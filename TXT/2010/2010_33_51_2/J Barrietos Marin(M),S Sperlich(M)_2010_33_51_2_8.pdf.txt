 The Size Problem of Bootstrap Tests when the Null is Non- or Semiparametric
Universidad de Antioquia; Institut für Statistik und Ökonometrie; Georg August Universität Göttingen
Abstract
In non- and semiparametric testing, the wild bootstrap is a standard method for determining the critical values of tests. If the null hypothesis is also semi- or nonparametric, then we know that at least asymptotically oversmoothing is necessary in the pre-estimation of the null model for generating the bootstrap samples. See Härdle & Marron (1990, 1991). However, in practice this knowledge is of little help. In this note we highlight that this bandwidth choice problem can become quite serious. As an alternative, we briegly discuss the possibility of subsampling.
Key words: Bandwidth choice, Bootstrap tests, Nonparametric specification tests.


2     Introduction
In both applied and mathematical statistics, non- and semiparametric specifi-
cation testing is still quite a popular research field. Unfortunately, only a few
papers address the problem of choosing an appropriate smoothing parameter.
This a problem is fundamental for the reasonable use of these methods. There
has been a growing amount of literature on adaptive testing where the adaptive-
ness refers to the smoothness of the alternative and deals with the smoothing
of the test or the alternative.
    However, these papers typically concentrate on testing problems where the
null hypothesis is fully parametric. Here we are interested in testing qualita-
tive restrictions, i.e. where the null hypothesis is semi- or nonparametric; think
e.g. of additivity tests. When bootstrap is used to determine the critical value,
these tests entail at least one more parameter choice problem: pre-estimating
the model under the null hypothesis to later generate the bootstrap samples.
This is necessary as in most cases the bandwidths for the estimation and the
bootstrap should have different rates. See Härdle & Marron (1990, 1991). As
in practical applications this problem has hardly been addressed, in most pub-
lished procedures for testing or constructing confidence bands with a semi- or
nonparametric null hypothesis, there is no guarantee that the bands meet the
nominal coverage probability. This has been confirmed in the work of Dette, von
Lieres, Wilkau & Sperlich (2005). In the latter paper, the problem is avoided
by using subsampling.
    To study the problem outlined in more detail, we concentrate on the problem
of testing additivity. We limit ourselves to two test statistics proposed in Dette
et al. (2005) and Roca & Sperlich (2007) but we extended this to different
modifications including subsampling. The aim is not to find the most efficient
additivity test or to propose new ones. Our focus is only directed at highlighting
the size problem when the null hypothesis and the resampling method are non- or
semiparametric. After a review of the additivity tests considered here, we study
some of the typically proposed procedures for bandwidth choice. Unfortunately,
we have not found a generally valid method. Our conclusion is that further
research is necessary to find a proper bootstrap bandwidth.


3     Estimators and test statistics for additive mod-
      els
Assume we face (not necessarily) independent and identically distributed (i.i.d.)
                n
data {(Xi, Yi )}i=1 ∈ Rd × R, where


  1 The authors gratefully acknowledge very helpful comments of two anonymous referees as

well as financial support from the Spanish MTM2008-03010 and the Deutsche Forschungsge-
meinschaft FOR916.




                                           2

                            Yi = m (Xi ) + ui        i = 1, 2, . . . , n,
    with m : Rd → R an unknown function of interest, m(x) = E(Y | X = x),
and ui i.i.d. random errors with E [ui ] = 0 and finite variance σ 2 (xi ). The
internalized Nadaraya-Watson estimator is defined as

                 n
                 X                                                 −1
     m
     b k (x) =         vk (x, Xi ) Yi , with vk (x, Xi ) = fbk (Xi )    Kk (x − Xi )
                 i=1

                           Pn
     where fbk (Xi ) = n1 i=1 Kk (Xj − Xi ) is a kernel density estimator with
a multiplicative kernel, i.e. for w = (w1 , . . . , wd ) ∈ Rd we think of Kk (w) =
Qd                               −1
                                    K wα k −1 . Commonly,
                                             
   α=1 Kk (wα ), Kk (wα ) = k                           R   the kernel isRassumed to
be Lipschitz continuous with compact support and |K(x)|dx < ∞, K(x)dx =
1. Furthermore, k is the bandwidth, assumed to go to zero for sample size n
going to infinity, but nknd going to infinity. Let Vk be the n × n matrix whose
(j, i) element is vk (Xj , Xi ), then m(x)
                                      b     = Vk (x)Y .
     We are interested in the additive model, which we write in terms of
                                                              d
                                                              X
                       E(Y | X = x) = mS (x) = ψ +                  mα (xα )
                                                              α=1
                                       R
    where we set EXα {mα (Xα )} = mα (x)fα (x)dx = 0∀α for identification.
Here, mα , α = 1, . . . , d are the marginal impact functions for each regressor.
Therefore, ψ is a constant equal to the unconditional expectation of Y . Writing
m(X) = mα (Xα ) + m−α (X−α ) where X−α is the vector X of all explanatory      
variables without Xα , i.e. X−α = Xi1 , . . . , Xi(α−1) , Xi(α+1) , . . . , Xid , we can
use the identification condition directly to estimate mα . The so called marginal
integration idea is based on that for xα fix we have

                                Z                                  Y
    EX−α [m (xα , X−α )] =          m (xα , x−α ) f−α (x−α )              dxβ = ψ + mα (xα )
                                                                   β̸=α

    Substituting for m(·) a nonparametric pre-estimator such as the one given
                                                                       Pn
in (2) , a sample average for the expectation, and for ψ simply ψb = n1 i=1 yi
gives
                                           n
                                           X
                             b α (xα ) =
                             m                   wαh (xα , Xiα ) Yi
                                           i=1

   where for a bandwidth h (the one fixing the smoothness of our H0 model)

                                                             fb−α (Xi,−α )
                   wh (xα , Xαi ) = Kh (xα − Xiα )
                                                            f (Xiα , Xi,−α )
                                                            b


                                                 3

                                    Pd
                    b S (Xj ) = ψb + α=1 m
    Finally, we set m                    b α (Xjα ) for each j = 1, 2, . . . , n. Note
                       Pd
that defining Wh = α=1 Wαh (xα ) with Wαh (xα ) being the n × n matrices
                                         b S (x) = ψ + Wh (x)Y .
with wαh (Xj , Xi ) as elements, one has m
    As mentioned before, we do not introduce new testing procedures but rather
study two modified statistics which have already been studied in the above
mentioned papers, and which performed excellently in the study by Roca &
Sperlich (2007) though in a different context. The null hypothesis of interest is
H0 : m(·) = mS (·) versus H1 : m(·) ̸= mS (·). We consider the following two
test statistics:
                   n
               1X                         2
          τ1 =        b (Xi ) − m
                     (m         b S (Xi )) w (Xi )
               n i=1
                                                            2
                  n         n
               1 X 1 X
          τ2 =                 Kk (Xi − Xj ) (Yj − m
                                                   b S (Xj )) w (Xi )
               n i=1 nk d j=1

    where ebi = Yi − mb S (Xi ), i.e. the residuals under the null hypothesis, and
bi = Yi − m
u          b (Xi ), the residuals without restrictions. We included also a weight
function w(·) which typically is just used for trimming at the boundaries or
regions where data are sparse. Note that in our simulation study we will make
use of the trimming at the boundaries. Obviously, τ1 calculates directly the
integrated squared difference between the null and alternative models. Alter-
natively, τ2 seeks to mitigate the bias problem inherited from the estimate m,  b
which suffers from the "curse of dimensionality". In Dette et al. (2005) it is
                                           d
proved that, for both tests τj , the nk 2 (τj − µj ) converge under the null to a
normal variable with mean zero and variances vj2 for j = 1, 2 with
                                  Z                Z                     
                               1       2               2              1
          µ1 = EH0 {τ1 } =           σ   (x)w(x)dx   K   (x)dx + o
                             nk d                                    nk d
                             Z                  Z
          µ2 = EH0 {τ2 } = (K ∗ K)2 (x)dx σ 2 (x)f 2 (x)w(x)dx

   and
                                   Z               Z
             v12 = VarH0 {τ1 } = 2 σ 4 (x)w2 (x)dx (K ∗ K)2 (x)dx
                                 Z
               2
             v2 = VarH0 {τ2 } = σ 4 (x)f 4 (x)w2 (x)dx

    All tests have been proven to be consistent in the sense that under the
alternative they converge with n to infinity. Let us also mention that we have
studied many more test statistics, e.g. those given in Dette et al. (2005) or
Roca & Sperlich (2007) but not presented here. These, however, showed even
less satisfactory performance, so we have skipped them in our presentation.




                                          4

4      The resampling
Asymptotic expressions are of little help in practice for several reasons: Bias
and variance contain unknown expressions which have to be estimated nonpara-
metrically, and the convergence rate is quite slow for large d. For this reason,
it is common to use resampling-mostly bootstrap-methods to approximate the
critical value for the particular sample statistic. These can be bootstrap meth-
ods or subsampling procedures. Unfortunately, for the bootstrap it is not known
how to choose the smoothing parameter in practice for the pre-estimation of the
model that is used to generate the bootstrap samples. From theory, it is known
that one should somewhat oversmooth.
    We give the general bootstrap procedure first and then discuss the details:

    1. With bandwidth h, calculate the estimate m        b S under the null hypothesis
       of additivity and its resulting residuals ebi , i = 1, . . . , n.
    2. With bandwidth k, calculate the estimator m   b for the conditional expec-
       tation without the additivity restriction, and the corresponding residuals
       bi , i = 1, . . . , n.
       u

    3. With the results from step 1 and 2, we can calculate our test statistics τ1
       and τ2 .
    4. Repeat step 1 with a bandwidth hb . We call the outcome m            b bS , respectively
                      b
       ϵi = Yi − m b S (Xi ) , i = 1, . . . , n.
                                                    h        i
                                                           j
    5. Draw random variables e∗i with E (e∗i ) = uji (respectively ebji or ϵji , see
       discussion below) for j = 1, 2, 3 (respectively j = 1, 2, see below again).
       Set Yi∗ = m   b bS (Xi ) + e∗i , i = 1, . . . , n, i.e. generate wild bootstrap sam-
       ples.
       n     Repeat onthis B times. This defines B different bootstrap samples
                 ∗,b
          Xi , Yi           , b = 1, . . . , B.
                       i=1

    6. For each bootstrap sample from steps 4 and 5 , calculate the test statistics
       τj∗,b , j = 1, 2, b = 1, . . . , B. Then, for each test statistic τj , j = 1, 2,
       the critical value is approximated by the corresponding quantiles n       of the
                                                                                     o
       distribution of the B bootstrap analogues: F ∗ (u) = B1 b=1 I τj∗,b ≤ u .
                                                                 PB

       Recall that they are generated under the null hypothesis.

    In step 2, the bandwidth k has simply to obey the different assumptions
required for each specific test. It can be chosen in such a way that it maxi-
mizes the power of the test for a given size. Therefore, different from Dette
et al. (2005) we apply the adaptive testing approach introduced         in Spokoiny
(1998, 1996). He considers simultaneously a family of tests τ k , k ∈ K , where
K = {k1 , k2 , . . . , kP } is a finite set of reasonable bandwidths. The theoretical
maximal number P depends on n, but is of no practical relevance. For details,
see Horowitz & Spokoiny (2001). They define


                                               5

                                                        
                                               τ k − E0 τ k
                                τ max = max
                                         k∈K       Var1/2 [τ k ]
    where E0 [·] indicates the expectation under H0 . A particularity of the re-
sampling analogues of τ max is that one first needs to calculate the resampling
                ∗,b                                          ∗,b
statistics τ k       for all k ∈ K to afterwards get (τ max ) . Note that for each
                                                                 ∗,b
k, the empirical moments of the resampling statistics τ k             can be used as a
substitute for E0 τ , respectively Var1/2 τ k , in practice.
                      k                            

    In step 5, the wild bootstrap (see Härdle & Mammen 1993) it is let open
which residuals should be taken u     bi , ebi or ϵi . While theory says clearly that
the best power can be reached when taking the residuals of the alternative,
i.e. u
     bi , our simulations (not shown) confirm the findings of Dette et al. (2005)
that in practice ϵi should be taken. Next, it is often sufficient if we allow for
heteroscedasticity of an unknown form using e∗i = εi ϵi , where the εi are i.i.d.,
drawn either from the golden-cut distribution, i.e.
                 ( √                                         √           √
                    −( 5 + 1)/2 with probability p = ( 5 + 1)/(2 5)
            εi = √
                    ( 5 + 1)/2      with probability 1 − p
    or from the Gaussian normal N (0, 1). This answers the question up to order
the moment of the bootstrap errors have to coincide with the residual moments.
In the simulation section, we will compare golden-cut with Gaussian bootstrap.
    In step 4 , bandwidth hb has to be chosen along the arguments of Härdle
& Marron (1990,1991): For the mean of m               b h (x) − m(x) under the conditional
distribution of Y1 , . . . , Yn | X1 , . . . , Xn , respectively of m b ∗h (x) − m
                                                                                 b hb (x) under
the conditional distribution of Y1∗ , . . . , Yn∗ | X1 , . . . , Xn , it is well known that

                                                    µ(K) ′′
                       E Y |X (m
                               b h (x) − m(x)) ≈ h2       m (x)
                                                       2
                                                   µ(K) ′′
                          b ∗h (x) − m
                     E ∗ (m          b h (x)) ≈ h2       m
                                                         b hb (x)
                                                     2
    where µ(K) = u2 K(u)du. Obviously, we need that m           b ′′hb (x) − m′′ (x) → 0.
                   R

The optimal bandwidth hb for estimating the second derivative must to be
larger (in rates) than bandwidth h for estimating the function itself. We can
even give the optimal rate. For example, the optimal rate to estimate m′′S is
of the order n−1/9 (instead of n−1/5 ), an observation we make use of in our
simulation studies. There it will be seen that the typical comment hb has to be
oversmoothing, is unhelpful in practice. Intuitively, one may think that a proper
choice for hb depends strongly on h. This might be true numerically, looking
at equations (5) and (6) in the asymptotics the " h-effect" seems to cancel
out as long as h/hb goes to zero (a necessary condition for the consistency of
bootstrap inference here) for n going to infinity. As one wants check whether the
best possible additive model is an adequate fit, one therefore can concentrate on
those bandwidth selectors for h which aim to optimize m       b S like cross validation
or some plug-in methods do.


                                               6

     After all, it might be interesting to also have a look at subsampling as an
alternative to bootstrapping (see Politis, Romano & Wolf 1999). Neumeyer &
Sperlich (2006) introduce subsampling in a slightly context, other than that
we discuss here, because there the bootstrap failed. There exists an automatic
choice of the adequate subsample size m. As we remodeled this method to serve
as a procedure for finding hb , we introduce subsampling and the automatic
choice of the subsample size m in more detail:
     Let Y = {(Xi , Yi ) | i = 1, . . . , n} be the original sample, and denoted by τ (Y)
the original statistic calculated from this sample, leaving aside index j = 1, 2, 3
for a moment. To determine the critical values we need to approximate
                                              √              
                              Q(z) = P n k d τ (Y) ≤ z
                                                                            
     Recall that under H0 this distribution converges to an N µj , vj2 , for µj and
vj , j = 1, 2, see above. For finite sample size n, drawing B subsamples Yb -each
of size m - we can approximate Q under H0 by
                                 B   q               
                            1    X
                                        d  km
                     Q(z)
                     b    =        I m km τ (Ym ) ≤ z
                            B
                                  b=1
    Note that the awkward notation comes from we have to adjust all bandwidths
for the new sample size m. For example, imagine k = k0 · n−δ for k0 being
constant. Then, τ km is calculated like τ but with  √ bandwidth km p  = k0 nδ m−δ .
                                                       d
    Certainly, under the alternative H1 , both n k τ (Y) and m km       d τ km (Y )
                                                                           √     m
converge to infinity. When demanding m/n → 0 guarantees that n k d τ (Y)
converges (much) faster to infinity than the subsample analogues. Then, Q           b
underestimates the quantiles of Q, which yields the rejection of H0 .
    The optimal m is actually a function of the level α. Again, one applies
resampling methods: Draw some pseudo sequences Y ∗,l , l = 1, . . . , L of Y of size
n with the same distribution as Y. For the desired level α, test H0∗ : m(x) −
mS (x) = m(x)
           b     −m b S (x) the same way as you want to test H0 : m(x) = mS (x),
i.e. applying your particular test statistic to H0∗ and using subsampling. From
the L repetitions you can determine the empirical rejection level (estimated
size) for your given α. Now, find an m such that this empirical rejection level is
≈ α. In practice, you choose from a grid of possible m the one whose estimated
rejection level for H0∗ is closest to α from below. Note that H0∗ is always true up
to an estimation error that should be almost the same as in your original test.
The only drawback of this procedure is the enormous computational effort. For
further details and examples, see Politis et al. (1999) or Delgado, Rodríguez &
Wolf (2001).


5     Simulation results
We give here only a summary of our large simulation study. The model consid-
ered is as follows: As in Dette et al. (2005), we draw n = 100 i.i.d. X ∈ R3
with


                                           7

                                                                 
                                                   1      0.2 0.4
              Xi ∼ N (0, ΣX )     with    ΣX =  0.2        1 0.6 
                                                 0.4      0.6   1
   to generate
                     2
        Yi = X1,i + X2,i + 2 sin (πX3,i ) + vX2,i X3,i + ei ,   i = 1, . . . , n
    with i.i.d. standard normal errors ei , v = 0 being an additive separable
model, or v = 2 for an alternative.
    In both test, statistics we use the weighting function w(·) for a possible
trimming: We cut the outer 5% or nothing (0%) of the sample, where "outer"
refers to the tails of the explanatory variables. This is done to get rid of the
boundary effects in the statistics. To speed up our simulation studies, the
presented results are calculated from 250 replications using only 200 bootstrap
samples (or subsamples respectively). We used the multiplicative quartic kernel
throughout but note that we know from our simulations in Dette et al. (2005)
as well as from three years simulation experiences for the studies in Barrientos
(2007), that the results change hardly for larger bootstrap samples.
    We first looked for an average cross validation bandwidth h, which turned
out to be hopt = 0.78 for the direction of interest, and 6hopt for the nuisance
directions, cf. Dette et al. (2005). This was done not only for computational
reasons but also because otherwise the size of the tests would also depend on the
randomness induced by the estimation of h. For the k-adaptive test procedure,
k ran over an equispaced grid of 10 bandwidths from kmin = 0.1 · range (X1 ) to
kmax = range (X1 ).
    We will study now the results for several choices of hb with different bootstrap
generating methods, i.e. golden-cut vs. Gaussian bootstrap errors. To have hb
as a function of h, to take also into account h/hb → 0, and validate the rate
n−1/9 (motivated above) we set hb = hn1/5−1/κ and try different κ ≤ 9.
    Table 1 shows the results for the k-adaptive bootstrap tests. We compare
the size and power for different hb , golden-cut vs Gaussian bootstrap, trimming
boundary effects vs no trimming, and finally also a bit τ1 vs τ2 (though the
latter is not the aim of this paper).
    First, the results basically show that the size problem is not solved simply
by different smoothing in the pre-estimation. Oversmoothing, in contrast to
the theoretical findings, seems to go in the wrong direction, at least for τ1 .
In particular, the hope that the ideas of Härdle & Marron (1990, 1991) (see
equations (5) and (6) ) might give us a hint or even provide a rule of thumb for
the choice of hb is not confirmed here.
    Second, following to some extent the findings of (Delgado et al. 2001), we
find a clear improvement for the Gaussian compared to the golden-cut bootstrap.
Actually, when using the golden-cut method, then τ1 does not hold the size for
several hb ( κ respectively). Even worse, it rejects more often under H0 than
it does under H1 . This phenomenon is not observed for the simpler Gaussian
wild bootstrap.


                                         8

    Third, boundary effects seem not to be the reason of our size and power
problems. Surely, we get different numerical results for different weighting (i.e.
trimming) functions, but cutting at the boundaries does not substantially change
our general findings.
    Finally, it is obvious that τ2 outperforms τ1 throughout. When recalling
the motivation of the construction of τ2 , cf. Neumeyer & Sperlich (2006) and
Roca & Sperlich (2007), it is obvious that the size problem comes from the bias
rather from the variance. Or, in other words, bootstrap can capture pretty well
the variance of a statistic but not its bias. There are two possible reasons for
the surprising fact that τ1 sometimes rejects more under H0 than under H1 .
First, while it is clear that the bias distorts the rejection level, it is not clear in
what direction; moreover, the distortion effect certainly changes with the true
underlying data generation TABLE 1: Rejection levels of the two k-adaptive
test statistics with and without trimming. Critical values are determined with
golden-cut respectively Gaussian wild bootstrap, using hb = hn1/5−1/κ for the
pre-estimation.



    process. Second, making the tests k-adaptive entails a normalization by the
estimated variance. In the unfortunate situation where the variance estimation
is getting larger, the power of the test decreases. Both effects together lead here
to the counter-intuitive performance of τ1 .
    In the last section we introduced subsampling as an alternative resampling
method to bootstrap. Therefore, we also provide a simulation study where the
critical values are approximated by subsampling, trying several subsample sizes
m. Recall that the different subsample sizes have a similar effect here like it
has the choice of hb for bootstrap tests. The results are given in Table 2 for k-
adaptive tests. For τ1 we see here basically the same bad behavior we observed
when using golden-cut bootstrap to determine the critical values. In contrast,
τ2 seems to TABLE 2: Rejection levels of the two k-adaptive test statistics with
and without trimming. Critical values are determined with subsampling, using
subsamples of sizes m.




                                          9

                                       H0 (v = 0)     H1 (v = 2)
                  Trim     α%    m     τ1     τ2      τ1     τ2
                   0%       5    80   .000 .000      .000 .000
                                 70   .000 .000      .000 .000
                                 60   .056 .000      .020 .036
                                 50   .276 .020      .076 .292
                                 40   .516 .212      .168 .732
                           10    80   .000 .000      .000 .000
                                 70   .020 .000      .016 .000
                                 60   .272 .008      .072 .144
                                 50   .584 .104      .256 .644
                                 40   .816 .476      .480 .912
                   5%       5    80   .000 .000      .000 .000
                                 70   .000 .000      .000 .000
                                 60   .016 .000      .000 .032
                                 50   .060 .020      .012 .276
                                 40   .152 .216      .024 .712
                           10    80   .000 .000      .000 .000
                                 70   .004 .000      .000 .000
                                 60   .060 .008      .016 .164
                                 50   .200 .092      .024 .636
                                 40   .380 .460      .120 .908

    work-though with less power than we observed when using Gaussian boot-
strap, cf. Table 1
    Recall that our main focus is the size distortion of resampling tests. There-
fore our last two studies are about the automatic choice of m in subsampling
and hb in (Gaussian) bootstrap, respectively.
    A quite time consuming simulation study evaluating the automatic choice of
m indicates that this procedure does unfortunately not work at all. Nevertheless,
our last study is to apply this idea for getting an automatic choice of hb . In
order to do so, we first have to adjust the procedure for an automatic choice of
the subsample size m to now find an adequate bootstrap bandwidth hb .
    This can be done as follows, described here in detail for τ2 . To make notation
and calculation easier, we consider the non-k-adaptive version but fix k = range
                              n
(X1 ) /2. Let now {Yi∗ , x∗i }i=1 := Y ∗ be a member of the pseudo sequence
introduced above. Then, for testing H0∗ : m(x) − mS (x) = m(x)  b     −mb S (x) with
sample Y ∗ , an analogue to τ2 would be

           
         n       n
      1 X
            1
                 X
τ2♯ =                                 b S Xj∗
                   Kh Xi∗ − Xj∗ Yj∗ − m
                                            
               d
      n i=1 nk j=1
                                                                          2
                                                             b S (Xj )}] w (Xi∗ )
                                       −Kh (Xi∗ − Xj ) {Yj − m

   Other statistics are thinkable certainly, e.g.


                                        10

     
   n      n
1 X 1 X
                               b S Xj∗
             Kh Xi − Xj∗ Yj∗ − m
                                     
        d
n i=1 nk j=1
                                                                                         2
                                                     −Kh (Xi − Xj ) {Yj − m
                                                                          b S (Xj )}] w (Xi )

    but they should all be asymptotically equivalent to (9). The procedure was
performed with only L = 100 pseudo samples Y ∗ . As the results varied widely
we were forced either to enlarge L considerably or to reduce σe considerably.
For computational reasons we decided on the second option and repeated the
study with σe = 0.1.
    Some results are summarized in Table 3. As et can be seen, this time we
emphasize the possibility of undersmoothing much more. You first have to look
at τ2♯ to find the κ giving the rejection level closest to α = 5% from below.
Here, this is always κ = 3. Note that this might also change depending on
the trimming, α, sample size, etc. It is important to understand that the lines
of τ2∗ can always be calculated, i.e. without knowing the true data generating
process. Therefore we call this method fully automatic. Now look at the lines
for τ2 , the test of interest. Obviously, κ = 3 is indeed the best possible choice; it
has the strongest power among all κ respecting the nominal level. This could be
taken as indicating that our suggestion for selecting hb works. Unfortunately,
this method does not work that well for all possible α; specifically, it becomes
quite incorrect for α ≥ 10%. Even worse, it did not work for τ1 (not shown).


6    Conclusions
Our main focus is the bootstrap and its size distortion in practice when the
sample size is small or moderate. These points are illustrated along the popular
problem of additivity testing. Naturally, one looks for an optimal trade-off be-
tween controlling for size under the null hypothesis H0 and maximizing power.
Even though these problems have already been discussed and studied in theory,
as yet, it is unclear how to set the smoothing parameter for the bootstrap prior
estimates in practice. We show that theory is not just unhelpful here; at present,
a reasonable application of bootstrap tests of these kinds is questionable. TA-
BLE 3: Rejection levels of τ2 and τ2♯ for α = 5%, with and without trimming,
using Gaussian bootstrap with hb = hn1/5−1/κ for the pre-estimation, and k =
range (X1 ) /2.




                                         11

                                                   κ
          Trim         1       2      3           4      5      6      7
        H0       0%    τ2♯   .012   .063        .028   .030   .032   .031   .029
                       τ2    .680   .392        .032   .012   .012   .012   .016
                 5%    τ2♯   .012   .062        .028   .030   .032   .031   .029
                       τ2    .676   .380        .024   .012   .012   .012   .020
        H1       0%    τ2♯   .001   .019        .042   .022   .015   .011   .009
      (v = 2)          τ2    .972   .932        .632   .380   .272   .260   .264
                 5%    τ2♯   .001   .019        .042   .023   .015   .011   .010
                       τ2    .968   .936        .620   .368   .260   .252   .264

    Further, we have shown that subsampling is an interesting alternative to
bootstrap which in addition provides a procedure for the analogue problem of
subsample size choices.
    Finally we introduced the idea of extending the procedure of subsample size
selection to smoothing parameter (hb ) selection in bootstrap testing problems.
However, further research is necessary to provide reliable procedures for the
nonparametric testing problems considered here.

             [Recibido: marzo de 2010 - Aceptado: octubre de 2010]


References
Barrientos J. Some Practical Problems of Recent Nonparametric Procedures: Testing Estimation and Application - Tesis doctoral.(2007). Universidad de Alicante.
Delgado M A, Rodríguez J M, Wolf M. Subsampling Cube Root Asymptotics with an Application to Manski’s MSE.(2001). Economics Letters.
Dette H, von Lieres C, Wilkau C, Sperlich S. A Comparison of Different Nonparametric Method for Inference on Additive Models.(2005). Journal of Nonparametric Statistics.
Härdle W, Mammen E. Comparing Nonparametric Versus Parametric Regression Fits.(1993). Annals of Statistics.
Härdle W, Marron J S. Semiparametric Comparison of Regression Curves.(1990). Annals of Statistics.
Härdle W, Marron J S. Bootstrap Simultaneous Bars For Nonparametric Regression.(1991). Annals of Statistics. 
Horowitz J L, Spokoiny V. An adaptive rate-optimal test of parametric mean-regression model against a nonparametric alternative.(2001). Econometrica.
Neumeyer N, Sperlich S. Comparison of separable components in different samples.(2006). Scandinavian Journal of Statistics.
Politis D N, Romano J P, Wolf M. Subsampling Springer Series in Statistics.(1999). Springer-verlag.
Roca J, Sperlich S. Testing the Link when the Index is Semiparamtric A Comparison Study.(2007). Computational Statistics and Data Analysis.
Spokoiny V. Adaptive Hypothesis Testing using Wavelets.(1996). Annals of Statistics.
Spokoiny V. Adaptive and spatially adaptive testing of a nonparametric hypothesis.(1998). Mathematical Methods of Statistics.
