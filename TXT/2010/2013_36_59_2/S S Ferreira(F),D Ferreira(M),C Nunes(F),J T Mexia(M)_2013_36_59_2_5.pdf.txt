Estimation of Variance Components in Linear Mixed Models with Commutative Orthogonal Block Structure.  Estimación de las componentes de varianza en modelos lineales mixtos con estructura de bloques ortogonal conmutativa
University of Beira Interior, Covilhã, Portugal.  University of Beira Interior, Covilhã, Portugal.  University of Beira Interior, Covilhã, Portugal.  New University of Lisbon, Covilhã, Portugal
Abstract
Segregation and matching are techniques to estimate variance components in mixed models. A question arising is whether segregation can be applied in situations where matching does not apply. Our motivation for this research relies on the fact that we want an answer to that question and to explore this important class of models that can contribute to the development of mixed models. That is possible using the algebraic structure of mixed models. We present two examples showing that segregation can be applied in situations where matching does not apply.
Key words: Commutative Jordan algebra, Mixed model, Variance components.
Resumen
La segregación y el emparejamiento son técnicas para estimar las componentes de varianza en modelos mixtos. Una pregunta que ha surgido es si la segregación puede ser aplicada en situaciones en las que el emparejamiento no es aplicable. Nuestra motivación para esta investigación se basa en el hecho de que se quiere una respuesta a esta pregunta y se quiere explorar esta importante clase de modelos con el fin de contribuir al desarrollo de los modelos mixtos. Esto es posible utilizando la estructura algebraica de los modelos mixtos con estructura de bloques ortogonal conmutativa. Se presentan dos ejemplos que muestran que la segregación puede ser aplicada en situaciones donde el emparejamiento no es aplicable.
Palabras clave: álgebra conmutativa Jordan, componentes de varianza, modelo mixto.




1. Introduction
   Mixed models have orthogonal block structure, OBS, when their variance co-
variance matrices are orthogonal all the linear combinations of known pairwise
projection matrices, POOPM, add up to In with non negative coefficients. These
models play an important role in design of experiments (Houtman & Speed 1983,
Mejza 1992) and were introduced by Nelder (1965a, 1965b), continuing to play
an important part in the theory of randomized block designs (see Caliński &
Kageyama 2000, Caliński & Kageyama 2003).
    A direct generalization of this class of models is that of models whose vari-
ance covariance matrices are linear combinations of known POOPM, we say these
models to have generalized orthogonal block structure, GOBS. Moreover if the
orthogonal projection matrix T on the space spanned by the mean vectors com-
mutes with these POOPM the model, (see Fonseca, Mexia & Zmyślony 2008) will
have commutative orthogonal block structure, COBS. Then, (see Zmyślony 1978),
its least square estimators, LSE, for estimable vectors will be best linear unbiased
estimators, BLUE, whatever the variance components.
    In what follows, we will present techniques for the estimation of variance com-
ponents in COBS. These techniques will be based on the algebraic structure of
the models then being quite distinct from other techniques that require normal-
ity. Moreover it has interesting developments, namely these related to model
segregation.
   The next Section presents the algebraic structure of the models considering
commutative Jordan algebras. Then we discuss, in section 3, the techniques for
the estimation of variance components: Matching and segregation. Segregation
displays the possibility of using the algebraic structure in estimation. Thus, in
subsections 3.1 and 3.2, we present two models in which this technique has to
be used to complete the structure based on estimation of variance components.
Lastly, we present some final remarks.


                                      Revista Colombiana de Estadística 36 (2013) 259–269

Variance Components in Linear Mixed Models                                              261

2. Algebras and Models
    Commutative Jordan Algebras, CJA, (of symmetric matrices) are linear spaces
constituted by symmetric matrices that commute and containing the square of
this matrices. Each CJA A has a principal unique basis (see, Seely 1971), pb(A),
constituted by pairwise orthogonal projection matrices. Any orthogonal projection
matrix belonging to A will be the sum of matrices in pb(A).
   Moreover, given a family W of symmetric matrices that commute, there is a
minimal CJA A(W ) containing W (see, Fonseca et al. 2008).
   Consider the model
                                              w
                                              X
                                        Y=          Xi βi                               (1)
                                              i=0

where β 0 is fixed and the β 1 , . . . , β w are independent, with null mean vectors and
variance covariance matrices

                                
                                    µ = X0 β0
                                           Pw                                           (2)
                                    V(θ) = i=1 θi Mi
                  0
with Mi = Xi Xi , i = 1, . . . , w. When the matrices in {T , M , . . . , M w } commute
we have the CJA A(W ) with principal basis

                                  Q = {Q1 , . . . , Qm }.
                                                          Pz
We can order the matrices in Q to have T =                  j=1 Qj . Moreover

                                    m
                                    X
                             Mi =         bi,j Qj , i = 1, . . . , w,
                                    j=1


so that
                                  w
                                  X                 m
                                                    X
                        V(θ) =          θ i Mi =          γ j Qj = V (γ)
                                  i=1               j=1
           Pw
with γ j = i=1 bi,j θ i , j = 1, . . . , m, thus the model will have COBS since its vari-
ance covariance matrices are linear combinations of known POOPM that commute
with the Q1 , . . . , Qm , belonging jointly to A(W ).


3. Segregation and Matching
    Since R(Qj ) ⊆ R(T ), j = 1, . . . , z we can estimate directly the γz+1 , . . . , γm ,
for which we have the unbiased estimators

                                  kQj Yk2
                           ej =
                           γ              , j = z + 1, . . . , m.                       (3)
                                   r(Qj )

                                         Revista Colombiana de Estadística 36 (2013) 259–269

262                        Sandra S. Ferreira, Dário Ferreira, Célia Nunes & João T. Mexia


   Partitioning matrix B = [bi,j ] as [B1 B2 ], where B1 has z columns, and
                                                                                         2 0
taking γ1 = (γ1 , . . . , γz )0 , γ2 = (γz+1 , . . . , γm )0 , and σ 2 = (σ12 , . . . , σw ) , with w ≤
m − z, we have
                                        γl = B0l σ 2 , l = 1, 2.                                     (4)
                                         0
When the column vectors of B2 are linearly independent we have

                                              σ 2 = (B02 )+ γ2 ,                                    (5)

as well as
                                             γ1 = B01 (B02 )+ γ2 ,                                  (6)
                                     2
allowing the estimation of σ and γ1 , through γ2 . It may be noted that if the
matrices Q1 , . . . , Qm can be ordered in such a way that the transition matrix is
                                                                   
                                                     B1,1    0
                                      B=                                    ,
                                                     B2,1   B2,2

with B1,1 a z × z matrix, the model is said to be segregated, see Ferreira, Ferreira
& Mexia (2007) and Ferreira, Ferreira, Nunes & Mexia (2010). It can be pointed
out that, in that case, sub-matrices B1,1 and B2,2 are regular.
   When B1 is a sub-matrix of B2 , B01 will be a sub-matrix of B02 and so γ1 will
be a sub-vector of γ2 , see Mexia, Vaquinhas, Fonseca & Zmyslony (2010). In this
case the match have between the components of γ1 and some components of γ2 .
When this happens we say that the model has matching. Thus γ1 and
                                                                   0
                                                     γ10    γ20
                                                 
                                         γ=                             ,

can be directly estimated from γ2 . If the row vectors of B are linearly independent,
we have

                                               σ 2 = (B0 )+ γ,                                      (7)
and we can also estimate σ 2 . Requiring the row vectors of B to be linearly in-
dependent is less restrictive than requiring the row vectors of B2 to be linearly
independent.
    Below we introduce two examples which show that segregation can be applied
in situations where matching does not apply.


3.1. Segregation without Matching: Stair Nesting
   We choose to present an example with stair nesting instead of the usual nesting
because stair nesting designs are unbalanced and use fewer observations than the
balanced case, and in addition, the degrees of freedom for all factors are more
evenly distributed, as was shown by Fernandes, Mexia, Ramos & Carvalho (2011).
Cox & Solomon (2003) suggested that having u factors, we will have u steps where
each step corresponds to one factor of the model.


                                                 Revista Colombiana de Estadística 36 (2013) 259–269

Variance Components in Linear Mixed Models                                                        263

    In order to describe the branching in such models, we can consider u + 1 steps.
The first step, with index 0, has a0 = c0 = u branches, one per factor. In the
second step, with index 1, we have c1 = a(1) + u − 1 branches, a(1) the number of
“active” levels for the first factor and u−1 the number of the remaining factors. We
point out that the branch for the first factors concerns its “active” levels. For the
third step, with index 2, we have c(2) = a(1) + a(2) + u − 2, where a(1) represents
the number of “active” levels for the first two factors resulting from the branching
for the first factor; a(2) is the number levels for the second factor and u − 2, the
number of the remaining factors. In this way, for the (i + 1)-th step, with index
                     Pi
i, we have c(i) = h=1 a(h) + u − i, i = 3, . . . , u branches. a(1), . . . , a(i) are the
number of “active” levels for the first i factors and u − i the number of remaining
factors. These designs are also studied in Fernandes, Ramos & Mexia (2010) and
some results of nesting may be seen, for example, in Bailey (2004).
    The model for stair nesting designs is given by
                                                  u
                                                  X
                                             Y=          Xi β i ,                                 (8)
                                                   i=0

with
           
           
            X 0 = D(1a(1) , . . . , 1a(i) , 1a(i+1) , . . . , 1a(u) )
             ..
           
           
           
            .
           
           
              X i = D(I a(1) , . . . , I a(i) , 1a(i+1) , . . . , 1a(u) ), i = 1, . . . , u − 1   (9)
            ...
           
           
           
           
           
           
            X = D(I
                 u      a(1) , . . . , I a(i) , I a(i+1) , . . . , I a(u) )


where D(A1 , . . . , Au ) is the block diagonal matrix with principal blocks A1 , . . . , Au
and 1a(s) is the vector with all a(s) components equal to 1.
   In this approach we will assume that β0 = 1u µ, where µ is the general mean
value and the vectors βi , i = 1, . . . , u, are independent normal with null mean
vectors and variance-covariance matrix σi2 Ic(i) , i = 1, . . . , u, and

                                       i
                                       X
                              c(i) =         a(h) + u − i, i = 1, . . . , u
                                       h=1


   Hence Y is normal P    distributed with mean vector µ = 1n µ, and variance-
                             u                                         0
covariance matrix V = i=1 σi2 Mi , where Mi = Xi Xi , i = 1, . . . , u, we have
     
     
       M 0 = D(J a(1) , . . . , J a(i) )
                      ..
     
     
     
     
     
                      .
        M i = D(I a(1) , . . . , I a(i) , J a(i+1) , . . . , J a(u) ), i = 1, . . . , u − 1 (10)
     
     
                      ..
     
     
                       .
     
      M = D(I
           u       a(1) , . . . , I a(i) , I a(i+1) , . . . , I a(u) )


                                              Revista Colombiana de Estadística 36 (2013) 259–269

264                      Sandra S. Ferreira, Dário Ferreira, Célia Nunes & João T. Mexia


with J s = 1s 10s . Now, the orthogonal projection matrix on r(X 0 ), will be T given
by
                                                                                            
                   1                    1                1                        1
     T=D              J a(1) , . . . ,      J a(i) ,          J a(i+1) , . . . ,      J a(u)   (11)
                a(1)                   a(i)          a(i + 1)                    a(u)

                                              1
   Moreover, with K a(i) = I a(i) − a(i)         J a(i) and 0a(i) the null a(i) × a(i) matrix,
i = 1, . . . , u, taking
                 (
                                            1
                    Qi = D(0a(1) , . . . , a(i) J a(i) , . . . , 0a(u) ), i = 1, . . . , u
                                                                                           (12)
                    Qi+u = D(0a(1) , . . . , K a(i) , . . . , 0a(u) ), i = 1, . . . , u

we will have
          
                 Xu
             T =      Qj
          
          
          
          
          
          
                j=1
          
                   i                   u
                  X                   X
             Mi =      (Qj + Qj+u ) +       a(j)Qj ,                     i = 1, . . . , u − 1.    (13)
          
                  j=1                j=i+1
          
          
                  Xu
          
             M   =     (Qj + Qj+u )
          
               u
          
          
          
                        j=1


      So we have
                                                                
                                        B=       B1        B2        ,

with                                                                                      
                  1       a(2)    ...   a(u)                        1          0    ...   0
                 1        1      ...   a(u)                      1          1    ...   0 
                                                                                          
                 .         ..     ..     ..                      .          ..    ..   .. 
                 ..
           B1 =             .      .      . ,                    ..
                                                             B2 =              .     .    . ,
                                                                                            
                                                                                          
                 1        1      ...   a(u)                      1          1    ...   0 
                  1        1      ...    1                          1          1    ...   1

so we have segregation but we do not have matching.
    Let us consider an example where u = 3, a(1) = 3, a(2) = 2 and a(3) = 3
“active” levels and the number of observations in the design is n = 3 + 2 + 3 = 8.
So, we have g(1) = 2, g(2) = 1 and g(3) = 2 degrees of freedom for the first,
second, and third factors, respectively. The design is shown in Figure 1.
      The random effects model for stair nesting can be summarized as
                                                  3
                                                  X
                                         Y=             Xi β i                                    (14)
                                                  i=0

where a(1) = 3, a(2) = 2 and a(3) = 3 are the levels for the 3 factors that nest.
We make the same assumptions on the random effects as we did in the section 3.1,


                                             Revista Colombiana de Estadística 36 (2013) 259–269

Variance Components in Linear Mixed Models                                        265




                         Figure 1: Stair nested design.



where                       
                            
                             X0 = D(13 , 12 , 13 )
                            
                            
                            
                            
                            
                             X1 = D(I3 , 12 , 13 )
                            
                            
                                                                                 (15)
                            
                            
                            
                             X2 = D(I3 , I2 , 13 )
                            
                            
                            
                            
                            
                              X3 = D(I3 , I2 , I3 )
                            


   From formula (13) we obtain
                            
                             M1 = D(I3 , J2 , J3 )
                            
                            
                            
                            
                            
                              M2 = D(I3 , I2 , J3 )                              (16)
                            
                            
                            
                            
                            
                              M3 = D(I3 , I2 , I3 )
                            

Considering m = 6, z = 3, we have the pairwise orthogonal projection matrices
                                    1
                             Q1 = { 3 J3 , 02 , 03 }
                            
                            
                            
                            
                            
                             Q2 = {03 , 21 J2 , 03 }
                            
                            
                            
                            
                            
                            
                            
                            
                             Q3 = {03 , 02 , 13 J3 }
                            
                            
                            

                              Q4 = {K 3 , 02 , 03 }
                            
                            
                            
                            
                            
                            
                            
                            
                            
                             Q5 = {03 , K 2 , 03 }
                            
                            
                            
                            
                            
                            
                            
                            
                              Q6 = {03 , 02 , K 3 }
                            


                                   Revista Colombiana de Estadística 36 (2013) 259–269

266                     Sandra S. Ferreira, Dário Ferreira, Célia Nunes & João T. Mexia


      and the matrices
                     
                     
                      M1 = Q1 + a(2)Q2 + a(3)Q3 + Q4
                     
                     
                     
                     
                       M2 = Q1 + Q2 + a(3)Q3 + Q4 + Q5
                     
                     
                     
                     
                     
                       M3 = Q1 + Q2 + Q3 + Q4 + Q5 + Q6
                     

It follows readily that
                                                             
                              1        a(2) a(3)        1 0 0
                          B= 1         1   a(3)        1 1 0 
                              1         1    1          1 1 1

considering                                                
                                    B=        B1       B2
where                                                       
                                      1       a(2)      a(3)
                               B1 =  1        1        a(3) 
                                      1        1         1
and                                                      
                                           1       0    0
                                    B2 =  1       1    0 
                                           1       1    1


3.2. Segregation without Matching: Crossing
    Let there be a first factor that crosses with a second that nests a third. The
factors will have a, b and c levels, respectively. The first and the third factors have
random effects and the second has fixed effects.
      The mean vector will then be

                      µ = (1a ⊗ 1b ⊗ 1c ) µ + (1a ⊗ Ib ⊗ 1c ) β (2)

where β (2) is the fixed vector of the effects for the second factor and ⊗ represent
the Kronecker matrix product.
      The random effects part of the model will be

         (Ia ⊗ 1b ⊗ 1c ) β (1) + (Ia ⊗ Ib ⊗ 1c ) β (1, 2) + (1a ⊗ Ib ⊗ Ic ) β (3) +

                                + (Ia ⊗ Ib ⊗ Ic ) β (1, 3) ,
where β (1), β (1, 2), β (3) and β (1, 3) correspond to the effects of the first factor,
to the interactions of the first and second factors, to the effects of the third factor
and to the interactions between the first and the third factors. As usual, we assume
these vectors to be independent, homoscedastic and represent the corresponding


                                        Revista Colombiana de Estadística 36 (2013) 259–269

Variance Components in Linear Mixed Models                                            267

variance components by σ 2 (1), σ 2 (1, 2), σ 2 (3) and σ 2 (1, 3). So the variance-
covariance matrix will be given by

V = σ 2 (1) Ia ⊗Jb ⊗Jc +σ 2 (1, 2) Ia ⊗Ib ⊗Jc +σ 2 (3) Ja ⊗Ib ⊗Ic +σ 2 (1, 3) Ia ⊗Ib ⊗Ic .

In this case the matrices in the principal basis will be
                            
                            
                              Q1 = a1 Ja ⊗ 1b Jb ⊗ 1c Jc
                               Q2 = Ka ⊗ 1b Jb ⊗ 1c Jc
                            
                            
                            
                            
                               Q3 = a1 Ja ⊗ Kb ⊗ 1c Jc
                            
                            
                                                   1
                             Q4 = Ka ⊗ Kb ⊗ c Jc
                            
                                     1      1
                            
                             Q5 = a Ja ⊗ b Jb ⊗ Kc
                            
                            
                            
                               Q6 = Ka ⊗ 1b Jb ⊗ Kc
                            


   Moreover the orthogonal projection matrix on Ω will be
                                1          1
                          T=      Ja ⊗ Ib ⊗ Jc = Q1 + Q3 .
                                a          c
We will also have

                  Ia ⊗ Jb ⊗ Jc = bcQ1 + bcQ2
               
               
               
                  Ia ⊗ Ib ⊗ Jc = cQ1 + cQ2 + cQ3 + cQ4
               
                  J  ⊗ Ib ⊗ Ic = aQ1 + aQ3 + aQ5
                a
               
               
                  Ia ⊗ Ib ⊗ Ic = Q1 + Q2 + Q3 + Q4 + Q5 + Q6

Therefore
                                          6
                                          X
                                    V=          γj Qj ,
                                          j=1

with             
                 
                  γ1 = bcσ 2 (1) + cσ 2 (1, 2) + aσ 2 (3) + σ 2 (1, 3)
                   γ2 = bcσ 2 (1) + cσ 2 (1, 2) + σ 2 (1, 3)
                 
                 
                 
                 
                   γ3 = cσ 2 (1, 2) + aσ 2 (3) + σ 2 (1, 3)
                 
                 
                 
                  γ4 = cσ 2 (1, 2) + σ 2 (1, 3)
                            2        2
                 
                  γ5 = aσ (3) + σ (1, 3)
                 
                 
                 
                   γ6 = σ 2 (1, 3)
                 

   Now γ1 and γ3 are different from all other canonical variance components so
there is no matching. Despite this we have
                    
                    
                      σ 2 (1, 3) = γ6
                     σ 2 (3) = γ5 −γ6
                    
                                    a
                    
                      σ 2 (1, 2) = γ4 −γ
                                       c
                                           6

                                         2      2
                       σ (1) = γ2 −cσ (1,2)−σ     (1,3)     −γ4
                                                        = γ2bc
                    
                     2
                                             bc

so all variance components either usual or canonic can be estimated.

                                       Revista Colombiana de Estadística 36 (2013) 259–269

268                     Sandra S. Ferreira, Dário Ferreira, Célia Nunes & João T. Mexia


4. Final Remarks
   COBS models consider important cases. In the second example in Section 3
we presented an example of a balanced crossing which, (see Fonseca, Mexia &
Zmyślony 2003, Fonseca, Mexia & Zmyślony 2007) can be extended to apply to all
models with balanced cross nesting, thus including a wide variety of well behaved
models.
    The first example in section 3, that of stair nesting, displays a different model
also with COBS. Besides the algebraic structure enables us to obtain unbiased
estimators without normality. The LSE for estimable vectors are BLUE, whatever
the variance components.


Acknowledgements
   This work was partially supported by the center of Mathematics, University of
Beira Interior, under the project PEst-OE/MAT/UI0212/2011.
   We thank the anonymous referees and the Editor for useful comments and sug-
gestions on a previous version of the paper, which helped to improve substantially
the initial manuscript.
                                                                           
               Recibido: octubre de 2012 — Aceptado: septiembre de 2013


References
Bailey, R. A. (2004), Association Schemes: Designed Experiments, Algebra and Combinatorics, Cambridge University Press, Cambridge.
Caliński, T. & Kageyama, S. (2000), Block Designs: A Randomization Approach Vol. I: Analysis, Springer-Verlag, New York.
Caliński, T. & Kageyama, S. (2003), Block Designs: A Randomization Approach Vol. II: Analysis, Springer-Verlag, New York.
Cox, D. & Solomon, P. (2003), Components of Variance, Chapman and Hall, New York.
Fernandes, C., Mexia, J., Ramos, P. & Carvalho, F. (2011), ‘Models with stair nesting’, AIP Conference Proceedings - Numerical Analysis and Applied Mathematics 1389, 1627–1630.
Fernandes, C., Ramos, P. & Mexia, J. (2010), ‘Algebraic structure of step nesting designs’, Discussiones Mathematicae. Probability and Statistics 30, 221–235.
Ferreira, S. S., Ferreira, D. & Mexia, J. T. (2007), ‘Cross additivity in balanced cross nesting models’, Journal of Statistical Theory and Practice (3), 377–392.
Ferreira, S. S., Ferreira, D., Nunes, C. & Mexia, J. T. (2010), ‘Nesting segregated mixed models’, Journal of Statistical Theory and Practice 4(2), 233–242.
Fonseca, M., Mexia, J. T. & Zmyślony, R. (2003), ‘Estimators and tests for variance components in cross nested orthogonal models’, Discussiones Mathematicae- Probability and Statistics 23(3), 175–201.
Fonseca, M., Mexia, J. T. & Zmyślony, R. (2007), ‘Jordan algebras generating pivot variables and orthogonal normal models’, Journal of Interdisciplinary Mathematics (10), 305–326.
Fonseca, M., Mexia, J. T. & Zmyślony, R. (2008), ‘Inference in normal models with commutative orthogonal block structure’, Acta et Commentationes Universitatis Tartuensis de Mathematica (12), 3–16.
Houtman, A. & Speed, T. (1983), ‘Balance in designed experiments with orthogonal block structure’, Annals of Statistics 11(4), 1069–1085.
Mejza, S. (1992), ‘On some aspects of general balance in designed experiments’, Statistica 52, 263–278.
Mexia, J. T., Vaquinhas, R., Fonseca, M. & Zmyslony, R. (2010), ‘COBS: Segregation, Matching, Crossing and Nesting’, Latest Trends on Applied Mathematics, Simulation, Modeling, 4th International Conference on Applied Mathematics, Simulation, Modelling (ASM’10) pp. 249–255.
Nelder, J. (1965a), ‘The analysis of randomized experiments with orthogonal block structure. I. Block structure and the null analysis of variance’, Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences 283(1393), 147–162.
Nelder, J. (1965b), ‘The analysis of randomized experiments with orthogonal block structure. II. Treatment structure and the general analysis of variance’, Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences 273(1393), 163–178.
Seely, J. (1971), ‘Quadratic subspaces and completeness’, The Annals of Mathematical Statistics 42, 710–721.
Zmyślony, R. (1978), ‘A characterization of best linear unbiased estimators in the general linear model’, Mathematical Statistics and Probability Theory 2, 365–373.
