Testing Equality of Several Correlation Matrices. Prueba de igualdad de varias matrices de correlaciÃ³n
Bowling Green State University, Bowling Green, USA. Experient Research Group, Severna Park, USA. Universidad de Antioquia, MedellÃ­n, Colombia
Abstract
In this article we show that the Kullbackâ€™s statistic for testing equality of several correlation matrices may be considered a modified likelihood ratio statistic when sampling from multivariate normal populations. We derive the asymptotic null distribution of Lâˆ— in series involving independent chisquare variables by expanding Lâˆ— in terms of other random variables and then inverting the expansion term by term. An example is also given to exhibit the procedure to be used when testing the equality of correlation matrices using the statistic Lâˆ— .
Key words: Asymptotic null distribution, Correlation matrix, Covariance matrix, Cumulants, Likelihood ratio test.
Resumen
En este artÃ­culo se muestra que el estadÃ­stico Lâˆ— de Kullback, para probar la igualdad de varias matrices de correlaciÃ³n, puede ser considerado como un estadÃ­stico modificado del test de razÃ³n de verosimilitud cuando se muestrean poblaciones normales multivariadas. Derivamos la distribuciÃ³n asintÃ³tica nula de Lâˆ— en series que involucran variables independientes chi-cuadrado, mediante la expansiÃ³n de Lâˆ— en tÃ©rminos de otras variables aleatorias y luego invertir la expansiÃ³n tÃ©rmino a tÃ©rmino. Se da tambiÃ©n un ejemplo para mostrar el procedimiento a ser usado cuando se prueba igualdad de matrices de correlaciÃ³n mediante el estadÃ­stico Lâˆ— .
Palabras clave: distribuciÃ³n asintÃ³tica nula, matriz de correlaciÃ³n, matriz de covarianza, razÃ³n de verosimilitud.

1. Introduction
    The correlation matrix is one of the foundations of factor analysis and has
found its way into such diverse areas as economics, medicine, physical science
and political science. There is a fair amount of literature on testing properties
of correlation matrices. Tests for certain structures in a correlation matrix have
been proposed and studied by several authors, e.g, see Aitkin, Nelson, and Rein-
furt (1968), Gleser (1968), Aitkin (1969), Modarres (1993), Kullback (1997) and
Schott (2007). In a series of papers, Konishi (1978, 1979a, 1979b) has developed
asymptotic expansions of correlation matrix and applied them to various problems
of multivariate analysis. The exact distribution of the correlation matrix, when
sampling from a multivariate Gaussian population, is derived in Ali, Fraser and
Lee (1970) and Gupta and Nagar (2000).
   If the covariance matrix of Î±-th population is given by Î£Î± and âˆ†Î± is a diagonal
matrix of standard deviations for the population Î±, then PÎ± = âˆ†âˆ’1         âˆ’1
                                                                  Î± Î£Î± âˆ†Î± is the
correlation matrix for the population Î±. The null hypothesis that all k populations
have the same correlation matrices may be stated as H : P1 = Â· Â· Â· = Pk .
    Let the vectors xÎ±1 , xÎ±2 , . . . , xÎ±NÎ± be a random sample of size NÎ± = nÎ± + 1
for Î± = 1, 2, . . . , k from k multivariate populations of dimensionality
                                                                   PNÎ±       p. Further,
we assume the independence of these k samples. Let xÎ± = i=1              xÎ±i /NÎ± , AÎ± =
PNÎ±                           0
   i=1 (xÎ±i âˆ’xÎ± )(xÎ±i âˆ’xÎ± ) and SÎ± = AÎ± /NÎ± . Further, let DÎ± be a diagonal matrix
of the square roots of the diagonal elements of SÎ± .PThe sample correlationPk matrix
                                                        k
RÎ± is then defined by RÎ± = DÎ±âˆ’1 SÎ± DÎ±âˆ’1 . Let n = Î±=1 nÎ± and R = Î±=1 nÎ± RÎ± .
                                                     Pk
    Kullback (1967) derived the statistic Lâˆ— = Î±=1 nÎ± ln{det(R)/ det(RÎ± )} for
testing the equality of k correlation matrices based on samples from multivariate
populations. This statistic was later examined by Jennrich (1970) who observed
that the statistic proposed by Kullback failed to have chi-square distribution as-
cribed to it. For further results on this topic the reader is referred to Browne (1978)
and Modarres and Jernigan (1992).
    Although the Kullbackâ€™s statistic Lâˆ— is not equal to the modified likelihood ratio
criterion, we here show that it may be considered an approximation of the modified
likelihood ratio statistic when sampling from multivariate normal populations.
    In Section 2, we show that Kullbackâ€™s statistic can be viewed as an approxima-
tion of the modified likelihood ratio statistic based on samples from multivariate
normal populations. Section 3 deals with some preliminary results and definitions
which are used in subsequent sections. In sections 4 and 5, we obtain asymptotic
null distribution of Lâˆ— by expanding Lâˆ— in terms of other random variables and
then inverting the expansion term by term. Finally, in Section 6, an example
is given to demonstrate the procedure to be used when testing the equality of
correlation matrices using the statistic Lâˆ— . Some results on matrix algebra and
distribution theory are given in the Appendix.




                                      Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

Testing Equality of Several Correlation Matrices                                               239

2. The Test Statistic
    In this section, we give an approximation of the likelihood ratio test statistic
Î» for testing equality of correlation matrices of several multivariate Gaussian pop-
ulations. The test statistic Î» was derived and studied by Cole (1968a, 1968b)
in two unpublished technical reports (see Browne 1978, Modarres and Jerni-
gan 1992, 1993). However, these reports are scarcely available, and therefore the
sake of completeness and for a better understanding it seems appropriate to first
give a concise step-by-step derivation of the test statistic Î».
   If the underlying populations follow multivariate normal distributions, then
the likelihood function based on the k independent samples, when all parameters
are unrestricted, is given by

     L(Âµ1 , . . . , Âµk , Î£1 , . . . , Î£k )
           Y k h                             iâˆ’1
       =            (2Ï€)pNÎ± /2 det(Î£Î± )NÎ± /2
            Î±=1
                    "  k                k
                                                                                           #
                    1X             1X
                          tr Î£âˆ’1           tr Î£âˆ’1                      0
                                             
            Ã— exp âˆ’           Î± AÎ± âˆ’           Î± (xÌ„Î± âˆ’ ÂµÎ± )(xÌ„Î± âˆ’ ÂµÎ± )
                    2 Î±=1            2 Î±=1

where for Î± = 1, . . . , k we have ÂµÎ± âˆˆ Rp and Î£Î± > 0. It is well known that for any
fixed value of Î£Î± the likelihood function is maximized with respect to the ÂµÎ± â€™s
when Âµb Î± = xÎ± .
    Let âˆ†Î± be a diagonal matrix of standard deviations for the population Î±.
Further, let PÎ± = âˆ†âˆ’1      âˆ’1
                    Î± Î£Î± âˆ†Î± be the population correlation matrix for the pop-
ulation Î±. The natural logarithm of the likelihood function, after evaluation at
b Î± = xÎ± , may then be written as
Âµ

   ln[L(xÌ„1 , . . . , xÌ„k , âˆ†1 P1 âˆ†1 , . . . , âˆ†k Pk âˆ†k )]
                             k                           k
            1             1X                          1X
         = âˆ’ N p ln(2Ï€) âˆ’       NÎ± ln[det(PÎ± âˆ†2Î± )] âˆ’       tr(NÎ± PÎ±âˆ’1 GÎ± RÎ± GÎ± )
            2             2 Î±=1                       2 Î±=1

              Pk                       âˆ’1
where N =       Î±=1 NÎ± and GÎ± = âˆ†Î± DÎ± . Further, when the parameters are
unrestricted, the likelihood function L(x1 , . . . , xk, âˆ†1 P1 âˆ†1 , . . . , âˆ†k Pk âˆ†k ) is max-
imized when âˆ’ ln[det(PÎ± âˆ†2Î± )] âˆ’ tr PÎ±âˆ’1 GÎ± RÎ± GÎ± is maximized for each Î±. This
is true when

                  ln[det(PÎ± âˆ†2Î± )] + tr PÎ±âˆ’1 GÎ± RÎ± GÎ±
                                                             

                        = ln[det(âˆ†Î± PÎ± âˆ†Î± )] + tr âˆ†âˆ’1 âˆ’1 âˆ’1
                                                                                 
                                                   Î± PÎ± âˆ†Î± DÎ± RÎ± DÎ±


is minimized for each Î±. This is achieved when âˆ†Î± PÎ± âˆ†Î± = DÎ± RÎ± DÎ± . From this
it follows that the maximum value of L(x1 , . . . , xk , âˆ†1 P1 âˆ†1 , . . . , âˆ†k Pk âˆ†k ), when
the parameters are unrestricted, is given by

                                               Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

240                                           Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar




                     ln[L(xÌ„1 , . . . , xÌ„k , D1 R1 D1 , . . . , Dk Rk Dk )]
                                                     k
                               1                  1X
                            = âˆ’ N p[ln(2Ï€) + 1] âˆ’       NÎ± ln[det(RÎ± DÎ±2 )].                      (1)
                               2                  2 Î±=1

Let P be the common value of the population correlation matrices under the null
hypothesis of equality of correlation matrices. The reduced parameter space for
the covariance matrices is the set of all covariance matrices that may be written
as âˆ†Î± P where P is a correlation matrix and âˆ†Î± is a diagonal matrix with positive
elements on the diagonal. The restricted log likelihood function is written as
      ln[L(xÌ„1 , . . . , xÌ„k , âˆ†1 P, . . . , âˆ†k P )]
                             k                          k
            1             1X                         1X
                                NÎ± ln[det(P âˆ†2Î± )] âˆ’      N tr P âˆ’1 GÎ± RÎ± GÎ± .
                                                                            
         = âˆ’ N p ln(2Ï€) âˆ’
            2             2 Î±=1                      2 Î±=1 Î±

      Let P âˆ’1 = (Ïij ). Since âˆ†Î± is a diagonal matrix,
                                                   " p     #    p
                                                    Y           X
                         2
             ln[det(âˆ†Î± ) ] = 2 ln[det(âˆ†Î± )] = 2 ln     ÏƒÎ±ii = 2   ln(ÏƒÎ±ii )
                                                                   i=1         i=1

Also, since GÎ± = âˆ†âˆ’1
                  Î± DÎ± is a diagonal matrix, we have
                                                p X
                                                  p
                                              X
                             tr P âˆ’1 GÎ± RÎ± GÎ± =     Ïij gÎ±j rÎ±ij gÎ±i
                                                              i=1 j=1

Thus,
             ln[L(xÌ„1 , . . . , xÌ„k , âˆ†1 P, . . . , âˆ†k P )]
                                     k      p                 k
                    1             1X       X               1X
                 = âˆ’ N p ln(2Ï€) âˆ’       NÎ±     ln(ÏƒÎ±ii ) âˆ’       NÎ± ln[det(P )]
                    2             2 Î±=1    i=1
                                                           2 Î±=1
                           k      p X p
                        1X       X
                    âˆ’         NÎ±         Ïij gÎ±j rÎ±ij gÎ±i
                        2 Î±=1    i=1 j=1

Since, gÎ±i = sÎ±ii /ÏƒÎ±ii , differentiation of ln[L(x1 , . . . , xk , âˆ†1 P, . . . , âˆ†k P )] with re-
spect to ÏƒÎ±ii yields
                                                                            p
       âˆ‚                                                      NÎ±      NÎ± X
            ln[L(x1 , . . . , xk , âˆ†1 P, . . . , âˆ†k P )] = âˆ’       +           gÎ±i gÎ±j Ïij rÎ±ij
      âˆ‚ÏƒÎ±ii                                                  2ÏƒÎ±ii   2ÏƒÎ±ii j=1
                                                   Pp
Further, setting this equal to zero gives j=1 gÎ±i gÎ±j Ïij rÎ±ij âˆ’1 = 0. Differentiating
ln[L(x1 , . . . , xk , âˆ†1 P, . . . , âˆ†k P )] with respect to the matrix P using Lemma 6, we
obtain
                                                                 k
  âˆ‚                                                 1         1X
    ln[L(x1 , . . . , xk , âˆ†1 P, . . . , âˆ†k P )] = âˆ’ N P âˆ’1 +       NÎ± P âˆ’1 GÎ± RÎ± GÎ± P âˆ’1
 âˆ‚P                                                 2         2 Î±=1


                                                   Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

Testing Equality of Several Correlation Matrices                                           241

Setting this equal to zero,Pmultiplying by 2, pre and post
                                                       Pk multiplying by P and
                            k                                    2
dividing by N gives P = Î±=1 NÎ± GÎ± RÎ± GÎ± /N so that Î±=1 NÎ± gÎ±i      /N = 1.
     The likelihood ratio test statistic Î» for testing H : P1 = Â· Â· Â· = Pk is now derived
as
                                    k
                                    Y det(RÎ± D2 )NÎ± /2
                                                  Î±
                             Î»=
                                            b b 2 NÎ± /2
                                    Î±=1 det(P âˆ†Î± )


where Pb and âˆ† b 2 are solutions of Pb = Pk NÎ± âˆ†       b âˆ’1 /N and Pp Ïij sÎ±ij âˆ’
                                               b âˆ’1 SÎ± âˆ†
                   Î±                      Î±=1    Î±       Î±          j=1
1 = 0, i = 1, . . . , p, respectively.
     To obtain an approximation of the likelihood ratio statistic we replace ÏƒÎ±ii
by its consistent estimator Ïƒ                                                    ÏƒÎ±ii and
                                      bÎ±ii . Then, it follows that gbÎ±ii = sÎ±ii /b
                                                                          Pk
GÎ± = diag(b
 b           gÎ±1 , . . . , gbÎ±p ), and the estimator of P is given by P = Î±=1 NÎ± G
                                                                      b               b Î± RÎ±
Gb Î± /N . Thus, an approximation of the maximum of ln[L(x1 , . . . , xk , âˆ†1 P, . . . , âˆ†k P )]
is given as
                                   k
             1                  1X              b Î± )2 ] âˆ’ 1 N ln[det(Pb)]
            âˆ’ N p[ln(2Ï€) + 1] âˆ’       NÎ± ln[det(âˆ†                                          (2)
             2                  2 Î±=1                      2

                                           ÏƒÎ±ii converges in probability to 1 so that
As the sample size goes to infinity, sÎ±ii /b
b Î± converges in probability to Ip . This suggest further approximation of (2) as
G
                            k
                                                      "                k
                                                                                      !#
      1                  1X                  2   1                     X NÎ±
     âˆ’ N p[ln(2Ï€) + 1] âˆ’       NÎ± ln[det(DÎ± ) ] âˆ’ N ln det                       RÎ±        (3)
      2                  2 Î±=1                   2                     Î±=1
                                                                             N

Now, using (1) and (3), the likelihood ratio statistic is approximated as
                                      Qk            NÎ± /2
                                        Î±=1 det(RÎ± )
                             Î»Ìƒ =       Pk                                                 (4)
                                    det( Î±=1 NÎ± RÎ± /N )N/2

Further, replacing NÎ± by nÎ± above, an approximated modified likelihood ratio
statistic is derived as
                          Qk            nÎ± /2
                                               Qk
                            Î±=1 det(RÎ± )            det(RÎ± )nÎ± /2
                 M=         Pk                = Î±=1                      (5)
                        det( Î±=1 nÎ± RÎ± /n)n/2     det(R)n/2
                 Pk
Since âˆ’2 ln M = Î±=1 nÎ± ln{det(R)/ det(RÎ± )} = Lâˆ— , the statistic proposed by
Kullback may be thought of as an approximated modified likelihood ratio statistic.


3. Preliminaries
   Let the vectors xÎ±1 , . . . , xÎ±NÎ± be a random sample of size nÎ± for Î± = 1, . . . , k
from k multivariate populations of dimensionality p and finite fourth moments.
The characteristic function for the population Î± is given by Ï†âˆ—Î± (t) = E[exp(Î¹ t0 x)]

                                        Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

242                                      Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar

         âˆš
where Î¹ = âˆ’1 and t = (t1 , . . . , tp )0 . The log characteristic function for population
Î± may be written as
                                   âˆ                                    p
                                   X                                    Y (Î¹tj )rj
               ln[Ï†âˆ—Î± (t)] =                   Îºâˆ—Î± (r1 , . . . , rp )                ,   rj âˆˆ I +         (6)
                               r1 +Â·Â·Â·+rp =1                            j=1
                                                                              rj !

where I + is the set of non-negative integers. The cumulants of the distribution are
the coefficients Îºâˆ—Î± (r1 , . . . , rp ). If r1 + Â· Â· Â· + rp = m, then the associated cumulant
is of order m. The relationship between the cumulants of a distribution and the
characteristic function provide a convenient method for deriving the asymptotic
distribution of statistic whose asymptotic expectations can be derived.
     The cumulants of order m are functions of the moments of order m or lower.
Thus if the mth order moment is finite, so is the mth order cumulant. Let Âµi =
E(Xi ), Âµij = E(Xi Xj ), Âµijk = E(Xi Xj Xk ), and Âµijk` = E(Xi Xj Xk X` ) and Îºi ,
Îºij , Îºijk , and Îºijk` be the corresponding cumulants. Then, Kaplan (1952) gives
the following relationship:
             Îºi = Âµi ,
           Îºij = Âµij âˆ’ Âµi Âµj ,
          Îºijk = Âµijk âˆ’ (Âµi Âµjk + Âµj Âµik + Âµk Âµij ) + 2Âµi Âµj Âµk ,
                            4
                            X                   3
                                                X                        6
                                                                         X
         Îºijk` = Âµijk` âˆ’          Âµi Âµjk` âˆ’            Âµij Âµk` + 2            Âµi Âµj Âµk` âˆ’ 6Âµi Âµj Âµk Âµ`

 where the summations are over the possible ways of grouping the subscripts, and
the number of terms resulting is written over the summation sign.
      Define the random matrix VÎ± as
                                                       
                              âˆš       1 âˆ’1
                        V Î± = nÎ±        âˆ†Î± AÎ± âˆ†âˆ’1
                                               Î±  âˆ’ P Î±                                                   (7)
                                     nÎ±
                                      (0)        (1)            (2)
Then, the random matrices VÎ± , VÎ±                      and VÎ±           are defined as
                                VÎ±(0) = diag(vÎ±11 , vÎ±22 , . . . , vÎ±pp )                                 (8)
                                           1          1
                               VÎ±(1) = VÎ± âˆ’ VÎ±(0) PÎ± âˆ’ PÎ± VÎ±(0)                                           (9)
                                           2          2
and
             1 (0)       1          1          3              3
   VÎ±(2) =    V P V (0) âˆ’ VÎ± VÎ±(0) âˆ’ VÎ±(0) VÎ± + (VÎ±(0) )2 PÎ± + PÎ± (VÎ±(0) )2                              (10)
             4 Î± Î± Î±     2          2          8              8
Konishi (1979a, 1979b) has shown that
                                    1         1 (2)
                         RÎ± = PÎ± + âˆš VÎ±(1) +   V    + Op (nâˆ’3/2 )
                                    nÎ±       nÎ± Î±          Î±


The pooled estimate of the common correlation matrix is
                                                       k
                                                       X
                                            R=               Ï‰Î± RÎ±
                                                       Î±=1


                                                Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

Testing Equality of Several Correlation Matrices                                                         243

so that
                               1 (1) 1 (2)
                    R=P +âˆš V          + V      + Op (nâˆ’3/2 )
                                n        n
                                 Pk              (1)       Pk âˆš          (1)
where Ï‰Î± = nÎ± /n, P =              Î±=1 Ï‰Î± PÎ± , V      =      Î±=1 Ï‰Î± VÎ±       and
   (2)  Pk     (2)                                   âˆš       âˆ’1     âˆ’1
                                                                               
V      = Î±=1 VÎ± . The limiting distribution of VÎ± = nÎ± âˆ†Î± AÎ± âˆ†Î± /nÎ± âˆ’ PÎ±
is normal with means 0 and covariances that depend on the fourth order cumulants
of the parent population (Anderson 2003, p. 88).
   Since âˆ†Î± is a diagonal matrix of population standard deviations, âˆ†âˆ’1
                                                                     Î± xÎ±1 , . . . ,
âˆ†âˆ’1
 Î± xÎ±NÎ±    may be thought of as NÎ± observations from a population with finite
fourth order cumulants and characteristic function given by
                                 âˆ                                   p
                                 X                                   Y (Î¹tj )rj
              ln[Ï†Î± (t)] =                   ÎºÎ± (r1 , . . . , rp )                  , rj âˆˆ I +           (11)
                             r1 +Â·Â·Â·+rp =1                           j=1
                                                                           rj !

where the standardized cumulants, ÎºÎ± (r1 , r2 , . . . , rp ), are derived from the expres-
sion (6) as
                                                  Îºâˆ—Î± (r1 , r2 , . . . , rp )
                ÎºÎ± (r1 , r2 , . . . , rp ) =
                                             ÏƒÎ±11 Ï‡r1 ÏƒÎ±22 Ï‡r2 Â· Â· Â· ÏƒÎ±pp Ï‡rp

with Ï‡rj = 1 if rj = 0, Ï‡rj = 1/Ïƒ (Î±)jj if rj 6= 0 and Î£âˆ’1
                                                        Î± = (Ïƒ
                                                               (Î±)jj
                                                                     ).
    K-statistics are unbiased estimates of the cumulants of a distribution, and
may be used to derive the moments of the statistics which are symmetric func-
tions of the observations (Kendall and Stuart 1969). Kaplan (1952) gives a series
of tensor formulaes for computing the expectations of various functions of the
k-statistics associated with a sample of size N from a multivariate population.
For the definition of the k-statistics, let N (r) = N (N âˆ’ 1) Â· Â· Â· (N âˆ’ r + 1).
   If si1 i2 Â·Â·Â·i` denotes the product Xi1 Xi2 Â· Â· Â· Xi` summed over the sample, the
tensor formulae for the k-statistics may be shown to be as follows:
                                                                              3
                                                              N 2 sijk âˆ’ N
                                                                              P
          si             N sij âˆ’ si sj                                              si sjk + 2si sj sk
      ki = ,       kij =               ,         kijk =
          N                 N (2)                                                 N (3)
                                 4                  3            6
                                   si sjk` ) âˆ’ N (2) sij sk` + 2N si sj sk` âˆ’ 6si sj sk s`
                                 P                  P            P
          N (N + 1)(N sijk` âˆ’
kijk` =
                                                 N (4)

                         Îº(ab, ij) = E[(kab âˆ’ Îºab )(kij âˆ’ Îºij )]
                                     Îºabij   Îºai Îºbj + Îºaj Îºbi
                                   =       +
                                      N           N âˆ’1


  Îº(ab, ij, pq) = E[(kab âˆ’ Îºab )(kij âˆ’ Îºij )(kpq âˆ’ Îºpq )]
                               12                         4
                                         X (N âˆ’ 2)Îºaip Îºbjq X Îºai Îºbp Îºjq                   8
                   Îºabijpq X Îºabip Îºjq
               =       2
                          +            +                   +
                    N       N (N âˆ’ 1)        N (N âˆ’ 1)2       (N âˆ’ 1)2


                                             Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

244                                  Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar


The summations are over the possible ways of grouping the subscripts, and the
number of terms resulting is written over the summation sign.
   The matrix VÎ± is constructed from observations from the standardized distri-
                        âˆš
bution so that vÎ±ij = nÎ± (kÎ±ij âˆ’ ÏÎ±ij ) where kÎ±ij is the related k-statistic for
standardized population Î±. Kaplanâ€™s formulae may be applied to derive the fol-
lowing expressions for the expectations of elements of the matrices VÎ± (note that
ÎºÎ±ij = ÏÎ±ij ). We obtain
                                   E(vÎ±ij ) = 0
                  E(vÎ±ij vÎ±k` ) = ÎºÎ±ijk` + ÏÎ±ik ÏÎ±j` + ÏÎ±i` ÏÎ±jk + O(nâˆ’1
                                                                      Î± )

and

                                              12               4
                              1               X                 X
        E(vÎ±ij vÎ±k` vÎ±ab ) = âˆš     ÎºÎ±ijk`ab +     ÎºÎ±ijka ÏÎ±`b +   ÎºÎ±ika ÎºÎ±j`b
                               nÎ±
                              X 8               
                            +     ÏÎ±ik ÏÎ±ja ÏÎ±`b + O(nâˆ’3/2
                                                        Î±    )

                            (0)    (1)           (2)
The random matrices VÎ± , VÎ± and VÎ± are defined in (8), (9), and (10), respec-
tively. The expectations associated with these random matrices are given as
                                                 (1)
                                             E(vÎ±ij ) = 0


            (2)      1              1                    3
        E(vÎ±ij ) =     ÏÎ±ij ÎºÎ±iijj âˆ’ (ÎºÎ±iiij + ÎºÎ±ijjj ) + ÏÎ±ij (ÎºÎ±iiii + ÎºÎ±jjjj )
                     4              2                    8
                        1 3                  âˆ’1
                     + (ÏÎ±ij âˆ’ ÏÎ±ij ) + O(nÎ± )
                        2

     (1) (1)                1
  E(vÎ±ij vÎ±k` ) = ÎºÎ±ijk` âˆ’ (ÏÎ±ij ÎºÎ±iik` + ÏÎ±ij ÎºÎ±jjk` + ÏÎ±k` ÎºÎ±ijkk + ÏÎ±k` ÎºÎ±ij`` )
                            2
                    1
                  + ÏÎ±ij ÏÎ±k` (ÎºÎ±iikk + ÎºÎ±ii`` + ÎºÎ±jjkk + ÎºÎ±jj`` )
                    4
                  âˆ’ (ÏÎ±k` ÏÎ±ik ÏÎ±jk + ÏÎ±k` ÏÎ±i` ÏÎ±j` + ÏÎ±ij ÏÎ±ik ÏÎ±i` + ÏÎ±ij ÏÎ±jk ÏÎ±j` )
                    1
                  + ÏÎ±ij ÏÎ±k` (Ï2Î±ik + Ï2Î±i` + Ï2Î±jk + Ï2Î±i` )
                    2
                  + (ÏÎ±ik ÏÎ±j` + ÏÎ±i` ÏÎ±jk ) + O(nâˆ’1
                                                   Î± )                               (12)

and
                                                                        
           (1) (1) (1)        1                   1     1     1
        E(vÎ±ij vÎ±k` vÎ±ab ) = âˆš               tÎ±1 âˆ’ tÎ±2 + tÎ±3 âˆ’ tÎ±4           + O(nâˆ’3/2
                                                                                  Î±    )
                               nÎ±                 2     4     8

where
                            12
                            X                       4
                                                    X                   8
                                                                        X
        tÎ±1 = ÎºÎ±ijk`ab +          ÎºÎ±ijka ÎºÎ±`b +         ÎºÎ±ika ÎºÎ±i`b +       ÏÎ±ik ÏÎ±ja ÏÎ±`b


                                              Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

Testing Equality of Several Correlation Matrices                                                       245

              3
              X                               12
                                               X
      tÎ±2 =          ÏÎ±ij ÎºÎ±iik`ab + ÎºÎ±jjk`a +    (ÎºÎ±iika + ÎºÎ±jjka )
                  3
                  X                                          8
                                                             X                                     
              +       (ÎºÎ±ika ÎºÎ±i`b + ÎºÎ±jka ÎºÎ±j`b ) +             (ÏÎ±ik ÏÎ±ia ÏÎ±`b + ÏÎ±jk ÏÎ±ja ÏÎ±`b )


              3
              X               
      tÎ±3 =          ÏÎ±ij ÏÎ±k` ÎºÎ±iikkab + ÎºÎ±ii``ab + ÎºÎ±jjkkab + ÎºÎ±jj``ab
                  12
                  X
              +      (ÎºÎ±iika ÏÎ±kb + ÎºÎ±ii`a ÏÎ±`b + ÎºÎ±jjka ÏÎ±kb + ÎºÎ±jj`a ÏÎ±`b )
                  3
                  X
              +     (ÎºÎ±ika ÎºÎ±ikb + ÎºÎ±i`a ÎºÎ±i`b + ÎºÎ±jka ÎºÎ±jkb + ÎºÎ±j`a ÎºÎ±j`b )
                8
                X                                                                     
              +   (ÏÎ±ik ÏÎ±ia ÏÎ±bk + ÏÎ±i` ÏÎ±ia ÏÎ±`b + ÏÎ±jk ÏÎ±ja ÏÎ±bk + ÏÎ±j` ÏÎ±ja ÏÎ±`b )

and
                             8 
                             X             12
                                           X                   3
                                                               X
        tÎ±4 = ÏÎ±ij ÏÎ±k` ÏÎ±ab    ÎºÎ±iikkaa +    (ÎºÎ±iika ÏÎ±ka ) +   (ÎºÎ±ika ÎºÎ±ikb )
                      8
                      X                       
                  +       (ÏÎ±ik ÏÎ±ia ÏÎ±ka )

                                                       (1)
Lemma 1. The diagonal elements of VÎ±                         are zero.
                                                     (0)
Proof . Using (9) and the fact that VÎ±                     is a diagonal matrix, we have

                                   (1)         1
                                  vÎ±ij = vÎ±ij âˆ’ ÏÎ±ij (vÎ±ii + vÎ±jj )
                                               2
The result follows by taking j = i above and noting that diagonal elements of PÎ±
are 1.
                                                       (2)
Lemma 2. The diagonal elements of VÎ±                         are zero.
                                                      (0)
Proof . Using (10) and the fact that VÎ±                      is a diagonal matrix, we get

               (2)      1 (0)        (0)  1                     3       2      2
              vÎ±ij =      vÎ±ii ÏÎ±ij vÎ±jj âˆ’ vÎ±ij (vÎ±jj + vÎ±ii ) + ÏÎ±ij (vÎ±ii + vÎ±jj )
                        4                 2                     8
The result follows by substituting j = i above and observing that ÏÎ±ii = 1.


4. Asymptotic Expansion of Lâˆ—
   In order to derive the asymptotic distribution for Lâˆ— the statistic is first ex-
panded in terms of other random variables (see Konishi and Sugiyama 1981).

                                                  Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

246                                            Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar


    The statistic Lâˆ— may be written as Lâˆ— = ng(R1 . . . , Rk ) where g(R1 , . . . , Rk ) =
             Pk
ln[det(R)] âˆ’ Î±=1 Ï‰Î± ln[det(RÎ± )]. Let
                                      1              1 âˆ’1 (2)
                                BÎ± = âˆš PÎ±âˆ’1 VÎ±(1) +   P V
                                      nÎ±            nÎ± Î± Î±
                 (1)           (2)
Since, PÎ± , VÎ± and VÎ± are all positive definite, so is BÎ± . This insures that the
eigenvalues of BÎ± exist and are positive. Also, as nÎ± becomes large, the elements
in BÎ± become small so that the characteristic roots may be assumed to be less
than one. Using Lemma 5,

             ln[det(RÎ± )] = ln[det(PÎ± + PÎ± BÎ± )] + Op (nâˆ’3/2
                                                        Î±    )
                                                    1
                          = ln[det(PÎ± )] + tr(BÎ± ) âˆ’ tr(BÎ± BÎ± ) + Op (nâˆ’3/2
                                                                       Î±    )
                                                    2
                                     (1)        (1)        âˆ’3/2
Now, BÎ± BÎ± = nâˆ’1 âˆ’1    âˆ’1
              Î± PÎ± VÎ± PÎ± VÎ± + Op (nÎ±                              ) so that
                                           1                     1
            ln[det(RÎ± )] = ln[det(PÎ± )] + âˆš tr(PÎ±âˆ’1 VÎ±(1) ) +       tr(PÎ±âˆ’1 VÎ±(2) )
                                           nÎ±                   nÎ±
                               1                         
                           âˆ’      tr PÎ±âˆ’1 VÎ±(1) PÎ±âˆ’1 VÎ±(1) + Op (nâˆ’3/2
                                                                   Î±   )
                              2nÎ±
A similar expansion for ln[det(R)] may be obtained by defining B by
                               k                  k
                           1 Xâˆš       âˆ’1       1 X âˆ’1 (2)
                        B=âˆš       Ï‰Î± P VÎ±(1) +       P VÎ±
                            n Î±=1              n Î±=1

Then

           ln[det(R)] = ln[det(P + P B)] + Op (nâˆ’3/2 )
                                              1
                      = ln[det(P )] + tr(B) âˆ’ tr(BB) + Op (nâˆ’3/2 )
                                              2
                  k     k   âˆš          âˆ’1 (1) âˆ’1 (1)
Since BB = nâˆ’1 Î±=1 Î²=1 Ï‰Î± Ï‰Î² P VÎ± P VÎ² + Op (nâˆ’3/2 ),
                P     P

                                       k                       k
                                   1 Xâˆš          âˆ’1         1X        âˆ’1
       ln[det(R)] = ln[det(P )] + âˆš       Ï‰Î± tr(P VÎ±(1) ) +       tr(P VÎ±(2) )
                                    n Î±=1                   n Î±=1
                                 k         k
                            1 XXâˆš           âˆ’1      âˆ’1 (1)
                       âˆ’          Ï‰Î± Ï‰Î² tr(P VÎ±(1) P VÎ² ) + Op (nâˆ’3/2 )
                           2n Î±=1
                                       Î²=1

Combining these expressions yields
                                                k                             k
                                                X                      1 Xâˆš
      g(R1 , . . . , Rk ) = ln[det(P )] âˆ’           Ï‰Î± ln[det(PÎ± )] + âˆš       Ï‰Î± tr(HÎ± VÎ±(1) )
                                                Î±=1
                                                                        n Î±=1
                                  k                     k
                               1X                    1 X Ï‰Î±
                           +         tr(HÎ± VÎ±(2) ) +          tr(PÎ±âˆ’1 VÎ±(1) PÎ±âˆ’1 VÎ±(1) )
                               n Î±=1                 2 Î±=1 nÎ±


                                                  Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

Testing Equality of Several Correlation Matrices                                                                247

                                      k       k
                                1 XXâˆš           âˆ’1      âˆ’1 (1)
                           âˆ’          Ï‰Î± Ï‰Î² tr(P VÎ±(1) P VÎ² ) + Op (nâˆ’3/2 )
                               2n Î±=1
                                          Î²=1

                                 âˆ’1âˆ’1      0
                                                                                            âˆš
where HÎ± = (hÎ±ij ) = (P âˆ’P        âˆšÎ± ) = HÎ± . LetâˆšG(R1 , . . . , Rk ) =                         n[g(R1 , . . . , Rk )
âˆ’g(P1 , . . . , Pk )]. Then, since n(Ï‰Î± /nÎ± ) = ( n)âˆ’1 , we obtain

                           k                                        k
                           X âˆš                        1 X
   G(R1 , . . . , Rk ) =          Ï‰Î± tr(HÎ± VÎ±(1) ) + âˆš       tr(HÎ± VÎ±(2) )
                           Î±=1
                                                       n Î±=1
                                 k
                             1 X
                           + âˆš      tr(PÎ±âˆ’1 VÎ±(1) PÎ±âˆ’1 VÎ±(1) )
                            2 n Î±=1
                                 k  k
                             1 XXâˆš              âˆ’1      âˆ’1 (1)
                           âˆ’ âˆš        Ï‰Î± Ï‰Î² tr(P VÎ±(1) P VÎ² ) + Op (nâˆ’1 )                                      (13)
                            2 n Î±=1
                                              Î²=1


Theorem 1. The expression G(R1 , . . . , Rk ) may be written as

                            k X                                         k
                            X   âˆš                       (1)    1 XX           (2)
    G(R1 , . . . , Rk ) =                     Ï‰Î± hÌ„Î±ij vÎ±ij + âˆš          hÌ„ v
                            Î±=1 i<j
                                                                n Î±=1 i<j Î±ij Î±ij
                                          k
                              1 XXX                  (1) (1)
                            +âˆš           qÎ± (ij, k`)vÎ±ij vÎ±k`
                               n Î±=1 i<j
                                                    k<`
                                         k XX
                                       k X
                              1        X                    âˆš                        (1) (1)
                            âˆ’âˆš                                  Ï‰Î± Ï‰Î² q(ij, k`)vÎ±ij vÎ²k` + Op (nâˆ’1 )
                               n Î±=1
                                              Î²=1 i<j k<`

                                 âˆ’1
where PÎ±âˆ’1 = (Ïij   Î± ), P      = (Ïij ), hÎ±ij = 2(Ïij âˆ’ Ïij                  i` jk
                                                          Î± ), qÎ± (ij, k`) = ÏÎ± ÏÎ± +
                           i` jk     ik j`
Ïik
 Î± Î±Ï j`
         and q(ij, k`) = Ï   Ï   + Ï   Ï   .

Proof . Using results on matrix algebra, we have
                     k                                    k             p
                                                                      p X
                     X âˆš                                  X âˆš         X                   (1)
                               Ï‰Î± tr(HÎ± VÎ±(1) ) =                Ï‰Î±             hÎ±ji vÎ±ij
                     Î±=1                                  Î±=1         i=1 j=1


and since HÎ± is symmetric, application of Lemma 3 yields
   k                              k                                       k
   X âˆš                            X âˆš               X               (1)
                                                                          X âˆš X       (1)
          Ï‰Î± tr(HÎ± VÎ±(1) ) =                  Ï‰Î±     (hÎ±ji + hÎ±ij )vÎ±ij =    Ï‰Î± hÎ±ij vÎ±ij
   Î±=1                            Î±=1               i<j                             Î±=1          i<j


In an entirely similar manner,
                                 k                         k X
                                                                              (2)
                                 X                         X
                                       tr(HÎ± VÎ±(2) ) =                  hÎ±ij vÎ±ij
                                 Î±=1                       Î±=1 i<j


                                                    Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

248                                            Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar

                                                                                         (1)
Using Lemma 4, results on matrix algebra and the symmetry of VÎ± , we have
               k
           1X
                 tr(PÎ±âˆ’1 VÎ±(1) PÎ±âˆ’1 VÎ±(1) )
           2 Î±=1
                      k       p       p    p     p
                   1 X X X X X i` jk (1) (1)
              =                  ÏÎ± ÏÎ± vÎ±ij vÎ±k`
                   2 Î±=1 i=1 j=1
                                          k=1 `=1
                     k XX
                   1 X                                                                (1) (1)
              =                           (Ïi` jk   j` ik   ik j`   i` jk
                                            Î± ÏÎ± + ÏÎ± ÏÎ± + ÏÎ± ÏÎ± + ÏÎ± ÏÎ± )vÎ±ij vÎ±k`
                   2 Î±=1 i<j
                                  k<`
                   k XX
                                                       (1) (1)
                   X
              =                        qÎ± (ij, k`)vÎ±ij vÎ±k`
                   Î±=1 i<j k<`

In a similar manner,
                          k        k
                      1 XXâˆš           âˆ’1      âˆ’1 (1)
                            Ï‰Î± Ï‰Î² tr(P VÎ±(1) P VÎ² )
                      2 Î±=1
                                  Î²=1
                                    k XX
                                  k X
                                  X      âˆš                                 (1) (1)
                          =                                Ï‰Î± Ï‰Î² q(ij, k`)vÎ±ij vÎ²k`
                              Î±=1 Î²=1 i<j k<`

Combining these expansions in (13) completes the proof.
Corollary 1. In the special case p = 2, G(R1 , . . . , Rk ) may be written as
                         k                           
                        X   âˆš        ÏÎ±         Ï         (1)
G(R1 , . . . , Rk ) = 2       Ï‰Î±         2
                                           âˆ’        2   vÎ±12
                        Î±=1
                                   1 âˆ’ Ï Î±   1 âˆ’  Ï
                              k                                      k
                                                                 1 X 1 + Ï2Î±
                                                     
                          2 X        ÏÎ±        Ï          (2)                        (1)
                      +âˆš                   âˆ’            v Î±12 + âˆš                  (v )2
                           n Î±=1 1 âˆ’ Ï2Î±     1 âˆ’ Ï2               n Î±=1 (1 âˆ’ Ï2Î± )2 Î±12
                                  k        k
                      1 X X 1 + Ï2 (1) (1)
                    âˆ’âˆš                 v v      + Op (nâˆ’1 ).
                       n Î±=1 (1 âˆ’ Ï2 )2 Î±12 Î²12
                                          Î²=1
                          P
Proof . For p = 2,         i<j consists of single term corresponding to i = 1, j = 2.
                                                                                âˆ’1
Also, PÎ± = ÏÎ± 1 so that PÎ±âˆ’1 = (1 âˆ’ Ï2Î± )âˆ’1 âˆ’Ï1 Î± âˆ’Ï1 Î± . Similarly, P
                 1 ÏÎ±                                           
                                                                                    =
                    
               1  âˆ’Ï
(1 âˆ’ Ï2 )âˆ’1 âˆ’Ï 1 . Thus, the off diagonal element of HÎ± is given by ÏÎ± (1 âˆ’
Ï2Î± )âˆ’1 âˆ’ Ï(1 âˆ’ Ï2 )âˆ’1 . Further, qÎ± (12, 12) = Ï12   21   11 22         2        2 2
                                                   Î± ÏÎ± + ÏÎ± ÏÎ± = (1 + ÏÎ± )/(1 âˆ’ ÏÎ± )
                         2         2 2
and q(12, 12) = (1 + Ï )/(1 âˆ’ Ï ) . The result follows by using these values in the
theorem.


5. Asymptotic Null Distribution of Lâˆ—
    In this section we derive asymptotic distribution of the statistic Lâˆ— when the
null hypothesis is true.

                                                     Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

Testing Equality of Several Correlation Matrices                                                            249

   Define the k Ã— k matrix W as W = (wij ) where wii = 1 âˆ’ Ï‰i and for i 6= j,
        âˆš
wij = âˆ’ Ï‰i Ï‰j = wji , 1 â‰¤ i, j, â‰¤ k. The matrix W has rank k âˆ’ 1 and each of its
non-zero eigenvalues is equal to 1.
Theorem 2. Let the k correlation matrices R1 , . . . , Rk be based on independent
samples of sizes N1 , . . . , Nk , respectively, with finite fourth order cumulants. De-
fine the kp(p âˆ’ 1)/2 Ã— 1 vector v(1) by
           (1)      (1)         (1)        (1)     (1)           (1)             (1)    (1)          (1)
v (1) = (v1,1,2 ,v1,1,3 ,. . .,v1,pâˆ’1,p ,v2,1,2 ,v2,1,3 ,. . .,v2,pâˆ’1,p ,. . .,vk,1,2 ,vk,1,3 ,. . .,vk,pâˆ’1,p )0
          (1)
where VÎ± is as defined in (9). Let Q = (q(ij, k`)) be the p(p âˆ’ 1)/2 Ã— p(p âˆ’ 1)/2
matrix of coefficients defined in Theorem 1.
                                                                        (1)
   Let TÎ± be the asymptotic dispersion matrix of VÎ± with entry (ij, k`) equal to
   (1) (1)
E(vÎ±ij vÎ±k` ) given in (12). Then, the asymptotic dispersion matrix of v(1) is

                                                               Â·Â·Â·
                                           ï£«
                                             T1          0             0
                                                                         ï£¶
                                           ï£¬ 0           T2    Â·Â·Â·     0 ï£·
                                      Tâˆ— = ï£¬ .
                                           ï£¬                             ï£·
                                                          ..
                                           ï£­ ..
                                                                         ï£·
                                                           .             ï£¸
                                             0           0     Â·Â·Â·     Tk
Under the null hypothesis
                                       p(pâˆ’1)(kâˆ’1)/2
                                             X
                               Lâˆ— =                       Î»i yi + Op (nâˆ’1/2 )
                                             i=1

where y1 , . . . , yp(pâˆ’1)(kâˆ’1)/2 are independent, yi âˆ¼ Ï‡21 , 1 â‰¤ i â‰¤ p(pâˆ’1)(kâˆ’1)/2 and
Î»1 , . . . , Î»p(pâˆ’1)(kâˆ’1)/2 are the eigenvalues of T âˆ— (Q âŠ— W ). If the standardized fourth
order cumulants of the populations are all equal, then TÎ± = T for Î± = 1, . . . , k and
                                          p(pâˆ’1)/2
                                             X
                                  Lâˆ— =               Î¸i ui + Op (nâˆ’1/2 ),
                                             i=1

where u1 , . . . , up(pâˆ’1)/2 are independent, ui âˆ¼ Ï‡2kâˆ’1 and Î¸1 , . . . , Î¸p(pâˆ’1)/2 are the
eigenvalues of T Q.

Proof . Under the null hypothesis we have PÎ± = P for Î± = 1, . . . , k so that
g(P1 , . . . , Pk ) = 0, hÎ±ij = 0 and qÎ± (ij, k`) = q(ij, k`) = Ïi` Ïjk + Ïik Ïj` for all Î±.
                                          Pk
Since g(R1 , . . . , Rk ) = ln[det(R)] âˆ’ Î±=1 Ï‰Î± ln[det(RÎ± )] = nâˆ’1 Lâˆ— , using Theo-
rem 1, one obtains
                Lâˆ— = ng(R1 , . . . , Rk ) = n[g(R1 , . . . , Rk ) âˆ’ g(P1 , . . . , Pk )]
                        k XX
                                                   (1) (1)
                        X
                    =                  q(ij, k`)vÎ±ij vÎ±k`
                        Î±=1 i<j k<`
                            k XX
                          k X
                          X      âˆš                                     (1) (1)
                      âˆ’                          Ï‰Î± Ï‰Î² q(ij, k`)vÎ±ij vÎ²k` + Op (nâˆ’1/2 )
                          Î±=1 Î²=1 i<j k<`


                                                 Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

250                                    Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar

                    k X
                      k
                                                        (1) (1)
                    X                XX
                =              wÎ±Î²             q(ij, k`)vÎ±ij vÎ²k` + Op (nâˆ’1/2 )
                    Î±=1 Î²=1          i<j k<`
                       (1) 0
                = (v      ) (Q âŠ— W )v (1) + Op (nâˆ’1/2 )

 Since Q is of rank p(p âˆ’ 1)/2 and W is of rank k âˆ’ 1, the matrix Q âŠ— W is of
                                                                          (1)
rank p(p âˆ’ 1)(k âˆ’ 1)/2. From (8) and (9) it is clear that elements of VÎ± are
linear functions of elements of VÎ± and the limiting distribution of VÎ± is normal
with means 0 and covariances that depend on the fourth order cumulants of the
parent population. Therefore, v (1) is asymptotically normal with means zero and
                                   Pp(pâˆ’1)(kâˆ’1)/2
dispersion matrix T âˆ— . Thus, Lâˆ— = i=1            Î»i yi + Op (nâˆ’1/2 ).
    If the standardized fourth order cumulants are the same for each underlying
population, then T âˆ— = T âŠ— I. Further, (T âŠ— I)(Q âŠ— W ) = T Q âŠ— W has as its
eigenvalues Î¸i j , i = 1, . . . , p(p âˆ’ 1)/2, j = 1, . . . , k where Î¸i are the eigenvalues
of T Q and j are the eigenvalues of W . Since there are p(p âˆ’ 1)/2 non-zero
eigenvalues of (T âŠ— I)(Q âŠ— W ) each occurring with multiplicity k âˆ’ 1, we have
       Pp(pâˆ’1)/2
Lâˆ— = i=1           Î¸i ui + Op (nâˆ’1/2 ).
Corollary 2. Let the k sample correlation coefficients r1 , r2 , . . . , rk be based on
independent samples of sizes N1 , N2 , . . . , Nk from bivariate populations with finite
fourth order cumulants. Let Ï be the hypothesized common correlation coefficient.
Define the k Ã— 1 vector v (1) by
                                                 (1)       (1)
                                     v (1) = (v1 , . . . , vk )0
        (1)
where vÎ± = vÎ±12 âˆ’ Ï(vÎ±11 + vÎ±22 ) as defined in (9). Let
                                                  
            2 2 1 2                            1 2
 tÎ± = (1 âˆ’ Ï ) + Ï (ÎºÎ±1111 + ÎºÎ±2222 ) + 1 + Ï ÎºÎ±1122 âˆ’ Ï(ÎºÎ±1113 + ÎºÎ±1222 )
                4                              2
and define T âˆ— = diag(t1 , . . . , tk ).Under the null hypothesis the statistic Lâˆ— is
asymptotically expanded as
                                             kâˆ’1
                                   1 + Ï2 X
                           Lâˆ— =                  Î»i yi + Op (nâˆ’1/2 )
                                  (1 âˆ’ Ï2 )2 i=1

where y1 , . . . , ykâˆ’1 are independent, yi âˆ¼ Ï‡21 and Î»1 , . . . , Î»,kâˆ’1 are the eigenvalues
of T âˆ— W . If the standardized fourth order cumulants are equal, then
                                                         
                    2 2   1 2                        1 2
    tÎ± = (1 âˆ’ Ï ) + Ï (Îº1111 + Îº2222 ) + 1 + Ï Îº1122 âˆ’ Ï(Îº1113 + Îº1222 )
                          4                          2
for Î± = 1, 2, . . . , k and
                                                                    
                 âˆ—            2 2   1 2                           1 2
              L = (1 âˆ’ Ï ) + Ï (Îº1111 + Îº2222 ) + 1 + Ï Îº1122
                                    4                             2
                                              1 + Ï2 2
                                           
                       âˆ’ Ï(Îº1113 + Îº1222 )             Ï‡    + Op (nâˆ’1/2 )
                                             (1 âˆ’ Ï2 )2 kâˆ’1


                                           Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

Testing Equality of Several Correlation Matrices                                     251

Proof . As shown in Corollary 1, when p = 2, Q is a scalar. If Ï is the common
correlation coefficient, then Q = (1 + Ï2 )/(1 âˆ’ Ï2 )2 . The asymptotic variance of
 (1)
vÎ±12 is given in (12). Upon simplification,
                                                                      
        (1) (1)                 2 2  1 2                           1 2
     E(vÎ±12 vÎ±12 ) = tÎ± = (1 âˆ’ Ï ) + Ï (ÎºÎ±1111 + ÎºÎ±2222 ) + 1 + Ï ÎºÎ±1122
                                     4                             2
                          âˆ’ Ï(ÎºÎ±1113 + ÎºÎ±1222 ) + Op (nâˆ’1/2 )                       (14)
so that T âˆ— is the asymptotic covariance matrix of v(1) . Further, T âˆ— (Q âŠ— W ) =
                                                                Pkâˆ’1
[(1 + Ï2 )/(1 âˆ’ Ï2 )2 ]T âˆ— W . Thus Lâˆ— = [(1 + Ï2 )/(1 âˆ’ Ï2 )2 ] i=1 Î»i yi + Op (nâˆ’1/2 ),
where Î»i are the eigenvalues of T âˆ— W . If the standardized fourth order cumulants
are identical, T = tI, so that there is one eigenvalue of T Q with multiplicity k.
This eigenvalue is merely t(1 + Ï2 )/(1 âˆ’ Ï2 )2 and the result follows immediately
from Theorem 2.
Corollary 3. Let the k sample correlation coefficients r1 , r2 , . . . , rk be based on
independent samples of sizes N1 , N2 , . . . , Nk from bivariate populations which are
elliptically contoured with a common curtosis of 3Îº and common correlation coef-
ficient Ï. Then
                                           1 + Ï2 2
             Lâˆ— = (1 âˆ’ Ï2 )2 + (1 + 2Ï2 )Îº                + Op (nâˆ’1/2 )
                 
                                                     Ï‡
                                           (1 âˆ’ Ï2 )2 kâˆ’1

Proof . For elliptically contoured distributions (Muirhead 1982, Anderson 2003,
Gupta and Varga 1993) the fourth order cumulants are such that Îºiiii = 3Îºiijj =
3Îº for i 6= j and all other cumulants are zero (Waternaux 1984). Substituting this
into the expression for t in Corollary 2 yields t = (1 âˆ’ Ï2 )2 + (1 + 2Ï2 )Îº. The result
then follows from Corollary 2.
Corollary 4. Let the k sample correlation coefficients r1 , . . . , rk be based on in-
dependent samples of sizes N1 , . . . , Nk from bivariate normal populations with a
common correlation coefficient Ï. Then
                           Lâˆ— = (1 + Ï2 )Ï‡2kâˆ’1 + Op (nâˆ’1/2 )

Proof . Normal distributions are special case of elliptically contoured distribu-
tions. The fourth order cumulants are all zero (Anderson 2003). The result follows
by setting Îº = 0 in Corollary 3.


6. An Example
   This example is included to demonstrate the procedure to be used when testing
the equality of correlation matrices by using the statistic Lâˆ— . The data represent
random samples from three trivariate populations each with identical correlation
matrix P given by
                                 ï£«                  ï£¶
                                   1.0 0.3      0.2
                             P = ï£­0.3 1.0 âˆ’0.3ï£¸
                                     0.2   âˆ’0.3     1.0

                                      Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

252                              Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar


Since the statistic Lâˆ— is an approximation of the modified likelihood ratio statistic
for samples from multivariate normal populations, it is particularly suited to pop-
ulations that are near normal. The contaminated normal model has been chosen
to represent such a distribution.
    Samples of size 25 from contaminated normal populations with mixing param-
eter  = 0.1 and Ïƒ = 2 were generated using the SAS system. These data are
tabulated in Gupta, Johnson and Nagar (2012). The density of a contaminated
normal model is given by

          Ï† (x, Ïƒ, Î£) = (1 âˆ’ )Ï†(x, Î£) + Ï†(x, ÏƒÎ£),   Ïƒ > 0,    0<<1

where Ï†(x, Î£) is the density of a multivariate normal distribution with zero mean
vector and covariance matrix Î£.
    If the data were known to be from three normal populations all that would
be required at this point would be the sample sizes and the matrix of corrected
sums of squares and cross products. A key element, however, of the modified
likelihood ratio procedure is that this assumption need not be made, but the
fourth order cumulant must be estimated. To do this the k-statistics are calculated
using Kaplanâ€™s formulae summarized in Section 3. The computations are made
considerably easier by standardizing the data so that all of the first order sums
are zero.
    The computation using original (or standardized) data yields the following
estimates of the individual correlation matrices:
                 ï£«                                ï£¶
                    1.0000     0.5105    0.3193
          R1 = ï£­ 0.5105        1.0000 âˆ’0.3485 ï£¸ , det(R1 ) = 0.4024
                    0.3193 âˆ’0.3485       1.0000
                 ï£«                                ï£¶
                    1.0000     0.1758    0.2714
          R2 = ï£­ 0.1758        1.0000 âˆ’0.2688 ï£¸ , det(R2 ) = 0.7975
                    0.2714 âˆ’0.2688       1.0000
                 ï£«                                ï£¶
                    1.0000     0.2457    0.3176
          R3 = ï£­ 0.2457        1.0000 âˆ’0.0331 ï£¸ , det(R3 ) = 0.8325
                    0.3176 âˆ’0.0331       1.0000

Since each sample is of size 25, Ï‰i = 1/3 for i = 1, 2, 3 and the pooled correlation
matrix is merely the average of these three matrices:
                 ï£«                              ï£¶
                  1.0000      0.3107     0.3028
            R = ï£­ 0.3107      1.0000    âˆ’0.2168 ï£¸ ,     det(R) = 0.7240
                  0.3028     âˆ’0.2168     1.0000

The value of the test statistic is now easily calculated as

          Lâˆ— = 72 ln(0.7240) âˆ’ 24[ln(0.4024) + ln(00.7975) + ln(0.8325)]
             = 8.7473


                                    Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

Testing Equality of Several Correlation Matrices                                       253

The null hypothesis is to be rejected if the value of the test statistic is too large.
The next step of the procedure is to estimate the coefficients in the linear combi-
nation of chi-square variables that make up the actual distribution under the null
hypothesis. The most arduous part is the computation of the estimates of fourth
order cumulants.
   Since the data are standardized, the formula for the k-statistic for the four way
product xi Ã— xj Ã— xk Ã— x` simplifies to

                   1
        kijk` =         [N 2 (N + 1)sijk` âˆ’ N (N âˆ’ 1)(sij sk` + sik sj` + si` sjk )]
                  N (4)

where N (4) = N (N âˆ’ 1)(N âˆ’ 2)(N âˆ’ 3). Using this to estimate the cumulant
corresponding to x21 x22 yields k1122 = 0.6670. The computation for other fourth
order cumulant are performed similarly. The resulting estimates are then pooled
across population to yield an estimate of the common fourth order cumulants
used in building the tau matrix (it is possible, of course, to drop the assumption
of common fourth order cumulants and use the nine by nine matrix that would
result if each separate tau matrix were joined in a block diagonal matrix). The
estimates of the fourth order cumulants are summarized in the Table 1.
    The pooled correlation matrix and these estimates are now used to build the
                                                                (1) (1)
estimated covariance matrix V (1) . The entry corresponding to vij vk` is given by

                       1
               kijk` âˆ’ (rij kiik` + rij kjjk` + rk` kijkk + rk` kij`` )
                       2
                       1
                     + rij rk` (kiikk + kii`` + kjjkk + kjj`` )
                       4
                     âˆ’ (rk` rik rjk + rk` ri` rj` + rij rik ri` + rij rjk rj` )
                       1           2    2        2      2
                     + rij rk` (rik  + ri`  + rjk  + rj`  ) + rik rj` + ri` rjk
                       2
where rij is the pooled estimate of the correlation value and kijk` is the correspond-
ing pooled fourth order cumulant. The entry corresponding to 12, 13 is given by
t12,13 = âˆ’0.3065. Similar calculations yield the following covariance matrix corre-
                (1)  (1)    (1)
sponding to (vÎ±12 , vÎ±13 , vÎ±23 )0 ,

                                              âˆ’0.3065 0.1800
                              ï£«                              ï£¶
                               1.0150
                        T = ï£­ âˆ’0.3065          0.7242 0.3974 ï£¸
                               0.1800          0.3974 0.8179

    To complete the example, the inverse of the pooled correlation matrix is used
to estimate the matrix Q. The entry corresponding to the element ij, k` is given
by rik rj` + ri` rjk where Râˆ’1 = (rij ). These matrices are as follows:

                                              âˆ’0.5198      âˆ’0.5113
                              ï£«                                    ï£¶
                             1.3163
                       âˆ’1
                      R = ï£­ âˆ’0.5198            1.2546       0.4294 ï£¸
                            âˆ’0.5113            0.4294       1.2479

                                        Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

254                                 Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar


                        Table 1: Estimated fourth order cumulants
            Variables     Population 1   Population 2    Population 3   Pooled
                1111            0.9077         0.1181          0.9355    0.6538
                1112            0.7765        -0.0387         -0.0565    0.2271
                1113           -0.3015         0.7008          0.0677    0.1105
                1122            0.6670         0.3595         -0.3663    0.2201
                1123           -0.3917         0.3519         -0.1333   -0.0574
                1133           -0.1848         0.6608         -0.7475   -0.0905
                1222            0.4896        -0.7128         -0.0178   -0.0803
                1223           -0.3005         0.1637         -0.2243   -0.1204
                1233           -0.0980         0.6343         -0.1394    0.1323
                1333           -0.3430         0.3973         -0.0773   -0.0077
                2222           -0.0787        -0.9989          0.8134   -0.0881
                2223           -0.2543         0.0750          0.1887    0.0032
                2233            0.3800        -0.1764         -0.5454   -0.1139
                2333           -0.8386         0.8496          0.2869    0.0993
                3333            0.9130        -0.9196          1.3068    0.4334


                                                        âˆ’0.8647
                              ï£«                                 ï£¶
                               1.9217 0.8310
                         Q= ï£­  0.8310 1.9041            âˆ’0.8682 ï£¸
                              âˆ’0.8647 0.8682             1.7500
Most eigenvalues extraction routines require that the matrix being analyzed be
symmetric. Let A be the Cholesky decomposition of Q, that is Q = A0 A where
A is an upper triangular matrix. Then the eigenvalues of T Q are the same as the
eigenvalues of AT A0 which is clearly symmetric. In this case

                               1.3863 0.5995 âˆ’0.6237
                           ï£«                             ï£¶

                      A=ï£­           0 1.2429 âˆ’0.3977 ï£¸
                                    0      0     1.0967

                                               âˆ’0.2877    âˆ’0.0246
                               ï£«                                  ï£¶
                              1.4111
                   AT A0 = ï£­ âˆ’0.2877            0.8552     0.1849 ï£¸
                             âˆ’0.0246            0.1849     0.9837
and the eigenvalue of this matrix are 1.55, 1.0473 and 0.6527. Using Theorem 2,
the distribution of the statistic is estimated to be that of Y = (1.55)Ï‡22 +(1.0473)Ï‡22
+(0.6527)Ï‡22 where each of the chi-square variate is independent. Using Lemma 7
the cumulative probability value associated with 8.7473 is obtained as 0.7665 so
that the observed significance level is 0.2335. Thus, if the test is performed at
the Î± = 0.1 level of significance the conclusion reached is that there is insufficient
evidence to reject the null hypothesis that the samples are from populations with
identical correlation matrices.


Acknowledgements
    The authors wish to thank three anonymous referees for their careful reading
of the manuscript and their fruitful comments and suggestions.


                                         Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

Testing Equality of Several Correlation Matrices                                    255
                                                                  
                 Recibido: junio de 2012 â€” Aceptado: julio de 2013




Appendix
Lemma 3. Let V = (vij ) be a p Ã— p symmetric matrix with zero on the diagonal
and let C = (cij ) be a p Ã— p symmetric matrix. Then
                                   p X
                                   X p                     X
                       tr(CV ) =             cij vij = 2         cij vij
                                   i=1 j=1                 i<j


Proof . The proof is obtained by noting that vjj = 0 and cij = cji .

Lemma 4. Let VÎ± = (vÎ±ij ) and VÎ² = (vÎ²ij ) be p Ã— p symmetric matrices with
zero on the diagonal. Then
   p X
 p X
 X     p
     p X                               XX
                   cijk` vÎ±ij vÎ²k` =             (cijk` + cij`k + cjik` + cji`k )vÎ±ij vÎ²k` .
 i=1 j=1 k=1 `=1                       i<j k<`


Proof . Using Lemma 3, the sum may be written as
                           p X
                         p X
                         X
                                        (cijk` + cij`k )vÎ±ij vÎ²k`
                          i=1 j=1 k<`

The proof is obtained by applying Lemma 3 second time.

Lemma 5. Let A be a real symmetric matrix with eigenvalues that are less than
one in absolute value, then
                                                  1          1
             âˆ’ ln[det(I âˆ’ A)] = tr(A) +             tr(A2 ) + tr(A3 ) + Â· Â· Â·
                                                  2          3
Proof . See Siotani, Hayakawa and Fujikoshi (1985).

Lemma 6. Let R be a correlation of dimension p. Then
                                  âˆ‚
                                    ln[det R] = Râˆ’1
                                 âˆ‚P
and
                              âˆ‚
                                tr(Râˆ’1 B) = Râˆ’1 BRâˆ’1
                             âˆ‚P
where B is a symmetric non-singular matrix of order p.

Proof . See Siotani, Hayakawa and Fujikoshi (1985).


                                          Revista Colombiana de EstadÃ­stica 36 (2013) 237â€“258

258                             Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar


Lemma 7. Let Y1 , Y2 and Y3 be independent random variables, Yi âˆ¼ Ï‡22 , i = 1, 2, 3.
Define Y = Î±1 Y1 + Î±2 Y2 + Î±3 Y3 where Î±1 , Î±2 and Î±3 are constants, Î±1 > Î±2 >
Î±3 > 0. Then, the cumulative distribution function FY (y) of Y is given by
                              3                  
                              X                 y
                 FY (y)   =       Ci 1 âˆ’ exp âˆ’        ,      y > 0,
                              i=1
                                               2Î±i

where C1 = Î±12 /(Î±1 âˆ’ Î±3 )(Î±1 âˆ’ Î±2 ), C2 = âˆ’Î±22 /(Î±2 âˆ’ Î±3 )(Î±1 âˆ’ Î±2 ) and
C3 = Î±32 /(Î±2 âˆ’ Î±3 )(Î±1 âˆ’ Î±3 )

Proof . We
        P3get the desired result by inverting the moment generating function
MY (t) = i=1 Ci (1 âˆ’ 2Î±i t)âˆ’1 , 2Î±1 t < 1.

References
Aitkin M. Some tests for correlation matrices.(1969). Biometrika.
Aitkin M A, Nelson W C, Reinfurt K H. Tests for correlation matrices. (1968).  Biometrika.
Ali M M, Fraser D A S, Lee Y S. Distribution of the correlation matrix.(1970). Journal of Statistical Research.
Anderson T W. An Introduction to Multivariate Statistical Analysis Wiley Series in Probability and Statistics.(2003).John Wiley Sons.
Browne M W. The likelihood ratio test for the equality of correlation matrices.(1978). The British Journal of Mathematical and Statistical Psychology.
Cole N. The likelihood ratio test of the equality of correlation matrices.(1968).University of North Carolina.
Cole N. On testing the equality of correlation matrices.(1968). University of North Carolina.
Gleser L J. On testing a set of correlation coefficients for equality: Some asymptotic results.(1968).Biometrika.
Gupta A K, Johnson B E, Nagar D K. Testing equality of several correlation matrices.(2012). Bowling Green State University.
Gupta A K, Nagar D K. Matrix Variate Distributions of Chapman Hall/CRC Monographs and Surveys in Pure and Applied Mathematics.(2000). Chapman Hall/CRC.
Gupta A K, Varga T. Elliptically Contoured Models in Statistics of Mathematics and its Applications.(1993). Kluwer Academic Publishers Group.
Jennrich R I. An asymptotic Ï‡2 test for the equality of two correlation matrices.(1970). Journal of the American Statistical Association.
Kaplan E L.Tensor notation and the sampling cumulants of k-statistics.(1952). Biometrika.
Kendall M G, Stuart A. The Advanced Theory of Statistics.(1969).Hafner Publishing.
Konishi S. An approximation to the distribution of the sample correlation coefficient.(1978). Biometrika.
Konishi S. Asymptotic expansions for the distributions of functions of a correlation matrix.(1979).Journal of Multivariate Analysis.
Konishi S. Asymptotic expansions for the distributions of statistics based on the sample correlation matrix in principal component analysis.(1979).Hiroshima Mathematical Journal.
Konishi S, Sugiyama T. Improved approximations to distributions of the largest and the smallest latent roots of a Wishart matrix.(1981). Annals of the Institute of Statistical Mathematics.
Kullback S. On testing correlation matrices.(1967). Applied Statistics.
Kullback S. Information Theory and Statistics.(1997). Dover Publications.
Modarres R. Testing the equality of dependent variances.(1993). Biometrical Journal.
Modarres R, Jernigan R W. Testing the equality of correlation matrices.(1992). Communications in Statistics - Theory and Methods.
Modarres R, Jernigan R W. A robust test for comparing correlation matrices.(1993). Journal of Statistical Computation and Simulation.
Muirhead R J. Aspects of Multivariate Statistical Theory.(1982). John Wiley Sons.
Schott J R. Testing the equality of correlation matrices when sample correlation matrices are dependent.(2007). Journal of Statistical Planning and Inference.
Siotani M, Hayakawa T, Fujikoshi Y. Modern Multivariate Statistical Analysis: A Graduate Course and Handbook.(1985). American Sciences Press Series in Mathematical and Management Sciences.
Waternaux C M. Principal components in the nonnormal case: The test of equality of q roots.(1984). Journal of Multivariate Analysis.