Testing Equality of Several Correlation Matrices. Prueba de igualdad de varias matrices de correlación
Bowling Green State University, Bowling Green, USA. Experient Research Group, Severna Park, USA. Universidad de Antioquia, Medellín, Colombia
Abstract
In this article we show that the Kullback’s statistic for testing equality of several correlation matrices may be considered a modified likelihood ratio statistic when sampling from multivariate normal populations. We derive the asymptotic null distribution of L∗ in series involving independent chisquare variables by expanding L∗ in terms of other random variables and then inverting the expansion term by term. An example is also given to exhibit the procedure to be used when testing the equality of correlation matrices using the statistic L∗ .
Key words: Asymptotic null distribution, Correlation matrix, Covariance matrix, Cumulants, Likelihood ratio test.
Resumen
En este artículo se muestra que el estadístico L∗ de Kullback, para probar la igualdad de varias matrices de correlación, puede ser considerado como un estadístico modificado del test de razón de verosimilitud cuando se muestrean poblaciones normales multivariadas. Derivamos la distribución asintótica nula de L∗ en series que involucran variables independientes chi-cuadrado, mediante la expansión de L∗ en términos de otras variables aleatorias y luego invertir la expansión término a término. Se da también un ejemplo para mostrar el procedimiento a ser usado cuando se prueba igualdad de matrices de correlación mediante el estadístico L∗ .
Palabras clave: distribución asintótica nula, matriz de correlación, matriz de covarianza, razón de verosimilitud.

1. Introduction
    The correlation matrix is one of the foundations of factor analysis and has
found its way into such diverse areas as economics, medicine, physical science
and political science. There is a fair amount of literature on testing properties
of correlation matrices. Tests for certain structures in a correlation matrix have
been proposed and studied by several authors, e.g, see Aitkin, Nelson, and Rein-
furt (1968), Gleser (1968), Aitkin (1969), Modarres (1993), Kullback (1997) and
Schott (2007). In a series of papers, Konishi (1978, 1979a, 1979b) has developed
asymptotic expansions of correlation matrix and applied them to various problems
of multivariate analysis. The exact distribution of the correlation matrix, when
sampling from a multivariate Gaussian population, is derived in Ali, Fraser and
Lee (1970) and Gupta and Nagar (2000).
   If the covariance matrix of α-th population is given by Σα and ∆α is a diagonal
matrix of standard deviations for the population α, then Pα = ∆−1         −1
                                                                  α Σα ∆α is the
correlation matrix for the population α. The null hypothesis that all k populations
have the same correlation matrices may be stated as H : P1 = · · · = Pk .
    Let the vectors xα1 , xα2 , . . . , xαNα be a random sample of size Nα = nα + 1
for α = 1, 2, . . . , k from k multivariate populations of dimensionality
                                                                   PNα       p. Further,
we assume the independence of these k samples. Let xα = i=1              xαi /Nα , Aα =
PNα                           0
   i=1 (xαi −xα )(xαi −xα ) and Sα = Aα /Nα . Further, let Dα be a diagonal matrix
of the square roots of the diagonal elements of Sα .PThe sample correlationPk matrix
                                                        k
Rα is then defined by Rα = Dα−1 Sα Dα−1 . Let n = α=1 nα and R = α=1 nα Rα .
                                                     Pk
    Kullback (1967) derived the statistic L∗ = α=1 nα ln{det(R)/ det(Rα )} for
testing the equality of k correlation matrices based on samples from multivariate
populations. This statistic was later examined by Jennrich (1970) who observed
that the statistic proposed by Kullback failed to have chi-square distribution as-
cribed to it. For further results on this topic the reader is referred to Browne (1978)
and Modarres and Jernigan (1992).
    Although the Kullback’s statistic L∗ is not equal to the modified likelihood ratio
criterion, we here show that it may be considered an approximation of the modified
likelihood ratio statistic when sampling from multivariate normal populations.
    In Section 2, we show that Kullback’s statistic can be viewed as an approxima-
tion of the modified likelihood ratio statistic based on samples from multivariate
normal populations. Section 3 deals with some preliminary results and definitions
which are used in subsequent sections. In sections 4 and 5, we obtain asymptotic
null distribution of L∗ by expanding L∗ in terms of other random variables and
then inverting the expansion term by term. Finally, in Section 6, an example
is given to demonstrate the procedure to be used when testing the equality of
correlation matrices using the statistic L∗ . Some results on matrix algebra and
distribution theory are given in the Appendix.




                                      Revista Colombiana de Estadística 36 (2013) 237–258

Testing Equality of Several Correlation Matrices                                               239

2. The Test Statistic
    In this section, we give an approximation of the likelihood ratio test statistic
λ for testing equality of correlation matrices of several multivariate Gaussian pop-
ulations. The test statistic λ was derived and studied by Cole (1968a, 1968b)
in two unpublished technical reports (see Browne 1978, Modarres and Jerni-
gan 1992, 1993). However, these reports are scarcely available, and therefore the
sake of completeness and for a better understanding it seems appropriate to first
give a concise step-by-step derivation of the test statistic λ.
   If the underlying populations follow multivariate normal distributions, then
the likelihood function based on the k independent samples, when all parameters
are unrestricted, is given by

     L(µ1 , . . . , µk , Σ1 , . . . , Σk )
           Y k h                             i−1
       =            (2π)pNα /2 det(Σα )Nα /2
            α=1
                    "  k                k
                                                                                           #
                    1X             1X
                          tr Σ−1           tr Σ−1                      0
                                             
            × exp −           α Aα −           α (x̄α − µα )(x̄α − µα )
                    2 α=1            2 α=1

where for α = 1, . . . , k we have µα ∈ Rp and Σα > 0. It is well known that for any
fixed value of Σα the likelihood function is maximized with respect to the µα ’s
when µb α = xα .
    Let ∆α be a diagonal matrix of standard deviations for the population α.
Further, let Pα = ∆−1      −1
                    α Σα ∆α be the population correlation matrix for the pop-
ulation α. The natural logarithm of the likelihood function, after evaluation at
b α = xα , may then be written as
µ

   ln[L(x̄1 , . . . , x̄k , ∆1 P1 ∆1 , . . . , ∆k Pk ∆k )]
                             k                           k
            1             1X                          1X
         = − N p ln(2π) −       Nα ln[det(Pα ∆2α )] −       tr(Nα Pα−1 Gα Rα Gα )
            2             2 α=1                       2 α=1

              Pk                       −1
where N =       α=1 Nα and Gα = ∆α Dα . Further, when the parameters are
unrestricted, the likelihood function L(x1 , . . . , xk, ∆1 P1 ∆1 , . . . , ∆k Pk ∆k ) is max-
imized when − ln[det(Pα ∆2α )] − tr Pα−1 Gα Rα Gα is maximized for each α. This
is true when

                  ln[det(Pα ∆2α )] + tr Pα−1 Gα Rα Gα
                                                             

                        = ln[det(∆α Pα ∆α )] + tr ∆−1 −1 −1
                                                                                 
                                                   α Pα ∆α Dα Rα Dα


is minimized for each α. This is achieved when ∆α Pα ∆α = Dα Rα Dα . From this
it follows that the maximum value of L(x1 , . . . , xk , ∆1 P1 ∆1 , . . . , ∆k Pk ∆k ), when
the parameters are unrestricted, is given by

                                               Revista Colombiana de Estadística 36 (2013) 237–258

240                                           Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar




                     ln[L(x̄1 , . . . , x̄k , D1 R1 D1 , . . . , Dk Rk Dk )]
                                                     k
                               1                  1X
                            = − N p[ln(2π) + 1] −       Nα ln[det(Rα Dα2 )].                      (1)
                               2                  2 α=1

Let P be the common value of the population correlation matrices under the null
hypothesis of equality of correlation matrices. The reduced parameter space for
the covariance matrices is the set of all covariance matrices that may be written
as ∆α P where P is a correlation matrix and ∆α is a diagonal matrix with positive
elements on the diagonal. The restricted log likelihood function is written as
      ln[L(x̄1 , . . . , x̄k , ∆1 P, . . . , ∆k P )]
                             k                          k
            1             1X                         1X
                                Nα ln[det(P ∆2α )] −      N tr P −1 Gα Rα Gα .
                                                                            
         = − N p ln(2π) −
            2             2 α=1                      2 α=1 α

      Let P −1 = (ρij ). Since ∆α is a diagonal matrix,
                                                   " p     #    p
                                                    Y           X
                         2
             ln[det(∆α ) ] = 2 ln[det(∆α )] = 2 ln     σαii = 2   ln(σαii )
                                                                   i=1         i=1

Also, since Gα = ∆−1
                  α Dα is a diagonal matrix, we have
                                                p X
                                                  p
                                              X
                             tr P −1 Gα Rα Gα =     ρij gαj rαij gαi
                                                              i=1 j=1

Thus,
             ln[L(x̄1 , . . . , x̄k , ∆1 P, . . . , ∆k P )]
                                     k      p                 k
                    1             1X       X               1X
                 = − N p ln(2π) −       Nα     ln(σαii ) −       Nα ln[det(P )]
                    2             2 α=1    i=1
                                                           2 α=1
                           k      p X p
                        1X       X
                    −         Nα         ρij gαj rαij gαi
                        2 α=1    i=1 j=1

Since, gαi = sαii /σαii , differentiation of ln[L(x1 , . . . , xk , ∆1 P, . . . , ∆k P )] with re-
spect to σαii yields
                                                                            p
       ∂                                                      Nα      Nα X
            ln[L(x1 , . . . , xk , ∆1 P, . . . , ∆k P )] = −       +           gαi gαj ρij rαij
      ∂σαii                                                  2σαii   2σαii j=1
                                                   Pp
Further, setting this equal to zero gives j=1 gαi gαj ρij rαij −1 = 0. Differentiating
ln[L(x1 , . . . , xk , ∆1 P, . . . , ∆k P )] with respect to the matrix P using Lemma 6, we
obtain
                                                                 k
  ∂                                                 1         1X
    ln[L(x1 , . . . , xk , ∆1 P, . . . , ∆k P )] = − N P −1 +       Nα P −1 Gα Rα Gα P −1
 ∂P                                                 2         2 α=1


                                                   Revista Colombiana de Estadística 36 (2013) 237–258

Testing Equality of Several Correlation Matrices                                           241

Setting this equal to zero,Pmultiplying by 2, pre and post
                                                       Pk multiplying by P and
                            k                                    2
dividing by N gives P = α=1 Nα Gα Rα Gα /N so that α=1 Nα gαi      /N = 1.
     The likelihood ratio test statistic λ for testing H : P1 = · · · = Pk is now derived
as
                                    k
                                    Y det(Rα D2 )Nα /2
                                                  α
                             λ=
                                            b b 2 Nα /2
                                    α=1 det(P ∆α )


where Pb and ∆ b 2 are solutions of Pb = Pk Nα ∆       b −1 /N and Pp ρij sαij −
                                               b −1 Sα ∆
                   α                      α=1    α       α          j=1
1 = 0, i = 1, . . . , p, respectively.
     To obtain an approximation of the likelihood ratio statistic we replace σαii
by its consistent estimator σ                                                    σαii and
                                      bαii . Then, it follows that gbαii = sαii /b
                                                                          Pk
Gα = diag(b
 b           gα1 , . . . , gbαp ), and the estimator of P is given by P = α=1 Nα G
                                                                      b               b α Rα
Gb α /N . Thus, an approximation of the maximum of ln[L(x1 , . . . , xk , ∆1 P, . . . , ∆k P )]
is given as
                                   k
             1                  1X              b α )2 ] − 1 N ln[det(Pb)]
            − N p[ln(2π) + 1] −       Nα ln[det(∆                                          (2)
             2                  2 α=1                      2

                                           σαii converges in probability to 1 so that
As the sample size goes to infinity, sαii /b
b α converges in probability to Ip . This suggest further approximation of (2) as
G
                            k
                                                      "                k
                                                                                      !#
      1                  1X                  2   1                     X Nα
     − N p[ln(2π) + 1] −       Nα ln[det(Dα ) ] − N ln det                       Rα        (3)
      2                  2 α=1                   2                     α=1
                                                                             N

Now, using (1) and (3), the likelihood ratio statistic is approximated as
                                      Qk            Nα /2
                                        α=1 det(Rα )
                             λ̃ =       Pk                                                 (4)
                                    det( α=1 Nα Rα /N )N/2

Further, replacing Nα by nα above, an approximated modified likelihood ratio
statistic is derived as
                          Qk            nα /2
                                               Qk
                            α=1 det(Rα )            det(Rα )nα /2
                 M=         Pk                = α=1                      (5)
                        det( α=1 nα Rα /n)n/2     det(R)n/2
                 Pk
Since −2 ln M = α=1 nα ln{det(R)/ det(Rα )} = L∗ , the statistic proposed by
Kullback may be thought of as an approximated modified likelihood ratio statistic.


3. Preliminaries
   Let the vectors xα1 , . . . , xαNα be a random sample of size nα for α = 1, . . . , k
from k multivariate populations of dimensionality p and finite fourth moments.
The characteristic function for the population α is given by φ∗α (t) = E[exp(ι t0 x)]

                                        Revista Colombiana de Estadística 36 (2013) 237–258

242                                      Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar

         √
where ι = −1 and t = (t1 , . . . , tp )0 . The log characteristic function for population
α may be written as
                                   ∞                                    p
                                   X                                    Y (ιtj )rj
               ln[φ∗α (t)] =                   κ∗α (r1 , . . . , rp )                ,   rj ∈ I +         (6)
                               r1 +···+rp =1                            j=1
                                                                              rj !

where I + is the set of non-negative integers. The cumulants of the distribution are
the coefficients κ∗α (r1 , . . . , rp ). If r1 + · · · + rp = m, then the associated cumulant
is of order m. The relationship between the cumulants of a distribution and the
characteristic function provide a convenient method for deriving the asymptotic
distribution of statistic whose asymptotic expectations can be derived.
     The cumulants of order m are functions of the moments of order m or lower.
Thus if the mth order moment is finite, so is the mth order cumulant. Let µi =
E(Xi ), µij = E(Xi Xj ), µijk = E(Xi Xj Xk ), and µijk` = E(Xi Xj Xk X` ) and κi ,
κij , κijk , and κijk` be the corresponding cumulants. Then, Kaplan (1952) gives
the following relationship:
             κi = µi ,
           κij = µij − µi µj ,
          κijk = µijk − (µi µjk + µj µik + µk µij ) + 2µi µj µk ,
                            4
                            X                   3
                                                X                        6
                                                                         X
         κijk` = µijk` −          µi µjk` −            µij µk` + 2            µi µj µk` − 6µi µj µk µ`

 where the summations are over the possible ways of grouping the subscripts, and
the number of terms resulting is written over the summation sign.
      Define the random matrix Vα as
                                                       
                              √       1 −1
                        V α = nα        ∆α Aα ∆−1
                                               α  − P α                                                   (7)
                                     nα
                                      (0)        (1)            (2)
Then, the random matrices Vα , Vα                      and Vα           are defined as
                                Vα(0) = diag(vα11 , vα22 , . . . , vαpp )                                 (8)
                                           1          1
                               Vα(1) = Vα − Vα(0) Pα − Pα Vα(0)                                           (9)
                                           2          2
and
             1 (0)       1          1          3              3
   Vα(2) =    V P V (0) − Vα Vα(0) − Vα(0) Vα + (Vα(0) )2 Pα + Pα (Vα(0) )2                              (10)
             4 α α α     2          2          8              8
Konishi (1979a, 1979b) has shown that
                                    1         1 (2)
                         Rα = Pα + √ Vα(1) +   V    + Op (n−3/2 )
                                    nα       nα α          α


The pooled estimate of the common correlation matrix is
                                                       k
                                                       X
                                            R=               ωα Rα
                                                       α=1


                                                Revista Colombiana de Estadística 36 (2013) 237–258

Testing Equality of Several Correlation Matrices                                                         243

so that
                               1 (1) 1 (2)
                    R=P +√ V          + V      + Op (n−3/2 )
                                n        n
                                 Pk              (1)       Pk √          (1)
where ωα = nα /n, P =              α=1 ωα Pα , V      =      α=1 ωα Vα       and
   (2)  Pk     (2)                                   √       −1     −1
                                                                               
V      = α=1 Vα . The limiting distribution of Vα = nα ∆α Aα ∆α /nα − Pα
is normal with means 0 and covariances that depend on the fourth order cumulants
of the parent population (Anderson 2003, p. 88).
   Since ∆α is a diagonal matrix of population standard deviations, ∆−1
                                                                     α xα1 , . . . ,
∆−1
 α xαNα    may be thought of as Nα observations from a population with finite
fourth order cumulants and characteristic function given by
                                 ∞                                   p
                                 X                                   Y (ιtj )rj
              ln[φα (t)] =                   κα (r1 , . . . , rp )                  , rj ∈ I +           (11)
                             r1 +···+rp =1                           j=1
                                                                           rj !

where the standardized cumulants, κα (r1 , r2 , . . . , rp ), are derived from the expres-
sion (6) as
                                                  κ∗α (r1 , r2 , . . . , rp )
                κα (r1 , r2 , . . . , rp ) =
                                             σα11 χr1 σα22 χr2 · · · σαpp χrp

with χrj = 1 if rj = 0, χrj = 1/σ (α)jj if rj 6= 0 and Σ−1
                                                        α = (σ
                                                               (α)jj
                                                                     ).
    K-statistics are unbiased estimates of the cumulants of a distribution, and
may be used to derive the moments of the statistics which are symmetric func-
tions of the observations (Kendall and Stuart 1969). Kaplan (1952) gives a series
of tensor formulaes for computing the expectations of various functions of the
k-statistics associated with a sample of size N from a multivariate population.
For the definition of the k-statistics, let N (r) = N (N − 1) · · · (N − r + 1).
   If si1 i2 ···i` denotes the product Xi1 Xi2 · · · Xi` summed over the sample, the
tensor formulae for the k-statistics may be shown to be as follows:
                                                                              3
                                                              N 2 sijk − N
                                                                              P
          si             N sij − si sj                                              si sjk + 2si sj sk
      ki = ,       kij =               ,         kijk =
          N                 N (2)                                                 N (3)
                                 4                  3            6
                                   si sjk` ) − N (2) sij sk` + 2N si sj sk` − 6si sj sk s`
                                 P                  P            P
          N (N + 1)(N sijk` −
kijk` =
                                                 N (4)

                         κ(ab, ij) = E[(kab − κab )(kij − κij )]
                                     κabij   κai κbj + κaj κbi
                                   =       +
                                      N           N −1


  κ(ab, ij, pq) = E[(kab − κab )(kij − κij )(kpq − κpq )]
                               12                         4
                                         X (N − 2)κaip κbjq X κai κbp κjq                   8
                   κabijpq X κabip κjq
               =       2
                          +            +                   +
                    N       N (N − 1)        N (N − 1)2       (N − 1)2


                                             Revista Colombiana de Estadística 36 (2013) 237–258

244                                  Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar


The summations are over the possible ways of grouping the subscripts, and the
number of terms resulting is written over the summation sign.
   The matrix Vα is constructed from observations from the standardized distri-
                        √
bution so that vαij = nα (kαij − ραij ) where kαij is the related k-statistic for
standardized population α. Kaplan’s formulae may be applied to derive the fol-
lowing expressions for the expectations of elements of the matrices Vα (note that
καij = ραij ). We obtain
                                   E(vαij ) = 0
                  E(vαij vαk` ) = καijk` + ραik ραj` + ραi` ραjk + O(n−1
                                                                      α )

and

                                              12               4
                              1               X                 X
        E(vαij vαk` vαab ) = √     καijk`ab +     καijka ρα`b +   καika καj`b
                               nα
                              X 8               
                            +     ραik ραja ρα`b + O(n−3/2
                                                        α    )

                            (0)    (1)           (2)
The random matrices Vα , Vα and Vα are defined in (8), (9), and (10), respec-
tively. The expectations associated with these random matrices are given as
                                                 (1)
                                             E(vαij ) = 0


            (2)      1              1                    3
        E(vαij ) =     ραij καiijj − (καiiij + καijjj ) + ραij (καiiii + καjjjj )
                     4              2                    8
                        1 3                  −1
                     + (ραij − ραij ) + O(nα )
                        2

     (1) (1)                1
  E(vαij vαk` ) = καijk` − (ραij καiik` + ραij καjjk` + ραk` καijkk + ραk` καij`` )
                            2
                    1
                  + ραij ραk` (καiikk + καii`` + καjjkk + καjj`` )
                    4
                  − (ραk` ραik ραjk + ραk` ραi` ραj` + ραij ραik ραi` + ραij ραjk ραj` )
                    1
                  + ραij ραk` (ρ2αik + ρ2αi` + ρ2αjk + ρ2αi` )
                    2
                  + (ραik ραj` + ραi` ραjk ) + O(n−1
                                                   α )                               (12)

and
                                                                        
           (1) (1) (1)        1                   1     1     1
        E(vαij vαk` vαab ) = √               tα1 − tα2 + tα3 − tα4           + O(n−3/2
                                                                                  α    )
                               nα                 2     4     8

where
                            12
                            X                       4
                                                    X                   8
                                                                        X
        tα1 = καijk`ab +          καijka κα`b +         καika καi`b +       ραik ραja ρα`b


                                              Revista Colombiana de Estadística 36 (2013) 237–258

Testing Equality of Several Correlation Matrices                                                       245

              3
              X                               12
                                               X
      tα2 =          ραij καiik`ab + καjjk`a +    (καiika + καjjka )
                  3
                  X                                          8
                                                             X                                     
              +       (καika καi`b + καjka καj`b ) +             (ραik ραia ρα`b + ραjk ραja ρα`b )


              3
              X               
      tα3 =          ραij ραk` καiikkab + καii``ab + καjjkkab + καjj``ab
                  12
                  X
              +      (καiika ραkb + καii`a ρα`b + καjjka ραkb + καjj`a ρα`b )
                  3
                  X
              +     (καika καikb + καi`a καi`b + καjka καjkb + καj`a καj`b )
                8
                X                                                                     
              +   (ραik ραia ραbk + ραi` ραia ρα`b + ραjk ραja ραbk + ραj` ραja ρα`b )

and
                             8 
                             X             12
                                           X                   3
                                                               X
        tα4 = ραij ραk` ραab    καiikkaa +    (καiika ραka ) +   (καika καikb )
                      8
                      X                       
                  +       (ραik ραia ραka )

                                                       (1)
Lemma 1. The diagonal elements of Vα                         are zero.
                                                     (0)
Proof . Using (9) and the fact that Vα                     is a diagonal matrix, we have

                                   (1)         1
                                  vαij = vαij − ραij (vαii + vαjj )
                                               2
The result follows by taking j = i above and noting that diagonal elements of Pα
are 1.
                                                       (2)
Lemma 2. The diagonal elements of Vα                         are zero.
                                                      (0)
Proof . Using (10) and the fact that Vα                      is a diagonal matrix, we get

               (2)      1 (0)        (0)  1                     3       2      2
              vαij =      vαii ραij vαjj − vαij (vαjj + vαii ) + ραij (vαii + vαjj )
                        4                 2                     8
The result follows by substituting j = i above and observing that ραii = 1.


4. Asymptotic Expansion of L∗
   In order to derive the asymptotic distribution for L∗ the statistic is first ex-
panded in terms of other random variables (see Konishi and Sugiyama 1981).

                                                  Revista Colombiana de Estadística 36 (2013) 237–258

246                                            Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar


    The statistic L∗ may be written as L∗ = ng(R1 . . . , Rk ) where g(R1 , . . . , Rk ) =
             Pk
ln[det(R)] − α=1 ωα ln[det(Rα )]. Let
                                      1              1 −1 (2)
                                Bα = √ Pα−1 Vα(1) +   P V
                                      nα            nα α α
                 (1)           (2)
Since, Pα , Vα and Vα are all positive definite, so is Bα . This insures that the
eigenvalues of Bα exist and are positive. Also, as nα becomes large, the elements
in Bα become small so that the characteristic roots may be assumed to be less
than one. Using Lemma 5,

             ln[det(Rα )] = ln[det(Pα + Pα Bα )] + Op (n−3/2
                                                        α    )
                                                    1
                          = ln[det(Pα )] + tr(Bα ) − tr(Bα Bα ) + Op (n−3/2
                                                                       α    )
                                                    2
                                     (1)        (1)        −3/2
Now, Bα Bα = n−1 −1    −1
              α Pα Vα Pα Vα + Op (nα                              ) so that
                                           1                     1
            ln[det(Rα )] = ln[det(Pα )] + √ tr(Pα−1 Vα(1) ) +       tr(Pα−1 Vα(2) )
                                           nα                   nα
                               1                         
                           −      tr Pα−1 Vα(1) Pα−1 Vα(1) + Op (n−3/2
                                                                   α   )
                              2nα
A similar expansion for ln[det(R)] may be obtained by defining B by
                               k                  k
                           1 X√       −1       1 X −1 (2)
                        B=√       ωα P Vα(1) +       P Vα
                            n α=1              n α=1

Then

           ln[det(R)] = ln[det(P + P B)] + Op (n−3/2 )
                                              1
                      = ln[det(P )] + tr(B) − tr(BB) + Op (n−3/2 )
                                              2
                  k     k   √          −1 (1) −1 (1)
Since BB = n−1 α=1 β=1 ωα ωβ P Vα P Vβ + Op (n−3/2 ),
                P     P

                                       k                       k
                                   1 X√          −1         1X        −1
       ln[det(R)] = ln[det(P )] + √       ωα tr(P Vα(1) ) +       tr(P Vα(2) )
                                    n α=1                   n α=1
                                 k         k
                            1 XX√           −1      −1 (1)
                       −          ωα ωβ tr(P Vα(1) P Vβ ) + Op (n−3/2 )
                           2n α=1
                                       β=1

Combining these expressions yields
                                                k                             k
                                                X                      1 X√
      g(R1 , . . . , Rk ) = ln[det(P )] −           ωα ln[det(Pα )] + √       ωα tr(Hα Vα(1) )
                                                α=1
                                                                        n α=1
                                  k                     k
                               1X                    1 X ωα
                           +         tr(Hα Vα(2) ) +          tr(Pα−1 Vα(1) Pα−1 Vα(1) )
                               n α=1                 2 α=1 nα


                                                  Revista Colombiana de Estadística 36 (2013) 237–258

Testing Equality of Several Correlation Matrices                                                                247

                                      k       k
                                1 XX√           −1      −1 (1)
                           −          ωα ωβ tr(P Vα(1) P Vβ ) + Op (n−3/2 )
                               2n α=1
                                          β=1

                                 −1−1      0
                                                                                            √
where Hα = (hαij ) = (P −P        √α ) = Hα . Let√G(R1 , . . . , Rk ) =                         n[g(R1 , . . . , Rk )
−g(P1 , . . . , Pk )]. Then, since n(ωα /nα ) = ( n)−1 , we obtain

                           k                                        k
                           X √                        1 X
   G(R1 , . . . , Rk ) =          ωα tr(Hα Vα(1) ) + √       tr(Hα Vα(2) )
                           α=1
                                                       n α=1
                                 k
                             1 X
                           + √      tr(Pα−1 Vα(1) Pα−1 Vα(1) )
                            2 n α=1
                                 k  k
                             1 XX√              −1      −1 (1)
                           − √        ωα ωβ tr(P Vα(1) P Vβ ) + Op (n−1 )                                      (13)
                            2 n α=1
                                              β=1


Theorem 1. The expression G(R1 , . . . , Rk ) may be written as

                            k X                                         k
                            X   √                       (1)    1 XX           (2)
    G(R1 , . . . , Rk ) =                     ωα h̄αij vαij + √          h̄ v
                            α=1 i<j
                                                                n α=1 i<j αij αij
                                          k
                              1 XXX                  (1) (1)
                            +√           qα (ij, k`)vαij vαk`
                               n α=1 i<j
                                                    k<`
                                         k XX
                                       k X
                              1        X                    √                        (1) (1)
                            −√                                  ωα ωβ q(ij, k`)vαij vβk` + Op (n−1 )
                               n α=1
                                              β=1 i<j k<`

                                 −1
where Pα−1 = (ρij   α ), P      = (ρij ), hαij = 2(ρij − ρij                  i` jk
                                                          α ), qα (ij, k`) = ρα ρα +
                           i` jk     ik j`
ρik
 α αρ j`
         and q(ij, k`) = ρ   ρ   + ρ   ρ   .

Proof . Using results on matrix algebra, we have
                     k                                    k             p
                                                                      p X
                     X √                                  X √         X                   (1)
                               ωα tr(Hα Vα(1) ) =                ωα             hαji vαij
                     α=1                                  α=1         i=1 j=1


and since Hα is symmetric, application of Lemma 3 yields
   k                              k                                       k
   X √                            X √               X               (1)
                                                                          X √ X       (1)
          ωα tr(Hα Vα(1) ) =                  ωα     (hαji + hαij )vαij =    ωα hαij vαij
   α=1                            α=1               i<j                             α=1          i<j


In an entirely similar manner,
                                 k                         k X
                                                                              (2)
                                 X                         X
                                       tr(Hα Vα(2) ) =                  hαij vαij
                                 α=1                       α=1 i<j


                                                    Revista Colombiana de Estadística 36 (2013) 237–258

248                                            Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar

                                                                                         (1)
Using Lemma 4, results on matrix algebra and the symmetry of Vα , we have
               k
           1X
                 tr(Pα−1 Vα(1) Pα−1 Vα(1) )
           2 α=1
                      k       p       p    p     p
                   1 X X X X X i` jk (1) (1)
              =                  ρα ρα vαij vαk`
                   2 α=1 i=1 j=1
                                          k=1 `=1
                     k XX
                   1 X                                                                (1) (1)
              =                           (ρi` jk   j` ik   ik j`   i` jk
                                            α ρα + ρα ρα + ρα ρα + ρα ρα )vαij vαk`
                   2 α=1 i<j
                                  k<`
                   k XX
                                                       (1) (1)
                   X
              =                        qα (ij, k`)vαij vαk`
                   α=1 i<j k<`

In a similar manner,
                          k        k
                      1 XX√           −1      −1 (1)
                            ωα ωβ tr(P Vα(1) P Vβ )
                      2 α=1
                                  β=1
                                    k XX
                                  k X
                                  X      √                                 (1) (1)
                          =                                ωα ωβ q(ij, k`)vαij vβk`
                              α=1 β=1 i<j k<`

Combining these expansions in (13) completes the proof.
Corollary 1. In the special case p = 2, G(R1 , . . . , Rk ) may be written as
                         k                           
                        X   √        ρα         ρ         (1)
G(R1 , . . . , Rk ) = 2       ωα         2
                                           −        2   vα12
                        α=1
                                   1 − ρ α   1 −  ρ
                              k                                      k
                                                                 1 X 1 + ρ2α
                                                     
                          2 X        ρα        ρ          (2)                        (1)
                      +√                   −            v α12 + √                  (v )2
                           n α=1 1 − ρ2α     1 − ρ2               n α=1 (1 − ρ2α )2 α12
                                  k        k
                      1 X X 1 + ρ2 (1) (1)
                    −√                 v v      + Op (n−1 ).
                       n α=1 (1 − ρ2 )2 α12 β12
                                          β=1
                          P
Proof . For p = 2,         i<j consists of single term corresponding to i = 1, j = 2.
                                                                                −1
Also, Pα = ρα 1 so that Pα−1 = (1 − ρ2α )−1 −ρ1 α −ρ1 α . Similarly, P
                 1 ρα                                           
                                                                                    =
                    
               1  −ρ
(1 − ρ2 )−1 −ρ 1 . Thus, the off diagonal element of Hα is given by ρα (1 −
ρ2α )−1 − ρ(1 − ρ2 )−1 . Further, qα (12, 12) = ρ12   21   11 22         2        2 2
                                                   α ρα + ρα ρα = (1 + ρα )/(1 − ρα )
                         2         2 2
and q(12, 12) = (1 + ρ )/(1 − ρ ) . The result follows by using these values in the
theorem.


5. Asymptotic Null Distribution of L∗
    In this section we derive asymptotic distribution of the statistic L∗ when the
null hypothesis is true.

                                                     Revista Colombiana de Estadística 36 (2013) 237–258

Testing Equality of Several Correlation Matrices                                                            249

   Define the k × k matrix W as W = (wij ) where wii = 1 − ωi and for i 6= j,
        √
wij = − ωi ωj = wji , 1 ≤ i, j, ≤ k. The matrix W has rank k − 1 and each of its
non-zero eigenvalues is equal to 1.
Theorem 2. Let the k correlation matrices R1 , . . . , Rk be based on independent
samples of sizes N1 , . . . , Nk , respectively, with finite fourth order cumulants. De-
fine the kp(p − 1)/2 × 1 vector v(1) by
           (1)      (1)         (1)        (1)     (1)           (1)             (1)    (1)          (1)
v (1) = (v1,1,2 ,v1,1,3 ,. . .,v1,p−1,p ,v2,1,2 ,v2,1,3 ,. . .,v2,p−1,p ,. . .,vk,1,2 ,vk,1,3 ,. . .,vk,p−1,p )0
          (1)
where Vα is as defined in (9). Let Q = (q(ij, k`)) be the p(p − 1)/2 × p(p − 1)/2
matrix of coefficients defined in Theorem 1.
                                                                        (1)
   Let Tα be the asymptotic dispersion matrix of Vα with entry (ij, k`) equal to
   (1) (1)
E(vαij vαk` ) given in (12). Then, the asymptotic dispersion matrix of v(1) is

                                                               ···
                                           
                                             T1          0             0
                                                                         
                                            0           T2    ···     0 
                                      T∗ =  .
                                                                        
                                                          ..
                                            ..
                                                                         
                                                           .             
                                             0           0     ···     Tk
Under the null hypothesis
                                       p(p−1)(k−1)/2
                                             X
                               L∗ =                       λi yi + Op (n−1/2 )
                                             i=1

where y1 , . . . , yp(p−1)(k−1)/2 are independent, yi ∼ χ21 , 1 ≤ i ≤ p(p−1)(k−1)/2 and
λ1 , . . . , λp(p−1)(k−1)/2 are the eigenvalues of T ∗ (Q ⊗ W ). If the standardized fourth
order cumulants of the populations are all equal, then Tα = T for α = 1, . . . , k and
                                          p(p−1)/2
                                             X
                                  L∗ =               θi ui + Op (n−1/2 ),
                                             i=1

where u1 , . . . , up(p−1)/2 are independent, ui ∼ χ2k−1 and θ1 , . . . , θp(p−1)/2 are the
eigenvalues of T Q.

Proof . Under the null hypothesis we have Pα = P for α = 1, . . . , k so that
g(P1 , . . . , Pk ) = 0, hαij = 0 and qα (ij, k`) = q(ij, k`) = ρi` ρjk + ρik ρj` for all α.
                                          Pk
Since g(R1 , . . . , Rk ) = ln[det(R)] − α=1 ωα ln[det(Rα )] = n−1 L∗ , using Theo-
rem 1, one obtains
                L∗ = ng(R1 , . . . , Rk ) = n[g(R1 , . . . , Rk ) − g(P1 , . . . , Pk )]
                        k XX
                                                   (1) (1)
                        X
                    =                  q(ij, k`)vαij vαk`
                        α=1 i<j k<`
                            k XX
                          k X
                          X      √                                     (1) (1)
                      −                          ωα ωβ q(ij, k`)vαij vβk` + Op (n−1/2 )
                          α=1 β=1 i<j k<`


                                                 Revista Colombiana de Estadística 36 (2013) 237–258

250                                    Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar

                    k X
                      k
                                                        (1) (1)
                    X                XX
                =              wαβ             q(ij, k`)vαij vβk` + Op (n−1/2 )
                    α=1 β=1          i<j k<`
                       (1) 0
                = (v      ) (Q ⊗ W )v (1) + Op (n−1/2 )

 Since Q is of rank p(p − 1)/2 and W is of rank k − 1, the matrix Q ⊗ W is of
                                                                          (1)
rank p(p − 1)(k − 1)/2. From (8) and (9) it is clear that elements of Vα are
linear functions of elements of Vα and the limiting distribution of Vα is normal
with means 0 and covariances that depend on the fourth order cumulants of the
parent population. Therefore, v (1) is asymptotically normal with means zero and
                                   Pp(p−1)(k−1)/2
dispersion matrix T ∗ . Thus, L∗ = i=1            λi yi + Op (n−1/2 ).
    If the standardized fourth order cumulants are the same for each underlying
population, then T ∗ = T ⊗ I. Further, (T ⊗ I)(Q ⊗ W ) = T Q ⊗ W has as its
eigenvalues θi j , i = 1, . . . , p(p − 1)/2, j = 1, . . . , k where θi are the eigenvalues
of T Q and j are the eigenvalues of W . Since there are p(p − 1)/2 non-zero
eigenvalues of (T ⊗ I)(Q ⊗ W ) each occurring with multiplicity k − 1, we have
       Pp(p−1)/2
L∗ = i=1           θi ui + Op (n−1/2 ).
Corollary 2. Let the k sample correlation coefficients r1 , r2 , . . . , rk be based on
independent samples of sizes N1 , N2 , . . . , Nk from bivariate populations with finite
fourth order cumulants. Let ρ be the hypothesized common correlation coefficient.
Define the k × 1 vector v (1) by
                                                 (1)       (1)
                                     v (1) = (v1 , . . . , vk )0
        (1)
where vα = vα12 − ρ(vα11 + vα22 ) as defined in (9). Let
                                                  
            2 2 1 2                            1 2
 tα = (1 − ρ ) + ρ (κα1111 + κα2222 ) + 1 + ρ κα1122 − ρ(κα1113 + κα1222 )
                4                              2
and define T ∗ = diag(t1 , . . . , tk ).Under the null hypothesis the statistic L∗ is
asymptotically expanded as
                                             k−1
                                   1 + ρ2 X
                           L∗ =                  λi yi + Op (n−1/2 )
                                  (1 − ρ2 )2 i=1

where y1 , . . . , yk−1 are independent, yi ∼ χ21 and λ1 , . . . , λ,k−1 are the eigenvalues
of T ∗ W . If the standardized fourth order cumulants are equal, then
                                                         
                    2 2   1 2                        1 2
    tα = (1 − ρ ) + ρ (κ1111 + κ2222 ) + 1 + ρ κ1122 − ρ(κ1113 + κ1222 )
                          4                          2
for α = 1, 2, . . . , k and
                                                                    
                 ∗            2 2   1 2                           1 2
              L = (1 − ρ ) + ρ (κ1111 + κ2222 ) + 1 + ρ κ1122
                                    4                             2
                                              1 + ρ2 2
                                           
                       − ρ(κ1113 + κ1222 )             χ    + Op (n−1/2 )
                                             (1 − ρ2 )2 k−1


                                           Revista Colombiana de Estadística 36 (2013) 237–258

Testing Equality of Several Correlation Matrices                                     251

Proof . As shown in Corollary 1, when p = 2, Q is a scalar. If ρ is the common
correlation coefficient, then Q = (1 + ρ2 )/(1 − ρ2 )2 . The asymptotic variance of
 (1)
vα12 is given in (12). Upon simplification,
                                                                      
        (1) (1)                 2 2  1 2                           1 2
     E(vα12 vα12 ) = tα = (1 − ρ ) + ρ (κα1111 + κα2222 ) + 1 + ρ κα1122
                                     4                             2
                          − ρ(κα1113 + κα1222 ) + Op (n−1/2 )                       (14)
so that T ∗ is the asymptotic covariance matrix of v(1) . Further, T ∗ (Q ⊗ W ) =
                                                                Pk−1
[(1 + ρ2 )/(1 − ρ2 )2 ]T ∗ W . Thus L∗ = [(1 + ρ2 )/(1 − ρ2 )2 ] i=1 λi yi + Op (n−1/2 ),
where λi are the eigenvalues of T ∗ W . If the standardized fourth order cumulants
are identical, T = tI, so that there is one eigenvalue of T Q with multiplicity k.
This eigenvalue is merely t(1 + ρ2 )/(1 − ρ2 )2 and the result follows immediately
from Theorem 2.
Corollary 3. Let the k sample correlation coefficients r1 , r2 , . . . , rk be based on
independent samples of sizes N1 , N2 , . . . , Nk from bivariate populations which are
elliptically contoured with a common curtosis of 3κ and common correlation coef-
ficient ρ. Then
                                           1 + ρ2 2
             L∗ = (1 − ρ2 )2 + (1 + 2ρ2 )κ                + Op (n−1/2 )
                 
                                                     χ
                                           (1 − ρ2 )2 k−1

Proof . For elliptically contoured distributions (Muirhead 1982, Anderson 2003,
Gupta and Varga 1993) the fourth order cumulants are such that κiiii = 3κiijj =
3κ for i 6= j and all other cumulants are zero (Waternaux 1984). Substituting this
into the expression for t in Corollary 2 yields t = (1 − ρ2 )2 + (1 + 2ρ2 )κ. The result
then follows from Corollary 2.
Corollary 4. Let the k sample correlation coefficients r1 , . . . , rk be based on in-
dependent samples of sizes N1 , . . . , Nk from bivariate normal populations with a
common correlation coefficient ρ. Then
                           L∗ = (1 + ρ2 )χ2k−1 + Op (n−1/2 )

Proof . Normal distributions are special case of elliptically contoured distribu-
tions. The fourth order cumulants are all zero (Anderson 2003). The result follows
by setting κ = 0 in Corollary 3.


6. An Example
   This example is included to demonstrate the procedure to be used when testing
the equality of correlation matrices by using the statistic L∗ . The data represent
random samples from three trivariate populations each with identical correlation
matrix P given by
                                                   
                                   1.0 0.3      0.2
                             P = 0.3 1.0 −0.3
                                     0.2   −0.3     1.0

                                      Revista Colombiana de Estadística 36 (2013) 237–258

252                              Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar


Since the statistic L∗ is an approximation of the modified likelihood ratio statistic
for samples from multivariate normal populations, it is particularly suited to pop-
ulations that are near normal. The contaminated normal model has been chosen
to represent such a distribution.
    Samples of size 25 from contaminated normal populations with mixing param-
eter  = 0.1 and σ = 2 were generated using the SAS system. These data are
tabulated in Gupta, Johnson and Nagar (2012). The density of a contaminated
normal model is given by

          φ (x, σ, Σ) = (1 − )φ(x, Σ) + φ(x, σΣ),   σ > 0,    0<<1

where φ(x, Σ) is the density of a multivariate normal distribution with zero mean
vector and covariance matrix Σ.
    If the data were known to be from three normal populations all that would
be required at this point would be the sample sizes and the matrix of corrected
sums of squares and cross products. A key element, however, of the modified
likelihood ratio procedure is that this assumption need not be made, but the
fourth order cumulant must be estimated. To do this the k-statistics are calculated
using Kaplan’s formulae summarized in Section 3. The computations are made
considerably easier by standardizing the data so that all of the first order sums
are zero.
    The computation using original (or standardized) data yields the following
estimates of the individual correlation matrices:
                                                 
                    1.0000     0.5105    0.3193
          R1 =  0.5105        1.0000 −0.3485  , det(R1 ) = 0.4024
                    0.3193 −0.3485       1.0000
                                                 
                    1.0000     0.1758    0.2714
          R2 =  0.1758        1.0000 −0.2688  , det(R2 ) = 0.7975
                    0.2714 −0.2688       1.0000
                                                 
                    1.0000     0.2457    0.3176
          R3 =  0.2457        1.0000 −0.0331  , det(R3 ) = 0.8325
                    0.3176 −0.0331       1.0000

Since each sample is of size 25, ωi = 1/3 for i = 1, 2, 3 and the pooled correlation
matrix is merely the average of these three matrices:
                                               
                  1.0000      0.3107     0.3028
            R =  0.3107      1.0000    −0.2168  ,     det(R) = 0.7240
                  0.3028     −0.2168     1.0000

The value of the test statistic is now easily calculated as

          L∗ = 72 ln(0.7240) − 24[ln(0.4024) + ln(00.7975) + ln(0.8325)]
             = 8.7473


                                    Revista Colombiana de Estadística 36 (2013) 237–258

Testing Equality of Several Correlation Matrices                                       253

The null hypothesis is to be rejected if the value of the test statistic is too large.
The next step of the procedure is to estimate the coefficients in the linear combi-
nation of chi-square variables that make up the actual distribution under the null
hypothesis. The most arduous part is the computation of the estimates of fourth
order cumulants.
   Since the data are standardized, the formula for the k-statistic for the four way
product xi × xj × xk × x` simplifies to

                   1
        kijk` =         [N 2 (N + 1)sijk` − N (N − 1)(sij sk` + sik sj` + si` sjk )]
                  N (4)

where N (4) = N (N − 1)(N − 2)(N − 3). Using this to estimate the cumulant
corresponding to x21 x22 yields k1122 = 0.6670. The computation for other fourth
order cumulant are performed similarly. The resulting estimates are then pooled
across population to yield an estimate of the common fourth order cumulants
used in building the tau matrix (it is possible, of course, to drop the assumption
of common fourth order cumulants and use the nine by nine matrix that would
result if each separate tau matrix were joined in a block diagonal matrix). The
estimates of the fourth order cumulants are summarized in the Table 1.
    The pooled correlation matrix and these estimates are now used to build the
                                                                (1) (1)
estimated covariance matrix V (1) . The entry corresponding to vij vk` is given by

                       1
               kijk` − (rij kiik` + rij kjjk` + rk` kijkk + rk` kij`` )
                       2
                       1
                     + rij rk` (kiikk + kii`` + kjjkk + kjj`` )
                       4
                     − (rk` rik rjk + rk` ri` rj` + rij rik ri` + rij rjk rj` )
                       1           2    2        2      2
                     + rij rk` (rik  + ri`  + rjk  + rj`  ) + rik rj` + ri` rjk
                       2
where rij is the pooled estimate of the correlation value and kijk` is the correspond-
ing pooled fourth order cumulant. The entry corresponding to 12, 13 is given by
t12,13 = −0.3065. Similar calculations yield the following covariance matrix corre-
                (1)  (1)    (1)
sponding to (vα12 , vα13 , vα23 )0 ,

                                              −0.3065 0.1800
                                                            
                               1.0150
                        T =  −0.3065          0.7242 0.3974 
                               0.1800          0.3974 0.8179

    To complete the example, the inverse of the pooled correlation matrix is used
to estimate the matrix Q. The entry corresponding to the element ij, k` is given
by rik rj` + ri` rjk where R−1 = (rij ). These matrices are as follows:

                                              −0.5198      −0.5113
                                                                  
                             1.3163
                       −1
                      R =  −0.5198            1.2546       0.4294 
                            −0.5113            0.4294       1.2479

                                        Revista Colombiana de Estadística 36 (2013) 237–258

254                                 Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar


                        Table 1: Estimated fourth order cumulants
            Variables     Population 1   Population 2    Population 3   Pooled
                1111            0.9077         0.1181          0.9355    0.6538
                1112            0.7765        -0.0387         -0.0565    0.2271
                1113           -0.3015         0.7008          0.0677    0.1105
                1122            0.6670         0.3595         -0.3663    0.2201
                1123           -0.3917         0.3519         -0.1333   -0.0574
                1133           -0.1848         0.6608         -0.7475   -0.0905
                1222            0.4896        -0.7128         -0.0178   -0.0803
                1223           -0.3005         0.1637         -0.2243   -0.1204
                1233           -0.0980         0.6343         -0.1394    0.1323
                1333           -0.3430         0.3973         -0.0773   -0.0077
                2222           -0.0787        -0.9989          0.8134   -0.0881
                2223           -0.2543         0.0750          0.1887    0.0032
                2233            0.3800        -0.1764         -0.5454   -0.1139
                2333           -0.8386         0.8496          0.2869    0.0993
                3333            0.9130        -0.9196          1.3068    0.4334


                                                        −0.8647
                                                               
                               1.9217 0.8310
                         Q=   0.8310 1.9041            −0.8682 
                              −0.8647 0.8682             1.7500
Most eigenvalues extraction routines require that the matrix being analyzed be
symmetric. Let A be the Cholesky decomposition of Q, that is Q = A0 A where
A is an upper triangular matrix. Then the eigenvalues of T Q are the same as the
eigenvalues of AT A0 which is clearly symmetric. In this case

                               1.3863 0.5995 −0.6237
                                                        

                      A=           0 1.2429 −0.3977 
                                    0      0     1.0967

                                               −0.2877    −0.0246
                                                                 
                              1.4111
                   AT A0 =  −0.2877            0.8552     0.1849 
                             −0.0246            0.1849     0.9837
and the eigenvalue of this matrix are 1.55, 1.0473 and 0.6527. Using Theorem 2,
the distribution of the statistic is estimated to be that of Y = (1.55)χ22 +(1.0473)χ22
+(0.6527)χ22 where each of the chi-square variate is independent. Using Lemma 7
the cumulative probability value associated with 8.7473 is obtained as 0.7665 so
that the observed significance level is 0.2335. Thus, if the test is performed at
the α = 0.1 level of significance the conclusion reached is that there is insufficient
evidence to reject the null hypothesis that the samples are from populations with
identical correlation matrices.


Acknowledgements
    The authors wish to thank three anonymous referees for their careful reading
of the manuscript and their fruitful comments and suggestions.


                                         Revista Colombiana de Estadística 36 (2013) 237–258

Testing Equality of Several Correlation Matrices                                    255
                                                                  
                 Recibido: junio de 2012 — Aceptado: julio de 2013




Appendix
Lemma 3. Let V = (vij ) be a p × p symmetric matrix with zero on the diagonal
and let C = (cij ) be a p × p symmetric matrix. Then
                                   p X
                                   X p                     X
                       tr(CV ) =             cij vij = 2         cij vij
                                   i=1 j=1                 i<j


Proof . The proof is obtained by noting that vjj = 0 and cij = cji .

Lemma 4. Let Vα = (vαij ) and Vβ = (vβij ) be p × p symmetric matrices with
zero on the diagonal. Then
   p X
 p X
 X     p
     p X                               XX
                   cijk` vαij vβk` =             (cijk` + cij`k + cjik` + cji`k )vαij vβk` .
 i=1 j=1 k=1 `=1                       i<j k<`


Proof . Using Lemma 3, the sum may be written as
                           p X
                         p X
                         X
                                        (cijk` + cij`k )vαij vβk`
                          i=1 j=1 k<`

The proof is obtained by applying Lemma 3 second time.

Lemma 5. Let A be a real symmetric matrix with eigenvalues that are less than
one in absolute value, then
                                                  1          1
             − ln[det(I − A)] = tr(A) +             tr(A2 ) + tr(A3 ) + · · ·
                                                  2          3
Proof . See Siotani, Hayakawa and Fujikoshi (1985).

Lemma 6. Let R be a correlation of dimension p. Then
                                  ∂
                                    ln[det R] = R−1
                                 ∂P
and
                              ∂
                                tr(R−1 B) = R−1 BR−1
                             ∂P
where B is a symmetric non-singular matrix of order p.

Proof . See Siotani, Hayakawa and Fujikoshi (1985).


                                          Revista Colombiana de Estadística 36 (2013) 237–258

258                             Arjun K. Gupta, Bruce E. Johnson Daya K. Nagar


Lemma 7. Let Y1 , Y2 and Y3 be independent random variables, Yi ∼ χ22 , i = 1, 2, 3.
Define Y = α1 Y1 + α2 Y2 + α3 Y3 where α1 , α2 and α3 are constants, α1 > α2 >
α3 > 0. Then, the cumulative distribution function FY (y) of Y is given by
                              3                  
                              X                 y
                 FY (y)   =       Ci 1 − exp −        ,      y > 0,
                              i=1
                                               2αi

where C1 = α12 /(α1 − α3 )(α1 − α2 ), C2 = −α22 /(α2 − α3 )(α1 − α2 ) and
C3 = α32 /(α2 − α3 )(α1 − α3 )

Proof . We
        P3get the desired result by inverting the moment generating function
MY (t) = i=1 Ci (1 − 2αi t)−1 , 2α1 t < 1.

References
Aitkin M. Some tests for correlation matrices.(1969). Biometrika.
Aitkin M A, Nelson W C, Reinfurt K H. Tests for correlation matrices. (1968).  Biometrika.
Ali M M, Fraser D A S, Lee Y S. Distribution of the correlation matrix.(1970). Journal of Statistical Research.
Anderson T W. An Introduction to Multivariate Statistical Analysis Wiley Series in Probability and Statistics.(2003).John Wiley Sons.
Browne M W. The likelihood ratio test for the equality of correlation matrices.(1978). The British Journal of Mathematical and Statistical Psychology.
Cole N. The likelihood ratio test of the equality of correlation matrices.(1968).University of North Carolina.
Cole N. On testing the equality of correlation matrices.(1968). University of North Carolina.
Gleser L J. On testing a set of correlation coefficients for equality: Some asymptotic results.(1968).Biometrika.
Gupta A K, Johnson B E, Nagar D K. Testing equality of several correlation matrices.(2012). Bowling Green State University.
Gupta A K, Nagar D K. Matrix Variate Distributions of Chapman Hall/CRC Monographs and Surveys in Pure and Applied Mathematics.(2000). Chapman Hall/CRC.
Gupta A K, Varga T. Elliptically Contoured Models in Statistics of Mathematics and its Applications.(1993). Kluwer Academic Publishers Group.
Jennrich R I. An asymptotic χ2 test for the equality of two correlation matrices.(1970). Journal of the American Statistical Association.
Kaplan E L.Tensor notation and the sampling cumulants of k-statistics.(1952). Biometrika.
Kendall M G, Stuart A. The Advanced Theory of Statistics.(1969).Hafner Publishing.
Konishi S. An approximation to the distribution of the sample correlation coefficient.(1978). Biometrika.
Konishi S. Asymptotic expansions for the distributions of functions of a correlation matrix.(1979).Journal of Multivariate Analysis.
Konishi S. Asymptotic expansions for the distributions of statistics based on the sample correlation matrix in principal component analysis.(1979).Hiroshima Mathematical Journal.
Konishi S, Sugiyama T. Improved approximations to distributions of the largest and the smallest latent roots of a Wishart matrix.(1981). Annals of the Institute of Statistical Mathematics.
Kullback S. On testing correlation matrices.(1967). Applied Statistics.
Kullback S. Information Theory and Statistics.(1997). Dover Publications.
Modarres R. Testing the equality of dependent variances.(1993). Biometrical Journal.
Modarres R, Jernigan R W. Testing the equality of correlation matrices.(1992). Communications in Statistics - Theory and Methods.
Modarres R, Jernigan R W. A robust test for comparing correlation matrices.(1993). Journal of Statistical Computation and Simulation.
Muirhead R J. Aspects of Multivariate Statistical Theory.(1982). John Wiley Sons.
Schott J R. Testing the equality of correlation matrices when sample correlation matrices are dependent.(2007). Journal of Statistical Planning and Inference.
Siotani M, Hayakawa T, Fujikoshi Y. Modern Multivariate Statistical Analysis: A Graduate Course and Handbook.(1985). American Sciences Press Series in Mathematical and Management Sciences.
Waternaux C M. Principal components in the nonnormal case: The test of equality of q roots.(1984). Journal of Multivariate Analysis.