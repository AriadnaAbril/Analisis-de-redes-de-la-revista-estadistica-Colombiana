The Distribution of a Linear Combination of Two Correlated Chi-Square Variables. Distribución de una combinación lineal de dos variables chi-cuadrado correlacionadas
King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia. Bowling Green State University, Ohio, United States of America
Abstract
The distribution of the linear combination of two chi-square variables is known if the variables are independent. In this paper, we derive the distribution of positive linear combination of two chi-square variables when they are correlated through a bivariate chi-square distribution. Some properties of the distribution, namely, the characteristic function, cumulative distribution function, raw moments, mean centered moments, coefficients of skewness and kurtosis are derived. Results match with the independent case when the variables are uncorrelated. The graph of the density function is presented.
Key words: Bivariate Chi-square Distribution, Correlated Chi-square Variables, Linear Combination, Characteristic Function, Cumulative Distribution, Moments.
Resumen
La distribución de una combinación lineal de dos variables chi cuadrado es conocida si las variables son independientes. En este artículo, se deriva la distribución de una combinación lineal positiva de dos variables chi cuadrado cuando estas están correlacionadas a través de una distribución chi cuadrado bivariada. Algunas propiedades de esta distribución como la función característica, la función de distribución acumulada, sus momentos, momentos centrados alrededor de la media, los coeficientes de sesgo y curtosis son derivados. Los resultados coinciden con el caso independiente cuando las variables son no correlacionadas. La gráfica de la función de densidad es también presentada.
Palabras clave: combinación lineal, distribución acumulada, distribución chi cuadrado bivariada, función característica, momentos, variables chi cuadrado correlacionadas.
 
1. Introduction
    Let X 1 , X 2 , . . . X n be (N > 2) two-dimensional independent normal random
                                                                           PN
vectors from N2 (µ, Σ) with mean vector X = (X 1 , X 2 )0 , where N X i = j=1 Xij ,
                                                                          PN
(i = 1, 2) so that sums of squares and cross product matrix is given by j=1 (X j −
X)(X j − X)0 = A. Let the matrix A be denoted by A = (aik ), i = 1, 2; k = 1, 2
where aii = mSi2 , (i = 1, 2), m = N − 1 and a12 = mRS1 S2 . That is, S1 and
S2 are the sample standard deviations based on the bivariate sample, and R is
the related product moment correlation coefficient. Also let Σ = (σik ), i = 1, 2;
k = 1, 2 where σ11 = σ12 , σ22 = σ22 , σ12 = ρσ1 σ2 with σ1 > 0, σ2 > 0. The quantity
ρ(−1 < ρ < 1) is the product moment correlation coefficient between X1j and
X2j (j = 1, 2, . . . , N ).
    The joint density function U = mS12 /σ12 and V = mS22 /σ22 , called the bivariate
chi-square distribution, was derived by Joarder (2009) in the spirit of Krishnaiah,
Hagis & Steinberg (1963) who studied the bivariate chi-distribution.
    The distribution of linear function of random variables is useful in the theory
of process capability indices and the study of two or more control variables. See,
for example, Glynn & Inglehart (1989) and Chen & Hsu (1995). It also occurs in
statistical hypothesis testing and high energy physics (See Bausch 2012).
    The density function of positive linear combination of independent chi-square
random variables was derived by Gunst & Webster (1973). Algorithms were writ-
ten by Davies (1980) and Farebrother (1984) for the distribution of the linear
combination of independent chi-square variables. The exact density function of a
general linear combination of independent chi-square variables is a special case of
a paper by Provost (1988) for a more general case of Gamma random variables.
Interested readers may go through Johnson, Kotz & Balakrishnan (1994) for a
detailed historical account.
    By application of the inversion formula to the characteristic function of the
sum of correlated chi-squares, Gordon & Ramig (1983) derived an integral form of
the cumulative distribution function (CDF) of the sum and the used trapezoidal
rule to evaluate it. Since this integral form of the CDF involves integration of
complex variables, the percentage points depends on the type of numerical tech-
nique you employ. Recently Bausch (2012) has developed an efficient algorithm for
numerically computing the linear combination of independent chi-square random
variables. He has shown its application in string theory.
   In Section 2, some mathematical preliminaries are provided. In Section 3,
we derive the density function and the Cumulative Distribution Function of the
positive linear combination of two correlated chi-square variables when they are
governed through a bivariate chi-square density function given by (6). In Section
4, we derive the characteristic function of the distribution. In Section 5, we also
derive some properties of the distribution, namely, raw moments, mean centered
moments, coefficient of skewness and kurtosis. The results match with the inde-
pendent case when the variables are uncorrelated. The results also match with
the special case of the sum of two correlated chi-square variables considered by


                                    Revista Colombiana de Estadística 36 (2013) 209–219

The Distribution of a Linear Combination of Two Correlated Chi-Square Variables 211


Joarder & Omar (2013). The graph of the density function of the sum is presented
at the end of the paper.


2. Mathematical Preliminaries
   Let fX,Y (x, y) be the joint density function of X and Y . Then the following
lemma is well known.
Lemma 1. Let X and Y be two random variables with common probability density
function fX,Y (x, y). Further let Z = X + Y . Then the density function of Z at z
is given by                         Z                   ∞
                                    hZ (z) =                fX,Y (z − y, y)dy                                 (1)
                                                    0

   The duplication of the Gamma function is given below:
                             √
                                                     
                                                    1
                        Γ(2z) π = 22z−1 Γ(z)Γ z +                                                             (2)
                                                    2

   The incomplete Gamma is defined by
                                   Z x
                         γ(α, x) =     tα−1 e−t dt                                                            (3)
                                                            0

where Re(α) > 0 (Gradshteyn & Ryzhik 1994, Equation 8.350, p. 949).

   The hypergeometric function p Fq (a1 , a2 , . . . , ap ; b1 , b2 , . . . , bq ; z) is defined by
                                                                  ∞
                                                                  X (a1 ){k} (a2 ){k} . . . (ap ){k} z k
       p Fq (a1 , a2 , . . . , ap ; b1 , b2 , . . . , bq ; z) =                                               (4)
                                                                        (b1 ){k} (b2 ){k} . . . (bq ){k} k!
                                                                  k=0

where a{k} = a(a + 1) . . . (a + k − 1)
   The following integral will be used:
       Z ∞
                                    dc Γ(a + c)
                                                                         
              a−1 −bx                                                  d
            x    e γ(c, dx)dx =                 2 F1 1, a + c; c + 1;                                         (5)
        0                          c(b + d)a+c                        b+d
with Re(a + b) > 0, b > 0, (a + c) > 0, (Gradshteyn & Ryzhik 1994).
   The following theorem is due to Joarder (2009), although it can be followed
from Krishnaiah et al. (1963).
Theorem 1. The random variables U and V are said to have a correlated bivariate
chi-square distribution each with m(> 2) degrees of freedom, if its density function
is given by

                     (uv)(m/2)−1                                      ρ2 uv
                                                                           
                                               u+v              m
  fU,V (u, v) = m 2                   exp −              0 F1     ;              (6)
               2 Γ (m/2)(1 − ρ2 )m/2          2 − 2ρ2           2 (2 − 2ρ2 )2
where 0 F1 (; b; z) is defined in 4 and −1 < ρ < 1.

                                                   Revista Colombiana de Estadística 36 (2013) 209–219

212                              Anwar H. Joarder, M. Hafidz Omar & Arjun K. Gupta


     In case ρ = 0, the density function of the joint probability distribution in
                                                              2             2
Theorem 1, would be fU,V (u, v) = fU (u)fv (v) where U ∼ Xm     and V ∼ Xm    . The
product moment correlation coefficient between U and V can be calculated to be
ρ2 . For the estimation of correlation coefficient ρ by modern techniques, we refer
to Ahmed (1992).


3. The Density Function and the Cumulative
   Distribution Function
   Let c1 and c2 be positive numbers so that T1 = c1 U + c2 V. Equivalently, let
T1 = c1 T where T = U + cV, c = c2 /c1 defines a general linear combination of the
variables U and V .

Theorem 2. Let U and V be two chi-square variables each having m degrees of
freedom with density function given in Theorem 1. Then for any positive constant
c, the density function of T = U + cV is given by

          Γ((m + 1)/2)tm−1
                                               
                                            t
  fT (t) =                     exp    −
        2m Γ(m)[c(1 − ρ2 )]m/2          2 − 2ρ2
    ∞
                             (tρ)2k
                                                                          
    X         1                                    m            (c − 1)t
  ×                                       1 F1 k +   ; 2k + m;                         (7)
      Γ(k + (m + 1)/2) (4 − 4ρ2 )2k ck k!          2           (2 − 2ρ2 )c
      k=0

where m > 2, −1 < ρ < 1 and 0 ≤ t < ∞.

Proof . It follows from (6) that the joint density function of X = U and Y = cV
is given by

                  (1 − ρ2 )−m/2  xy (m/2)−1
  fX,Y (x, y) =
                   2m Γ2 (m/2)     c
                                                                    ρ2
                                                                           
                                        1        y          m            xy 1
                             exp −             x+      0 F1     ;
                                     2 − 2ρ2      c           2 (2 − 2ρ2 )2 c   c

so that, by Lemma 1, the density function of T = X + Y is given by

                        (c(1 − ρ2 ))−m/2
                                                      
                                                  t
               fT (t) =                  exp −          I(t; m, ρ, c)                  (8)
                          2m Γ2 (m/2)          2 − 2ρ2

where
                                 ∞
                                 X          1             ρ2k
        I(t; m, ρ, c) = Γ(m/2)                                         J(t; m, ρ, c)   (9)
                                       Γ[k + (m/2)] (2 − 2ρ2 )2k ck k!
                                 k=0

                       Rt                                                   
                                                                  (c−1)y
with J(t; m, ρ, c) =   0
                         (t − y)k−1+(m/2) y k−1+(m/2) exp        c(2−2ρ2 )       dy


                                        Revista Colombiana de Estadística 36 (2013) 209–219

The Distribution of a Linear Combination of Two Correlated Chi-Square Variables 213


   Substituting y = st we have
                                              ∞
                                     2k+m−1
                                              X Γ[(k + j + (m/2))]      (c − 1)j tj
  J(t; m, ρ, c) = Γ(k + (m/2))t                                                         (10)
                                              j=0
                                                     Γ(2k + j + m)    (2 − 2ρ2 )j cj j!

   which, by (4), can be expressed as
                                 2
                                                                                    
                         2k+m−1 Γ (k + (m/2))                m            (c − 1)t
     J(t; m, ρ, c) = t                              1 F1 k +   ; 2k + m;
                                    Γ(2k + m)                2           (2 − 2ρ2 )c

   Plugging this in (9) and simplifying, we have
                           √      ∞
                    Γ(m/2) π m−1 X          1              (tρ)2k
  I(t; m, ρ, c) =       m−1
                             t
                      2              Γ(k + (m + 1)/2) (4 − 4ρ2 )2k ck k!
                                 k=0
                                                                               
                                                      m              (c − 1)t
                                          × 1 F1 k + ; 2k + m;
                                                       2            (2 − 2ρ2 )c

   Substituting this in (8) and simplifying, we have (7).
   Figure 1 provides a graphical display of this density function for m = 5 and
various values of c and ρ.




Figure 1: Linear combination of chi-square variables for m = 5 and various values of ρ.


Theorem 3. Let T have a density function given by (7). Then the Cumulative
Distribution Function of T is given by
                              Γ((m + 1)/2)
              FT (t) =
                          2m Γ(m)(c(1 − ρ2 ))m/2
                          ∞                                                              (11)
                          X          1                 ρ2k
                      ×                                             I(k; m, ρ)
                                Γ(k + (m + 1)/2) (4 − 4ρ2 )2k ck k!
                          k=0



                                          Revista Colombiana de Estadística 36 (2013) 209–219

214                               Anwar H. Joarder, M. Hafidz Omar & Arjun K. Gupta

                  Rt                                                        
                                         y                m            (c−1)y
where I(k; m, ρ) = y=0 y m+2k−1 exp − (2−2ρ 2)   1 F1 k + 2 ; 2k + m; (2−2ρ2 )c dy,
0 ≤ t < ∞, −1 < ρ < 1, m > 2 and c is any positive constant.

Proof . It is immediate from Theorem 2

   The CDF in (11) is not in closed form. However, if ρ = 0, a closed form
expression is presented in Theorem 5.
Theorem 4. Let U and V be two independent chi-square variables each having
m(> 2) degrees of freedom. Then for any positive constant c, the density function
of T = U + cV is given by
                      tm−1 e−t/2
                                                      
                                       m      (c − 1)t
            fT (t) = m m/2        F
                                 1 1     ; m;            , 0≤t<∞             (12)
                     2 c    Γ(m)       2         2c

Proof . Putting ρ = 0 in Theorem 2, we have (12).
                                                              2
   If c = 1, then (12) simplifies to the density function of X2m as expected. The
equation (10) is a special case of Provost (1988)
Theorem 5. Let U and V be two independent chi-square variables each having
m(> 2) degrees of freedom. Then the Cumulative Density Function of T = U + cV
is given by
                             ∞
                          1 X (m/2){k} (c − 1)k
                F (t) = m/2                     γ(k + m, t/2)            (13)
                        c       Γ(k + m) ck k!
                                 k=0
where m > 2 and γ(α, x) is defined in (3).

Proof . By substituting ρ = 0 in (12), we have
                             Z t                                   
                     1             m−1                   m (c − 1)y
        F (t) = m                y     exp (−y/2) 1 F1     ;          dy
               2 Γ(m)cm/2 0                              2    2c
which simplifies to (13).

   By substituting c = 1 in (13), we have F (t) = γ(m, t/2)/Γ(m) which is the
                                    2
Cumulative Distribution Function X2m   . Bausch (2012) developed and efficient
algorithm for computing linear combination of independent chi-square variables.


4. The Characteristic Function
                                                                                 √
      The quantity i in this section is defined by the imaginary number i =          −1.
Theorem 6. Let U and V be two chi-square variables each having m(> 2) degrees
of freedom −1 < ρ < 1 with density function given in Theorem 1. Then the
characteristic function φU,V (w1 , w2 ) = E(eiw1 U +iw2 V ) of U and V at w1 and w2
is given by
                φU,V (w1 , w2 ) = [(1 − 2iw1 )(1 − 2iw2 ) + 4w1 w2 ρ2 ]−m/2           (14)
where m > 2 and −1 < ρ < 1.


                                       Revista Colombiana de Estadística 36 (2013) 209–219

The Distribution of a Linear Combination of Two Correlated Chi-Square Variables 215


Proof . See Omar & Joarder (2010).

   The characteristic function of the linear combination of two correlated chi-
square variables is derived below.

Theorem 7. Let U and V be two chi-square variables each having m degrees of
freedom. Then for any known constant c, the characteristic function of T = U +cV
at w is given by the following:

                   φT (w) = [(1 − 2iw)(1 − 2icw) + 4w2 cρ2 ]−m/2                   (15)

where m > 2 and −1 < ρ < 1.

Proof . By definition,the characteristic function of T = U + cV is given by
φT (w) = E(eiwT ) = E[eiw(U +cV ) ] = E[ei(wU +cwV ].

    By (14), E[ei(wU +cwV ) ] = φU,V (w, cw) and can be written as φU,V (w, cw) =
[(1 − 2iw)(1 − 2iw) + 4wcwρ2 ]−m/2 , which is (15).
   The corollary below follows from Theorem 7.

Corollary 1. Let U and V be two independent chi-square variables each having
the same degrees of freedom m. Then for any positive constant c, the characteristic
function of T = U + cV at w is given by the following:

                    φT (w) = [(1 − 2iw)(1 − 2iwc)]−m/2 , m > 2                     (16)

    Since the above can be expressed as φT (w) = φU (w)φcV (w), clearly the random
variable T is the linear combination of two independent random variables U and
V . In case c = 1, the equation (16) will be specialized to the characteristic function
of a chi-square variable with 2m degrees of freedom.
   The following results are for any general bivariate distribution.

Theorem 8. Let X and Y have a bivariate distribution with density function
fX,Y (x, y) and characteristic function ϕX,Y (w1 , w2 ) = E(eiw1 X+iw2 Y ). Then for
any constant c, the characteristic function of T = X + cY at w is given by the
following:
                               φT (w) = φX,Y (w, cw)                            (17)

Proof . By definition, the characteristic function of T = X + cY is given by
φT (w) = E(eiwT ) = E[eiw(X+cY ) ] = E[ei(wX+cwY ) ] = φX,Y (w, cw).

Corollary 2. Let X and Y have a bivariate distribution with density function
fX,Y (x, y) and characteristic function ϕX,Y (w1 , w2 ) = E(eiw1 X+iw2 Y ). Then, the
characteristic function of T = X + Y at w is given by the following:

                                φT (w) = φX,Y (w, w)                               (18)


                                     Revista Colombiana de Estadística 36 (2013) 209–219

216                                   Anwar H. Joarder, M. Hafidz Omar & Arjun K. Gupta


5. Moments, Coefficient of Skewness and Kurtosis
      The following theorem is due to Joarder, Laradji, & Omar (2012).
Theorem 9. Let U and V have the bivariate chi-square distribution with density
function with common degrees of freedom m and density function in Theorem 1.
Then for a > −m/2, b > −m/2 and −1 < ρ < 1, the (a, b)-th product moment of
                      0
U and V , denoted by µa,b;ρ (U, V ) = E(U a V b ), is given by

                                                  Γ(a + (m/2))Γ(b + (m/2))
  µ0a,b;ρ (U, V ) = 2a+b (1 − ρ2 )a+b+(m/2)
                                                           Γ2 (m/2)
                                                                   m    m m     
                                                        × 2 F1 a + , b + ; ; ρ 2            (19)
                                                                    2    2 2
where m > 2, −1 < ρ < 1 and 2 F1 (a1 , a2 ; b; z) is defined in (4).
Theorem 10. Let T have a density function given by (7). Then the first four
moments of T are respectively given by

        E(T ) = (c + 1)m                                                                    (20)
           2       2                                        2
       E(T ) = (c + 1)m(m + 2) + 2cm(m + 2ρ )                                               (21)
           3       3                                                                    2
       E(T ) = (c + 1)m(m + 2)(m + 4) + 3c(c + 1)(m(m + 2)(m + 4ρ ))                        (22)
           4       4
       E(T ) = (c + 1)[m(m + 2)(m + 4)(m + 6)]
               + 4c(c2 + 1)[m(m + 2)(m + 4)(m + 6ρ2 )]                                      (23)
                   2                                                2           4
               + 6c m(m + 2)[m(m + 2) + 8(m + 2)ρ + 8ρ ]

where c > 0, m > 2 and −1 < ρ < 1.

Proof . The moment expressions between (20) and (23) inclusively follow from
Theorem 9 by tedious algebraic simplification.

   Let T have a density function given by (7). Then the a-th moment of T denoted
by E(T a ) = E(U + cV )a , where c is any non-negative constant, is given by
                                      a    
                                      X   a
                         µ0a (T ) =                   ca−j µ0j,a−j;ρ (U, V )                (24)
                                              j
                                      j=0


where µ0j,a−j;ρ (U, V ) = E(U j V a−j ) is given by Theorem 9.
    The centered moments of T of order a is given by µa = E(T − E(T ))a , a =
1, 2, . . . That is the second, third and fourth order mean corrected moments are
respectively given by

                       µ2 = E(T 2 ) − µ2                                                    (25)
                                  3               2             3
                       µ3 = E(T ) − 3E(T )µ + 2µ                                            (26)
                                  4               3                     2   2       4
                       µ4 = E(T ) − 4E(T )µ + 6E(T )µ − 3µ                                  (27)


                                            Revista Colombiana de Estadística 36 (2013) 209–219

The Distribution of a Linear Combination of Two Correlated Chi-Square Variables 217


   Where µ = E(T ). The explicit forms for the centered moments of the linear
combination of bivariate chi-square random variables are given in the following
theorem.
Theorem 11. Let T have a density function given by (7). The second to fourth
centered moments of T are given by the following:

               µ2 = 2m(1 + c2 + 2cρ2 )                                              (28)
                                  2                2
               µ3 = 8(c + 1)m(c − c + 1 + 3cρ )                                     (29)
               µ4 = 12m 2c2 m + (c4 + 1)(m + 4)
                        
                                                                                    (30)
                  + 4c(4c2 + 4c + 4 + c2 m + m)ρ2 + 4c2 (m + 2)ρ4
                                                                       

where m > 2, c is any positive constant and −1 < ρ < 1.

Proof . The moments between (28) to (30) inclusively follow from (25),(26) and
(27) with tedious algebraic simplifications.

   In case ρ = 0,the moments match with that of T = U + cV where U and V
have independent chi-square distributions each with degrees of freedom m(> 2).
    The skewness and kurtosis of a random variable T are given by the moment
                     −i/2
ratios αi (T ) = µi µ2 , i = 3, 4. The theorem below follows from Theorem 11.
Theorem 12. Let T have a density function given by (7). The coefficient of
skewness and kurtosis of T where c is any non-negative constant, are given by
                               √
                              2 2(c + 1)(3cρ2 + c2 − c + 1)
                    α3 (T ) =    √                                          (31)
                                   m(2cρ2 + c2 + 1)3/2

and
                               12
        α4 (T ) = 3 +                        (2c2 ρ4 + 4c(c2 + c + 1) + c4 + 1)     (32)
                        m(2ρ2 c + c2 + 1)2
respectively, where m > 2, c is any positive constant and −1 < ρ < 1.

    In case ρ = 0, the above coefficient of skewness and kurtosis simplifies to, as
expected, that for T = U +cV where U and V are independent chi-square with the
same degrees of freedom m(> 2). In case c = 1, ρ decreases to 0 and the degrees
of freedom m increases indefinitely, then the coefficient of skewness and that of
kurtosis converges to 0 and 3 as expected.


6. Conclusion
    We have developed the distributional characteristics of linear combination of
correlated chi-square variables. Based on the results in the paper, efficient com-
putational algorithms can be developed along the line of Bausch (2012) who de-
veloped an efficient algorithm for computing linear combination of independent
chi-square variables.


                                      Revista Colombiana de Estadística 36 (2013) 209–219

218                            Anwar H. Joarder, M. Hafidz Omar & Arjun K. Gupta


Acknowledgments
   The authors are grateful to two anonymous referees and the Editor for many
constructive suggestions that led to the current version of the paper. The first two
authors gratefully acknowledge the excellent research facility provided by King
Fahd University of Petroleum and Minerals, Saudi Arabia especially through the
project IN111019.
                                                                  
               Recibido: febrero de 2013 — Aceptado: julio de 2013


References
Ahmed, S. (1992), ‘Large sample pooling procedure for correlation’, The Statistician 41, 415–428.
Bausch, J. (2012), On the efficient calculation of a linear combination of chi-square variables with an application in counting string vacua. arXiv:1208.2691.
Chen, S. & Hsu, N. (1995), ‘The asymptotic distribution of the process capability index cpmk ’, Communications in Statistics - Theory and Methods 24, 1279–1291.
Davies, R. (1980), ‘Algorithm as 155. The distribution of a linear combination ofχ2 random variables’, Applied Statistics 29, 332–339.
Farebrother, R. (1984), ‘Algorithm AS 204. The distribution of a positive linear combination of χ2 random variables’, Applied Statistics 33, 332–339.
Glynn, P. & Inglehart, D. (1989), ‘The optimal linear combination of control variates in the presence of asymptotically negligible bias’, Naval Research Logistics Quarterly 36, 683–692.
Gordon, N. & Ramig, P. (1983), ‘Cumulative distribution function of the sum of correlated chi-squared random variables’, Journal of Statistical Computation and Simulation 17(1), 1–9.
Gradshteyn, I. & Ryzhik, I. (1994), Table of Integrals, Series and Products, Academic Press.
Gunst, R. & Webster, J. (1973), ‘Density functions of the bivariate chi-square distribution’, Journal of Statistical Computation and Simulation 2, 275–288.
Joarder, A. (2009), ‘Moments of the product and ratio of two correlated chi-square random variables’, Statistical Papers 50(3), 581–592.
Joarder, A., Laradji, A., & Omar, M. (2012), ‘On some characteristics of bivariate chi-square distribution’, Statistics 46(5), 577–586.
Joarder, A. & Omar, M. (2013), ‘Exact distribution of the sum of two correlated chi-square variables and its application’, Kuwait Journal of Science and Engineering 40(2), 60–81.
Johnson, N., Kotz, S. & Balakrishnan, N. (1994), Continuous Univariate Distributions, Vol. 1, Wiley.
Krishnaiah, P., Hagis, P. & Steinberg, L. (1963), ‘A note on the bivariate chi distribution’, SIAM Review 5, 140–144.
Omar, M. & Joarder, A. (2010), Some properties of bivariate chi-square distribution and their application, Technical Report 414, Department of Mathematics and Statistics, King Fahd University of Petroleum and Minerals, Saudi Arabia.
Provost, S. (1988), ‘The exact density of a general linear combination of gamma variables’, Metron 46, 61–69.
