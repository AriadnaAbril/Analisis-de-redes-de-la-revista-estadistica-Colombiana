Partial Least Squares Regression on Symmetric. Positive-Definite Matrices. RegresiÃ³n de mÃ­nimos cuadrados parciales sobre matrices simÃ©tricas definidas positiva
Universidad Nacional de Colombia, MedellÃ­n, Colombia.  CIMAT-MÃ©xico Unidad Monterrey, Monterrey Nuevo LeÃ³n, MÃ©xico
Resumen
Recently there has been an increased interest in the analysis of different types of manifold-valued data, which include data from symmetric positive definite matrices. In many studies of medical cerebral image analysis, a major concern is establishing the association among a set of covariates and the manifold-valued data, which are considered as responses for characterizing the shapes of certain subcortical structures and the differences between them. The manifold-valued data do not form a vector space, and thus, it is not adequate to apply classical statistical techniques directly, as certain operations on vector spaces are not defined in a general Riemannian manifold. In this article, an application of the partial least squares regression methodology is performed for a setting with a large number of covariates in a euclidean space and one or more responses in a curved manifold, called a Riemannian symmetric space. To apply such a technique, the Riemannian exponential map and the Riemannian logarithmic map are used on a set of symmetric positive-definite matrices, by which the data are transformed into a vector space, where classic statistical techniques can be applied. The methodology is evaluated using a set of simulated data, and the behavior of the technique is analyzed with respect to the principal component regression.
Palabras clave: Matrix theory, Multicollinearity, Regression, Riemann manifold.
Abstract
Recientemente ha habido un aumento en el interÃ©s de analizar diferentes tipos de datos variedad-valuados, dentro de los cuÃ¡les aparecen los datos de matrices simÃ©tricas definidas positivas. En muchos estudios de anÃ¡lisis de imÃ¡genes mÃ©dicas cerebrales, es de interÃ©s principal establecer la asociaciÃ³n entre un conjunto de covariables y los datos variedad-valuados que son considerados como respuesta, con el fin de caracterizar las diferencias y formas en ciertas estructuras sub-corticales. Debido a que los datos variedad-valuados no forman un espacio vectorial, no es adecuado aplicar directamente las tÃ©cnicas estadÃ­sticas clÃ¡sicas, ya que ciertas operaciones sobre espacio vectoriales no estÃ¡n definidas en una variedad riemanniana general. En este artÃ­culo se realiza una aplicaciÃ³n de la metodologÃ­a de regresiÃ³n de mÃ­nimos cuadrados parciales, para el en torno de un nÃºmero grande de covariables en un espacio euclÃ­deo y una o varias respuestas que viven una variedad curvada llamada espacio simÃ©trico Riemanniano. Para poder llevar a cabo la aplicaciÃ³n de dicha tÃ©cnica se utilizan el mapa exponencial Riemanniano y el mapa log Riemanniano sobre el conjunto de matrices simÃ©tricas positivas definida, mediante los cuales se transforman los datos a un espacio vectorial en donde se pueden aplicar tÃ©cnicas estadÃ­sticas clÃ¡sicas. La metodologÃ­a es evaluada por medio de un conjunto de datos simulados en donde se analiza el comportamiento de la tÃ©cnica con respecto a la regresiÃ³n por componentes principales.
Key words: multicolinealidad, regresiÃ³n, teorÃ­a de matrices, variedad Riemanniana.


1. Introduction
    In studies of diffusion tensor magnetic resonance imaging (TD-MRI), a diffusion
tensor (DT) is calculated in each voxel of an imaging space, which describes the
local diffusion of water molecules in various directions over this region of the brain.
A sequence of images is used to measure this diffusion. The sequence includes a
noise that produces uncertainty in the tensor estimation and in the estimation of
certain quantities inherent to water molecules, such as eigenvalues, eigenvectors,
the anisotropic fraction rate (FA) and the fiber trajectories, which are constructed
based on these last parameters. The diffusion-tensor imaging (DTI) is a powerful
tool to quantitatively evaluate the integrity of the anatomic connectivity in the
white matter of clinical populations. The methods used for the analysis of DTI
at a group level include the statistical analysis of certain invariant measures, such
as eigenvalues, eigenvectors or principal directions, the anisotropic fraction, and
the average diffusivity, among others. However, these invariant measures do not
capture all of the information about the complete DTs, which leads to a decrease in
the statistical power of the DTI to detect subtle changes in white matter. Hence,
new statistical methods are being developed to fully analyze the DTs as responses
and to establish their association with a set of covariates (Li, Zhu, Chen, Ibrahim,
An, Lin, Hall & Shen 2009, Zhu, Chen, Ibrahim, Li & Lin 2009, Yuan, Zhu,
Lin & Marron 2012). In some of these development the log-euclidean metric has
been used with the transformation of the DTs from a non-linear space into their


                                      Revista Colombiana de EstadÃ­stica 36 (2013) 177â€“192

PLS-Regression on SPD Matrices                                                      179

logarithmic matrices on a Euclidean space. Semi-parametrical models have been
proposed to study the relationship between the set of covariates and the DTs as
responses. Estimation processes and hypotheses test based on test statistics and re-
sampling methods have been developed to simultaneously evaluate the statistical
significance of linear hypotheses throughout large regions of interest (ROI).
    An appropriate statistical analysis of DTs is important to understand the nor-
mal development of the brain, the neurological bases of neuropsychiatric disorders
and the joined effects of environmental and genetic factors on the brain struc-
ture and function. In addition, any statistical method for complete diffusion ten-
sors can be directly applied to positive-definitive tension matrices in computa-
tional anatomy to understand the variations in shapes of brain-structure imaging
(Grenander & Miller 1998, Lepore, Brun, Chou, Chiang, Dutton, Hayashi, Luders,
Lopez, Aizenstein, Toga, Becker & Thompson 2008).
    Symmetric positive-definite (SPD) matrix-valued data occur in a wide variety
of applications, such as DTI for example, where a SPD 3x3 DT, which tracks the
effective diffusion of the water molecules in certain brain regions, is estimated at
each voxel of an imaging space. Another application of SPD matrix-valued data
can be seen in studies on functional magnetic resonance imaging (fMRI), where
an SPD covariance matrix is calculated to delineate the functional connectivity
between different neuronal assembles involved in the execution of certain complex
cognitive tasks or in perception processes (Fingelkurts & Kahkonen 2005). De-
spite the popularity of SPD matrix-valued data, there are few statistical methods
to analyze SPD matrices as response variables in a Riemannian manifold. Data
considered as responses with a small number of covariates of interest in a Euclid-
ian space can be found from the following studies in the literature for statistical
analysis using regression models of SPD matrices: Batchelor, Moakher, Atkinson,
Calamante & Connelly (2005), Pennec, Fillard & Ayache (2006), Schwartzman
(2006), Fletcher & Joshi (2007), Barmpoutis, Vemuri, Shepherd & Forder (2007),
Zhu et al. (2009) and Kim & Richards (2010). However, because the SPD matrix
data do not form a vector space, classical multivariate regression techniques can-
not be applied directly to establish the relationship between these types of data
and a set of covariates of interest.
    In a setting with a large number of covariates with a high multicollinearity pres-
ence and few available observations, no regression methods have been previously
proposed to study the relationship between such covariates and the response vari-
ables of SPD matrices living in non-Euclidian spaces. In this article, partial least
squares (PLS) regression is proposed using a strategy of exponential and Rieman-
nian logarithmic maps to transform data into Euclidian spaces. The development
of the technique is similar to the scheme for the analysis of SPD matrices data
as responses in a classical regression model and in a local polynomial regression
model, as proposed in Zhu et al. (2009) and Yuan et al. (2012). The PLS regression
model is initially evaluated using a set of simulated data and statistical validation
techniques which currently exist, such as cross validation techniques. The behavior
of the PLS regression technique is analyzed by comparing it to that of the classic
dimension-reduction technique, called principal component (PC) regression.



                                     Revista Colombiana de EstadÃ­stica 36 (2013) 177â€“192

180                                      RaÃºl Alberto PÃ©rez & Graciela GonzÃ¡lez-Farias


    The article is structured as follows: In Section 2, a brief revision of the existing
theory for PC and the PLS regression classical model is outlined. In Section 3, some
properties of the Riemannian geometric structure of SPD matrices are reviewed.
An outline of the regression models, as well as the estimation methods of their
regression coefficients are also presented. In Section 4, our PLS regression model
is presented, along with the estimation process used and their application and
evaluation on a simulated-data set. In Section 5, conclusions and recommendations
for future work are given.


2. Regression Methods
2.1. Classical Regression
     We will examine the following data set {(xi , yi ) : i = 1, 2, . . . , n}, composed
of a response yi and a k Ã— 1 covariate vector xi = (xi1 , xi2 , . . . , xik )T , where the
response can be continuous, discrete, or qualitative observations, and the covariates
can be qualitative or quantitative. A regression model often includes two key
elements: A link function Âµi (Î²) = E[y|xi ] = g(xi , Î²) and a residual i = yi âˆ’Âµi (Î²),
where Î² qÃ—1 is a regression-coefficients vector and g(. , .): from Rk Ã— Rq â†’ R,
(xi , Î²) â†’ g(xi , Î²) with q = k + 1, can be known or unknown according to the
type of model: Parametric, not-parametric or semi-parametric. The parametric
regression model can be defined as: yi = g(xi , Î²) + i , with g(xi , Î²): Known and
E[i |xi ] = 0, âˆ€i = 1, 2, . . . , n, where the expectation is taken with respect to the
conditional distribution of  given x. The non-parametric model can be defined as
yi = g(xi ) + i , with g(xi ): Unknown and E[i |xi ] = 0.
   For inference on Î² in the parametric case (or on g(.), in the non-parametric
case), at least three statistical procedurals are needed. First, an estimation method
needs to be developed to calculate the estimate of the coefficients of vector Î²,
denoted by Î²Ì‚. Second, it needs to be proven that Î²Ì‚ is a consistent estimator of
Î² and that it has certain asymptotic properties. Third, test statistics need to be
developed for testing hypotheses with the form:

                        H0 : HÎ² = b0       v.s Ha : HÎ² 6= b0

where normally HrÃ—s , Î² sÃ—1 and b0rÃ—1 are a constant matrix, a regression-coefficients
vector and a constant vector respectively.


2.2. Regression in Sub-Spaces of Variables
   In many practical situations, the number of variates is much greater than the
quantity of available observations in the data set for a regression model, caus-
ing the problem of multicollinearity between the predictors. Among the available
options for handling this problem are techniques based in explicit or implicit sub-
spaces and the Bayesian approach, which includes additional information about
the parameters of the model. In the case of the sub-spaces, the regression is


                                       Revista Colombiana de EstadÃ­stica 36 (2013) 177â€“192

PLS-Regression on SPD Matrices                                                             181

realized within a feasible space of a lesser dimension. The sub-space may be con-
structed explicitly with a geometric-type motivation derived from the use of latent
variables, or implicitly using regularization techniques to avoid the problem of
multicollinearity. A latent variable is a non-observable variable that is inferred
from other variables by being directly observed and measured. The introduction
of latent variables allows to capture more relevant information about the covari-
ates matrix, denoted by X, or information about the structure of the interaction
between X and the response variables matrix, denoted by Y.
     In this approach, latent, non-correlated variables are introduced, denoted by
t1 , t2 , . . . , ta and u1 , u2 , . . . , ua , where a is the number of componets retained. The
use of latent variables allows for the factorization of low ranges of the predictor
and/or the response matrix, which allows for the adjustment of a linear regression
model by least squares upon this set of latent variables.
   The vectors loadings pk and qk , with k = 1, 2, . . . , a, generate a-dimensional
spaces, where the coefficients tk nÃ—1 and uk nÃ—1 are considered as latent variables.
Among the approaches based on latent variables are PCR and PLS regression,
which are briefly described below.
    In PC regression, which was introduced in Massy (1965), latent variates called
principal components are obtained out of the correlation matrix X, denoted by R.
PC regression avoids the problem of multicollinearity by reducing the dimension
of the predictors. The loadings {pk }ak=1 are taken as a-first eigenvectors of the
spectral decomposition of R matrix, and these vectors are the directions that
maximize the variance of the principal components. The principal components
are defined using the projections of the Xâ€™s upon these directions. That is, the
ith principal component of X is defined as tk = Xpk so that pk maximizes the
variance of tk ,
                       maxhXpk , Xpk i = max pTk XT Xpk
                            pk                     pk

with pTk pk    = 1 y pTk pl      = 0, l < k. The principal components represent the
selection of a new coordinate system obtained when rotating the original system of
axes, X1 , X2 , . . . , Xp . All of the loadings or principal directions are then obtained,
P = [p1 |p2 | Â· Â· Â· |pa ]pÃ—a , as are the projections of the Xi0 s on p0k s, that is, all of the
principal components, T = [t1 |t2 | Â· Â· Â· |ta ]nÃ—a , with the restrictions htk , tl i = 0 and
htk , tk i = V ar(tk ) = Î»k , with Î»k : the eigenvalues associated with the eigenvectors
Pk with Î»1 â‰¥ Î»2 â‰¥ . . . , Î»a . A regression model of Y is then adjusted against
the latent variates T. Then, the response for Y-new ones is predicted associated
with new observations of the predictors vector. In PC regression, the principal
components in the predictor space Xâ€™s are used without taking into account the
information of the responses Yâ€™s.
    PLS regression was introduced in Wold (1975) and applied in the economic
and social sciences fields. However, due to the contributions made by his son
in Wold, Albano, Dunn, Edlund, Esbensen, Geladi, Hellberg, Johansson, Lind-
berg & SjÃ¶strÃ¶m (1984), it gained great popularity in the area of chemometrics,
where data characterized by many predictor variables with multicollinearity prob-
lems and few available observations are analyzed. This happens in many studies
of imaging analysis. The PLS regression methodology generalizes and combines


                                         Revista Colombiana de EstadÃ­stica 36 (2013) 177â€“192

182                                     RaÃºl Alberto PÃ©rez & Graciela GonzÃ¡lez-Farias


characteristics of Principal Component Analysis (PCA) and Multiple Regression
Analysis (MLR). Its demand and evidence has increased and it is being applied in
many scientific areas. PLS regression is similar to the canonic correlation analysis
(CCA), but instead of maximizing the correlation, it maximizes the covariance
between the components. That is, p and q directions are found so that

                           maxhXp, Yqi = max pT XT Yq
                            p,q               p,q


subject to kpk = kqk = 1
    In general, the PLS regression is a two-phase process. First, the predictor ma-
trix X is transformed with the help of the vector of response variables, Y, in a
matrix of latent, non-correlated variables T = (t1 , t2 , . . . , tp ), called PLS compo-
nents. This distinguishes it from the PLS regression, in which the components
are obtained using only the predictor matrix, X. Second, the estimated regression
model is adjusted using the original response vector and the PLS components as
predictors, and then, response for Yâ€™new ones associated with future observations
of the repetition vector are of predict. A reduction of dimensionality is obtained
directly on the PLS components because they are orthogonal, and the number of
components necessary for the regression analysis is much lower than the number
of original predictors. The process of maximizing the covariance instead of the
correlation prevents the possible problem of numeric instability that can appear
when using correlation, which is due to the division of covariances by variances
that may be too small. The directions of the maximum covariance p and q among
the PLS components can be found by the following eigen-decomposition problem:

                    XT YYT Xp = Î»p and YT XXT Yq = Î»q

with kpk = kqk = 1. The latent variates (or PLS components) are calculated
by projecting the X and Y data in the p and q directions, that is, t = Xp and
u = Yq results in all latent components being obtained such that T = XP and
U = YQ.


3. Geometrical Structure of Sym+ (m)
    A summary will now be given of some of the basic results of (Schwartzman
2006) on the geometric structure of the Sym+ (m) set as a Riemannian manifold.
The Sym+ (m) space is a sub-manifold of the Euclidian space Sym(m). Geometri-
cally, the Sym+ (m) and Sym(m) spaces are differential manifolds of m(m + 1)/2
dimensions, and they are homeomorphically related by an exponential and log-
arithmic transformation matrix. For any matrix A âˆˆ Sym(m), its exponential
                             Pâˆž      k
matrix is given by exp(A) = k=1 Ak! âˆˆ Sym+ (m). Reciprocally, for any matrix
S âˆˆ Sym+ (m), there is a log(S) = A âˆˆ Sym(m), such that exp(A) = S.
    For responses in Euclidian spaces in non-parametric standard regression mod-
els, E[S|X = x] is estimated. However, for responses on a curved space, the con-
ditional expectancy of S, given x = x, cannot be defined. For Âµ(x) = E[S|X = x],

                                      Revista Colombiana de EstadÃ­stica 36 (2013) 177â€“192

PLS-Regression on SPD Matrices                                                             183

a tangent vector is introduced in Âµ(x) on Sym+ (m). For a small scalar Î´ > 0,
the differentiable map C : (âˆ’Î´, Î´) âˆ’â†’ Sym+ (m), t â†’ C(t), is considered such
that C(0) = Âµ(x). A tangent vector in Âµ(x) is defined as the derivative of the
soft curve C(t), with respect to t, valued at t = 0. The set of all tangent
vectors in Âµ(x) is called the tangent space of Sym+ (m) in Âµ(x), and it is de-
noted by TÂµ(x) Sym+ (m). This space can be identified by a copy of Sym(m).
The TÂµ(x) Sym+ (m) space is equipped with an internal product h . , . i, called
a Riemannian metric, which varies softly from point to point. For example, the
Frobenius metric can be used as a Riemannian metric. For a given Riemannian
metric, hu , vi is calculated for any u and v in TÂµ(x) Sym+ (m), and then, the
length of the soft qcurve C(t) : [t0 , t1 ] âˆ’â†’ Sym+ (m) is calculated, which is equal
               R t1    .     .                 .
to: kC(t)k = t0 hC(t), C(t)idt, where C(t) is the derivative of C(t), with re-
spect to t. A geodesic is a soft curve in Sym+ (m) with tangent vectors that do
not change in length or direction along the curve. For any u âˆˆ TÂµ(x) Sym+ (m),
there is a single geodesic, denoted by Î³Âµ(x) (t; u), with a dominion that contains
                                                     .
the range [0, 1], such that Î³Âµ(x) (0; u) = Âµ(x) and Î³ Âµ(x) (0; u) = u.
   The exponential Riemannian map is defined as

  ExpÂµ(x) : TÂµ(x) Sym+ (m) âˆ’â†’ Sym+ (m) ; u âˆ’â†’ ExpÂµ(x) (u) = Î³Âµ(x) (1; u)                   (1)

The inverse of the exponential Riemannian map, called a Riemannian logarithmic
map, is defined as

         LogÂµ(x) : Sym+ (m) âˆ’â†’ TÂµ(x) Sym+ (m) ; S âˆ’â†’ LogÂµ(x) (S) = u                       (2)

such that ExpÂµ(x) (u) = S. Finally, the shortest distance between 2 points Âµ1 (x)
and Âµ2 (x) in Sym+ (m), is called the geodesic distance and is denoted by
g(Âµ1 (x), Âµ2 (x)), which satisfies

      d2g (Âµ1 (x), Âµ2 (x)) = hLogÂµ1 (x) Âµ2 (x), LogÂµ1 (x) Âµ2 (x)i = kLogÂµ1 (x) Âµ2 (x)k2g   (3)

where d2g (. , .), denoted the geodesic distance.
    The residual from S with respect to Âµ(x), denoted by ÎµÂµ (x), is defined as
ÎµÂµ (x) = LogÂµ(x) S âˆˆ TÂµ(x) Sym+ (m). The vectorization of C = [cij ] âˆˆ Sym(m) is
                                                                    T    m(m+1)
defined as Vecs(C) = c11 c12 . . . c1m c22 . . . c2m . . . cmm âˆˆ R 2 .
The conditional expectancy of S, given x = x, is defined as the matrix Âµ(x) âˆˆ
Sym+ (x), such that

                   E[LogÂµ(x) S|X = x] = E[ÎµÂµ (x)|X = x] = 0mÃ—m                             (4)

where the expectancy is taken component by component with respect to the m(m+
                                                  m(m+1)
1)-vector aleatory multivaried Vecs[LogÂµ(x) S] âˆˆ R 2 .


3.1. Regression Model for Response Data in Sym+ (m)
    Because the DTs are in a non-linear space, it is theoretically and computation-
ally difficult to develop a formal statistical framework that includes estimation


                                        Revista Colombiana de EstadÃ­stica 36 (2013) 177â€“192

184                                         RaÃºl Alberto PÃ©rez & Graciela GonzÃ¡lez-Farias


theory and hypothesis tests where by a set of covariates are used to directly pre-
dict DTs as responses. With the recently developed log-Euclidian metric Arsigny,
Fillard, Pennec & Ayache (2006), DTs can be transformed from non-linear space
into logarithmic matrices in a Euclidian space. Zhu et al. (2009) developed a
regression model with the log-transformation of the DTs as the response. The
model was based on a semi-parametric method, which avoids the specification
of parametric distributions for aleatory log-transformed DTs. Inference processes
have been proposed for estimating the regression coefficients and test statistics of
this model to contrast linear hypotheses of unknown parameters as well as to test
processes based on re-sampling methods to simultaneously evaluate the statisti-
cal significance of linear hypotheses throughout large ROIs. The procedure for
the laying out of the local intrinsic polynomial regression model (RPLI) for SPD
matrices as a response is described below, ver Zhu et al. (2009).
   The procedure to estimate Âµ(x) = E[S|X = x0 ] in the RPLI model will now
be described. Because Âµ(x) is on a curved space, it cannot be directly expand to
Âµ(x) in x = x0 using a Taylor series. Instead, the Riemannian logarithmic map of
Âµ(x) in Âµ(x0 ) on the space TÂµ(x) Sym+ (m) is considered, that is, we are considering
LogÂµ(x0 ) Âµ(x) âˆˆ TÂµ(x) Sym+ (m). Because LogÂµ(x0 ) Âµ(x) occupies a different tangent
space for each value of X, it can be transported from the common tangent space
TIm Sym+ (m) through the parallel transport given by:

  Î¦Âµ(x0 ) : TÂµ(x0 ) Sym+ (m) âˆ’â†’ TIm Sym+ (m);
                                  LogÂµ(x0 ) Âµ(x) âˆ’â†’ Î¦Âµ(x0 ) (LogÂµ(x0 ) Âµ(x)) = Y (x) (5)

Its inverse is given by LogÂµ(x0 ) Âµ(x) = Î¦âˆ’1                          +
                                          Âµ(x0 ) (Y (x)) âˆˆ TÂµ(x0 ) Sym (m).
   For LogÂµ(x0 ) Âµ(x0 ) = Om âˆˆ TÂµ(x0 ) Sym+ (m), because Î¦Âµ(x0 ) (Om ) = Y (x0 ) =
Om and because Y (x) y Y (x0 ) are in the same tangent space TIm Sym+ (m), a
Taylor series expansion can be used for Y (x) in x0 . The following is obtained:
                                                    k0
                                                                          !
                                                    X
                           âˆ’1                âˆ’1         (k)             k
         LogÂµ(x0 ) Âµ(x) = Î¦Âµ(x0 ) (Y (x)) â‰ˆ Î¦Âµ(x0 )    Y (x0 )(x âˆ’ x0 )          (6)
                                                          k=1

with k0 as a whole and Y (k) as the kth derivative of Y (x) with respect to x divided
by por k!. Equivalently,
                                 
  Âµ(x) = ExpÂµ(x0 ) Î¦âˆ’1
                    Âµ(x0 ) (Y (x)) =
                                      k0
                                                                    !!
                                      X
                ExpÂµ(x0 )   Î¦âˆ’1
                             Âµ(x0 )         Y (k) (x0 )(x âˆ’ x0 )k        = Âµ (x, Î±(x0 ), k0 ) (7)
                                      k=1

where Î±(x0 )-contains all the parameters in {Âµ(x0 ), Y (1) (x0 ), . . . , Y (k) (x0 )}.
    For a set of vectors in TÂµ(x) Sym+ (m), various Riemannian metrics can be de-
fined. Among these metrics is the log-Euclidian metric, and some of its basic
properties will now be reviewed. Notations exp(.) and log(.) are used to rep-
resent the exponential and log matrices, respectively; Exp and Log are used to


                                        Revista Colombiana de EstadÃ­stica 36 (2013) 177â€“192

PLS-Regression on SPD Matrices                                                                 185

represent the exponential and logarithmic maps, respectively. The differential of
the logarithmic matrix in Âµ(x) âˆˆ Sym+ (m) is denoted by âˆ‚Âµ(x) log.(u), which acts
on an infinitesimal movement u âˆˆ TÂµ(x) Sym+ (m). The log-Euclidian metric on
Sym+ (m) is defined as:
                                                             
                      hu, vi := tr (âˆ‚Âµ(x) log.u)(âˆ‚Âµ(x) log.v)                 (8)
for u, v âˆˆ TÂµ(x) Sym+ (m).
   The geodesic Î³Âµ(x) (t; u)-is given by:
                                                        
             Î³Âµ(x) (t; u) := exp log(Âµ(x)) + tâˆ‚Âµ(x) log.v , âˆ€t âˆˆ R                             (9)
The differential of the exponential matrix is denoted by âˆ‚log(Âµ(x)) exp.(A), in
log(Âµ(x)) âˆˆ Sym(m) = TÂµ(x) Sym+ (m) which acts on an infinitesimal movement
A âˆˆ Tlog(Âµ(x)) Sym+ (m). The exponential and logarithmic Riemannian maps are
defined, respectively, as follows: for S âˆˆ Sym+ (m),
                                              
  ExpÂµ(x) (u) := exp log(Âµ(x)) + âˆ‚Âµ(x) log.(u) ;
                                  LogÂµ(x) (S) := âˆ‚log(Âµ(x)) exp [log(S) âˆ’ log(Âµ(x))] (10)

   For Âµ(x) and S âˆˆ Sym+ (m), the geodesic distance is given by:
                   d2g (Âµ(x), S) := tr (log Âµ(x) âˆ’ log(S))âŠ—2
                                                            
                                                                                           (11)

with aâŠ—2 = aaT and with a-vector. For two matrices Âµ(x) and Âµ(x0 ) âˆˆ Sym+ (m)
and any uÂµ(x0 ) âˆˆ TÂµ(x0 ) Sym+ (m), the parallel transport is defined as follows:

  Î¦Âµ(x0 ) : TÂµ(x0 ) Sym+ (m) âˆ’â†’ TIm Sym+ (m);
                                                                                               
                                       uÂµ(x0 ) âˆ’â†’ Î¦Âµ(x0 ) (uÂµ(x0 ) ) := âˆ‚Âµ(x0 ) log. UÂµ(x0 )
If uÂµ(x0 ) = LogÂµ(x0 ) Âµ(x) âˆˆ TÂµ(x0 ) Sym+ (m), then
                                             
               Y (x) = Î¦Âµ(x0 ) LogÂµ(x0 ) Âµ(x) = log Âµ(x) âˆ’ log Âµ(x0 )                      (12)

and Âµ(x) = exp [log Âµ(x0 ) + Y (x)].
   The residual of S with respect to Âµ(x) is defined as: ÎµÂµ (x) := log(S)âˆ’log(Âµ(x))
with E[logS|X = x] = log Âµ(x). The model RPLI is defined as:
                               log(S|x) = log(Âµ(x)) + ÎµÂµ (x)                               (13)
with E[ÎµÂµ (x)] = 0, which indicates that E[logS|X = x] = log(Âµ(x)).


4. The PLS Regression Model
   Suppose we have n DTs, denoted by Ti : i = 1, 2, . . . , n, obtained from a voxel
correspondent with a normalized and especially re-oriented DTI from n subjects.
The log-transformation of Tk is then obtained, which is denoted by
              LT,i = (LiT(1,1) , LiT(1,2) , LiT(1,3) , LiT(2,2) , LiT(2,3) , LiT(3,3) )T   (14)


                                           Revista Colombiana de EstadÃ­stica 36 (2013) 177â€“192

186                                        RaÃºl Alberto PÃ©rez & Graciela GonzÃ¡lez-Farias


where LiT(j,k) -denotes the (j, k)-element of the logarithm matrix of Tk . For each
individual, a set of covariates of interest is observed as well.
    In studies of medical images, many demographic or clinical measurements are
normally observed for different patients considered in a certain study. The amount
of available information is abundant, and there may be problems of linear depen-
dences between the covariates of interest, which generates the problem of multi-
collinearity. In addition, available data to analyze the information are scarce. For
the log-transformed DTs, a linear model is considered, which is given by:

                           LT,i = xi Î² + Îµi , i = 1, 2, . . . , n                     (15)
                           1Ã—6    1Ã—ppÃ—6    1Ã—6

or
                                    LT = X B + Îµ                                      (16)
                                    nÃ—6 nÃ—ppÃ—6 nÃ—6

with E[Îµ|x] = 0nÃ—p and Cov(Îµ|x) = Î£npÃ—np and where X, Y=L, B, Îµ and Î£, are
matrices representing the covariates, responses, regression coefficients, the model
errors and covariance of Îµ|x.
    Compared to the general lineal model, the model, based on the conditional
mean and covariance in equation (16)does not assume any distributional supposi-
tions for the image measurements.
    If Î¸ (6p+21)Ã—1 is the vector of unknown parameters contained in Î² and Î£, then
to estimate Î¸, the objective function given by:
                              n
                           1X
                                 log|Î£| + (LT,i âˆ’ Î²xi )T Î£âˆ’1 (LT,i âˆ’ Î²xi )
                                                                           
            ln (Î¸Î¸ ) = âˆ’                                                              (17)
                           2 i=1

is maximized using the iterative algorithm proposed by Li et al. (2009).
   The regression model (16) has been adjusted using existing algorithms for PC
and PLS regression, following the steps described in Section 2.2 and taking into
account the log-transformations on the original data to transfer them to a Euclidian
space.


4.1. Evaluation of the PLS Regression Model with Simulated
     Data
   The behavior of the PLS regression model is evaluated with sets of simulated
data, and predicted results are compared with those obtained using the PC tech-
nique in the case of a design matrix of full range.
    The settings considered to simulate the data are the following. First, a sample
of SPD matrices with a size of n = 20 with k = 15 covariates was generated from
a multivariate normal distribution with a mean of zero and a covariance structure
given by Î£ = 0.6I6 . Then, the sample size was increased to n = 30, and the
number of covariates was increased from k = 15 to k = 40, with a covariance
structure given by Î£ = 0.3I6 + 0.616 1T6 , with 16 , a vector of ones. In both
settings, the values for the coefficients of beta were used in the matrix given by


                                        Revista Colombiana de EstadÃ­stica 36 (2013) 177â€“192

PLS-Regression on SPD Matrices                                                      187

p Ã— 6, Î²k = [1 + 0.1 Ã— (k âˆ’ 1)]T . The exponential of Î£ was calculated to ensure its
positive definiteness. Results obtained in each scenario are expounded below.
    For the first setting, shown in Table 1, the percentages of variance explained
by each of the latent components through PC and PLS regression demonstrate
that PC explains more of the variability of X than PLS regression, which is a
typical result. In Table 2, the PLS components explain a higher percentage of the
variability of Y than the PC components; with two components, more than 80%
of the variability in Y and approximately 20% of the variability in X is explained.
Figure 1 shows the graphs of the square root of the prediction middle quadratic
error (RMSEP) against the number of components used in the cross validation
(CV). Here, it can be observed that in PC, approximately four components would
be needed to explain a majority of the variability in the data. However, in PLS re-
gression, three components are needed in most cases. In general, few repetition are
shown through this illustration of the repetition results obtained by each method,
when compared with the simulation. Figure 2 shows the graphs of the predicted
data with the observed responses. A greater precision in the adjustment can be
observed when PLS regression is used. For the second setting, Table 3 shows the
percentages of variance explained by each of the latent components using PC and
PLS regression. Again, PC explains more of the variability of X than PLS regres-
sion. Table 4 shows that the PLS components explain a greater percentage of the
variability of Y than the PLS components. In five components, more than 60% of
the variability in Y and approximately 35% of the variability in X is explained.
Figure 3 shows the graphs for the RMSEP against the number of components. It
can be observed that in PC, approximately 7 components would be needed to ex-
plain most of the variability of the data, while in PLS regression, five components
are needed in most cases. Figure 4 shows the graphs of the predicted data along
with the observed values of the responses; a greater precision in the adjustment
can be observed when PLS regression is used.

           Table 1: Percentages of variance explained by each component.
        Comp 1     Comp 2   Comp 3   Comp 4   Comp 5    Comp 6   Comp 7    Comp 8
    PC   17.57      15.55    13.59    12.46    11.16     9.16     6.81      4.64
    PLS  14.27      9.93     10.16    13.45    12.60     5.75     4.46      7.07




                                     Revista Colombiana de EstadÃ­stica 36 (2013) 177â€“192

188                                                                                         RaÃºl Alberto PÃ©rez & Graciela GonzÃ¡lez-Farias

                      Y1                            Y2                             Y3                          Y1                            Y2                           Y3
        3.2
                                   3.6                                                                                      3.5
                                                                                                 3.0                                                          4
        3.0                        3.4
                                                                                                                            3.0
                                                                     4.0
                                                                                                 2.5
                                   3.2
        2.8                                                                                                                 2.5                               3

                                   3.0                                                           2.0
        2.6                                                                                                                 2.0
                                                                     3.5
                                   2.8
                                                                                                                                                              2
                                                                                                 1.5                        1.5
        2.4                        2.6

                                                                                                                            1.0
                                   2.4                                                           1.0
        2.2                                                          3.0                                                                                      1

                                                                                                                            0.5
                                   2.2

                                                                                                       0   2   4    6   8          0    2    4    6       8       0   2   4    6   8
RMSEP




              0   2   4    6   8          0    2    4    6       8         0   2   4    6   8


                      Y4                            Y5                             Y6                          Y4                            Y5                           Y6
                                                                                                                             5

        4.5
                                                                                                  4
                                                                     4.5                                                                                      4
                                   4.5                                                                                       4


        4.0                                                                                       3
                                                                                                                                                              3
                                                                     4.0                                                     3
                                   4.0


                                                                                                  2                                                           2
        3.5                                                                                                                  2
                                                                     3.5
                                   3.5

                                                                                                  1                                                           1
                                                                                                                             1
        3.0
                                                                     3.0
                                   3.0

              0   2   4    6   8          0    2    4    6       8         0   2   4    6   8          0   2   4    6   8          0    2    4    6       8       0   2   4    6   8

                                         NÃºmero de Componentes                                                                    NÃºmero de Componentes


                               (a) PC regression                                                                        (b) PLS regression
Figure 1: RMSEP versus number of components by PC regression and PLS regression


Table 2: Percentages of variance explained cumulated of X and Y for the components
         by PC and PLS regression.
                               Comp 1 Comp 2 Comp 3 Comp 4                                                 Comp 5           Comp 6                Comp 7 Comp 8
              PC           X     17.57  33.11  46.70  59.16                                                  70.32            79.48                 86.28  90.93
              PLS          X     14.27  24.20  34.37  47.82                                                  60.43            66.17                 70.64  77.70
              PC          Y1      7.69  18.38  33.74  51.79                                                  52.72            52.91                 54.64  57.04
              PLS         Y1     66.85  82.64  88.85  89.51                                                  90.13            91.21                 95.17  95.26
              PC          Y2     14.95  22.65  36.98  58.96                                                  60.99            61.09                 62.21  62.29
              PLS         Y2     74.87  87.30  96.16  96.35                                                  96.46            97.01                 98.04  98.05
              PC          Y3      7.45  20.12  34.30  55.21                                                  56.38            56.43                 56.44  56.77
              PLS         Y3     70.51  88.00  94.72  95.05                                                  96.33            97.57                 97.67  97.78
              PC          Y4      7.30  19.10  41.57  58.71                                                  60.19            60.20                 61.57  61.78
              PLS         Y4     74.39  91.05  95.78  96.90                                                  96.92            97.87                 99.36  99.39
              PC          Y5      7.44  19.65  45.13  60.30                                                  60.66            61.30                 62.61  62.93
              PLS         Y5     74.38  89.10  93.22  95.70                                                  96.19            96.62                 97.51  98.38
              PC          Y6     13.89  20.83  40.31  62.35                                                  63.45            63.46                 63.46  63.47
              PLS         Y6     77.35  90.32  97.51  97.60                                                  97.63            99.12                 99.12  99.38


                           Table 3: Percentages of variance explained by each component, 2.
      Comp 1 Comp 2 Comp 3 Comp 4 Comp 5 Comp 6 Comp 7 Comp 8 Comp 9 Comp 10
   PC   12.81   9.33   8.74   7.42   7.22   6.39   6.33   5.12   4.97   4.44
  PLS   10.63   8.65   6.39   5.21   3.85   5.34   4.88   5.36   5.32   5.00



5. Conclusions and Recommendations
    A PLS linear regression model is proposed in this article to study the relation-
ship between a large set of covariates of interest in a Euclidian space with a set of
response variables in a symmetric Riemannian space. The theory of exponential
and Riemannian maps has been used to transform data from a non-Euclidian space
into a Euclidian space of symmetrical matrices, where the methodology has been
developed. Results indicate support for the proposed methodology as compared to


                                                                                        Revista Colombiana de EstadÃ­stica 36 (2013) 177â€“192

PLS-Regression on SPD Matrices                                                                                                             189

a technique using regression by major components, as has been observed in classic
situations of data analysis in euclidean spaces with matrices of covariates present-
ing high multicollinearity, or in problems with a low number of observations and
many covariates. In future works, we will investigate more realistic models, such
as non-linear PLS models for the types of SPD matrix data discussed in this study
and other types of manifold-valued data, such as data obtained by geometric repre-
sentations of objects via medial axial representation (m-rep), orthogonal rotation
groups, and other methods. The illustration presented in this article for simulated
data favorably sheds light on the results that can be obtained by applying these
types of models to real data.

        4                                 6                                  4                                 6
                                          4                                                                    4
        2                                 2                                  2                                 2
  Y1




                                    Y2




                                                                       Y1




                                                                                                         Y2
        0                                 0                                  0                                 0
       âˆ’2                                âˆ’2                                 âˆ’2                                âˆ’2
       âˆ’4                                âˆ’4                                 âˆ’4                                âˆ’4

            5        10        15             5        10         15             5        10        15             5        10        15




        5                                 5                                  5                                 5
  Y3




                                    Y4




                                                                       Y3




                                                                                                         Y4
        0                                 0                                  0                                 0

       âˆ’5                                âˆ’5                                 âˆ’5                                âˆ’5

            5        10        15             5        10         15             5        10        15             5        10        15




       10                                                                   10
        5                                 5                                  5                                 5
  Y5




                                    Y6




                                                                       Y5




                                                                                                         Y6
        0                                 0                                  0                                 0
       âˆ’5                                âˆ’5                                 âˆ’5                                âˆ’5

            5        10        15             5        10         15             5        10        15             5        10        15

                Observations                      Observations                       Observations                      Observations


                        (a) PC regression                                                  (b) PLS regression
Figure 2: Predicted values with the observables values by PC regression and PLS re-
          gression. Solid lines: Observed, dashed lines: Predicted.



Table 4: Percentages of variance explained cumulated of X and Y for the components
         by PC and PLS regression, 2.
            Comp 1 Comp 2 Comp 3 Comp 4 Comp 5 Comp 6 Comp 7 Comp 8 Comp 9 Comp 10
 PC X         12.81  22.14  30.88  38.30  45.52  51.90  58.23  63.35  68.33   72.77
 PLS X        10.63  19.28  25.67  30.88  34.73  40.07  44.95  50.32  55.64   60.64
 PC Y1        26.52  50.85  51.17  56.31  59.51  59.69  79.40  80.74  80.83   82.65
 PLS Y1       83.70  93.81  97.39  98.80  99.37  99.59  99.66  99.66  99.66   99.67
 PC Y2        26.97  51.87  51.99  57.41  61.87  62.25  80.86  82.42  82.52   83.83
 PLS Y2       84.85  94.65  97.57  98.58  99.09  99.19  99.37  99.72  99.74   99.74
 PC Y3        24.82  50.72  51.34  57.02  61.40  61.56  81.08  82.05  82.05   83.70
 PLS Y3       83.92  95.16  97.72  98.91  99.38  99.38  99.52  99.54  99.70   99.73
 PC Y4        27.00  51.74  52.05  57.50  61.39  61.65  80.51  81.84  81.99   84.23
 PLS Y4       84.74  94.50  97.54  98.67  99.23  99.44  99.66  99.73  99.74   99.81
 PC Y5        25.11  50.70  50.90  56.36  59.61  59.97  81.14  81.93  81.96   83.96
 PLS Y5       83.80  94.97  97.77  98.77  99.14  99.37  99.38  99.54  99.74   99.75
 PC Y6        26.75  53.38  53.80  59.58  63.02  63.15  82.70  83.96  84.18   85.90
 PLS Y6       86.10  95.97  98.12  99.03  99.37  99.53  99.69  99.71  99.73   99.85




                                                                 Revista Colombiana de EstadÃ­stica 36 (2013) 177â€“192

190                                                                                                                                       RaÃºl Alberto PÃ©rez & Graciela GonzÃ¡lez-Farias

                                       Y1                                          Y2                                            Y3                                        Y1                                             Y2                                                   Y3

           7                                                                                                                                         7
                                                                                                                                                                                                                                                       8
                                                                                                              8                                                                                     7
                                                                 7
                                                                                                                                                     6
                                                                                                                                                                                                    6
           6                                                                                                  7
                                                                                                                                                     5                                                                                                 6
                                                                 6                                                                                                                                  5

                                                                                                              6                                      4
           5                                                                                                                                                                                        4
                                                                 5                                                                                                                                                                                     4
                                                                                                                                                     3
                                                                                                                                                                                                    3
                                                                                                              5
           4                                                                                                                                         2
                                                                 4                                                                                                                                  2
                                                                                                                                                                                                                                                       2
                                                                                                              4
                                                                                                                                                     1                                              1
           3                                                     3
  RMSEP




                   0       2       4        6    8     10             0    2   4        6       8   10             0    2    4        6    8   10         0        2   4        6        8    10         0        2   4        6     8       10            0      2        4        6    8        10


                                       Y4                                          Y5                                            Y6                                        Y4                                             Y5                                                   Y6
           9
                                                                                                             10                                                                                                                                       10
                                                                 9                                                                                   8
           8                                                                                                                                                                                        8
                                                                                                              9
                                                                                                                                                                                                                                                       8
                                                                 8
           7                                                                                                  8                                      6
                                                                                                                                                                                                    6
                                                                 7                                                                                                                                                                                     6
                                                                                                              7
           6
                                                                                                                                                     4
                                                                 6                                                                                                                                  4                                                  4
                                                                                                              6
           5
                                                                 5                                                                                   2
                                                                                                              5                                                                                     2                                                  2
           4
                                                                 4                                            4


                   0       2       4        6    8     10             0    2   4        6       8   10             0    2    4        6    8   10         0        2   4        6        8    10         0        2   4        6     8       10            0      2        4        6    8        10

                                                                     NÃºmero de Componentes                                                                                                              NÃºmero de Componentes


                                                       (a) PC regression                                                                                                                      (b) PLS regression
Figure 3: RMSEP versus number of components by PC regression and PLS regression,
          2.



           15                                                                            15                                                               15                                                                        15
           10                                                                            10                                                               10                                                                        10
            5                                                                             5                                                                5                                                                         5
  Y1




                                                                               Y2




                                                                                                                                                Y1




                                                                                                                                                                                                                          Y2
            0                                                                             0                                                                0                                                                         0
           âˆ’5                                                                            âˆ’5                                                               âˆ’5                                                                        âˆ’5
          âˆ’10                                                                           âˆ’10                                                              âˆ’10                                                                       âˆ’10

                       0       5            10    15        20        25                        0        5        10    15       20       25                   0       5            10       15    20        25                          0        5        10         15            20       25




           20                                                                            20                                                               20                                                                        20
           15                                                                                                                                             15
           10                                                                            10                                                               10                                                                        10
            5                                                                                                                                              5
  Y3




                                                                               Y4




                                                                                                                                                Y3




                                                                                                                                                                                                                          Y4
            0                                                                               0                                                              0                                                                         0
           âˆ’5                                                                                                                                             âˆ’5
          âˆ’10                                                                           âˆ’10                                                              âˆ’10                                                                       âˆ’10
          âˆ’15                                                                                                                                            âˆ’15
                       0       5            10    15        20        25                        0        5        10    15       20       25                   0       5            10       15    20        25                          0        5        10         15            20       25




           20                                                                            20                                                               20                                                                        20
           10                                                                            10                                                               10                                                                        10
  Y5




                                                                               Y6




                                                                                                                                                Y5




                                                                                                                                                                                                                          Y6




               0                                                                          0                                                                0                                                                         0
          âˆ’10                                                                           âˆ’10                                                              âˆ’10                                                                       âˆ’10
                                                                                        âˆ’20                                                                                                                                        âˆ’20
                       0       5            10    15        20        25                        0        5        10    15       20       25                   0       5            10       15    20        25                          0        5        10         15            20       25

                                            Observations                                                          Observations                                                      Observations                                                               Observations


                                                     (a) PC regression                                                                                                                       (b) PLS regression
Figure 4: Predicted values with the observables values by PC regression and PLS re-
          gression, 2. Solid lines: Observed, dashed lines: Predicted.




                                                                                                                             Revista Colombiana de EstadÃ­stica 36 (2013) 177â€“192

PLS-Regression on SPD Matrices                                                      191

                   Recibido: junio de 2012 â€” Aceptado: mayo de 2013
                                                                     




References
Arsigny, V., Fillard, P., Pennec, X. & Ayache, N. (2006), â€˜Log-euclidean metrics for fast and simple calculus on diffusion tensorsâ€™, Magnetic Resonance in Medicine, 56, 411â€“421.
Barmpoutis, A., Vemuri, B. C., Shepherd, T. M. & Forder, J. R. (2007), â€˜Tensor splines for interpolation and approximation of DT-MRI with applications to segmentation of isolated rat hippocampiâ€™, IEEE Transations on Medical Imaging, 26, 1537â€“1546.
Batchelor, P., Moakher, M., Atkinson, D., Calamante, F. & Connelly, A. (2005), â€˜A rigorous framework for diffusion tensor calculusâ€™, Magnetic Resonance in Medicine, 53, 221â€“225.
Fingelkurts, A. A. & Kahkonen, S. (2005), â€˜Functional connectivity in the brain -is it an elusive concepts?â€™, Neuroscience and Biobehavioral Reviews, 28, 827â€“836.
Fletcher, P. T. & Joshi, S. (2007), â€˜Riemannian geometry for the statistical analysis of diffusion tensor dataâ€™, Signal Processing, 87, 250â€“262.
Grenander, U. & Miller, M. I. (1998), â€˜Computational anatomy: An emerging disciplineâ€™, Quarterly of Applied Mathematics, 56, 617â€“694.
Kim, P. T. & Richards, D. S. (2010), â€˜Deconvolution density estimation on spaces of positive definite symmetric matricesâ€™, IMS Lecture Notes Monograph Series. A Festschrift of Tom Hettmansperger .
Lepore, N., Brun, C. A., Chou, Y., Chiang, M., Dutton, R. A., Hayashi, K. M., Luders, E., Lopez, O. L., Aizenstein, H. J., Toga, A. W., Becker, J. T. & Thompson, P. M. (2008), â€˜Generalized tensor-based morphometry of HIV/AIDS using multivariate statistics on deformation tensorsâ€™, IEEE Transactions in Medical Imaging, 27, 129â€“141.
Li, Y., Zhu, H., Chen, Y., Ibrahim, J. G., An, H., Lin, W., Hall, C. & Shen, D. (2009), RADTI: Regression analysis of diffusion tensor images, in E. Samei & J. Hsieh, eds, â€˜Progress in Biomedical Optics and Imaging - Proceedings of SPIEâ€™, Vol. 7258.
Massy, W. F. (1965), â€˜Principal components regression in exploratory statistical researchâ€™, Journal of the American Statistical Association, 64, 234â€“246.
Pennec, X., Fillard, P. & Ayache, N. (2006), â€˜A Riemannian framework for tensor computingâ€™, International Journal of Computer Vision, 66, 41â€“66.
Schwartzman, A. (2006), Random ellipsoids and false discovery rates: Statistics for diffusion tensor imaging data, PhD thesis, Stanford University.
Wold, H. (1975), â€˜Soft modeling by latent variables; the non-linear iterative partial least squares approachâ€™, Perspectives in Probability and Statistics, pp. 1â€“2.
Wold, S., Albano, C., Dunn, W.J., I., Edlund, U., Esbensen, K., Geladi, P., Hellberg, S., Johansson, E., Lindberg, W. & SjÃ¶strÃ¶m, M. (1984), Multivariate data analysis in chemistry, in B. Kowalski, ed., â€˜Chemometricsâ€™, Vol. 138 of NATO ASI Series, Springer Netherlands, pp. 17â€“95.
Yuan, Y., Zhu, H., Lin, W. & Marron, J. S. (2012), â€˜Local polynomial regression for symmetric positive-definite matricesâ€™, Journal of the Royal Statistical Society: Series B (Statistical Methodology) 74(4), 697â€“719.
Zhu, H. T., Chen, Y. S., Ibrahim, J. G., Li, Y. M. & Lin, W. L. (2009), â€˜Intrinsic regression models for positive-definite matrices with applications to diffusion tensor imagingâ€™, Journal of the American Statistical Association 104, 1203â€“1212.
