Improved Linear Combination of Two Estimators for a Function of Interested Parameter. Estimación eficiente de una función de un parámetro a través de una combinación lineal de dos estimadores
Imam Khomeini International University, Qazvin, Iran
Abstract
In this paper, we consider the problem of improving the efficiency of a linear combination of two estimators when the population coefficient of variation is known. We generalized the discussion from the case of a parameter to a function of are interested parameter. We show that two estimators obtained from a improved linear combination of two estimators and a linear combination of two improved estimators are equivalent in terms of efficiency. We also show how a doubly-improved linear combination of two estimators can be constructed when the population coefficient of variation is known.
Key words: Coefficient of variation, Mean squared error, Efficiency, Linear combination.
Resumen
En este artículo, se considera el problema de mejorar la eficiencia de una combinación lineal de dos estimadores cuando el coeficiente de variación poblacional es conocido. Se generaliza el caso de un solo parámetro al de una función del parámetro. Se muestra que hay equivalencia, en términos de eficiencia, entre usar combinaciones lineales mejoradas y combinaciones lineales de estimadores mejorados. También se muestra como construir una combinación lineal doblemente mejorada cuando el coeficiente de variación poblacional es conocido.
Palabras clave: coeficiente de variación, combinación lineal, eficiencia, error cuadrado medio.

1. Introduction
    In many practical inferential studies some prior information such as coefficient
of variation (CV), kurtosis or skewness of population are available in advance.
Using prior information to improve the efficiency of a given estimator has been
considered in literature, repeatedly Searls (1964) and Arnholt & Hebert (2001)
proposed an improved estimators for the population mean given the population
CV. Wencheko & Wijekoon (2007) improved their results and obtained a shrunken
estimator for the mean of one parameter exponential families. Also, given the
population CV, Khan (1968) constructed a convex combination of two uncorre-
lated and unbiased estimators of the population mean with minimum mean square
error (MSE). Improved estimators for the population variance that utilize the
population kurtosis have been discussed by many authors notably Searls (1964),
Kleffe (1985), Searls & Intarapanich (1990), Kanefuji & Iwase (1998), Wencheko
& Chipoyera (2005) and Subhash & Cem (2013). In this regard, Laheetharan &
Wijekoon (2010) proposed an improved estimator for the population variance and
compared it with other estimators based on the scaled squared error loss function.
The problem of finding improved estimators given an additional information has
also been considered, for situations in which the dimension of sufficient statistics
is grater than the dimension of the interested parameter. Gleser & Healy (1976)
considered the problem of minimizing the MSE of a non-convex combination of
two uncorrelated and unbiased estimators given a known population coefficient of
variation. Samuel-Cahn (1994) expand their solution to a more general case for
two correlated and unbiased estimators. Also, Arnholt & Hebert (1995) discussed
non-convex combination of two correlated and biased estimators for an unknown
parameter when the CV of both two estimators are known. It should be noted
that the process of finding improved estimator usually leads to a biased estimator;
therefore, the MSE criterion plays a main role in all results due to its empha-
sis on both variance and biasness of estimators. Some important results related
to improving biased estimators are given by Bibby (1972), Bibby & Toutenburg
(1977) and Bibby & Toutenburg (1978). The following theorems provide some of
the most important results related to the problem of finding improved estimators
in the presence of some prior information.

Theorem 1 (Arnholt & Hebert, 2001 and Laheetharan & Wijekoon, 2011).
                          0
Let X = (X1 , . . . , Xn ) be a random sample from a population with distribu-
tion f (x|θ) and T1 (X) and T2 (X) be estimators of θ, possibly correlated with
E(Ti (X)) = ki θ, i = 1, 2. Suppose that the ratios νi = V ar(T
                                                              θ2
                                                                 i)
                                                                    , i = 1, 2 are
free from θ and V ar(T1 (X)) < V ar(T2 (X)). Under these conditions, the estima-
tor T ∗ (X) = α1∗ T1 (X) + α2∗ T2 (X) uniformly has the minimum MSE among all
estimators that are linear in T1 (X) and T2 (X), where

                                         1 − ρλ
                    α1∗ =                                         ,
                          k1 (1 − 2ρλ + λ2 + (1 − ρ2 )(ν1 /k12 ))
                                       λ(λ − ρ)
                    α2∗ =                                         .
                          k2 (1 − 2ρλ + λ2 + (1 − ρ2 )(ν1 /k12 ))


                                     Revista Colombiana de Estadística 39 (2016) 229–245

Improved Linear Combination of Two Estimators                                             231

                     k2 ν
Furthermore, λ2 = k22 ν12 and ρ = √ Cov(T1 (X),T2 (X))            are known and free from θ.
                      1                V ar(T1 (X))V ar(T2 (X))
                                                                                      0
Theorem 2 (Laheetharan & Wijekoon, 2010). Let X = (X1 , . . . , Xn ) be a
random sample from a population with distribution f (x|θ) and g(θ) be a real-
valued function on the parameter space Θ. Let T1 (X) and T2 (X) be point es-
timators of g(θ) with E(Ti (X)) = ki g(θ), where ki ∈ <. Then, the estimators
Ti∗ (X) = αi∗ Ti (X), i = 1, 2 uniformly have the minimum MSE among all esti-
mators in class of CTi (αi ) = {αi Ti (X) | 0 < αi < ∞}, where αi∗ = k2k+τ
                                                                         i
                                                                           2 and
                                                                                  i   i

τi2 = V ar(T i (X))
         [g(θ)]2    , i = 1, 2 are free from θ. Furthermore, if k2 < k1 (τ2 /τ1 ) then
  ∗
T1 (X) has smaller MSE and if k2 > k1 (τ2 /τ1 ), then T2∗ (X) has smaller MSE.

    In this paper, we consider the problem of improving the efficiency of a linear
combination of two estimators, when the population CV is known. The rest of pa-
per is organized as follows: in Section 2, we briefly review the main results related
to the improved linear combination of estimators. In Section 3 we generalized the
discussion from the case of a parameter to a function of an interested parameter by
expanding the results of Gleser & Healy (1976) and and Arnholt & Hebert (2001).
In section 4, we show that two estimators obtained from a improved linear combi-
nation of two estimators and a linear combination of two improved estimators are
the same in terms of efficiency. In section 5, we show that how a doubly-improved
linear combination of two estimators can be construct when the population CV is
known. In section 6, we provide some illustrative examples.


2. Improved Linear Combination of Two Estimators
   for a Parameter
   In this section, we briefly review the main results related to an improved linear
combination of estimators, when some additional information is available.
    Using some prior information may reduce the dimension of parameter space.
For example, when the coefficient of variation ν = σµ is known, the distribution of
N (µ, σ 2 ), µ 6= 0 can be written as N (µ, ν 2 µ2 ) due to the equation σ 2 = ν 2 µ2 . It
can be seen that the dimension of sufficient statistics, (X̄, S 2 ), is more than the
dimension of the parameter of interest, µ. In this situation using only a part of
the sufficient statistics leads to a loss of some information about the parameter
of interest. Therefore, the simultaneous use of two or more estimator is necessary
to achieve more possible information about the parameter of interest. One can
use a combination of estimators to construct an efficient estimator. Khan (1968)
proposed the optimal combinations of two independent and unbiased estimators of
the population mean when the sampling distribution is normal and the population
coefficient of variation, ν, is known. Consider T1 (X) = X̄n , T2 (X) = cn S, cn =
(n1/2 Γ((n − 1)/2))/((2a)1/2 Γ(n/2)), as two unbiased and√independent estimators
for µ, where S is the sample standard deviation and a = ν. Then, the shrinkage
estimator
                            T (X) = α∗ X̄ + (1 − α∗ )cn S,

                                       Revista Colombiana de Estadística 39 (2016) 229–245

232                                                         Afshin Fallah & Hamid Khoshtarkib


is the optimal combination of estimators X̄ and cn S, where α∗ = dn /(dn + n−1 a)
and dn = [n−1 (n − 1)acn 2 − 1]. Of course, it is not necessary to restrict these
combinations to be convex. Gleser & Healy (1976) considered a more general case
with T = α1 T1 + α2 T2 , where Ti are any independent and unbiased estimators of
θ and α1 + α2 is not necessarily equal to 1. The only restriction is that the ratios
νi2 = θ2 V ar(Ti ), i = 1, 2 are free from θ, where νi denotes the CV of estimator
Ti . This restriction holds, for example, when the Ti , i = 1, 2 are unbiased and
ν is known. Since the estimator T is not necessarily convex, it is not necessarily
an unbiased estimator for θ. The authors showed that the optimal weights in this
case are given by
                             ν2                                  ν1
               α1∗ =                       ,       α2∗ =                   .
                       ν1 + ν2 + ν1 ν2                     ν1 + ν2 + ν1 ν2
Samuel-Cahn (1994) studied another generalized case of optimizing a convex com-
bination of two unbiased, dependent estimators with a known correlation coeffi-
cient ρ. They derived the optimal weight as α∗ = (1 − ρλ)/(1 − 2ρλ + λ2 ), where
λ2 = V ar(T1 )/V ar(T2 ). The authors assumed that λ2 is known and free from θ.
It should be noted that when the estimators CV are known and free from θ for
both estimators, this restriction is held.


3. Improved Linear Combination of Two Estimators
   for a Function of a Parameter
     In a population with distribution f (x|θ) there are different interested param-
eters such as mean, variance, etc. these appear as different functions of θ, hence
it is interesting to look for improved estimators for a function of a parameter. In
recent years some authors, notably Laheetharan & Wijekoon (2010), have con-
sidered the problem of finding improved estimators for a function of an interested
parameter, say g(θ). In this section, we derived an optimal shrinkage estimator for
a function of a parameter with assumption of known population CV. The following
lemma, which is left without proof, provides a preliminary necessary fact for the
next theorem.
Lemma 1. Let T (X) be an estimator of parameter θ and g(·) be a real valued
function, where E(T (X)) = kg(θ). If the population CV is known, then the ratio
τ 2 = V ar(T (X))
        [g(θ)]2   is free from θ.

   Using the Lemma 1, we improved the Gleser & Healy (1976) and Arnholt
& Hebert (2001) results to estimate a function of parameter, g(θ), in the next
theorem.
                                               0
Theorem 3. Let X = (X1 , . . . , Xn ) be a random sample from a population
with distribution f (x|θ) and let T1 (X) and T2 (X) be estimators for g(θ), possi-
bly correlated with E(Ti (X)) = ki g(θ), and i = 1, 2. Under these conditions,
  ∗
TLC (X) = α1∗ T1 (X) + α2∗ T2 (X) uniformly has the minimum MSE among all es-
timators in the class CT1 ,T2 (α1 , α2 ) = {α1 T1 (X) + α2 T2 (X) | 0 < α1 , α2 < ∞},


                                         Revista Colombiana de Estadística 39 (2016) 229–245

Improved Linear Combination of Two Estimators                                                  233

where
                                            1 − ρλ
                       α1∗ =                                          ,
                             k1 (1 − 2ρλ + λ2 + (1 − ρ2 )(τ12 /k12 ))
                                                                                                (1)
                                           λ(λ − ρ)
                       α2∗ =                                          .
                             k2 (1 − 2ρλ + λ2 + (1 − ρ2 )(τ12 /k12 ))
                                          k2 τ 2
In addition, τi2 = V ar(T i (X))
                      [g(θ)]2    , λ2 = k22 τ12 and ρ = √ Cov(T1 (X),T2 (X))              are known
                                           1 2                V ar(T1 (X))V ar(T2 (X))
and free from θ.

Proof . Let TLC (X) = α1 T1 (X) + α2 T2 (X); 0 < α1 , α2 < ∞. Without loss of
generality, we assume that k1 , k2 > 0, then
                M SE(TLC (X)) = V ar(TLC (X)) + bias2 (TLC (X))
                                     = V ar(α1 T1 (X) + α2 T2 (X))
                                    + [E(α1 T1 (X) + α2 T2 (X)) − g(θ)]2
                                                                                                (2)
                                    = α12 V ar(T1 (X)) + α22 V ar(T2 (X))
                                              p
                                    + 2α1 α2 ρ V ar(T1 (X))V ar(T2 (X))
                                    + (α1 k1 + α2 k2 − 1)2 [g(θ)]2 .
Differentiating (2) with respect to α1 and α2 and equating it to zero leads to the
following system of equations:
    ∂M SE(T (X))                                   p
              LC
   
           ∂α1       = 2α1 V ar(T1 (X))) + 2α2 ρ V ar(T1 (X))V ar(T2 (X))
                         +2k1 [g(θ)]2 (α1 k1 + α2 k2 − 1) = 0,
   
   
                                                                               (3)
    ∂M SE(T  LC (X))
                                                    p
   
           ∂α2       = 2α2 V ar(T2 (X))) + 2α1 ρ V ar(T1 (X))V ar(T2 (X))
                         +2k2 [g(θ)]2 (α1 k1 + α2 k2 − 1) = 0.
   

The solutions of equations (3) are given by
                                      2
                                              p
             ∗    (1 − α2∗ k2 )k1 g(θ) − α2∗ ρ V ar(T1 (X))V ar(T2 (X))
            α1 =                                          2
                                   V ar(T1 (X)) + k12 g(θ)
                                                                                                (4)
               k1 − α2∗ (k1 k2 + ρτ1 τ2 )
                       k12 + τ12

                                       2
                                                   p
                   (1 − α1∗ k1 )k2 g(θ) − α1∗ ρ V ar(T1 (X))V ar(T2 (X))
             α2∗ =                                         2
                                    V ar(T2 (X)) + k22 g(θ)
                                                                                                (5)
                          ∗
                   k2 − α1 (k1 k2 + ρτ1 τ2 )
                 =                           .
                           k22 + τ22
Substituting (4) in (5), we have
                       k1 −α∗
                            2 (k1 k2 +ρτ1 τ2 )
                   k2 −       k12 +τ12
                                               (k1 k2 + ρτ1 τ2 )
             ∗
            α2 =
                                   k22 + τ22
                 k2 τ1 − ρk1 τ1 τ2 + α2∗ k12 k22 + 2α2∗ ρk1 k2 τ1 τ2 + α2∗ ρ2 τ12 τ22
                     2
               =                                                                      .
                                       (k12 + τ12 )(k22 + τ22 )


                                          Revista Colombiana de Estadística 39 (2016) 229–245

234                                                      Afshin Fallah & Hamid Khoshtarkib


Therefore
                                       k2 τ12 − ρk1 τ1 τ2
             α2∗ =
                     (k1 + τ1 )(k2 + τ2 ) − k12 k22 − 2ρk1 k2 τ1 τ2 − ρ2 τ12 τ22
                       2    2    2    2

                                  λ(λ − ρ)
                =                                 τ    2    .
                     k2 (1 + λ2 − 2ρλ − (1 − ρ2 ) k12 )
                                                    1


Similarly, we have
                                          λ(λ−ρ)
                          k1 −                         τ2
                                                              (k1 k2 + ρτ1 τ2 )
                                 k2 (1+λ2 −2ρλ−(1−ρ2 ) 12 )
                                                       k1
                 α1∗ =
                                             k12 + τ12
                                        1 − ρλ
                      =                                       τ2
                                                                    .
                          k1 (1 + λ2 − 2ρλ − (1 − ρ2 ) k12 )
                                                                1


The second order partial derivations of (2) with respect to α1 and α2 given by
               2
               ∂ M SE(T2LC (X)) = 2V ar(T1 (X)) + 2k 2 [g(θ)]2
                        ∂α1                              1
                   2                                                          (6)
               ∂ M SE(T2LC (X)) = 2V ar(T2 (X)) + 2k22 [g(θ)]2 ,
                           ∂α2

which are both positive, therefore α1∗ and α2∗ minimize the value of M SE(TLC (X)),
                     ∗
and the estimator TLC   (X) = α1∗ T1 (X) + α2∗ T2 (X) uniformly has the minimum
MSE among all estimators in the class CT1 ,T2 (α1 , α2 ) = {α1 T1 (X) + α2 T2 (X) |
0 < α1 , α2 < ∞}.

    Obviously, Theorem 3 is assumptions are culmination of the required assump-
tions for Theorems 1 and 2, which are provided in the section. The next corollary
is an immediate consequence of Theorem 3.
Corollary 1. The estimators proposed by Arnholt & Hebert (2001) can be obtained
as special cases, in Theorem 3, for g(θ) = θ and τi2 = νi ,. Also, for g(θ) = θ,
ki = 1, ρ = 0 and τi2 = νi , we obtained Gleser & Healy (1976) results.


4. Linear Combination of Two Improved
   Estimators
    One may expected, intuitively, that using two estimators with improved effi-
ciency to construct an optimal linear combination, leads to a more efficient esti-
mator. In the other words, it may be expected that improving the two estimators
T1 (X) and T2 (X) by using Theorem 2 and then constructing an optimal combi-
nation of these improved estimators by Theorem 3 leads to a more efficient linear
combination. The following theorem shows that this intuitive expectation is not
true. In fact, it shows that two estimators obtained from an improved linear com-
bination of two estimators and a linear combination of two improved estimators
are equivalent, in terms of efficiency.


                                         Revista Colombiana de Estadística 39 (2016) 229–245

Improved Linear Combination of Two Estimators                                         235

Theorem 4. Suppose based on Theorem 2, T1∗ (X) and T2∗ (X) are improved ver-
                                                            ∗
sions of estimators T1 X) and T2 (X), respectively. Let TLC   (X) = α1∗ T1 (X) +
  ∗
α2 T2 (X) is the optimal Linear Combination (LC) of T1 (X) and T2 (X), based
                      ∗
on Theorem 3. If TLCI    (X) = α1∗∗ T1∗ (X) + α2∗∗ T2∗ (X) be the optimal Linear
Combination of Improved (LCI) estimators T1∗ (X) and T2∗ (X), respectively, then
  ∗           ∗
TLCI  (X) = TLC (X).

Proof . Let TLCI (X) = α1 T1∗ (X) + α2 T2∗ (X) = α1 b1 T1 (X) + α2 b2 T2 (X); 0 <
α1 , α2 < ∞.
Without loss of generality, we assume that k1 , k2 > 0. Then,
            M SE(TLCI (X)) = V ar(TLCI (X)) + bias2 (TLCI (X))
                                = V ar(α1 b1 T1 (X) + α2 b2 T2 (X))
                                + (E(α1 b1 T1 (X) + α2 b2 T2 (X)) − g(θ))2 ,
                                                                                      (7)
                                = α12 b21 V ar(T1 (X)) + α22 b22 V ar(T2 (X))
                                                p
                                + 2α1 α2 b1 b2 ρ V ar(T1 (X))V ar(T2 (X))
                                + (α1 b1 k1 + α2 b2 k2 − 1)2 (g(θ))2 .
Differentiating (7) with respect to α1 and α2 and equating to zero leads to the
following system of equations:
  ∂M SE(T (X))                                                 p
 
         ∂α
             LCI
             1
                      = 2α1 b21 V ar(T1 (X))) + 2α2 b1 b2 ρ V ar(T1 (X))V ar(T2 (X))
                           +2k1 [g(θ)]2 (α1 k1 + α2 k2 − 1) = 0,
 
 
     ∂M SE(TLCI (X))
                                                         p
 
 
         ∂α2         = 2α2 V ar(T2 (X))) + 2α1 ρ V ar(T1 (X))V ar(T2 (X))
                           +2k2 [g(θ)]2 (α1 k1 + α2 k2 − 1) = 0.
 
                                                                                   (8)
The solutions of equation (8) are given by
                                            2
                                                         p
          ∗∗     (1 − α2∗ b2 k2 )b1 k1 g(θ) − α2∗ b1 b2 ρ V ar(T1 (X))V ar(T2 (X))
         α1 =                                                     2
                                      b21 [V ar(T1 (X)) + k12 g(θ) ]
                                                                                   (9)
                 k1 − α2∗ b2 (k1 k2 + ρτ1 τ2 )
             =                                  ,
                        b1 (k12 + τ12 )

                                        2
                                                  p
             (1 − α1∗ b1 k1 )b2 k2 g(θ) − α1∗ b1 b2 ρ V ar(T1 (X))V ar(T2 (X))
      α2∗∗ =                                                  2
                                  b22 [V ar(T2 (X)) + k22 g(θ) ]
                                                                                     (10)
             k2 − α1∗ b1 (k1 k2 + ρτ1 τ2 )
           =                                .
                    b2 (k22 + τ22 )
Substituting (9) in (10) we have the following,
                          1              1 − ρλ                 1
                 α1∗∗ =                                  τ 2   = α1∗ ,
                          b1 k1 (1 + λ2 − 2ρλ − (1 − ρ2 ) 12 )  b1
                                                           k1
                          1             λ(λ − ρ)                  1
                 α2∗∗ =                                          = α2∗ ,
                          b2 k2 (1 + λ2 − 2ρλ − (1 − ρ2 ) τ122 )  b2
                                                           k1



                                       Revista Colombiana de Estadística 39 (2016) 229–245

236                                                     Afshin Fallah & Hamid Khoshtarkib


where α1∗ and α2∗ presented in equation (1). Then
                           ∗
                          TLCI (X) = α1∗∗ T1∗ (X) + α2∗∗ T2∗ (X)
                                     α∗             α∗∗
                                   = 1 T1∗ (X) + 2 T2∗ (X)
                                     b1             b2
                                       ∗            ∗
                                   = α1 T1 (X) + α2 T2 (X)
                                        ∗
                                     = TLC (X).




5. A Doubly-Improved Linear Combination of Two
   Estimators
    In this section, we show how a Doubly-Improved (DI) linear combination of
two estimators of a parameter can be construct when the population CV is known.
In the next theorem, we try to further improve the improved linear combination
estimator that resulted from Theorem 3 by applying Theorem 2.
                                                                  ∗
Theorem 5. Consider the assumptions of Theorem 3. Suppose TLC       (X) = α1∗ T1 (X)+
  ∗
α2 T2 (X) are the optimal linear combination of estimators T1 (X) and T2 (X) where
α1∗ and α2∗ are given in equation (1). Then,

                                    ∗
  a) The doubly-improved estimator TDI           ∗
                                       (X) = α∗ TLC (X) uniformly has the min-
     imum MSE among all estimators of g(θ) that are in the class CTLC ∗ (α) =

         ∗                          ∗     k
     {αTLC (X) | α ∈ (0, ∞)} where α = k2 +τ 2 .
                                                                     2
                               ∗
  b) The minimum value of MSE(TDI (X)) is given by k2τ+τ 2 [g(θ)]2

Proof . a) Since
               ∗          ∗
              TDI (X) = αTLC (X);          0 < α < ∞,
             ∗
          E(TLC (X)) = (α1∗ k1 + α2∗ k2 )g(θ) = kg(θ),
                   k = α1∗ k1 + α2∗ k2 ,
                               ∗
                       V ar(TLC   (X))
                  τ2 =            2
                                          = α1∗ 2 τ12 + α2∗ 2 τ22 + 2α1∗ α2∗ ρτ1 τ2 ,
                                [g(θ)]

hence,
                           ∗             ∗
                        E(TDI (X)) = αE(TLC (X))
                                      = α(α1∗ k1 g(θ) + α2∗ k2 g(θ))
                                      = α(α1∗ k1 + α2∗ k2 )g(θ)
                                      = αkg(θ).


                                         Revista Colombiana de Estadística 39 (2016) 229–245

Improved Linear Combination of Two Estimators                                               237

            ∗                   ∗                 ∗
Also, V ar(TDI (X)) = V ar(αTLC   (X)) = α2 V ar(TLC (X))). Therefore the MSE of
 ∗
TDI (X) ∈ CTs∗ (α) that is obtained is:

                  ∗               ∗                 ∗
            M SE(TDI (X)) = V ar(TDI (X)) + bias2 (TDI (X))
                                         ∗              ∗
                                = V ar(αTLC (X)) + [E(αTLC (X)) − g(θ)]2
                                           ∗
                                = α2 V ar(TLC (X)) + (αk − 1)2 [g(θ)]2 .

Due to following system of equations
           (             ∗
               ∂M SE(TDI    (X))               ∗
                       ∂α         =    2αV ar(TLC (X)) + 2k(αk − 1)[g(θ)]2 ,
                          ∗
               ∂ 2 M SE(TDI  (X))             ∗
                                                                                            (11)
                      ∂α2         =    2V ar(TLC (X)) + 2k 2 [g(θ)]2 > 0,

                                           ∗
it can be easily shown that the estimator TDI                        ∗
                                              (X) = k(k 2 + τ 2 )−1 TLC (X) has the
minimum MSE in the class CTLC  ∗ (α).

b) We have

              ∗
    min
     ∗
        M SE(TDI (X)) = min
                         ∗
                                     ∗
                            M SE(α∗ TLC (X))
     α                          α

                            = min
                               ∗
                                 {α∗ 2 V ar(TLC
                                             ∗                ∗
                                                (X)) + [E(α∗ TLC (X)) − g(θ)]2 }
                                α
                                      k2              ∗                τ4
                            =                   V ar(TLC (X)) +                   [g(θ)]2
                                (k 2 + τ 2 )2                     (k 2 + τ 2 )2
                                                  ∗
                                g(θ) 2 2 V ar(TLC     (X))
                            =[ 2    2
                                      ] (k           2
                                                           + τ 4)
                              k +τ             [g(θ)]
                                g(θ) 2 2 2
                            =[ 2      ] [k τ + τ 4 ]
                              k + τ2
                                τ2
                            = 2      [g(θ)]2 .
                             k + τ2




6. Illustrative Examples
     Using Theorem, 2 it is possible to obtain optimal shrunken estimators for both
the population mean, say Tµ∗ (X), and the population variance, say Tσ∗2 (X). Note
that if the population CV, ν, is known, then one can easily use the mean based
estimator Tσ∗ν 2 (X) = ν 2 [Tµ∗ (X)]2 as another estimator for the population variance.
Laheetharan & Wijekoon (2010) compared the MSE of estimators Tσ∗2 (X) and
Tσ∗ν 2 (X).
    Suppose E(Tµ (X)) = k1 µ and E(Tσ2 (X)) = k2 σ 2 are estimators of the popu-
lation mean and variance, respectively. Since the population CV is known, then
the estimator Tσν 2 (X) = ν 2 [Tµ (X)]2 can be considered as another estimator for


                                           Revista Colombiana de Estadística 39 (2016) 229–245

238                                                          Afshin Fallah & Hamid Khoshtarkib


the population variance. Hence, if V ar(Tµ (X)) = cσ 2 , we have the following
                        E(Tσ2 (X)) = k2 σ 2 ,
                       E(Tσν 2 (X)) = E(ν 2 [Tµ (X)]2 )
                                        = ν 2 [V ar(Tµ (X)) + E 2 (Tµ (X))]
                                        = cν 2 σ 2 + k12 ν 2 µ2
                                        = (cν 2 + k12 )σ 2
                                        = k σν 2 σ 2 ,
where k1 , k2 , kσ2 ν and c are known constants. Using the above information,
and based on Theorems 4 and 5, we have the following theorem to estimate the
population variance.
                                               0
Theorem 6. Let X = (X1 , . . . , Xn ) be a random sample from a population with
distribution f (x|θ), and let Tσν 2 (X) and Tσ2 (X) be estimators of σ 2 , possibly cor-
related with E(Tσν 2 (X)) = kσν 2 σ 2 and E(Tσ2 (X)) = k2 σ 2 . Then,
                                                               ∗
   i) Based on Theorem 4, the linear combination TLC             (X) = ασν 2 ∗ Tσν 2 (X) +
         ∗
      ασ2 Tσ2 (X) uniformly has the minimum MSE of all estimators in the class
      CTσν 2 ,Tσ2 (ασν 2 , ασ2 ) = {ασν 2 Tσν 2 (X) + ασ2 Tσ2 (X) | 0 < ασν 2 , ασ2 < ∞},
      where
                                                     1 − ρλ
                       ασ∗ ν 2 =                                                      ,
                                   kσν 2 (1 − 2ρλ + λ2 + (1 − ρ2 )(τσ2ν 2 /kσ2 ν 2 ))
                                                                                            (12)
                                                  λ(λ − ρ)
                        ασ∗ 2 =                                                   .
                                   k2 (1 − 2ρλ + λ + (1 − ρ2 )(τσ2ν 2 /kσ2 ν 2 ))
                                                  2


                       V ar(Tσν 2 )        V ar(T )          k22 τ 2 2
      Also, τσ2ν 2 =      [σ 2 ]2   ,τσ22 = [σ2 ]2σ2 , λ2 = k2 στν2         and
                                                                  2σν 2 σ


                                            Cov(Tσν 2 (X), Tσ2 (X))
                                   ρ= p
                                           V ar(Tσν 2 (X))V ar(Tσ2 (X))

      are known and free from σ 2 .
  ii) Since
                               ∗
                            E(TLC (X)) = (ασν 2 ∗ + ασ2 ∗ k2 )σ 2 = kσ 2 ,
      and
                                  ∗
                            V ar(TLC  (X))
                   τ2 =           2
                                [σ ]2
                        = [ασν 2 ∗ ]2 τσ2ν 2 + [ασ2 ∗ ]2 τσ22 + 2ασν 2 ∗ ασ2 ∗ ρτσν 2 τσ2

      is free from σ 2 and known, based on Theorem 5, the doubly-improved es-
                 ∗
      timator TDI              ∗
                    (X) = α∗ TLC (X) uniformly has the minimum MSE of all σ 2
                                         ∗ (α) = {αT
                                                     ∗
      estimators that are in the class CTLC          LC (X) | α ∈ (0, ∞)}, where
        ∗      k
      α = k2 +τ 2 .

                                              Revista Colombiana de Estadística 39 (2016) 229–245

Improved Linear Combination of Two Estimators                                           239

Example 1. Let X = (X1 , . . . , Xn )0 be a random sample from a population with
a location-scale exponential distribution E(θ, θ), given by
                                 1      x−θ
                       f (x) =     exp(     )I(θ,+∞) (x).
                                 θ       θ
                                                       Pn
Since the estimators T1 (X) = X(1) and T2 (X) =           i=1 (Xi − X(1) ) are jointly
sufficient statistics for g(θ) = θ, our motivation is to use a combination of these
two estimators. We can to estimate an interested parameter. It is easy to shaw
that the mean and variance of T1 and T2 are given by
                                            n+1
                                  E(T1 (X)) =       θ,
                                               n                                       (13)
                                             1
                              V ar(T1 (X)) = 2 θ2 ,
                                            n
and
                                 E(T2 (X)) = (n − 1)θ,
                                                                                       (14)
                             V ar(T2 (X)) = (n − 1)θ2 ,

respectively. Hence, based on the notation of Theorem 3.1, we have k1 = n+1     n ,
                                             1
k2 = n − 1, τ12 = n12 , τ22 = n − 1, λ2 = (n+1)2 , ρT ,T
                                                     1 2
                                                         = −1. Therefore, according
to equation (1), the improved linear combination of two estimators T1 and T2 is
           ∗
given by TLC  (X) = α1∗ T1 (X) + α2∗ T2 (X), where
                                          n
                                 α1∗ =       ,
                                         n+2
                                               1                                       (15)
                                 α2∗ =                  .
                                         (n − 1)(n + 2)
This improved estimator uniformly has the minimum MSE among all estimators
in the class CT1 ,T2 (α1 , α2 ) = {α1 T1 (X) + α2 T2 (X) | 0 < α1 , α2 < ∞}. The value
of MSE for an improved estimator has been computed for different sample sizes
and plotted in Figure 1. Decreasing the value of MSE by increasing the sample
size, indicates that the improved shrinkage estimator will become more consistent.

Example 2. Let X = (X1 , . . . , Xn ) be a random sample from a population with
normal distribution N (θ, θ2 ). This is a curved exponential family with a two-
                                                                                    2
                            Pn The joint 2minimal sufficient statistic for g(θ) = θ
dimensional sufficient statistic.
                         2
is (T1 (X), T2 (X)) = (X̄ , i=1 (Xi − X̄) ) and the following equations hold for
these estimators:

                                           n+1 2
                               E(T1 (X)) =       θ ,
                                             n
                                           12n2 + 2 4
                            V ar(T1 (X)) =           θ ,                               (16)
                                              n2
                                                   2
                              E(T2 (X)) = (n − 1)θ ,
                            V ar(T2 (X)) = 2(n − 1)θ4 .


                                         Revista Colombiana de Estadística 39 (2016) 229–245

240                                                                            Afshin Fallah & Hamid Khoshtarkib




                                        0.0025
                                                                                        Theta=0.3




                                        0.0020
                                                                                        Theta=0.5
                                                                                        Theta=0.7




                     Mean Squre Error

                                        0.0015
                                        0.0010
                                        0.0005
                                        0.0000


                                                  10        20          30         40          50

                                                                 Sample Size

Figure 1: MSE of the improved shrinkage estimator for estimation of g(θ) = θ2 versus
          sample size in E(θ,θ) distribution.



Hence, based on Theorem 3.1 is notation, we obtain k1 = n+1               2
                                                         n , k2 = n − 1, τ1 =
                                                   2
12n2 +2
  n2
                                  +1)(n−1)
        , τ22 = 2(n−1), λ2 = (6n (n+1)2    and ρT1 ,T2 = 0 due to the independence of
           2
X̄ and S in normal distribution. Therefore, according to Equation (1), improved
                                                            ∗
linear combination of two estimators T1 and T2 is given by TLC (X) = α1∗ T1 (X) +
  ∗
α2 T2 (X), where

                                                                               3
                                                             n(n + 1)
                                        α1∗ =           4                                    2,
                                                 (n + 1) + 2(n − 1)(6n2 + 1)
                                                                   2
                                                       (n + 1) (6n2 + 1)
                                        α2∗ =           4                                    2.
                                                 (n + 1) + 2(n − 1)(6n2 + 1)

Again, this improved estimator uniformly has the minimum MSE among all esti-
mators in the class CT1 ,T2 (α1 , α2 ) = {α1 T1 (X) + α2 T2 (X) | 0 < α1 , α2 < ∞}. The
value for the improved estimator has been computed for different sample sizes and
plotted in Figure 2. Decreasing the value of MSE by increasing the sample size
indicates that the improved shrinkage estimator becoming more consistent.



Example 3. Let X = (X1 , . . . , Xn )0 be a random sample from a population with
Inverse Gaussian distribution IG(θ, θ). Let

                                                                       n
                                                                  1X 1
                                                   T1 (X) =                ,
                                                                  n i=1 Xi
                                                                                                            (17)
                                                           1
                                                   T2 (X) = ,
                                                           X̄

                                                       Revista Colombiana de Estadística 39 (2016) 229–245

Improved Linear Combination of Two Estimators                                                             241




                                       400
                                                                                 Theta=0.3
                                                                                 Theta=0.5
                                                                                 Theta=0.7




                                       300
                    Mean Squre Error

                                       200
                                       100

                                                   10        20          30     40           50

                                                                  Sample Size

Figure 2: MSE of the improve shrinkage estimator for estimation of g(θ) = θ versus
          sample size in N(θ,θ2 ) distribution.


and g(θ) = θ1 . It is easy to see that the mean and variance of T1 and T2 are given
by
                                                                  2
                                                        E(T1 (X)) = ,
                                                                  θ                                      (18)
                                                                    3
                                                    V ar(T1 (X)) = 2 ,
                                                                  nθ
and
                                                                 n+1
                                                        E(T2 (X)) =   ,
                                                                  nθ                                     (19)
                                                                 n+2
                                                   V ar(T2 (X)) = 2 2 ,
                                                                 n θ
respectively.
     Pn       To compute the coefficient of correlation between T1 and T2 , let
                 1
V = i=1 ( X1i − X̄ ), Then
                                n
                               X   1   1
               V ar(V ) = V ar( (     − ))
                               i=1
                                   Xi  X̄
                                             n
                                             X             1   1
                                        =          V ar(      − )
                                             i=1
                                                           Xi  X̄
                                                1           1         1 1
                                        = n(V ar(  ) + V ar( ) − 2Cov( , ))
                                                Xi          X̄        Xi X̄
                                             3  n+2            1 1
                                        = n( 2 + 2 2 − 2Cov( , )).
                                            θ   n θ            Xi X̄
Therefore,
                                       1 1    3  n+2     1
                 Cov(                    , )= 2 + 2 2 −    V ar(V ).                                     (20)
                                       Xi X̄ 2θ  2n θ   2n

                                                           Revista Colombiana de Estadística 39 (2016) 229–245

242                                                       Afshin Fallah & Hamid Khoshtarkib

                                                                          p Γ( n+2p−1 )
From Inverse Gaussian distribution we know that E(V p ) = θ2p                    2
                                                                             Γ( n−1
                                                                                          , (see for
                                                                                 2 )
example, Singh & Pandit 2008). Therefore,
                                                                2
                             V ar(V ) = E(V 2 ) − E(V )
                                         4                                                     (21)
                                      = 2 c(n),
                                         θ
            h n+3
                          Γ( n+1
                                    i
             Γ( 2 )           2 ) 2
where c(n) = Γ( n−1 )
                      − ( Γ( n−1 )
                                 )
                                     . Substituting (21) in (20), we have
                    2             2

                          1 1         3      n+2        2
                            , ) = 2 + 2 2 − 2 c(n)
                         Cov(
                         Xi X̄       2θ     2n θ      nθ
                                          1
                                   = k(n) 2 ,
                                          θ
             h                 i
                         2c(n)
where k(n) = 32 + n+2
                   2n2 −   n     . Therefore, the coefficient of correlation between
T1 and T2 obtained is:
                                              Cov(T1 , T2 )
                                ρT1 ,T2 = p
                                             V ar(T1 )V ar(T2 )
                                                  Pn
                                          Cov( n1 i=1 X1i , X̄1 )
                                        = p
                                             V ar(T1 )V ar(T2 )
                                           1
                                             Pn          1   1
                                               i=1 Cov( Xi , X̄ )                              (22)
                                        = np
                                             V ar(T1 )V ar(T2 )
                                                     1
                                          Cov( X1i , X̄ )
                                       =p
                                         V ar(T1 )V ar(T2 )
                                       = h(n),

where h(n) = qk(n)
               3n+6
                    is a free from θ quantity. Considering the equations (18), (19)
                   n3

and (22), and based on the notation of Theorem 3.1, we have k1 = 2, k2 = n+1
                                                                          n ,
                                         2
                              3n(n+1)
τ12 = n3 , τ22 = n+2    2
                  n2 , λ = 4(n+2)2 and ρT1 ,T2 = h(n). Therefore, according to
equation (1), the improved linear combination of two estimators T1 and T2 is
             ∗
given by TLC   (X) = α1∗ T1 (X) + α2∗ T2 (X) where,
                                                         √
                                       1 − h(n) (n+1)  3n
                                                  2(n+2)
           α1∗ =                   √              2                       ,
                                                               2
                   2 1 − h(n)(n+1)
                              n+2
                                     3n
                                        + 3n(n+1)
                                           4(n+2)2
                                                    + (1 − h(n) )( 4n9 2 )
                                       √         √
                                (n+1) 3n (n+1) 3n           
                                  2(n+2)     2(n+2) − h(n)
           α2∗ =                      √              2                       .
                                                                  2
                   n+1
                    n    1 − h(n)(n+1)
                                  n+2
                                        3n
                                           + 3n(n+1)
                                              4(n+2)2
                                                       + (1 − h(n) )( 4n9 2 )

    This improved estimator, uniformly has the minimum MSE among all estima-
tors in class CT1 ,T2 (α1 , α1 ) = {α1 T1 + α2 T1 }. The value of MSE for improved
estimators has been computed for different sample sizes and plotted in Figure
3. Decreasing the value of MSE by increasing the sample size indicates that the
improved shrinkage estimator becomes more consistent.


                                             Revista Colombiana de Estadística 39 (2016) 229–245

Improved Linear Combination of Two Estimators                                                      243




                                         2.0
                                                                               Theta=0.3
                                                                               Theta=0.5
                                                                               Theta=0.7




                                         1.5
                      Mean Squre Error

                                         1.0
                                         0.5

                                               20      40           60    80          100

                                                            Sample Size
Figure 3: MSE of improved shrinkage estimator for the estimation of g(θ) = 1/θ versus
          sample size in IG(θ,θ) distribution.



7. Discussion and Results
    Sometimes the complete information about the interested parameter is dis-
tributed in two or more different estimators. In these situations, using only one
of given estimators leads to loss of information of other estimators. Therefore,
a combination of estimators must be employed to achieve a more efficient esti-
mator. Moreover, it is interesting to look for improved estimators for a general
form function of interested parameters, say g(θ). In recent years, some authors,
notably Laheetharan & Wijekoon (2010), have been considered the problem in
term of finding improved estimators for a function of an interested parameter. In
this context, we have presented an optimal shrinkage estimator for a general form
function of an interested parameter with an assumption of a known population
coefficient of variation. We have also showed that two estimators obtained from
the improved linear combination of two estimators and the linear combination of
two improved estimators are equivalent, in terms of efficiency.
    We think that using other coefficients of distributions, as additional information
to be able to achieve a more efficient linear combination of two or more estimators,
is an interesting field of research. Future studies will need to address this problem.
Of course, it is our opinion that using the coefficient of variation in this direction,
as an informative coefficient of distribution, will remain forever interesting. In fact,
whenever prior information about the size of coefficient of variations is available,
the shrinkage procedure could be useful. The possible results for some distributions
with particular properties may be more interesting. For example, considering
one-parameter exponential family of distributions is quite interesting. In some
members of this distributions family such as normal, Poisson, gamma, binomial
and negative binomial, it is known that variance is at most a quadratic function
of the mean. Therefore, identifying the pertinent coefficients in the quadratic
function is equivalent to determining the coefficient of the variations. As is obvious
from theorems’ assumptions, one can use any correlated or uncorrelated pair of


                                                    Revista Colombiana de Estadística 39 (2016) 229–245

244                                              Afshin Fallah & Hamid Khoshtarkib


estimators to construct an optimal linear combination to estimate any parametric
function of an interested parameter. The results show that the efficiency of the
proposed improved shrinkage estimator increase when the sample size increases.
                                                            
                   Received: June 2015 — Accepted: April 2016


References
Arnholt, A. T. & Hebert, J. L. (1995), ‘Estimating the mean with known coefficient of variation’, Journal of the American Statistical Associatio 49, 367–369.
Arnholt, A. T. & Hebert, J. L. (2001), ‘Combinations of pairs of estimator’, Statistics on the Internet pp. 1–8.
Bibby, J. (1972), ‘Minimum means square error estimation, ridge regression and some unanswered questions’, Progress in Statistics 1, 107–121.
Bibby, J. & Toutenburg, H. (1977), Prediction and Improved Estimation in Linear Models, 1 edn, Wiley, New York.
Bibby, J. & Toutenburg, H. (1978), ‘Improved estimation and prediction’, Zeitschriftur Angewandte Mathematik and Mechanik 58, 5–49.
Gleser, L. J. & Healy, J. D. (1976), ‘Estimating the mean of a normal distribution with known coefficient of variation’, Journal of the American Statistical Association 71, 977–981.
Kanefuji, K. & Iwase, K. (1998), ‘Estimation for a scale parameter with known coefficient of variation’, Statistical Papers 39, 377–388.
Khan, R. A. (1968), ‘A note on estimating the mean of a normal distribution with known coefficient of variation’, Journal of the American Statistical Association 63, 1039–1041.
Kleffe, J. (1985), Some remarks on improving unbiased estimators by multiplication with a constant, in T. Alinski & W. Klonecki, eds, ‘Linear Statistical Inference’, Cambridge.
Laheetharan, A. & Wijekoon, P. (2010), ‘Improved estimation of the population parameters when some additional information is available’, Statistical Papers 51, 889–914.
Laheetharan, A. & Wijekoon, P. (2011), ‘Mean square error comparison among variance estimators with known coefficient of variation’, Statistical Papers 52, 171–201.
Samuel-Cahn, E. (1994), ‘Combining unbiased estimators’, The American Statistician 48, 34–36.
Searls, D. T. (1964), ‘The utilization of known coefficient of variation in the estimation procedure’, Journal of the American Statistical Association 59, 1225–1226.
Searls, D. T. & Intarapanich, P. (1990), ‘A note on an estimator for the variance that utilizes the kurtosis’, The American Statistician 44, 295–296.
Singh, H. P. & Pandit, S. (2008), ‘Estimation of the reciprocal of the mean of the inverse gaussian distribution with prior information’, Statistica 2, 201–216.
Subhash, K. Y. & Cem, K. (2013), ‘Improved exponential type ratio estimator of population variance’, Revista Colombiana de Estadística 36(1), 145–152.
Wencheko, E. & Chipoyera, H. W. (2005), ‘Estimation of the variance when kurtosis is known’, Statistical Papers 50, 455–464.
Wencheko, E. & Wijekoon, P. (2007), ‘Improved estimation of the mean in oneparameter exponential families with known coefficient of variation’, Statistical Papers 46, 101–115.
