An Iterative Method for Curve Adjustment Based on Optimization of a Variable and its Application. Un método iterativo para el ajuste de curvas basado en la optimización en una variable y su aplicación al caso lineal en una variable independiente
Universidad de Las Tunas, Las Tunas, Cuba.Universitat Politecnica de Valencia, Valencia, Spain.  Universidad de las Ciencias Informáticas, La Habana, Cuba
Abstract
An iterative method for the adjustment of curves is obtained by applying the least squares method reiteratively in functional subclasses, each defined by one parameter, after assigning values to the rest of the parameters which determine a previously determined general functional class. To find the minimum of the sum of the squared deviations, in each subclass, only techniques of optimization are used for real functions of a real variable.The value of the parameter which gives the best approximation in an iteration is substituted in the general functional class, to retake the variable character of the following parameter and repeat the process, getting a succession of functions. In the case of simple linear regression, the convergence of that succession to the least squares line is demonstrated, because the values of the parameters that define each approximation coincide with the values of the parameters obtained when applying the method of Gauss - Seidel to the normal system of equations. This approach contributes to the teaching objective of improving the treatment of the essential ideas of curve adjustment, which is a very important topic in applications, what gives major importance to the optimization of variable functions.
Key words: Curve estimation, Iterative method, Least Squares Method, Linear regression, Teaching materials.
Resumen
Se obtiene un método iterativo para el ajuste de curvas al aplicar reiteradamente el método de los mínimos cuadrados en subclases funcionales, cada una definida por un parámetro, luego de asignar valores a los restantes parámetros que determinan una clase funcional general, seleccionada previamente. Para hallar el mínimo de la suma de las desviaciones cuadráticas, en cada subclase, solo se utilizan técnicas de optimización para funciones reales de una variable real. El valor del parámetro, que proporciona la mejor aproximación en una iteración, se sustituye en la clase funcional general, para retomar el carácter variable del siguiente parámetro y repetir el proceso, obteniéndose una sucesión de funciones. En el caso de la regresión lineal simple se demuestra la convergencia de esa sucesión a la recta mínimo cuadrática, pues coinciden los valores de los parámetros que definen cada aproximación con los que se obtienen al aplicar el método de Gauss - Seidel al sistema normal de ecuaciones. Este enfoque contribuye al objetivo docente de adelantar el tratamiento de las ideas esenciales del ajuste de curvas, temática muy importante en las aplicaciones, lo que le confiere mayor significación a la optimización de funciones de una variable.
Palabras clave: estimación de curvas, materiales de enseñanza, método de mínimos cuadrados, método iterativo, regresión lineal.



1. Introduction
    The method of regression is one of the most important statistical methods for
higher education graduates. Its comprehension facilitates obtaining and correctly
interpreting the results of different types of models to be applied in their profes-
sional careers. Bibliographic research in the scientific literature shows the wide
interest and use of regression methods. From such bibliographic analysis, three
approaches can be differentiated: The largest one, related to the application of
regression methods to different fields and topics of science; see Braga, Silveira,
Rodríguez, Henrique de Cerqueira, Aparecido & Barros (2009), Guzmán, Boli-
var, Alepuz, González & Martin (2011), Ibarra & Arana (2011) and Santos da
Silva, Estraviz, Caixeta & Carolina (2006) a second approach related to the the-
oretical aspects of the topic; see Núñez, Steyerberg & Núñez (2011), Vega-Vilca
& Guzmán (2011), Donal (2001), Ranganatham (2004), Kelley (1999), Schmidt
(2005). Lastly, a third group related to the teaching of the method, i.e., how to
help students, and professionals in general, in correctly applying regression and
interpreting its results see Batanero, Burrill & Reading (2011), Gutiérrez de Ravé,
Jiménez-Hornero & Giráldez (2011) and Wei, De Quan & Jian (2001).
    Applications of regression methods are found in scientific papers related to
agriculture, medicine, environment, economics, sociology and different engineering
areas. Using a random sample of one hundred papers published during 2012 and
obtained by the authors from the Web of Knowledge, in 32% of them there was a
direct application of these methods and in almost half of them (46%) there was a
reference to regression.


                                       Revista Colombiana de Estadística 37 (2014) 111–125

Iterative Method for Curve Adjustment                                                     113

   Such significant use of regression supports its inclusion in the largest part of
university curricula. It is generally included in the statistics subject.
    In terms of teaching, curve adjustment is generally explained once the methods
of optimization of real functions of several real variables are known, along with the
solution of linear equation systems. This makes possible the support of procedures
that permit to determine the values of the parameters characterizing the functional
class of the best adjustment curve sought. Usually, in practice, computer packages
are used to determine these parameters.
     Taking into consideration the importance of curve adjustment for applications,
it is advisable to teach the students these ideas much more before it is usually done
in university curricula. How can this purpose be achieved if curve adjustment is
preceded by mathematical requirements which seem not to be possible to sever?
This paper presents an approach that permits to teach in advance the consideration
of these basic ideas of regression in at least one semester, what is justified assuming
the following hypotheses:
    -The methods of optimization for the real functions of many variables are ne-
glected, what has the immediate implication of not requiring the partial derivation.
   -A system of linear equations is not stated, so the corresponding theory is not
necessary.
   -The reiterative application of the least squares method in functional classes
determined by only one parameter, so that in each of them, the corresponding
sum of the squared deviations is function of a unique variable. Consequently,
optimization techniques for real functions of a real variable are only required.
    Though sufficient and varied bibliography about the least squares method is
available, it was considered necessary to make explicit some of its basic aspects
initially, such as the expression that takes the sum of the squared deviations, as
well as the normal system of equations that is formed at stating the necessary
conditions for extremes, both in the case of the simple linear regression.
    Curve adjustment is, possibly, the most frequently used mathematical resource
for solving one of the fundamental problems related to numerous scientific areas:
“reconstructing” a function starting from experimental data. Essentially, for the
case of one variable functions, this problem may be formulated through the follo-
wing statement:
    “Given the set of n points {(x1 , y1 ); (x2 , y2 ); . . . (xn , yn )}, where n is a natural
number and every two xk abscissas are different, the goal is determining the y =
f (x) function which, within a given prefixed class of functions, best adjusts them”.


2. Materials and Methods
2.1. Curve Adjustment and the Least Squares Method
   Generally, the prefixed functional class depends on various parameters, and
the purpose of the method used for their estimation is to satisfy some criterion of


                                         Revista Colombiana de Estadística 37 (2014) 111–125

114                                                                                        Rogelio Acosta, et al.


optimization, which is characteristic of the method; particularly, the objective of
the least squares method is to minimize the sum of the squared deviations. Two
other alternatives, which are also frequently used are the Maximum Likelihood
Method; see Yoshimori & Lahiri (2014), Seo & Lindsay (2013) and Han & Phillips
(2013) and for the Bayesian regression method; see Zhao, Valle, Popescu, Zhang
& Mallick (2013), Mudgal, Hallmark, Carriquiry & Gkritza (2014) and Choi &
Hobert (2013).
    In the probably most renowned and significant case of finding the best-adjusting
function within the class of linear functions of one independent variable f (x) =
a1 x + a2 this problem is solved through the least squares method by determining
the values of the a1 and a2 parameters (the slope and the intercept with the y-
axis, respectively), which provide the minimum value to the sum of the S(a1 , a2 )
squared deviations:
                                      n
                                      X                             n
                                                                    X
                    S(a1 , a2 ) =          (f (xk ) − yk )2 =              (a1 xk + a2 − yk )2 .
                                      k=1                           k=1

Determining the minimum of S(a1 , a2 ) requires applying optimization techniques
for real functions of two real variables, which initially require the use of the nec-
essary condition on extreme points:
                n                                                                n
   ∂S     X                                                       ∂S     X
       =2   xk (a1 xk + a2 − yk ) = 0;                                =2   (a1 xk + a2 − yk ) = 0            (1)
   ∂a1                                                            ∂a2
            k=1                                                              k=1

Afterwards, it requires the resolution of the system of two linear equations resulting
from it with a1 and a2 as unknowns. This system is called the normal equation
system, which is expressed as follows:
                n
                X                    n
                                     X            n
                                                  X                        n
                                                                           X                  n
                                                                                              X
           a1            x2k + a2          xk =         xk yk ;      a1          xk + na2 =         yk       (2)
                k=1                  k=1          k=1                      k=1                k=1

When applying any of the existing techniques for the resolution of system (2), the
                                  (0)       (0)
result is a single solution a1 = a1 , a2 = a2 , given by the expressions:
                         n
                         X                 n
                                           X        n
                                                    X
                     n        xk yk −          xk       yk                           n          n
         (0)             k=1               k=1 k=1                   (0)      1X        X 
        a1 =                n                n               ;     a2 =           yk − a  xk                 (3)
                           X               X   2                            n
                         n         x2k −          xk                                 k=1       k=1

                             k=1            k=1

The procedure herein presented is equivalent to applying the Gauss - Seidel Method
(McCracken & Dorn 1974) to the normal system of equation (2). This is an iter-
ative method for the resolution of linear equation systems, as it happens with the
system in equation (2), or the one resulting from applying the necessary condition
in equation (1) to the sum of the squared deviations, when the adjustment takes
place in a functional class that is linear with respect to the parameters defining it.


                                                    Revista Colombiana de Estadística 37 (2014) 111–125

Iterative Method for Curve Adjustment                                                                   115

    In terms of teaching organization, this approach provides more significance to
the optimization methods of real functions of one real variable. At the same time,
it permits the introduction of an important application such as curve adjustment,
advancing one semester, at least.


2.2. An Iterative Method for the Process of Curve
     Adjustment
    Solving the normal equation system of equation (2) is not possible until a
method that permits optimizing a derivable function of two real variables is avail-
able. Therefore, a sequence, that only requires applying different variable opti-
mization techniques, one at a time, may be followed. Such method results from
realizing the following steps:

  1. Prefix the functional class in which the adjustment process will be carried
     out.
     As it is known, the functional class is characterized by a functional expression
     involving the independent variable and the p parameters that define it, being
     p a positive integer. This class of functions is denoted by:

                                    y = f (x, a1 , a2 , . . . ap ),                                     (4)

     where x is the independent variable, and the parameters have been denoted
     by a1 , a2 , . . . , ap , for which it is necessary to previously establish an order
     among them.
  2. Keep the variable character of a1 and assign values to the rest of the para-
     meters.
                                                                                   (0)    (0)           (0)
     The values assigned to the parameters are denoted by a2 , a3 , . . . , ap ,
     where the sub-index of each identifies the parameter, and the supra - index 0
     indicates that it is the initial assignment. These values may be arbitrary or
     follow a certain criterion, but this is irrelevant to the method being described.
     Thus, the set of functions is defined as:
                                                          (0)
                                 y1 = f1 (x, a1 , a2 , . . . , a(0)
                                                                p )                                     (5)

     which is formed by functions of the independent variable x depending on the
     parameter a1 that obviously constitutes a subclass of the pre-fixed functional
     class.
                                                                                    (0)           (0)
  3. Form the sum of the quadratic differences in y1 = f1 (x, a1 , a2 , . . . , ap ).
     Given the set {(x1 , y1 ); (x2 , y2 ); . . . ; (xn , yn )}, of n points, the corresponding
     sum of the quadratic differences to be minimized is formed, which is a func-
     tion of the a1 parameter, and is defined by the expression:
                     n                    2       n                                            2
                                                                         (0)
                     X                              X
          S(a1 ) =         f1 (xk ) − yk        =          f1 (xk , a1 , a2 , . . . , a(0)
                                                                                       p ) − yk
                     k=1                            k=1



                                        Revista Colombiana de Estadística 37 (2014) 111–125

116                                                                                    Rogelio Acosta, et al.


   4. Apply the necessary extreme condition to S(a1 ).
      As S(a1 ) is a one variable function, it is enough to state S 0 (a1 ) = 0 to thus
      determine the solution of this equation. This gives the value of parameter a1 ,
                    (1)           (1)
      denoted by a1 so that S(a1 ) is the lowest value of S(a1 ). It is important to
                                      (1)
      note that the supra-index 1 in a1 means that this is the first value calculated
      for a1 .


3. Results and Discussion
    The implementation of the previous process guarantees obtaining the func-
                                                                (0)    (0)
tion of better adjustment within the y1 = f1 (x, a1 , a2 , . . . , ap ) subclass, in the
initially pre-fixed functional class y = f (x, a1 , a2 , . . . , ap ).
                                                         (1)              (1)    (0)     (0)      (0)
    In general, it is not expected that the y1 = f1 (x, a1 , a2 , a3 , . . . , ap ) func-
                                      (0)         (0)                  (1)
tion obtained from y1 = f1 (x, a1 , a2 , . . . , ap ) by substituting a1 by a1 , to be
a good approximation to the better adjustment within the general prefixed class
y = f (x, a1 , a2 , . . . , ap ).
    The described process is repeated, leaving the next parameter as arbitrary (in
                                                          (1)
this case a2 ) and taking for a1 the calculated value a1 , and for the rest of the
                                          (0)         (0)
parameters the initially assumed values a3 , . . . , ap .
                                                                          (1)
   As a result, the value of parameter a2 , denoted by a2 , will be obtained, offering
the best adjustment function within the subclass:
                                            (1)               (0)
                           y2 = f2 (x, a1 , a2 , a3 , . . . , a(0)
                                                               p )

Once the whole set of parameters has been recovered, by proceeding similarly, the
following p functions would be obtained:
                          (1)                  (1)      (0)    (0)         (0)
                         y1 = f1 (x, a1 , a2 , a3 , . . . , ap )
                          (1)               (1) (1) (0)                  (0)
                         y2 = f2 (x, a1 , a2 , a3 , . . . , ap )
                            .............................................
                          (1)               (1) (1) (1)                  (1)
                         yp = fp (x, a1 , a2 , a3 , . . . , ap )

where each of them is the best adjustment function within the corresponding
functional class.
    It can be verified that each of these functions is not a worse, but a better
                                                 (1)                (1)     (1)
approximation than the previous one. Indeed, y2 is reduced to y1 taking a2 =
 (0)                                                            (1)
a2 for it. So with this value for that parameter, function y2 provides a value
                                                                        (1)
for the sum of squared differences that is similar to the one given by y1 . This
              (1)                                        (1)
proves that y2 is an approximation not worse than y1 . This also holds for the
rest of the functions and this step completes the first iteration.
                                         (1)      (1)               (1)
   As the values of parameters a2 , a3 , . . . , ap have a similar purpose to the
                           (0) (0)           (0)
one followed with numbers a2 , a3 , . . . , ap , the above process may be repeated


                                          Revista Colombiana de Estadística 37 (2014) 111–125

Iterative Method for Curve Adjustment                                                        117

to find the second iteration, which will be completed once the following new p
functions have been determined:
                             (2)              (2)    (1)    (1)               (1)
                           y1 = f1 (x, a1 , a2 , a3 , . . . , ap )
                            (2)               (2) (2) (1)                  (1)
                           y2 = f2 (x, a1 , a2 , a3 , . . . , ap )
                              .............................................
                            (2)               (2) (2) (2)                  (2)
                           yp = fp (x, a1 , a2 , a3 , . . . , ap )

                                                             (2)        (2)           (2)
For a third iteration, the calculated values a2 , a3 , . . . , ap will be used, and
the process would be the same successively. A possible stop criterion for the
iterative process would be that each of the values determined for the parameters in
a given iteration were sufficiently close to the corresponding value in the preceding
iteration1 .
    Sufficiently close means here, as is common in mathematics, that a certain
pre-fixed degree  > 0 of accuracy is fulfilled for all j index (j = 1, 2, . . . , p), and
              (m)    (m−1)
inequality |aj − aj        | < , where m is a natural number indicating the number
of order of the iteration.
    The objective is proving that this process converges with the function of best
adjustment within the pre-fixed functional class y = f (x, a1 , a2 , . . . , ap ), which
is the initial step in the iterative method. For the case of pre-fixing the linear
functions, the method may be proven to converge.


3.1. Geometrical Interpretation of the Case of Linear
     Adjustment
   If in the initial step of the process the linear function class is pre-fixed in an
independent variable f (x) = ax + b, where a and b are real numbers, an interesting
geometrical interpretation may be given from the described iterative method.
    By keeping the notation used in the description of the iterative method, the
slope is denoted by a1 and the intercept with the y axis by a2 , so that this general
functional class is then expressed as:

                                           y = a1 x + a2
                                                           (0)
Prefixing the value of parameter a2 equal to a2 implies taking the point of coordi-
           (0)
nates (0, a2 ) of the y - axis in the system of Cartesian coordinates, and considering
the family of all straight lines that pass through such point, except the very axis
of the ordinates (y - axis).
                                                                                      (0)
    This family is formed by the graphics of the y1 = a1 x + a2 subclass functions,
for all the possible values of the a1 slope.
                                                     (1)          (1)           (0)         (0)
   Once the best adjustment function y1 = a1 x + a2 in y1 = a1 x + a2 is
determined, we come back to the arbitrary character of the intercept with the y
   1 This is one of the stop - criteria used in some numeric methods, such as Gauss - Seidel, for

the resolution of a linear system.


                                            Revista Colombiana de Estadística 37 (2014) 111–125

118                                                                                      Rogelio Acosta, et al.


- axis, taking the new subclass within the y = f (x) = a1 x + a2 class, defined by
       (1)
y2 = a1 x + a2 to carry out the new adjustment process. As a result, the value of
                                      (1)                          (1)   (1)      (1)
parameter a2 will be determined as a2 , which determines the y2 = a1 x + a2
                                                                              (1)
equation, which is a straight line parallel to the one initially determined, y1 =
  (1)      (0)
a1 x + a2 .
      Therefore, the first iteration concludes, when the following functions are ready:
                          (1)       (1)           (0)            (1)   (1)         (1)
                         y1 = a1 x + a2 ,                       y2 = a1 x + a2
                   (1)      (1)           (0)
Initially, with y1 = a1 x + a2 , we determine the angle of inclination of the
                                                                 (0)
straight line that passes through the point of coordinates (0, a2 ) with the positive
direction of the axis of abscissas. This is subsequently transferred parallel to itself
                                                             (1)       (1)    (1)
until it occupies the graphic position corresponding to y2 = a1 x + a2 that
                                              (1)
passes through the point of coordinates (0, a2 ). In turn, this is used to implement
                                       (2)                           (2)
the second iteration: the new slope a1 and the new intercept a2 , and so on.


3.2. One Example of the Application of the Iterative Method
    A table with arbitrary or hypothetical data, which determine five points of
integer coordinates: A(1, 1), B(2, 3), C(3, 3), D(4, 5) y E(5, 5), is taken.

                                Y

                                                                   D
                                5                                          E




                                4



                                                   B
                                3                           C




                                2




                                1             A




                                          1        2    3          4   5       X

                                    Figure 1: Regression line.


   In Figure 1, a regression line y = x + 0.4 is represented. It was obtained by
the the least squares method, in the general functional class y = a1 x + a2 , where
the parameters are the slope a1 and the intercept a2 . The sum of the squared
deviations is function of these two parameters, so to obtain y = x + 0.4 (it means,
a1 = 1 and a2 = 0.4) techniques of optimization for the functions of some variables
were required and the exact resolution of the normal system of equations.
    In Figure 2, a segment of the first approximation is represented. It is the line
that by the origin (of slope 61/55) better adjusts to the five points. It is optimized
in the functional class y = a1 x, where the parameter is the a1 slope, what follows


                                                  Revista Colombiana de Estadística 37 (2014) 111–125

Iterative Method for Curve Adjustment                                                        119


                         Y

                                                                 D
                         5                                                   E




                         4


                                             B
                         3                               C




                         2



                         1           A




                                 1           2       3           4       5           X
                   Figure 2: Segment of the first approximation.



from assigning, in y = a1 x + a2 , the zero value to a2 parameter. Geometrically,
it means that it optimizes in the functional class of all no vertical lines that pass
through the origin. The sum of the squared deviations is only function of a1 , so that
this optimum (minimum) is determined by techniques of optimization of functions
of one variable (it not even requires the ordinary derivative, observing that the sum
of the squared deviations is a quadratic function in a1 variable, whose graphic is
a parabola that opens upwards, so that the optimum (minimum) is reached in the
abscissa of the vertex (value 61/55). It is maintained with purposes of comparison,
the segment of the regression line.

                             Y

                                                                     D
                             5                                                   E




                             4



                                                 B
                             3                               C




                             2




                             1           A




                                     1           2       3           4       5           X

                  Figure 3: Segment of the second approximation.


    In Figure 3 a segment of the second approximation, which best adjusts to the
five points among all the lines with slope 61/55, is represented. It is optimized in
the functional class y = (61/55)x + a2 , which is obtained from the functional class
y = a1 x+a2 replacing a1 = 61/55 and retaking the variable character of a2 (notice
that for a2 the value 0 was initially assumed). Geometrically it means that the line
of equation y = (61/55)x is paralleled displaced itself up to a position that betters

                                             Revista Colombiana de Estadística 37 (2014) 111–125

120                                                                    Rogelio Acosta, et al.


the adjustment (provides the minimal for the sum of the squared deviations which
now depends on a2 ). The resulting value for the parameter is a2 = 4/55.

                          Y

                                                       D
                          5                                    E




                          4



                                         B
                          3                        C




                          2




                          1          A




                                 1       2     3       4   5       X

                   Figure 4: Segment of the third approximation.


    For a new approximation a2 = 4/55 in the general class y = a1 x+a2 is replaced
to optimize in the subclass y = a1 x + 4/55, in which the slope a1 is variable again,
so that what is looked for is the line that better adjusts to the data (in the sense of
minimizing the corresponding sum of squared deviations) among all those that pass
through the axis point (0, 4/55). The result is the new slope value a1 = 659/605,
what permits the line with best equation adjustment y = (659/605)x + 4/55, a
segment of which is represented in Figure 4 together with the one of the least
regression line.
    The process continues similarly, so that the new adjustment would take place
in the functional subclass y = (659/605)x + a2 , where the variable character of the
second of the parameters is retaken.


3.3. The Gauss - Seidel Method and Convergence in the Case
     of Linear Adjustment
    The issue related to the convergence of the described iterative method has an
affirmative answer in the case of linear adjustment, if the set of points fulfills the
initially described characteristics; i.e., if within the full set, every pair of points
has different abscissas.
                                                                          (0)      (0)
    As the best adjusting straight line, with the equation y = a1 x + a2 , does
exist, and the parameters are analytically determined as the only solution by (3)
in the standard equation system, an iterative method convergent to the solution
of such system would obviously provide, after an adequate number of iterations,
          (0)       (0)
an α ≈ a1 , β ≈ a2 approximation. This offers the possibility of defining a
                                                                  (0)     (0)
y = αx + β approximation for the best adjustment equation y = a1 x + a2 .
   One of the simplest iterative methods for the resolution of a linear equation
system, easily programmed for its computerized application, is the Gauss - Seidel
Method.


                                         Revista Colombiana de Estadística 37 (2014) 111–125

Iterative Method for Curve Adjustment                                                        121

   For the case of a two - equation system with two unknowns:

                                         b11 a1 + b12 a2 = c1
                                         b21 a1 + b22 a2 = c2

The method is described as follows:
    Supposing that in the coefficients matrix, those of the main diagonal are not
null, it is possible to find the unknowns a1 and a2 :
                             1                                1
                     a1 =       (c1 − b12 a2 );       a2 =       (c2 − b21 a1 )              (6)
                            b11                              b22
                                                                         (0)       (0)
An arbitrary approximation is now defined for the a1 = a1 , a2 = a2 solution,
and it is used to find a new approximation for the a1 unknown value stemming
from the first of the expressions in (6):

                                        (1)    1             (0)
                                    a1 =          (c1 − b12 a2 )
                                              b11
      (1)
The a1 calculated value is substituted in the second of the expressions (6) to
determine an approximation to the a2 unknown value:

                                        (1)    1             (1)
                                    a2 =          (c2 − b21 a1 )
                                              b22
At this point the first iteration is fulfilled.
    The second iteration is implemented by taking the calculated approximation
 (1)  (1)                                   (0)       (0)
a1 , a2 with the same role played by a1 = a1 , a2 = a2 in the first iteration.
    In this way it is possible to reach the order m iteration defined by the expre-
ssions:
             (m)      1             (m−1)     (m)    1             (m−1)
            a1 =         (c1 − b12 a2     ), a2 =       (c2 − b21 a1     )      (7)
                     b11                            b22
A sufficient condition for convergence to the solution of the iterations produced
through the Gauss - Seidel Method lies in the matrix of the coefficients being dia-
gonally dominant, which means in this case that the |b11 b22 | > |b21 b12 | inequality
has to be fulfilled (McCracken & Dorn 1974).
                           Xn                   n
                                                X
   If we now define b11 =     x2k ; b12 = b21 =   xk ; a = a1 , and b = a2 then the
                                  k=1                    k=1
standard equation system (2) can be expressed as:

                                         b11 a1 + b12 a2 = c1
                                         b21 a1 + b22 a2 = c2
             n
             X                    n
                                  X
where c1 =         xk yk , c2 =         yk so that the expressions in (7) allow determining
             k=1                  k=1
an approximation to its solution.


                                              Revista Colombiana de Estadística 37 (2014) 111–125

122                                                                     Rogelio Acosta, et al.


   It is not difficult to verify that the values given by the expressions (7) for
the unique solution of the standard system (2) match in each m iteration, those
provided for the a1 slope and the a2 intercept by each iteration of the iterative
adjustment process here described. Neither is it difficult to prove that the system
matrix (2) is diagonally dominant if in the {(x1 , y1 ); (x2 , y2 ); . . . ; (xn , yn )} set of
points, where n is a natural number, every two of the xk abscissas are different,
                         Xn        Xn    2
which means that the n       x2k >      xk inequality is fulfilled.
                            k=1          k=1
    Indeed, according to Bronshtein & Semendiaev (1971), in the inequality (which
is strict if there are at least two different xk values):
                                               r
                       |x1 + x2 + . . . xn |     x21 + x22 + . . . + x2n
                                             ≤
                                n                          n
it would suffice to square both sides to obtain, first:
                      (x1 + x2 + . . . + xn )2  x2 + x22 + . . . + x2n
                                 2
                                               ≤ 1
                               n                         n
Then, multiplying the two sides by n2 and expressing the sums in a compact form,
it results:
                                Xn       Xn     2
                              n    x2k ≥      xk
                                     k=1           k=1
As every two of the xk numbers are supposed to be different, the fulfillment of
      n
      X       X n     2
          2
the n    xk >       xk    inequality is finally guaranteed. This proves that the
      k=1           k=1
matrix of the standard equation system (2) is diagonally dominant, and in turn
implies that the expressions (7), obtained by applying the Gauss - Seidel Method,
converge to the unique solution of such system.
   At the same time, each iteration of the method was observed to coincide with
the parameter values that result, in each step, from the function of better adjust-
ment within the corresponding subclass. Therefore, a conclusion can be advanced
so that these functions converge to the least squares straight line of equation
     (0)     (0)
y = a1 x + a2 .


4. Conclusions
   An iterative method has been proposed to obtain an approximation of the best
adjustment function to a given set of points, consisting of determining the best
adjustment function within a certain subclass of the pre-fixed functional class each
time. Each subclass is dependent on a single parameter.
   As optimization is used only on one variable, it is not required to explicitly
write the standard equation system.
   For the case of linear adjustment with one independent variable, the iterative
method is revealed to be equivalent to the application of the standard equation


                                           Revista Colombiana de Estadística 37 (2014) 111–125

Iterative Method for Curve Adjustment                                              123

system of the Gauss - Seidel Method, which permits to show its convergence to
                                               (0)    (0)
the lowest quadratic straight line of the y = a1 x + a2 equation.
    Everything suggests that for other linear functional classes, with respect to the
parameters that define them, similar results should be obtained, in the sense of
the equivalence between the iterative method and the Gauss - Seidel one. Also,
in this way, it may be possible to show that the iterative method is convergent to
the best adjustment function obtained when applying the least squares m-ethod.
    The proposed approach offers the possibility of focusing the least squares
method along with that of curve adjustment, as an application of optimization
techniques of the real functions of one real variable, developed during the first
semester of higher education diplomas. This would permit speeding the approach
of the significant topic of curve adjustment.


Acknowledgments
    The authors acknowledge the financial support of AECID (Spain) under Project
No A2/039476/11 - “Institutional Strengthening in the Teaching of Subjects of
Statistics, Operative Investigation, Liability and Quality, and Scientific Applica-
tion in Topics of Regional Interest”.
                                                                  
              Recibido: agosto de 2013 — Aceptado: febrero de 2014


References
Batanero C, Burrill G, Reading C. Teaching Statistics in School Mathematics - Challenges for Teaching and Teacher Education: A Joint ICMI/IASE Study.(2011). Springer.
Braga G, Silveira C, Rodríguez V, Henrique de Cerqueira P, Aparecido W, Barros F. Quantifying herbage mass on rotationally stocked palisadegrass pastures using indirect methods.(2009). Scientia Agricola.
Bronshtein I, Semendiaev K. Mathematics Manual.(1971). Mir.
Choi H M, Hobert J P. Analysis of MCMC algorithms for Bayesian linear regression with Laplace errors.(2013). Journal of Multivariate Analysis.
Donal R J. A taxonomy of global optimization methods bases on response surfaces.(2001). Journal of Global Optimization.
Gutiérrez de Ravé E, Jiménez Hornero F J, Giráldez J V. A computer application for interpolation algorithms of curves.(2011). Computer Applications in Engineering Education.
Guzmán K P, Bolivar I, Alepuz M T, González D, Martin M. Impacto en el tiempo asistencial y el estadio tumoral de un programa de diagnóstico y tratamiento rápido del cáncer colorrectal.(2011). Revista Española de Enfermedades Digestivas.
Han C, Phillips P C. First difference maximum likelihood and dynamic panel estimation.(2013). Journal of Econometrics.
Ibarra M, Arana P. Crecimiento del camarón excavador Parastacus pugnax (Poeppig, 1835) determinado mediante técnica de marcaje.(2011). Latin American Journal of Aquatic Research.
Kelley C T. Iterative Methods for Optimization.(1999). Society for Industrial and Applied Mathematics.
McCracken D D,  Dorn W S. Métodos Numéricos y Programación Fortran.(1974). Editorial Pueblo y Educación.
Mudgal A, Hallmark S, Carriquiry A, Gkritza K. Driving behavior at a roundabout: A hierarchical Bayesian regression analysis.(2014). Transportation Research Part D: Transport and Environment.
Núñez E, Steyerberg E W, Núñez J. Estrategias para la elaboración de modelos estadísticos de regresión.(2011).Revista Española de Cardiología.
Ranganatham A. The Levenberg - Marquardt algorithm.(2004). Tutorial on LM Algorithm.
Santos da Silva L M, Estraviz L C, Caixeta J V, Carolina S B. Fitting a Taper function to minimize the sum of absolute deviations.(2006). Scientia Agricola.
Schmidt M. Least Squares Optimization with L1-Norm Regularization.(2005). CS542B Project Report.
Seo B, Lindsay B G. Nearly universal consistency of maximum likelihood in discrete models.(2013). Statistics and Probability Letters.
Vega Vilca J, Guzmán J. Regresión PLS y PCA como solución al problema de multicolinealidad en regresión múltiple.(2011). Revista de Matemática: Teoría y Aplicaciones.
Wei W, De Quan L, Jian L. On the system of multiple linear regression of higher education tuition in China.(2001). Journal Advanced Materials Research.
Yoshimori M, Lahiri P. A new adjusted maximum likelihood method for the Fay-Herriot small area model.(2014). Journal of Multivariate Analysis.
Zhao K, Valle D, Popescu S, Zhang X, Mallick B. Hyperspectral remote sensing of plant biochemistry using Bayesian model averaging with variable and band selection.(2013). Remote Sensing of Environment.
