Robust Mixture Regression Based on the Skew t Distribution. Mixtura robusta de modelos de regresiÃ³n basada en la distribuciÃ³n t asimÃ©trica
Giresun University, Giresun, Turkey.  Ankara University, Ankara, Turkey
Abstract
In this study, we explore a robust mixture regression procedure based on the skew t distribution in order to model heavy-tailed and/or skewed errors in a mixture regression setting. We present an EM-type algorithm to compute the maximum likelihood estimators for the parameters of interest using the scale mixture representation of the skew t distribution. The performance of proposed estimators is demonstrated by a simulation study and a real data example.
Key words: EM Algorithm, Maximum Likelihood, Mixture Regression Model, Skew t Distribution.
Resumen
En este estudio se explora una mixtura robusta de modelos de regresiÃ³n basada en la distribuciÃ³n t asimÃ©trica, con el propÃ³sito de modelar colas pesadas o asimÃ©tricas en los errores, en un escenario de mixtura de regresiones. Se usa un algoritmo EM para obtener los estimadores mÃ¡ximo verosÃ­miles empleando una mixtura de escala de la distribuciÃ³n t asimÃ©trica. El comportamiento de los estimadores propuestos se ilustra a travÃ©s de une estudio de simulaciÃ³n y de un ejemplo con datos reales.
Palabras clave: Algoritmo EM, mÃ¡xima verosimilitud, mixtura de regresiones, distribuciÃ³n t asimÃ©trica.


1. Introduction
    Mixture regression models are used to investigate the relationship between
variables that come from some unknown latent groups. These models were first
introduced by Quandt (1972) and Quandt & Ramsey (1978) as switching regres-
sion models and are widely used in areas such as engineering, genetics, biology,
econometrics and marketing. The parameter estimation of a mixture regression
model is usually based on the normality assumption. It is well-known that the
estimators that are based on the normality assumption perform well when the
error distribution is normal, but they are very sensitive to departures from nor-
mality (outliers, heavy-tailedness, skewness). To deal with the departures from
normality, robust mixture regression procedures have been proposed. Some of
these works can be summarized as follows: Markatou (2000) and Shen, Yang &
Wang (2004) used a weight function to estimate the parameters robustly in the
mixture regression models. Bashir & Carter (2012) used the S-estimation method
for the mixture linear regression model. Bai (2010) and Bai, Yao & Boyer (2012)
proposed a robust estimation procedure based on M-regression estimation to esti-
mate the parameters of the mixture regression model. Wei (2012) and Yao, Wei
& Yu (2014) explored the mixture regression model based on t distribution, which
is an extension of the mixtures of t distributions studied by Peel & McLachlan
(2000). Furthermore, Zhang (2013) studied the robust mixture regression model
using the Pearson Type VII distribution, and Song, Yao & Xing (2014) proposed
a robust estimation procedure for mixture regression model using the mixtures of
Laplace distributions. As it is pointed out by these authors, the robust mixture
regression estimation procedure based on the Laplace distribution can be regarded
as the application of the least absolute deviation (LAD) regression estimation to
the mixture regression models. Liu & Lin (2014) proposed mixture regression
model based on the skew normal distribution. Also, Pereira, Marques & da Costa
(2012) studied the performance of the estimates procedure for the mixtures of skew
normal distributions.
    In this paper, we examine a robust mixture regression procedure based on the
skew t distribution to efficiently deal with heavy-tailedness and skewness in the
mixture regression model setting. This is an extension of the mixtures of skew
t distributions proposed by Lin, Lee & Hsieh (2007) to the mixture regression
models. We will use the skew t distribution results from the scale mixture of
the skew normal distribution that was introduced by Gupta, Chang & Huang
(2002), Gupta (2003) and Azzalini & Capitaino (2003). The scale mixture repre-
sentation of the skew t distribution enables to easily implement an Expectation-
Maximization (EM) algorithm to obtain the maximum likelihood (ML) estimators
for the parameters of interest in the mixture regression model. For the mixture
regression model based on the skew t distribution, refer to the works by DoÄŸru &
Arslan (2014) and DoÄŸru (2015).
    Recently, Zeller, Cabral & Lachos (2016) have proposed in robust mixture
regression model based on scale mixtures of skew normal distributions. They con-
sider the problem in general for the scale mixtures of skew normal distributions
and compare the performance of the skew normal, skew t, and skew slash distri-


                                     Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

Robust Mixture Regression Based on the Skew t Distribution                                      47

butions via simulation studies and real data example. In their simulation study
and real data example, they assumed equal variance, which differs from our exten-
sive simulation study and real data example. Also, in our paper, we consider the
performance of the estimators for the outlier case, apart from heavy-tailedness,
(see simulation study Case the V) which is not considered in their paper. We also
explore the outlier case in real data example by adding ten extra outliers to the
data to illustrate the performance of the estimators that are considered in this
study. Furthermore, we compute the standard errors using Fisher information
based theory in real data example.
    The paper is organized as follows: In Section 2, we give the basic definition
of the mixture regression model. In Section 3, we present the robust mixture
regression results based on the skew t distribution. In Sections 4 and 5, we give
a simulation study and a real data example to compare the performance of the
proposed estimation procedure with the other estimation procedures obtained from
normal, t (Yao et al. 2014), and skew normal (Liu & Lin 2014) distributions. The
paper ends with a conclusion section.


2. Mixture Regression Model
    The model setting for a general mixture of linear regression model can be
defined as follows. Let x be a p-dimensional vector of observed values of the
explanatory variables, Y be the response variable, and Z be a latent class variable
independent of x. Suppose that given Z = i, the response variable Y depends on
the explanatory variable x in a linear way

                               Y = x0 Î² i + i , i = 1, 2, . . . , g,                          (1)

where Î² i = (Î²i1 , Î²i2 , . . . , Î²ip )0 is the regression parameters, i is the error term,
and g is the number of components in the mixture regression model. It is assumed
that i and x are independent and x includes both predictors and constant 1. In
the literature, it is often assumed that the random errors (i ) have distributions
from the location-scale family with zero means and Ïƒi scale parameters. Suppose
Pg P (Z = i | x) = wi , i = 1, 2, . . . , g denote the mixing probabilities with
that
   i=1 wi = 1, then the conditional density function of Y given x is

                                              g
                                              X
                             f (y | x, Î˜) =         wi fi (y; x0 Î² i , Ïƒi ),                   (2)
                                              i=1

where fi (y; x0 Î² i , Ïƒi ) is the probability density function (pdf) of the ith component
with some shape parameters (e.g. degrees of freedom for t distribution), and
Î˜ = (w1 , . . . , wg , Î² 1 , . . . , Î² g , Ïƒ1 , . . . , Ïƒg )0 is the unknown parameter vector. This
model is called as a g-component mixture regression model. The ML estimation
method is used to estimate the unknown parameter vector Î˜ in model (2). Let
{(x1 , y1 ), (x2 , y2 ),. . ., (xn , yn )} be a given sample. Then, the ML estimator of Î˜
is obtained by maximizing the following log-likelihood function with respect to Î˜


                                              Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

48                                                                     Fatma Zehra DoÄŸru & Olcay Arslan


                                         n
                                         X             g
                                                      X                                
                             `(Î˜) =             log           wi fi (yj ; x0j Î² i , Ïƒi ) .                     (3)
                                          j=1           i=1

However, it should be noted that the ML estimators cannot be explicitly obtained.
The EM algorithm (Dempster, Laird & Rubin 1977) is used to find the ML esti-
mates.


3. Robust Mixture Regression Based on the Skew t
   Distribution
    In this section, we will use the skew t distribution in order to model possible
skewed and heavy-tailed errors in the mixture regression model. By doing so, we
will obtain more robust estimators for the mixture regression model parameters.
We will use the Azzalini type skew t distribution (Gupta et al. 2002, Gupta 2003,
Azzalini & Capitaino 2003) with the pdf
                                              r        
                         2        2              Î½+1          
                 f (; Ïƒ , Î», Î½) = tÎ½ (Î·)TÎ½+1 Î»Î·         , Î· = ,  âˆˆ R,                                        (4)
                                  Ïƒ              Î·2 + Î½       Ïƒ

where Î» âˆˆ R is the skewness parameter, tÎ½ (Â·) is the pdf of the t distribution with
Î½ âˆˆ (0, âˆž) degrees of freedom, and TÎ½+1 (Â·) is the cumulative density function (cdf)
of the t distribution with Î½ + 1 degrees of freedom.
    In the mixture regression model given in (2), assume that the errors have a skew
t distribution with zero location, and Ïƒi2 , Î»i and Î½i scale, skewness, and degrees
of freedom parameters, respectively. In contrast to the symmetric case, the mean
                                                          p Î“( Î½i âˆ’1 )
E(i ) 6= 0. For the skew t distribution, E(i ) = Ïƒi Î´Î»i Î½Ï€i Î“( Î½2i ) when Î½i > 1,
                 p                                                2

where Î´Î»i = Î»i / (1 + Î»2i ). Thus, E(yj ) = x0j Î² i + E(i ), which only affects the
intercept. Thus, when we estimate the intercept, we will take this into account
                          [
and correct Î²Ì‚0 by using E( i ). In order to estimate the unknown parameters, we
should maximize the following log-likelihood function
                                    n
                                    X            g
                                                X                                           
                         `(Î˜) =           log           wi fi (yj ; x0j Î² i , Ïƒi2 , Î»i , Î½i ) ,                (5)
                                    j=1           i=1


where Î˜ = (w1 , . . . , wg , Î² 1 , . . . , Î² g , Ïƒ12 , . . . , Ïƒg2 , Î»1 , . . . , Î»g , Î½1 , . . . , Î½g )0 . However,
the maximizer of the above log-likelihood function cannot be explicitly obtained,
so an EM-type algorithm should be used to estimate Î˜. Here, we will use the
following EM algorithm to obtain the estimators.
     Let Zj = (Z1j , . . . , Zgj )0 be the latent variables such that

                                 1, if j th observation is from ith component
                             
                   Zij =
                                 0, otherwise,

                                                      Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

Robust Mixture Regression Based on the Skew t Distribution                                            49

where j = 1, . . . , n and i = 1, . . . , g. To simplify the EM algorithmâ€™s steps, we will
use the stochastic representation of the skew t distribution given by Azzalini &
Capitaino (2003) (see Appendix for more detailed explanations). This stochastic
representation yields the following hierarchical formulation in terms of the condi-
tional distributions
                                                              Îº2i
                                                                 
                                                  0
                            Yj | Î³j , Ï„j âˆ¼ N xj Î² i + Î±i Î³j ,       ,
                                                              Ï„j
                                                          
                                                 1
                            Î³j | Ï„j âˆ¼ TN 0, ; (0, âˆž) ,
                                                 Ï„j
                              Ï„j âˆ¼ Gamma(Î½i /2, Î½i /2),

where TN(Â·) denotes the truncated normal distribution, Î±i = Ïƒi Î´Î»i , and Îº2i =
Ïƒi2 (1 âˆ’ Î´Î»2 i ). Then, considering (Î³, Ï„ ) and zj are missing data, the complete data
log likelihood function for (y, Î³, Ï„ , zj ) given X can be written as
                              g
                            n X
                                                         log Îº2i
                            X           
    `c (Î˜; y, Î³, Ï„ , z) =           zij log wi âˆ’ log Ï€ âˆ’
                            j=1 i=1
                                                             2
                           Î½i      Î½i    Î½i                                                        (6)
                         + log            + log Ï„j
                            2        2      2
                                                            0
                                                    (yj âˆ’ xj Î² i âˆ’ Î±i Î³j )2   Ï„j Î³j2
                                 Î½  Î½ Ï„                                          
                                      i       i j
                         âˆ’ log Î“           âˆ’      âˆ’                         âˆ’         ,
                                     2        2            2Îº2i /Ï„j             2

where X =(x1 , . . . , xn )0 , y = (y1 , . . . , yn ), Î³ = (Î³1 , . . . , Î³n ) and Ï„ = (Ï„1 , . . . , Ï„n ).
Moreover, based on the theory of the EM algorithm, the conditional expectation
of the complete data log-likelihood function, given the observed data and the
                                       (k)
current parameter estimate Î˜Ì‚ , should be calculated. That is, we have to find
the following conditional expectation
                                        g
                                      n X
                                                                         log Îº2i
                                                            
                                      X                                              Î½i   Î½ 
                                                                                            i
   E (`c (Î˜; y, Î³, Ï„ , zj ) | yj ) =          E(Zij | yj ) log wi âˆ’              + log
                                      j=1 i=1
                                                                           2         2     2
                                           Î½  Î½                          Î½i
                                               i       i
                                   âˆ’ log Î“        + E(log Ï„j | yj ) âˆ’ E(Ï„j | yj )
                                              2       2                     2
                                                         0
                                                              2
                                     E(Ï„j | yj ) yj âˆ’ xj Î² i          Î±i2 E(Ï„j Î³j2 | yj )
                                   âˆ’                              âˆ’
                                                 2Îº2i                       2Îº2i
                                                              0
                                     Î±i E(Ï„j Î³j | yj )(yj âˆ’ xj Î² i )
                                                                     
                                   +                                  .
                                                   Îº2i
                                                                                              (7)

The conditional expectations E(Ï„j | yj ), E(Ï„j Î³j | yj ), E(Ï„j Î³j2 | yj ), and E(log Ï„j |
yj ) can be calculated using the conditional expectations presented in the Appendix.
The conditional expectation E(Zij | yj ) can be computed using the classical theory
of mixture modeling. Then, the steps of the EM algorithm can be given as follows.


                                                Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

50                                                               Fatma Zehra DoÄŸru & Olcay Arslan


EM algorithm:
1. Take initial parameter estimate Î˜(0) and a stopping rule âˆ†.
                                                                       (k)   (k)    (k)        (k)                 (k)
2. E step: Compute the conditional expectations zij , s1ij , s2ij , s3ij and s4ij for
k = 0, 1, 2, . . . using the following equations for k = 0, 1, 2, . . . iteration
                                                              (k)
                                                        0
                                                                                          
                                  (k)
                                        wÌ‚i(k) fi yj ; xj Î²Ì‚ i , ÏƒÌ‚i2(k) , Î»Ì‚(k)
                                                                              i
                                                                                      (k)
                                                                                  , Î½Ì‚i
            (k)
          zÌ‚ij = E Zij | yj , Î˜Ì‚        =                            (k)
                                                                                           , (8)
                                                     f yj ; xj , Î˜Ì‚


                                                                                         r        
                                                                                              (k)
                                                                                      (k)   Î½Ì‚i +3
                                                                      T  (k)       MÌ‚ ij
                                                        Î½Ì‚ik + 1                              (k)
                                                                     Î½Ì‚i +3
                               (k)
                                    
                                          (k)                                               Î½Ì‚i +1
     sÌ‚1ij = E Zij Ï„j | yj , Î˜Ì‚       = zÌ‚ij          2(k)      (k)
                                                                                                   ,                   (9)
                                                                                           (k)
                                                    Î·Ì‚ij + Î½Ì‚i              TÎ½Ì‚ (k) +1 MÌ‚ij
                                                                              i




                                                (k)
                                                    
                 sÌ‚2j = E Zij Î³j Ï„j | yj , Î˜Ì‚
                                           (k)
                                      0
                                               
                          (k)                       (k)
                        Î´Ì‚Î»i yj âˆ’ xj Î²Ì‚ i         sÌ‚1ij
                      =               (k)
                                   ÏƒÌ‚i                                                                               (10)
                                                                                               !
                                                                                      (k)
                              q                                                    Î½Ì‚
                                                                         !âˆ’          i
                          (k)         2(k)                2(k)                        2 +1
                        zÌ‚ij    1 âˆ’ Î´Ì‚Î»i                Î·Ì‚ij
                      +       (k)
                                                                    +1                           ,
                           Ï€ÏƒÌ‚ fË†(yj )
                              i               Î½Ì‚i
                                                 (k)
                                                        1 âˆ’ Î´Ì‚
                                                               2(k)
                                                               Î»i



                                    (k)
                                       
       (k)
     sÌ‚3ij = E Zij Î³j2 Ï„j | yj , Î˜Ì‚
                             0   (k) !2               (
               2(k) yj âˆ’ xj Î²Ì‚ i          (k)     (k)            2(k)
           = Î´Ì‚Î»i          (k)
                                        sÌ‚1ij + zÌ‚ij (1 âˆ’ Î´Ì‚Î»i )
                         ÏƒÌ‚i                                                                                         (11)
                                                                                                       !
                                                                                               (k)
                              (k)
                                   q                                                     Î½Ì‚
               (k)         0                  2(k)                            !âˆ’               i
             Î´Ì‚Î»i yj âˆ’ xj Î²Ì‚ i          1 âˆ’ Î´Ì‚Î»i              2(k)                              2 +1
                                                                                                           )
                                                            Î·Ì‚ij
           +             2(k) Ë†                         (k)         2(k)
                                                                           +1                                  ,
                     Ï€ÏƒÌ‚  i    f (yj )(k)             Î½Ì‚ (1 âˆ’ Î´Ì‚
                                                          i          Î»i  )




                                                Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

Robust Mixture Regression Based on the Skew t Distribution                                                                                                        51



                                            (k)
                                               
           (k)
         sÌ‚4ij = E Zij log(Ï„j ) | yj , Î˜Ì‚
                                                         2(k)        (k)
                     (        (k)     !                                  !
                              Î½Ì‚i + 1                  Î·Ì‚ij + Î½Ì‚i
               = zÌ‚ij DG                    âˆ’ log
                                   2                             2
                                     ï£«                           r                   ï£¶
                                                                         (k)
                                   ! T (k)            (k) (k)          Î½Ì‚i +3
                         (k)         ï£¬ Î½Ì‚i +3      Î»Ì‚ i    Î·Ì‚ ij      (k)    2(k)
                       Î½Ì‚i + 1                                      Î½Ì‚i +Î·Ì‚ij          ï£·
               +      2(k)     (k)
                                     ï£¬                           r                âˆ’ 1ï£·
                                                                         (k)
                    Î·Ì‚ij + Î½Ì‚i
                                     ï£­                (k) (k)          Î½Ì‚i +1
                                                                                       ï£¸
                                       TÎ½Ì‚ (k) +1 Î»Ì‚i Î·Ì‚ij            (k)    2(k)
                                                                 i                                    Î½Ì‚i       +Î·Ì‚ij
                                                                                                                     r                          !             (12)
                                                                                                                                 (k)
                                                                                                      (k) (k)                  Î½Ì‚i     +1
                                   (k) (k)        2(k)                           tÎ½Ì‚ (k) +1         Î»Ì‚i Î·Ì‚ij                 (k)        2(k)
                                 Î»Ì‚i Î·Ì‚ij (Î·Ì‚ij            âˆ’ 1)                    i                                     Î½Ì‚i         +Î·Ì‚ij
                +q                                                                                                  r
                                                                                                                                 (k)
                                                                                                                                                
                                 (k)              (k)                2(k)     (k) (k)                                          Î½Ì‚i     +1
                            (Î½Ì‚i          + 1)(Î½Ì‚i         + Î·Ì‚ij )3 T (k)  Î»Ì‚i Î·Ì‚ij                                         (k)         2(k)
                                                                      Î½Ì‚ +1         i                                    Î½Ì‚i         +Î·Ì‚ij
                                                                                              (k)
                                                                                         MÌ‚ij                                                   )
                                                                                          Z
                                                     1
                +                         r
                                                (k)
                                                             gÎ½Ì‚ (k) (x)tÎ½Ì‚ (k) +1 (x)dx ,
                                   (k) (k)    Î½Ì‚i +1             i          i
                      TÎ½Ì‚ (k) +1 Î»Ì‚i Î·Ì‚ij    (k)    2(k)   âˆ’âˆž
                            i                                  Î½Ì‚i       +Î·Ì‚ij

                        0
                (Â·)
where DG(Â·) = Î“Î“(Â·) is the digamma function and
                                                                                                              v
                                      (k)                                  (k)                                                                   (k)
                (yj âˆ’ x0j Î²Ì‚ i )
                                                                                                              u
       (k)                                                               Î»Ì‚i                  (k)     (k) (k)
                                                                                                              u                                Î½Ì‚i
     Î·Ì‚ij =                     (k)
                                             , Î´Ì‚Î»(k) = q                                , MÌ‚  ij = Î»Ì‚ Î·Ì‚ t      i      ij               (k)           2(k)
                                                                                                                                                              ,
                       ÏƒÌ‚i                       i
                                                                      1 + Î»Ì‚i
                                                                                 2(k)                                                  Î½Ì‚i      + Î·Ì‚ij

                                                           x2        x2 (Î½Ì‚ + 1) âˆ’ Î½Ì‚ âˆ’ 1
                                                               
                       Î½Ì‚ + 2         Î½Ì‚ + 1
    gÎ½Ì‚ (x) = DG(             ) âˆ’ DG(        ) âˆ’ log 1 +          +                        ,
                          2              2               Î½Ì‚ + 1     (Î½Ì‚ + 1)(Î½Ì‚ + 1 + x2 )
                g
                X       (k)           2                  (k)                           (k)
 fË†(yj )(k) =         wÌ‚i           (k) Î½Ì‚i
                                            t (k) (Î·Ì‚ij )TÎ½Ì‚ (k) +1 (MÌ‚ij ).
                i=1               ÏƒÌ‚i                                i



                                                       (k)
                                                          
Then, we form the following objective function Q Î˜; Î˜Ì‚

                                             n X g        
                               (k)
                                           X         (k)         1          Î½i   Î½ 
                                                                                    i
             Q Î˜; Î˜Ì‚                      =         zÌ‚ij log wi âˆ’ log(Îº2i ) + log
                                            j=1 i=1
                                                                  2          2     2
                                                               Î½ 
                                                                  i
                                                        âˆ’ log Î“
                                                                 2
                                                 (k)                     (k)            (k)                 0
                                            Î½i sÌ‚1ij   Î½i sÌ‚4ij   sÌ‚1ij (yj âˆ’ xj Î² i )2                                                                       (13)
                                          âˆ’          +          âˆ’
                                                2          2               2Îº2i
                                                  (k)                      0                    (k)
                                            Î±i sÌ‚2ij (yj âˆ’ xj Î² i ) Î±i2 sÌ‚3ij
                                          +                        âˆ’          .
                                                       Îº2i           2Îº2i



                                                                      Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

52                                                                                Fatma Zehra DoÄŸru & Olcay Arslan

                                  (k)
                                     
3. M step 1: Maximize the Q Î˜; Î˜Ì‚       with respect to the unknown parameters
(wi , Î² i , Ïƒi2 ), assuming that (Î»i , Î½i ) are fixed, in order to obtain (k + 1)th values for
the parameters (wi , Î² i , Ïƒi2 ). This maximization yields
                                                                     n
                                                                     P          (k)
                                                                           zÌ‚ij
                                                       (k+1)   j=1
                                                     wÌ‚i     =                        ,                                           (14)
                                                                          n

                       ï£«                 ï£¶âˆ’1 ï£«                               ï£¶
                          n                     n 
                (k+1)                  0
                                                                         
                               (k)                       (k)     (k) (k)
                         X                    X
             Î²Ì‚ i     =ï£­     sÌ‚1ij xj xj ï£¸ ï£­        yj sÌ‚1ij âˆ’ Î´Ì‚Î»i sÌ‚2ij xj ï£¸,                                                   (15)
                         j=1                   j=1


                                                           n                          0   (k)
                                                           P       (k)
                                                                 sÌ‚2ij (yj âˆ’ xj Î²Ì‚ i )
                                       (k+1)               j=1
                                  Î±Ì‚i                 =              n                              ,                             (16)
                                                                     P          (k)
                                                                           sÌ‚3ij
                                                                     j=1


                  n                          0       (k)                                        0       (k)
                         (k)                                         (k) (k)                                      2(k) (k)
                        sÌ‚1ij (yj âˆ’ xj Î²Ì‚ i )2 âˆ’ 2Î±Ì‚i sÌ‚2ij (yj âˆ’ xj Î²Ì‚ i ) + Î±Ì‚i
                  P
                                                                                                                      sÌ‚2ij
       2(k+1)   j=1
     ÎºÌ‚i      =                                                    n                                                          ,   (17)
                                                                   P      (k)
                                                                         zÌ‚ij
                                                                   j=1


                                         2(k+1)                  2(k+1)           2(k+1)
                                       ÏƒÌ‚i                 = ÎºÌ‚i          + Î±Ì‚i             .                                     (18)

4. M step 2: Using the new values for (wi , Î² i , Ïƒi2 ) that were obtained in M step
1, the following equations are solved to obtain new estimates for the parameters
(Î»i , Î½i )
                                         ï£«                                            ï£¶
                           n                n                   0 (k+1) 2    n
                          X     (k)
                                          X      (k) (y j âˆ’  x  j Î²Ì‚ i )    X     (k)
         Î´Î»i (1 âˆ’ Î´Î»2 i )     zÌ‚ij âˆ’ Î´Î»i ï£­     sÌ‚1ij          2(k+1)
                                                                          +     sÌ‚3ij ï£¸
                          j=1              j=1             ÏƒÌ‚ i             j=1

                                               n                 0                                        (k+1)
                                                    (k) (yj âˆ’ xj Î²Ì‚ i      )
                                              X
                                                     +(1 + Î´Î»2 i )
                                                  sÌ‚2ij         (k+1)
                                                                             = 0,                                                 (19)
                                              j=1            ÏƒÌ‚i
                                                    Pn  (k)           (k)
                                                                           
                            Î½           Î½          j=1   sÌ‚4ij âˆ’ sÌ‚1ij
                              i             i
                        log      + 1 âˆ’ DG       +         Pn       (k)
                                                                             = 0.                                                 (20)
                             2             2
                                                            j=1 zÌ‚ij

Also the (k + 1)th estimate of Î»i can be obtained by using the following equation
                                          q
                         (k+1)     (k+1)         2(k+1)
                       Î»Ì‚i     = Î´Ì‚Î»i    / 1 âˆ’ Î´Ì‚Î»i     ,                    (21)
        (k+1)       (k+1)          (k+1)
where Î´Ì‚Î»i      = Î±Ì‚i          / ÏƒÌ‚i             .


                                                              Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

Robust Mixture Regression Based on the Skew t Distribution                                       53

                                                                                  (k+1)    (k)
5. Repeat E and M steps until the convergence criteria k Î˜Ì‚            âˆ’ Î˜Ì‚      k<
âˆ† is satisfied. Alternatively, the absolute difference of the actual log-likelihood
      (k+1)         (k)                      (k+1)              (k)
k `(Î˜Ì‚   ) âˆ’ `(Î˜Ì‚         ) k< âˆ† or k `(Î˜Ì‚           ) / `(Î˜Ì‚         ) k< âˆ† can be used (see Dias
& Wedel 2004).
    Note that the equation given in (20) can be used to estimate the degrees of
freedom of the skew t distribution. However, for the sake of robustness, we will
assume that the degrees of freedom are fixed throughout this paper. We take all the
degrees of freedom as 2. This is suggested by Lange, Little & Taylor (1989). Also,
it is pointed out by Lucas (1997) that the estimators based on the t distribution
are not locally robust when the degrees of freedom are estimated.


4. Simulation Study
    In this section, we present a simulation study to demonstrate the performance
of the proposed mixture regression model based on skew t distribution (MixregST)
over the mixture regression model based on normal distribution (MixregN), mix-
ture regression model based on t distribution (Mixregt) and mixture regression
model based on skew normal distribution (MixregSN) in terms of bias and mean
squared error (MSE). The formulas of bias and MSE are given

                               bias(
                               d Î¸Ì‚)      =     Î¸Ì„ âˆ’ Î¸,
                                                       N
                                                1 X Ë†
                              M
                              \ SE(Î¸Ì‚)    =           (Î¸i âˆ’ Î¸)2 ,
                                                N i=1

where Î¸ is the
             Ptrue   parameter value, Î¸Ë†i is the estimate of Î¸ for the ith simulated
                N
data, Î¸Ì„ = N1 i=1 Î¸Ë†i , and N = 500 is the number of replicates. In the simulation
study, the sample sizes are taken as 200 and 400. The simulation study and
real data example are conducted using M AT LAB R2013a. For all numerical
calculations, the stopping rule âˆ† is taken as 10âˆ’6 .
   We generate the data {(x1j , x2j , yj ), j = 1, . . . , n} from the following two com-
ponent mixture regression models (Bai et al. 2012)
                                  
                                      0 + X1 + X2 + 1 , Z = 1,
                            Y =
                                      0 âˆ’ X1 âˆ’ X2 + 2 , Z = 2,

where P (Z = 1) = 0.25 = w1 , X1 âˆ¼ N(0, 1) and X2 âˆ¼ N(0, 1). Furthermore, the
                                               0           0                            0
model coefficients are Î² 1 = (Î²10 , Î²11 , Î²12 ) = (0, 1, 1) and Î² 2 = (Î²20 , Î²21 , Î²22 ) =
           0
(0, âˆ’1, âˆ’1) .
   We take the following error distributions:
Case I: 1 , 2 âˆ¼ N(0, 1), standard normal distribution.
Case II: 1 , 2 âˆ¼ t3 (0, 1), t distribution with the degrees of freedom 3.
Case III: 1 , 2 âˆ¼ 0.95N(0, 1) + 0.05N(0, 25), contaminated normal distribution.
Case IV: 1 , 2 âˆ¼ ST(0, 1, 0.5, 3), skew t distribution.

                                           Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

54                                               Fatma Zehra DoÄŸru & Olcay Arslan


Case V: 1 , 2 âˆ¼ N(0, 1), standard normal distribution with %5 outliers, X1 =
20, X2 = 20 and Y = 100.
    We use Case I to compare the estimators with the traditional MLE (MixregN)
when the error terms have the normal distribution and there are no outliers. Case
II is the example for the heavy-tailed error distribution case. The distribution
given in Case III is to create outliers. This distribution is often considered in
literature as an outlier model. Case IV is to examine the behavior of the estimators
when the error term is skewed and heavy-tailed. Case V is considered to test the
performances of the estimators to deal with the high leverage points. In this case
%5 of the observations are replaced by X1 = 20, X2 = 20 and Y = 100.
    Table 1 and 2 show the simulation results for the sample sizes 200 and 400.
The tables include the MSEs, and the biases of the parameter estimates, and
the true parameter values. We can observe from the simulation study results
that the MixregN has the best result in Case I. Moreover, the other estimators
obtained from Mixregt, MixregSN, and MixregST have similar performances when
the errors have a normal distribution. In Case II, Mixregt performs best, as
expected. Also, MixregST has a lower bias and MSE values compared to the
MixregN and MixregSN for almost all cases. For Case III, MixregN and MixregSN
are drastically affected by the contamination. However, Mixregt and MixregST
perform better than the other estimators and Mixregt is comparable with the
MixregST. Similarly, MixregN and MixregSN have the worst performance and
Mixregt, and MixregST have similar performance in Case IV. Finally, in the outlier
case, all estimators are affected by the outliers. However, Mixregt and MixregST
have the lowest bias and MSE values in almost all cases. In summary, concerning
all the estimators, the Mixregt and MixregST are resistant to the skewness and the
heavy tailedness in the data, and they behave better than MixregN and MixregSN
in the case of outliers in x direction.




                                      Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

Robust Mixture Regression Based on the Skew t Distribution                                          55

                  Table 1: MSE (bias) values of estimates for n = 200.

              MixregN                    Mixregt             MixregSN            MixregST
              Case I:1 , 2 âˆ¼ N(0, 1)
   Î²10 :0     0.0456 (0.0150)            0.0587 (0.0134)     0.1726 (âˆ’0.3560)    0.1317 (0.2306)
   Î²20 :0     0.0090 (0.0019)            0.0098 (0.0039)     0.1447 (âˆ’0.3678)    0.0575 (âˆ’0.2084)
   Î²11 :1     0.0348 (âˆ’0.0013)           0.0495 (âˆ’0.0064)    0.0349 (âˆ’0.0016)    0.0546 (âˆ’0.0036)
   Î²21 :âˆ’1    0.0085 (âˆ’0.0004)           0.0103 (0.0031)     0.0085 (âˆ’0.0004)    0.0118 (0.0212)
   Î²12 :1     0.0401 (âˆ’0.0243)           0.0483 (âˆ’0.0308)    0.0401 (âˆ’0.0242)    0.0617 (âˆ’0.0296)
   Î²22 :âˆ’1    0.0089 (âˆ’0.0062)           0.0107 (0.0024)     0.0089 (âˆ’0.0062)    0.0125 (0.0201)
   w:0.25     0.0021 (0.0079)            0.0023 (0.0059)     0.0021 (0.0079)     0.0035 (âˆ’0.0063)
              Case II:1 , 2 âˆ¼ t3 (0, 1)
   Î²10 :0     11.5674 (âˆ’0.2939)          0.0930 (âˆ’0.0121)    11.6586 (âˆ’0.9305)   0.3151 (0.2406)
   Î²20 :0     1.2217 (0.0796)            0.0136 (âˆ’0.0050)    1.3914 (âˆ’0.5527)    0.1397 (âˆ’0.3327)
   Î²11 :1     7.6108 (0.4273)            0.0959 (âˆ’0.0180)    7.6526 (0.3704)     0.1415 (0.0036)
   Î²21 :âˆ’1    1.2984 (âˆ’0.0331)           0.0145 (âˆ’0.0064)    1.2011 (0.0192)     0.0171 (0.0259)
   Î²12 :1     8.2789 (0.1660)            0.0981 (0.0027)     8.2956 (0.2624)     0.1678 (0.0282)
   Î²22 :âˆ’1    1.9409 (0.1250)            0.0137 (âˆ’0.0031)    1.6075 (0.0762)     0.0167 (0.0283)
   w:0.25     0.0226 (âˆ’0.0372)           0.0033 (0.0112)     0.0214 (âˆ’0.0352)    0.0055 (âˆ’0.0067)
              Case III:1 , 2 âˆ¼ 0.95N(0, 1) + 0.05N(0, 25)
   Î²10 :0     6.0158 (âˆ’0.0052)           0.0634 (âˆ’0.0062)    6.1249 (âˆ’0.6206)    0.1517 (0.2053)
   Î²20 :0     0.6299 (0.0054)            0.0118 (âˆ’0.0080)    0.6282 (âˆ’0.5670)    0.0911 (âˆ’0.2711)
   Î²11 :1     4.5781 (0.2371)            0.0599 (0.0078)     4.8849 (0.2067)     0.0727 (0.0119)
   Î²21 :âˆ’1    0.2236 (0.0418)            0.0106 (âˆ’0.0068)    0.1302 (0.0649)     0.0124 (0.0155)
   Î²12 :1     2.9126 (âˆ’0.0271)           0.0620 (0.0021)     2.7706 (0.0830)     0.0774 (0.0192)
   Î²22 :âˆ’1    0.1607 (0.0628)            0.0090 (0.0033)     0.0614 (0.0778)     0.0108 (0.0250)
   w:0.25     0.0167 (âˆ’0.0472)           0.0026 (0.0039)     0.0136 (âˆ’0.0526)    0.0034 (âˆ’0.0098)
              Case IV:1 , 2 âˆ¼ ST(0, 1, 0.5, 3)
   Î²10 :0     8.4499 (1.0601)            0.2783 (0.4422)     6.1264 (0.3167)     0.9691 (0.7550)
   Î²20 :0     0.3472 (0.4787)            0.1524 (0.3759)     0.1323 (âˆ’0.0886)    0.0231 (0.0590)
   Î²11 :1     2.9291 (0.2448)            0.0851 (âˆ’0.0296)    2.7053 (0.2225)     0.1605 (âˆ’0.0107)
   Î²21 :âˆ’1    0.0600 (0.0432)            0.0120 (âˆ’0.0133)    0.0540 (0.0381)     0.0146 (0.0230)
   Î²12 :1     5.9774 (âˆ’0.1412)           0.0862 (âˆ’0.0195)    5.6460 (âˆ’0.0863)    0.1911 (0.0005)
   Î²22 :âˆ’1    0.0789 (0.0798)            0.0115 (âˆ’0.0029)    0.0731 (0.0715)     0.0154 (0.0336)
   w:0.25     0.0125 (âˆ’0.0296)           0.0033 (0.0118)     0.0116 (âˆ’0.0260)    0.0050 (âˆ’0.0156)
              Case V:1 , 2 âˆ¼ N(0, 1) (% 5 outliers)
   Î²10 :0     2.2247 (0.1553)            1.3245 (0.1820)     2.5926 (âˆ’0.4879)    5.9114 (2.1745)
   Î²20 :0     0.0146 (0.0111)            0.0106 (0.0072)     0.2401 (âˆ’0.4728)    0.0392 (âˆ’0.1678)
   Î²11 :1     3.2773 (1.5211)            2.8341 (1.5030)     3.3162 (1.5107)     2.6095 (1.4250)
   Î²21 :âˆ’1    0.0833 (0.2528)            0.0234 (0.1077)     0.0826 (0.2519)     0.0296 (0.1283)
   Î²12 :1     3.1162 (1.4674)            2.7897 (1.4869)     3.2436 (1.4870)     2.7237 (1.4655)
   Î²22 :âˆ’1    0.0798 (0.2482)            0.0225 (0.1055)     0.0786 (0.2472)     0.0281 (0.1244)
   w:0.25     0.0093 (âˆ’0.0937)           0.0061 (âˆ’0.0751)    0.0094 (âˆ’0.0939)    0.0112 (âˆ’0.1029)
   Note: Value in parentheses indicates the bias.




                                                   Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

56                                                              Fatma Zehra DoÄŸru & Olcay Arslan

                    Table 2: MSE (bias) values of estimates for n = 400.

                MixregN                    Mixregt             MixregSN            MixregST
                Case I:1 , 2 âˆ¼ N(0, 1)
     Î²10 :0     0.0203 (0.0088)            0.0265 (0.0114)     0.1564 (âˆ’0.3687)    0.0782 (0.2081)
     Î²20 :0     0.0043 (0.0044)            0.0050 (0.0058)     0.1427 (âˆ’0.3716)    0.0565 (âˆ’0.2211)
     Î²11 :1     0.0149 (0.0008)            0.0192 (âˆ’0.0019)    0.0149 (0.0009)     0.0227 (0.0034)
     Î²21 :âˆ’1    0.0040 (âˆ’0.0028)           0.0048 (0.0016)     0.0040 (âˆ’0.0027)    0.0057 (0.0197)
     Î²12 :1     0.0160 (âˆ’0.0100)           0.0213 (âˆ’0.0185)    0.0161 (âˆ’0.0100)    0.0245 (âˆ’0.0110)
     Î²22 :âˆ’1    0.0044 (0.0009)            0.0053 (0.0070)     0.0044 (0.0010)     0.0065 (0.0247)
     w:0.25     0.0012 (0.0035)            0.0013 (0.0006)     0.0012 (0.0035)     0.0018 (âˆ’0.0123)
                Case II:1 , 2 âˆ¼ t3 (0, 1)
     Î²10 :0     14.3296 (âˆ’0.3312)          0.0365 (âˆ’0.0137)    14.2254 (âˆ’0.9701)   0.0830 (0.1669)
     Î²20 :0     0.6052 (0.0125)            0.0066 (âˆ’0.0066)    0.6330 (âˆ’0.6752)    0.1411 (âˆ’0.3601)
     Î²11 :1     10.6597 (0.4839)           0.0321 (âˆ’0.0054)    10.1135 (0.4010)    0.0427 (0.0239)
     Î²21 :âˆ’1    0.5987 (0.0527)            0.0068 (âˆ’0.0052)    0.1809 (0.0921)     0.0083 (0.0272)
     Î²12 :1     12.1779 (0.3384)           0.0334 (0.0052)     11.8293 (0.5888)    0.0421 (0.0288)
     Î²22 :âˆ’1    1.5732 (0.0903)            0.0062 (âˆ’0.0041)    0.9058 (0.0454)     0.0078 (0.0273)
     w:0.25     0.0161 (âˆ’0.0602)           0.0014 (0.0049)     0.0143 (âˆ’0.0591)    0.0020 (âˆ’0.0134)
                Case III:1 , 2 âˆ¼ 0.95N(0, 1) + 0.05N(0, 25)
     Î²10 :0     4.6683 (âˆ’0.0431)           0.0287 (0.0004)     5.1651 (âˆ’0.7278)    0.0729 (0.1830)
     Î²20 :0     0.0088 (0.0037)            0.0056 (âˆ’0.0012)    0.3555 (âˆ’0.5817)    0.0848 (âˆ’0.2769)
     Î²11 :1     4.2093 (0.1003)            0.0229 (0.0038)     4.2202 (0.0962)     0.0278 (0.0214)
     Î²21 :âˆ’1    0.0313 (0.0872)            0.0053 (0.0024)     0.0319 (0.0875)     0.0066 (0.0243)
     Î²12 :1     3.2445 (0.1817)            0.0251 (0.0166)     3.1090 (0.1611)     0.0327 (0.0303)
     Î²22 :âˆ’1    0.0328 (0.0886)            0.0054 (0.0064)     0.0325 (0.0878)     0.0069 (0.0292)
     w:0.25     0.0093 (âˆ’0.0572)           0.0014 (âˆ’0.0020)    0.0093 (âˆ’0.0570)    0.0019 (âˆ’0.0160)
                Case IV:1 , 2 âˆ¼ ST(0, 1, 0.5, 3)
     Î²10 :0     7.8868 (0.9770)            0.2082 (0.4344)     5.1906 (0.0754)     0.4395 (0.6371)
     Î²20 :0     0.2110 (0.4604)            0.1461 (0.3853)     0.0373 (âˆ’0.1476)    0.0105 (0.0455)
     Î²11 :1     5.0109 (0.1370)            0.0247 (âˆ’0.0192)    5.6461 (0.1695)     0.0400 (0.0140)
     Î²21 :âˆ’1    0.0280 (0.0717)            0.0053 (âˆ’0.0065)    0.0259 (0.0686)     0.0066 (0.0263)
     Î²12 :1     6.6126 (0.3814)            0.0301 (âˆ’0.0120)    7.0245 (0.3604)     0.0485 (0.0140)
     Î²22 :âˆ’1    0.0308 (0.0723)            0.0049 (âˆ’0.0044)    0.0276 (0.0691)     0.0069 (0.0290)
     w:0.25     0.0081 (âˆ’0.0459)           0.0014 (0.0040)     0.0073 (âˆ’0.0436)    0.0021 (âˆ’0.0168)
                Case V:1 , 2 âˆ¼ N(0, 1) (% 5 outliers)
     Î²10 :0     1.5208 (0.2485)            1.0975 (0.2305)     1.6056 (âˆ’0.3105)    6.9413 (2.5194)
     Î²20 :0     0.0094 (0.0158)            0.0059 (0.0056)     0.2483 (âˆ’0.4883)    0.0419 (âˆ’0.1880)
     Î²11 :1     2.6872 (1.4449)            2.4663 (1.4533)     2.6444 (1.4307)     2.3970 (1.4530)
     Î²21 :âˆ’1    0.0783 (0.2591)            0.0175 (0.1066)     0.0770 (0.2572)     0.0239 (0.1284)
     Î²12 :1     2.9720 (1.5383)            2.7078 (1.5341)     3.0209 (1.5560)     2.3044 (1.4204)
     Î²22 :âˆ’1    0.0813 (0.2646)            0.0176 (0.1072)     0.0810 (0.2639)     0.0230 (0.1279)
     w:0.25     0.0098 (âˆ’0.0974)           0.0069 (âˆ’0.0814)    0.0098 (âˆ’0.0976)    0.0138 (âˆ’0.1159)
     Note: Value in parentheses indicates the bias.




                                                     Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

Robust Mixture Regression Based on the Skew t Distribution                                                          57

5. Real Data Example
    In this section, we will analyze the tone perception data set (Cohen 1984) in
order to further illustrate the performance of the mixture regression model based
on the skew t distribution on a real data set. In Cohen (1984) tone perception
experiment, a pure fundamental tone was played to a trained musician. Also, elec-
tronically obtained overtones were added, which were determined by a stretching
ratio. This ratio is between the adjusted tone and the fundamental tone. In the
experiment, 150 trials were performed by the same musicians. The aim of this
experiment was to find out how the tuning ratio affects the perception of the tone
and to decide if either of two musical perception theories was reasonable (see Co-
hen (1984) for more detailed explanations). This data set can be accessed by using
a fpc package (Henning 2013) in R. The variable perceived tone ratio is taken as
the response variable and the actual tone ratio variable is taken as the explanatory
variable. This data set has also been analyzed by Yao et al. (2014) and Song et al.
(2014) to test the performance of the mixture regression estimates based on the
t and Laplace distributions, respectively. Figure 1 shows the scatter plot and the
histogram of the perceived tone ratio. From these plots, it is clear that there are
two groups in the data; non-normality is also shows.

                                                                          4
                             3.4
                             3.2                                         3.5

                              3                                           3
                             2.8
      Perceived tone ratio




                                                                         2.5
                             2.6
                             2.4                                          2
                             2.2
                                                                         1.5
                              2
                             1.8                                          1

                             1.6
                                                                         0.5
                             1.4
                                                                          0
                                   1.5      2            2.5     3             1   1.5        2           2.5   3
                                         Actual tone ratio                            Perceived tone ratio
Figure 1: (a) The scatter plot of the data. (b) Histogram of the perceived tone ratio.


   We use this data set to compare the estimatorâ€™s performances both with and
without outliers cases. For the comparison of the mixture regression models, we
use the Akaike information criterion (AIC) (Akaike 1973), consistent AIC (CAIC)
(Bozdogan 1993), and the Bayesian information criterion (BIC) (Schwarz 1978)
values, which have the following form

                                                           âˆ’2`(Î˜Ì‚) + mcn ,

where `(Â·) is the maximized log-likelihood, m is the number of parameters to be
estimated, and cn is the penalty term. We use cn = 2 for AIC, cn = log(n) for BIC
and log(n) + 1 for CAIC. We present the scatter plots with the fitted regression
lines obtained from the MixregN, Mixregt, MixregSN, and MixregST procedures in

                                                                 Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

58                                                        Fatma Zehra DoÄŸru & Olcay Arslan


Figure 2 for the tone perception data set. Also, we summarize the ML estimates,
standard errors (SE) of estimates, and some information criteria in Table 3. For
all mixture regression models, the standard errors of estimates are calculated using
the Fisher information-based method (see Basford, Greenway, McLachlan & Peel
1997). Note that in real data example, we set the normal mixture regression
estimates as initial values for the mixing probability and regression coefficients.
We also take small value from skewness parameters and we assume that in both
groups the degrees of freedom are equal to 2. We try other values of degrees of
freedom and get similar results. We observe that MixregST has the best fit than
the other mixture regression models in terms of AIC, CAIC, and BIC criterion
values.
        3.5                                     3.5
                  MixregN                                    MixregN
         3        Mixregt                        3           MixregSN

        2.5                                     2.5

         2                                       2

        1.5                                     1.5

         1                                       1
              1   1.5         2    2.5   3            1      1.5         2    2.5   3
                             (a)                                        (b)

        3.5                                     3.5
                  MixregN                                    MixregSN
         3        MixregST                       3           MixregST

        2.5                                     2.5

         2                                       2

        1.5                                     1.5

         1                                       1
              1   1.5         2    2.5   3            1      1.5         2    2.5   3
                             (c)                                        (d)
Figure 2: Fitted mixture regression lines for the tone perception data set. (a): dashed
          line- MixregN, solid line-Mixregt, (b): dashed line- MixregN, solid line-
          MixregSN, (c): dashed line- MixregN, solid line-MixregST, (d): dashed line-
          MixregSN, solid line-MixregST.


    Next, we add ten pairs of outliers at (0,5). These points are shown in Figure
3 by an asterisk. These outliers can be considered as high leverage points. By
adding these points, we want to see the performance of the estimators against the
high leverage points. Figure 3 displays the scatter plots of the data set with the
fitted regression lines obtained from MixregN, Mixregt, MixregSN, and MixregST
procedures. The Table 4 presents the ML estimation results. We can see that
MixregN and MixregSN are drastically affected by the high leverage points. On
the other hand, the estimators based on the t and the skew t distributions (Mixregt
and MixregST) give fits to the majority of the data without influencing from the
high leverage points. Also, MixregST gives the best results in terms of information
criteria. Note that the estimates, including the estimates for skewness parameters
with and without outliers, are very similar (see Tables 3 and 4).


                                         Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

Robust Mixture Regression Based on the Skew t Distribution                                                           59



          5                                                      5
                                        MixregN                                                  MixregN
                                        Mixregt                                                  MixregSN
          4                                                      4

          3                                                      3

          2                                                      2

          1                                                      1

          0                                                      0
              0       0.5   1     1.5   2       2.5   3              0    0.5   1      1.5   2      2.5     3
                                 (a)                                                  (b)


          5                                                      5
                                        MixregN                                              MixregSN
                                        MixregST                                             MixregST
          4                                                      4

          3                                                      3

          2                                                      2

          1                                                      1

          0                                                      0
              0       0.5   1     1.5   2       2.5   3              0    0.5   1      1.5   2      2.5     3
                                 (c)                                                  (d)
Figure 3: Fitted mixture regression lines with ten outliers at (0,5). (a): dashed line-
          MixregN, solid line-Mixregt; (b): dashed line- MixregN, solid line-MixregSN;
          (c): dashed line- MixregN, solid line-MixregST; (d): dashed line-MixregSN,
          solid line-MixregST.




Table 3: ML estimates, SE of estimates and some information criteria for fitting mixture
         regression models to the tone perception data set.

                      MixregN                   Mixregt                  MixregSN                   MixregST

              Estimate          SE      Estimate           SE     Estimate          SE       Estimate           SE

   Î²Ì‚10       1.91637       0.02259     1.95857       0.01755        1.92157    1.08982      1.95232        0.05878
   Î²Ì‚11       0.04254       0.01044     0.02642       0.00782        0.04254    0.01053      0.03094        0.02299
   Î²Ì‚20       âˆ’0.01927      0.12571     0.01776       0.03975     âˆ’0.05439      0.49340      0.00544        0.01118
   Î²Ì‚21       0.99229       0.04721     0.99181       0.01835        0.99096    0.69809      0.99815        0.00331
   ÏƒÌ‚1        0.04619       0.00382     0.02805       0.00389        0.04648    0.11867      0.03903        0.00192
   ÏƒÌ‚2        0.13283       0.00836     0.02096       0.00328        0.13817    0.13592      0.00327        0.00335
   Î»Ì‚1            -             -           -               -        âˆ’0.4106    29.89308     âˆ’0.22245       0.00003
   Î»Ì‚2            -             -           -               -        0.36875    5.71451      0.44809        0.28316
   wÌ‚1        0.69772       0.04724     0.55182       0.05352        0.69775    0.06488      0.64037        0.43502
  `(Î˜Ì‚)           141.19840                 190.81770                    141.24909                 211.65935
  AIC             âˆ’268.39680                âˆ’367.63541                   âˆ’264.49818               âˆ’405.31870
 CAIC             âˆ’240.32235                âˆ’339.56096                   âˆ’228.40247               âˆ’369.22298
  BIC             âˆ’247.32235                âˆ’346.56096                   âˆ’237.40247               âˆ’378.22298

 Note: Bold value indicates the smallest values of AIC, CAIC and BIC.




                                                          Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

60                                                              Fatma Zehra DoÄŸru & Olcay Arslan

Table 4: ML estimates, SE of estimates and some information criteria for fitting mixture
         regression models to the tone perception data set with ten outliers at (0,5).

                    MixregN                  Mixregt             MixregSN               MixregST

             Estimate          SE     Estimate          SE   Estimate        SE    Estimate     SE

     Î²Ì‚10    1.90577        0.02686   1.95289     0.02635    1.92316    1.56013    1.96201    0.03431
     Î²Ì‚11    0.04707        0.01294   0.02877     0.01177    0.04722    0.01745    0.02960    0.01683
     Î²Ì‚20    4.40096        0.41131   0.02512     0.04855    4.37160    4.75098    0.00567    0.00324
     Î²Ì‚21    -0.79538       0.17740   0.98808     0.02157    -0.79334   0.24530    0.99810    0.00189
      ÏƒÌ‚1    0.05060        0.00391   0.03999     0.00426    0.05946    0.15277    0.05356    0.00504
      ÏƒÌ‚2    0.85912        0.14761   0.02795     0.00496    1.06486    2.30277    0.00306    0.00002
      Î»Ì‚1       -              -         -              -    0.12257    33.27402   -0.27627   0.19334
      Î»Ì‚2       -              -         -              -    0.57263    7.37487    0.45105    0.48747
      wÌ‚1    0.73677        0.03747   0.60833     0.05431    0.73624    0.07519    0.67532    0.05843
     `(Î˜Ì‚)          54.09971                 77.57685             39.83069             108.07958
     AIC        âˆ’94.19942                âˆ’141.15371              âˆ’61.66139             âˆ’198.15916
  CAIC          âˆ’65.67320                âˆ’112.62749              âˆ’24.98483             âˆ’161.48260
     BIC        âˆ’72.67320                âˆ’119.62749              âˆ’33.98483             âˆ’170.48260

  Note: Bold value indicates the smallest values of AIC, CAIC and BIC.




6. Conclusions
    In this paper, we have explored a robust mixture regression procedure based on
the skew t distribution. For the proposed mixture regression model, we have given
an EM-type algorithm to compute the estimates. We have presented a simulation
study to compare the performance of the estimators based on the skew t distribu-
tion with the performance of estimators obtained from mixture regression model
based on normal, t, and skew normal distributions. The simulation results con-
firm that when heavy-tailedness and skewness are present, the proposed estimators
behave better than their counterparts. We have also given a real data example
to further illustrate the capability of the proposed estimators in dealing with the
outliers and/or high leverage points in the data. Likewise, for the real data, our
proposed estimators show superiority over the estimators based on normal, t and,
skew normal.



Acknowledgements
   We would like to thank two anonymous referees and the Associate Editor for
their valuable comments and suggestions that have greatly improved the paper.

                          Received: April 2015 â€” Accepted: February 2016
                                                                                  




                                                   Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

Robust Mixture Regression Based on the Skew t Distribution                          61

Appendix
    If a random variable Y has the skew t distribution (ST(Î¾, Ïƒ 2 , Î», Î½)) with the
location parameter Î¾ âˆˆ R, scale parameter Ïƒ 2 âˆˆ (0, âˆž), skewness parameter Î» âˆˆ R
and degrees of freedom Î½, it has the following stochastic representation (Azzalini
& Capitaino 2003)

                          Z
                Y = Î¾ + Ïƒ âˆš , Z âˆ¼ SN(Î»), Ï„ âˆ¼ Gamma(Î½/2, Î½/2),
                           Ï„

where Z and Ï„ are independent and SN shows the skew normal distribution, re-
spectively. Also, we can further give the following stochastic representation for Z,
which has already given by (Azzalini 1986, p. 201) and (Henze 1986, Theorem 1)
                                                 q
                               Z = Î´Î» | U1 | +       1 âˆ’ Î´Î»2 U2 ,

where U1 and U2 are independent standard normal random variables and | U1 |
will have truncated normal distribution. This stochastic representation can be
used to obtain the following conditional distributions

                                                 1 âˆ’ Î´Î»2 2
                      Y | Î³, Ï„ âˆ¼ N(Î¾ + ÏƒÎ´Î» Î³,           Ïƒ ),
                                                   Ï„
                                                 1
                                    Î³ | Ï„ âˆ¼ TN(0, ; (0, âˆž)).
                                                 Ï„
These conditional distributions will help us to undertake the steps of the EM
algorithm. According to Proposition 2 of Lin et al. (2007), we can obtain the
following conditional expectations for Ï„, Î³Ï„, Î³ 2 Ï„ , and log(Ï„ ) given Y = y
                                          q     
                                            Î½+3
                                    TÎ½+3 M Î½+1
                          Î½+1
         E(Ï„ | y) =                            ,
                          Î·2 + Î½  TÎ½+1 (M )
                                         p                      âˆ’( Î½2 +1)
                                          1 âˆ’ Î´Î»2   Î·2
                                                  
                      (y âˆ’ Î¾)
       E(Î³Ï„ | y) = Î´Î»         E(Ï„ | y) +                     +1            ,
                         Ïƒ               Ï€Ïƒf (y) Î½(1 âˆ’ Î´Î»2 )


                                          Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

64                                                         Fatma Zehra DoÄŸru & Olcay Arslan




                              (y âˆ’ Î¾)2
             E(Î³ 2 Ï„ | y) = Î´Î»2          E(Ï„ | y) + (1 âˆ’ Î´Î»2 )
                                 Ïƒ2
                                       p                            âˆ’ Î½2 +1
                           Î´Î» (y âˆ’ Î¾) 1 âˆ’ Î´Î»2         Î·2
                                                
                         +                                     +1            ,
                                 Ï€Ïƒ 2 f (y)        Î½(1 âˆ’ Î´Î»2 )
                                                 2       
                                  Î½+1               Î· +Î½
            E(log Ï„ | y) = DG               âˆ’ log
                                    2                  2
                                                q           
                                     TÎ½+3 Î»Î· Î·2 +Î½  Î½+3          
                              Î½+1
                         +                                   âˆ’   1
                             Î·2 + Î½                 q
                                          TÎ½+1 Î»Î· Î·Î½+1  2 +Î½

                                                        q            
                                                                Î½+1
                                                  tÎ½+1   Î»Î·    Î· 2 +Î½
                                Î»Î·(Î· 2 âˆ’ 1)
                         +p                             q            
                              (Î½ + 1)(Î· 2 + Î½)3
                                                  TÎ½+1 Î»Î· Î·Î½+1   2 +Î½

                                                          Z M
                                              1
                             +                q               gÎ½ (x)tÎ½+1 (x)dx,
                                                 Î½+1      âˆ’âˆž
                                  TÎ½+1       Î»Î· Î·2 +Î½

               q
                    Î½+1
where M = Î»Î·       Î· 2 +Î½   and

                                           x2      x2 (Î½ + 1) âˆ’ Î½ âˆ’ 1
                                         
              Î½+2         Î½+1
 gÎ½ (x) = DG        âˆ’ DG        âˆ’ log 1 +       +                      .
               2           2              Î½+1     (Î½ + 1)(Î½ + 1 + x2 )

Note that these conditional expectations will be used in the EM algorithm pre-
sented in Section 3.


                                               Revista Colombiana de EstadÃ­stica 40 (2017) 45â€“64

References
Akaike H. Information theory and an extension of the maximum likelihood principle in B N Petrov  F Caski eds Proceeding of the Second International Symposium on Information Theory.(1973). Akademiai Kiado, Budapest.
Azzalini A. Further results on a class of distributions which includes the normal ones.(1986). Statistica.
Azzalini A, Capitaino A. Distributions generated by perturbation of symmetry with emphasis on a multivariate skew t distribution.(2003). Journal of the Royal Statistical Society.
Bai X. Robust mixture of regression models.(2010). Kansas State University.
Bai X, Yao W,Boyer J E. Robust fitting of mixture regressionmodels.(2012). Computational Statistics and Data Analysis.
Basford K E, Greenway D R, McLachlan, G J Peel D. Standard errors of fitted means under normal mixture.(1997). Computational Statistics.
Bashir S, Carter E. Robust mixture of linear regression models.(2012). Communications in Statistics-Theory and Methods.
Bozdogan H. Choosing the number of component clusters in the mixture model using a new informational complexity criterion of the inverse-fisher information matrix in Information and Classification.(1993). Springer.
Cohen A C. Some effects of inharmonic partials on interval perception.(1984). Music Perception.
Dempster A P, Laird N M, Rubin D B. Maximum likelihood from incomplete data via the E-M algorithm. (1977). Journal of the Royal Statistical Society.
Dias J G, Wedel M. An empirical comparison of em sem and mcmc performance for problematic gaussian mixture likelihoods.(2004). Statistics and Computing.
DoÄŸru F Z. Robust Parameter Estimation in Mixture Regression Models.(2015).Ankara University.
DoÄŸru F Z, Arslan O. Robust mixture regression modelling based on the skew t distribution in International Conference on Robust Statistics.(2014). Martin-Luther-University Halle-Wittenberg/Germany.
Gupta A. Multivariate skew t distribution.(2003). Statistics.
Gupta A, Chang F, Huang, W. Some skew symmetric models.(2002). Random Operators Stochastic Equations.
Henning C. fpc: Flexible procedure for clustering.(2013). R Package Version 2.1-5.
Henze N. A probabilistic representation of the skew-normal distribution.(1986). Scandinavian Journal of Statistics.
Lange K L, Little J A, Taylor M G J. Robust statistical modeling using the t distribution.(1989). Journal of the American Statistical Association.
Lin T I, Lee J C, Hsieh W J. Robust mixture modeling using the skew t distribution.(2007). Statistics and Computing.
Liu M, Lin T I. A skew-normal mixture regression model.(2014). Educational and Psychological Measurement.
Lucas A. Robustness of the student t based m-estimator.(1997). Communications in Statistics: Theory and Methods.
Markatou M. Mixture models robustness and the weighted likelihood methodology.(2000). Biometrics.
Peel D, McLachlan G, J. Robust mixture modelling using the t distribution.(2000). Statistics and Computing.
Pereira J R, Marques L A, da Costa J M. An empirical comparison of EM initialization methods and model choice criteria for mixtures of skew normal distributions.(2012). Revista Colombiana de Estadistica.
Quandt R E. A new approach to estimating switching regressions.(1972). Journal of the American Statistical Association.
Quandt R E, Ramsey J B. Estimating mixtures of normal distributions and switching regressions.(1978). Journal of the American Statistical Association.
Schwarz G. Estimating the dimension of a model.(1978). Annals of Statistics.
Shen H, Yang J, Wang S. Outlier detecting in fuzzy switching regression models in International Conference on Artificial Intelligence: Methodology - Systems - and Applications.(2004). Springer.
Song W, Yao W, Xing Y. Robust mixture regression model fitting by laplace distribution.(2014). Computational Statistics and Data Analysis.
Wei Y. Robust mixture regression models using t-distribution.(2012).Kansas State University.
Yao W, Wei Y, Yu C. Robust mixture regression using the t distribution.(2014). Computational Statistics and Data Analysis.
Zeller C B, Cabral C R B, Lachos V H. Robust mixture regression modeling based on scale mixtures of skew normal distributions.(2016). Test.
Zhang J. Robust mixture regression modeling with pearson type vii distribution.(2013).Kansas State University.
