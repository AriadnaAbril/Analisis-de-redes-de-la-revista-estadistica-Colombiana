Bimodal Regression Model. Modelo de regresiÃ³n Bimodal. Universidad de CÃ³rdoba, CÃ³rdoba, Colombia. Universidad de Atacama, CopiapÃ³, Chile. Universidad de Sao Paulo, Sao Paulo, Brasil
Abstract
Regression analysis is a technique widely used in different areas of human knowledge, with distinct distributions for the error term. It is the case, however, that regression models with the error term following a bimodal distribution are not common in the literature, perhaps due to the lack of simple to deal with bimodal error distributions. In this paper, we propose a simple to deal with bimodal regression model with a symmetric-asymmetric distribution for the error term for which for some values of the shape parameter it can be bimodal. This new distribution contains the normal and skew-normal as special cases. A real data application reveals that the new model can be extremely useful in such situations.
Key words: Bimodal Distribution, Generalized Gaussian Distribution, Linear Regression, Power Regression Model.
Resumen
El anÃ¡lisis de regresiÃ³n es una tÃ©cnica muy utilizada en diferentes Ã¡reas de conocimiento humano, con diferentes distribuciones para el tÃ©rmino de error, sin embargo los modelos de regresiÃ³n con el termino de error siguiendo una distribuciÃ³n bimodal no son comunes en la literatura, tal vez por la simple razÃ³n de no tratar con errores con distribuciÃ³n bimodal. En este trabajo proponemos un camino sencillo para hacer frente a modelos de regresiÃ³n bimodal con una distribuciÃ³n simÃ©trica - asimÃ©trica para el tÃ©rmino de error para la cual para algunos valores del parÃ¡metro de forma esta puede ser bimodal. Esta nueva distribuciÃ³n contiene a la distribuciÃ³n normal y la distribuciÃ³n normal asimÃ©trica como casos especiales. Una aplicaciÃ³n con datos reales muestra que el nuevo modelo puede ser extremadamente Ãºtil en algunas situaciones.
Palabras clave: distribuciÃ³n bimodal, distribuciÃ³n gaussiana generalizada, regresiÃ³n lineal, modelo de regresiÃ³n exponenciado.



1. Introduction
    To study the relationship between variables in different areas of human knowl-
edge, linear and nonlinear regression models have been substantially used. It is
typically considered that the error term follows a normal distribution although
more general symmetric error distributions have also been considered. One of
those alternatives is to consider that the errors follow distributions with heavier
tails than those of normal distribution, in order to reduce the influence of outly-
ing observations. In this context, Lange, Little and Taylor (1989) proposed the
Student-t model with unknown degrees of freedom for parameter Î½. Cordeiro, Fer-
rari, Uribe-Opazo and Vasconcellos (2000), and Galea, Paula and Cysneiros (2005)
present results from the study of inferential aspects of symmetrical nonlinear mod-
els. For the asymmetric nonlinear model, we use the work of Cancho, Lachos and
Ortega (2010). Symmetrical measurement error models have been investigated in
Arellano-Valle, Bolfarine and Vilca-Labra (1996).
    One situation in which we encounter an anomaly in the error term of the model
occurs when it is of interest to explain the fat percentage in the human body as
a function of the individual weight. It is the case, however, that given inherent
gender peculiarities, the exclusion of the gender variable can lead to a bimodal
error distribution model. That is, not taking into account the sex variable , leads
to a regression model for which the distribution of the error term is no longer
unimodal.
    A viable alternative to this situation is to use a mixture of normal distributions
for the error term. According to this alternative, there are two models to estimate,
one for each component of the normal mixture, namely (j for j = 1, 2); they are
both normally distributed with mean zeros and variance Ïƒj for j = 1, 2. According
to De Veaux (1989), for the special case of two explanatory variables X1 and X2 ,
the response variable can be written as
                 (
                   Î²10 + Î²11 x1i + Î²12 x2i + 1i , with probability p,
            yi =
                   Î²20 + Î²21 x1i + Î²22 x2i + 2i , with probability 1 âˆ’ p

where the ji âˆ¼ N (0, Ïƒj2 ) are independent, j = 1, 2, i = 1, 2, Â· Â· Â· , n. Consequently,
response yi has a pdf

                                                                                        
            p          yi âˆ’ Î²10 âˆ’ Î²11 x1i âˆ’ Î²12 x2i    1âˆ’p     yi âˆ’ Î²20 âˆ’ Î²21 xi âˆ’ Î²22 x2i
f (yi ) =      Ï†                                     +     Ï†                                 ,
            Ïƒ1                     Ïƒ1                   Ïƒ2                Ïƒ2

                                              Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

Bimodal Regression Model                                                             67

for i = 1, 2, . . . , n. Although widely recommended, this alternative has some im-
portant drawbacks. The first results from lack of identifiability of some model
parameters, more specifically, Î²10 and Î²20 . Another difficulty is related to con-
vergence problems with the algorithm for parameter estimation, including the
proportion of data points for each model. Moreover, the model is not parsimo-
nious at all, since by increasing the number of explanatory variables, the number
of parameters in the model jumps to nine. For instance, for one explanatory vari-
able there are seven parameters to be estimated, making algorithm convergence
difficult. De Veaux (1989) presents an EM-algorithm for the mixture of regression
models. Further results can be found in Quandt (1958), Turner (2000), and Young
& Hunter (2010), among others.
    In this paper, we suggest using the symmetric-asymmetric bimodal alpha-power
model, considered in Bolfarine, MartÃ­nez and Salinas (2012), to adjust data with a
linear relation. Results from two real data applications are reported the illustrate
the usefulness of the models developed. One alternative, clearly, is to undertake
data transformation or use mixtures of distributions, as mentioned above.
    The paper is organized as follows. Section 2 is devoted to describing the bi-
modal symmetric-asymmetric alpha-power distribution and some of its main prop-
erties. The model considered generalizes both the skew-normal model (Azzalini
1985) and the power-normal model (Pewsey, GÃ³mez and Bolfarine 2012). The
extension of the normal multiple regression model to the case in which the error
term follows the bimodal symmetric-asymmetric power-normal (ABPN) model is
considered in Section 3. Maximum likelihood estimation is discussed in Section 4.
In particular, it is shown that the Fisher information matrix is nonsingular and
allows for normality to be tested using the likelihood ratio statistics. A real appli-
cation considered in Section 5 illustrates the fact that the model considered can
outperform traditional symmetric models that have been previously considered in
the literature, in specifically the mixture of normals.


2. The Bimodal Symmetric-Asymmetric
   Alpha-Power Distribution
   The alpha-power distribution was first considered in Durrans (1992), and its
pdf is given by

                       g(z; Î±) = Î±Ï†(z){Î¦(z)}Î±âˆ’1 ,       z âˆˆ R,                      (1)
             +
where Î± âˆˆ R is a shape parameter, and Î¦ and Ï† are the density and distribution
functions of the standard normal, respectively. We use the notation Z âˆ¼ P N (Î±).
The location-scale extension of Z, Y = Âµ + ÏƒZ, where Î¾ âˆˆ R and Ïƒ âˆˆ R+ , have a
probability density function given by
                                                    Î±âˆ’1
                                Î±     yâˆ’Âµ         yâˆ’Âµ
                 Ï•(y; Âµ, Ïƒ, Î±) = Ï†             Î¦               .             (2)
                                Ïƒ      Ïƒ            Ïƒ
We use the notation Y âˆ¼ P N (Âµ, Ïƒ, Î±).

                                       Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

68                     Guillermo MartÃ­nez-FlÃ³rez, Hugo S. Salinas & Heleno Bolfarine


   Several authors, for example, Gupta and Gupta (2008), Pewsey, GÃ³mez &
Bolfarine (2012), Rego, Cintra & Cordeiro (2012), studied properties of this model,
but the fact that this model can be seen as an aditive generalized model seems to be
unknown. This model can be further extended by considering Âµi = x0i Î² replacing
Âµ, where Î² is an unknown vector of regression coefficients and xi a vector of known
regressors, possibly correlated with the response vector.
  MartÃ­nez-FlÃ³rez, Bolfarine & GÃ³mez (2015) considered the multiple regression
model represented by

                        yi = x0i Î² + i ,     i = 1, 2, . . . , n,                            (3)

where Î² is a vector of unknown constants, xi are values of known explanatory vari-
ables, and the error terms i are independent random variables with power-normal
distribution, P N (0, Ïƒ, Î±). This model becomes a viable alternative to the ordinary
regression models under normality for the situation of asymmetrically distributed
errors with kurtosis above 3 (normal distribution). These authors studied the
main properties of this model, obtained equations to estimate model parameters
via maximum likelihood, and deduced its information matrices. They found that
the Fisher information matrix is nonsingular. Although the new proposal is a vi-
able alternative to model data with low and high asymmetry, this model can only
be applied to unimodal situations.
   As an extension of the PN model to bimodal data, Bolfarine, MartÃ­nez-FlÃ³rez
& Salinas (2012) introduced the family of bimodal distributions, one symmetric
and the other asymmetric. The corresponding density function of the bimodal
power-normal distribution is given by

                                                         Î±âˆ’1
                         Î± 2Î±âˆ’1
                                                
                                          yâˆ’Âµ        yâˆ’Âµ
         Ï•(y; Âµ, Ïƒ, Î±) =          Ï†               Î¦            ,                     x âˆˆ R,   (4)
                         Ïƒ 2Î± âˆ’ 1          Ïƒ          Ïƒ
where Âµ is the location parameter and Ïƒ is the scale parameter. We use the
notation BP N (Âµ, Ïƒ, Î±). Note that for Î± = 1 the normal distribution N (Âµ, Ïƒ 2 )
follows.
     The r-th moment of the random variable Y âˆ¼ BP N (0, 1, Î±) is given by
                                     
                                            0,      if r is odd,
                         E(Z r ) =
                                          2Âµr (0), if r is even.

where
                             Z âˆž
                                                     Î±âˆ’1
              Âµr (0) = Î±cÎ±         z r Ï†(z) {Î¦(z)}         dz,       r = 0, 1, 2, . . .       (5)
                             0

Hence, it follows that E(Z) = E(Z 3 ) = 0. The authors show that the pdf is
symmetric and, moreover, if Î± > 1, then its density function is bimodal. Fur-
thermore, maximum likelihood estimation is considered for model parameters and
the Fisher information matrix is derived and shown to be nonsingular. Under


                                            Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

Bimodal Regression Model                                                                   69

these conditions,
        âˆš         and given that it is a regular continuous function, it also fol-
lows the n-normal approximation for the maximum likelihood estimators for the
parameter vector.
   Bolfarine et al. (2012) also studied a distribution for fitting symmetric-asymmetric
data with bimodal behaviour and this distribution was termed the bimodal symmetric-
asymmetric power-normal model.
     The density function for the location-scale version of the model can be written
as
                                                    Î±âˆ’1       
                           2Î±cÎ±          yâˆ’Âµ        yâˆ’Âµ          yâˆ’Âµ
        Ï•(y; Âµ, Ïƒ, Î±, Î») =      Ï†                Î¦           Î¦ Î»       ,
                            Ïƒ             Ïƒ          Ïƒ            Ïƒ

where y âˆˆ R, Âµ âˆˆ R is a location parameter, Ïƒ > 0 is a scale parameter, Î± âˆˆ R+
is a shape parameter, Î» âˆˆ R is an asymmetry parameter, and cÎ± = 2Î±âˆ’1 /(2Î± âˆ’ 1)
is the normalizing constant. We use the notation Y âˆ¼ ABP N (Âµ, Ïƒ, Î±, Î»).
                                                    h             i
                                                            Ï†(Î»z)
    The authors show that for Î± > 1 and Î» satisfying 1 âˆ’ Î»z Î¦(Î»z)   > 0, this model
is bimodal asymmetric; whereas for Î± > 1 and Î» = 0, it is bimodal symmetric.
Conversely, for Î± â‰¤ 1 the resulting model is unimodal. We note that for Î± = 1
the skew-normal model follows, for Î± = 1 and Î» = 0 the normal case follows, and
for Î» = 0 the bimodal power-normal model follows.
     The r-th moment of a random variable Z âˆ¼ ABP N (0, 1, Î±, Î») is given by
                               
                                         2Âµr (0),        if r is even,
                   E(Z r ) =
                                   2Âµr (0) + 2Âµr (Î², Î±), if r is odd,

where                                    Z âˆž
                                                                 Î±âˆ’1
                  Âµr (Î», Î±) = 2Î±cÎ±             z r Ï†(z) {Î¦(z)}         Î¦(Î»z)dz.
                                         0

    In addition to these results, these authors have shown that the information
matrix is nonsingular at the vicinity of symmetry, that is, Î± = 1 and Î» = 0. This
leads to large sample normal distribution for the maximum likelihood estimators
for which the asymptotic covariance matrix is the inverse of the Fisher information
matrix.


3. The Multiple Regression Model With
   ABPN Errors
    We assume, under the ordinary multiple regression model, that the error term
follows a ABPN distribution with parameters Âµ = 0, Ïƒ, Î± and Î», that is, for i =
1, 2, . . . , n, the Îµi are independent random variables with Îµi âˆ¼ ABP N (0, Ïƒ, Î±, Î»).
Hence, it follows that the density function of Îµi is given by

                                     2Î±cÎ±  Îµi  n  Îµi oÎ±âˆ’1  Îµi 
              Ï•(Îµi ; 0, Ïƒ, Î±, Î») =       Ï†        Î¦          Î¦ Î»     ,
                                      Ïƒ     Ïƒ        Ïƒ           Ïƒ

                                             Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

70                         Guillermo MartÃ­nez-FlÃ³rez, Hugo S. Salinas & Heleno Bolfarine


for i = 1, . . . , n. Therefore, it follows that yi given xi (denoted yi | xi ) also follows
a ABPN distribution, that is, yi | xi âˆ¼ ABP N (x0i Î², Ïƒ, Î±, Î»), for i = 1, 2, . . . , n.
In this model, x0i Î² is a location parameter, Ïƒ is a scale parameter, Î± is a shape
parameter, and Î» is an asymmetry parameter where Î² is a vector of unknown
constants and xi are values of known explanatory variables.
    The interpretation of the systematic part of the model, namely (Î²0 , Î²1 , Â· Â· Â· , Î²p ),
is similar to that of the model under the ordinary normal assumption, and Ïƒ is a
scale parameter related to the error terms.
    Under the ABPN model, E(yi ) 6= x0i Î², and we have to make the following cor-
rection to obtain the regression line as the expected value of the response variable
Î²0âˆ— = Î²0 + ÂµÎµ , where ÂµÎµ = E(Îµi ). Thus, E(yi ) = x0i Î² âˆ— where Î² âˆ— = (Î²0âˆ— , Î²1 , . . . , Î²p )0 .
   As special cases this model contains the model with normal errors, that is,
Î» = 0 and Î± = 1, as well as the model with skew-normal errors for Î± = 1 and the
bimodal symmetric error model for Î» = 0.


4. Inference for the Multiple Linear ABPN Model
4.1. Likelihood and Score Functions
    Considering a matrix notation where Y denotes a (n Ã— 1)-dimensional vector
with entries yi and X the (n Ã— (p + 1))-matrix with rows x0i , the likelihood function
for Î¸ = (Î² 0 , Ïƒ, Î±, Î»)0 , given a random sample of size n, Y = (Y1 , Y2 , . . . , Yn )0 , can
be written as

           `(Î²; Y)     = n[ln(2Î±) + ln(cÎ± ) âˆ’ ln(Ïƒ)]
                          1
                       âˆ’      (Y âˆ’ XÎ²)0 (Y âˆ’ XÎ²) + 10 [(Î± âˆ’ 1)U1 + U2 ] ,
                         2Ïƒ 2
where 10 is na n-dimensional       n U1 and
                                vector,           U2 are n-dimensional vectors with
                 yi âˆ’x0i Î²              yi âˆ’x0i Î²
                           o                      o
elements ln Î¦        Ïƒ       and ln Î¦ Î» Ïƒ            , respectively, for i = 1, 2, . . . , n.
The score function, U = (U (Î²), U (Ïƒ), U (Î±), U (Î»)), has elements that are given by

                âˆ‚`(Î²; Y)           1 0
        U (Î²) =                  =   X [Z âˆ’ (Î± âˆ’ 1)SÎ›1Î± âˆ’ Î»Î›1Î» ] ,
                   âˆ‚Î²              Ïƒ
                                             
                âˆ‚`(Î²; Y)               1
        U (Î±) =                  = n     + U 1 + n ln(2)(1 âˆ’ (1 âˆ’ 2âˆ’Î± )âˆ’1 ),
                   âˆ‚Î±                  Î±
                âˆ‚`(Î²; Y)           1                         0
                                      âˆ’n + Z0 Z âˆ’ (Î± âˆ’ 1) |Z| Î›1Î± âˆ’ Î»Z0 Î›1Î» ,
                                                                            
        U (Ïƒ) =                  =
                   âˆ‚Ïƒ              Ïƒ
                âˆ‚`(Î²; Y)
        U (Î») =                  = Z0 Î›1Î» ,
                   âˆ‚Î»
where S = diag {sgn(z1 ), . . . , sgn(zn )},
                       0                                 0
                    Zk = (z1k , . . . , znk ),       Zk =      z1k , . . . , znk
                                                                                   
                                                                                       ,

                                                 Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

Bimodal Regression Model                                                                         71
                                    0                                     0
         Ï†(|z1 |)           Ï†(|zn |)               Ï†(Î»z1 )           Ï†(Î»zn )                Pn
Î›1Î± =    Î¦(|z1 |) , . . . , Î¦(|zn |)    , Î› 1Î» =   Î¦(Î»z1 ) , . . . , Î¦(Î»zn )    and U 1 = n1 i=1 U1i
          y âˆ’x0 Î²
with zi = i Ïƒ i for i = 1, . . . , n.
   After some algebraic manipulations, maximum likelihood estimating equations
are given by

                                                                                   1
             Î² = Î² M Q + Ïƒ(X0 X)âˆ’1 X0 [(Î± âˆ’ 1)SÎ›1Î± + Î»Î›1Î» ] , Î± = âˆ’
                                                                                   U1

                1 0     Î±âˆ’1    0     Î»
                  ZZ=1+     |Z| Î›1Î± + Z0 Î›1Î» and Z0 Î›1Î» = 0
                n        n           n
where Î² M Q = (X0 X)âˆ’1 X0 Y. Hence the MLE of parameter vector Î² is equal to the
least squares estimator for Î², plus the asymmetry and bimodal correcting terms.
Non analytical solutions are available for the likelihood (score) equations, and,
hence, they have to be solved numerically using iterative procedures such as the
Newton-Raphson or quase-Newton type algorithms.
    Hence, the maximum likelihood estimator for Î¸ can be obtained by implement-
ing the following iterative procedure:

                             Î¸Ì‚(k+1) = Î¸Ì‚(k) + [J(Î¸Ì‚(k) )]âˆ’1 U (Î¸Ì‚(k) ),                        (6)

                âˆ‚ 2 `(Î¸)
where J(Î¸) = âˆ’           is the observed information matrix. There are however,
                âˆ‚Î¸ âˆ‚Î¸>
other numerical procedures based on the expected (Fisher) information matrix.
    These optimization algorithms can be found in the following packages: nlm,
optim, maxLik or optimx of the R software (R Development Core Team. (2015)).
These are procedures that are based on the function score for parameter estima-
tion.
   To initialize the estimation process, the following algorithm is considered.
Firstly, the ordinary normal linear regression model is fitted and model errors
are estimated. Using these estimates, the ABPN model is fitted, from which esti-
mates for Î» and Î± are computed. Then, ÂµÎµ can be estimated. For Îµâˆ—i = Îµi âˆ’ ÂµÎµ , we
have E(Îµâˆ— ) = 0 and V ar(Îµâˆ— ) = Ïƒ 2 Î¦2 (Î±, Î»), where Î¦2 is the variance of the random
variable ABP N (0, 1, Î±, Î»).
    Hence, the errors sum of squares are minimized, namely,
                              n             n
                                                                2
                              X             X
                                     Îµâˆ—2
                                      i =         (yi âˆ’ x0i Î² âˆ— ) .
                               i=1          i=1

We the obtain the least squares estimators of Î² âˆ— and Ïƒ, which are given by:
                                              n 
              âˆ—    0  âˆ’1 0         Î¦âˆ’1
                                    2 (bÎ±, Î»)
                                           b X               âˆ— 2
                                                              
             Î² = (X X) X Y and Ïƒ
             b                   2
                               b =                yi âˆ’ x0i Î²b .
                                     n âˆ’ 2 i=1

                     âˆ—
Moreover, V ar(Î²b ) = Ïƒ 2 Î¦2 (Î±, Î»)(X0 X)âˆ’1 .


                                              Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

72                                Guillermo MartÃ­nez-FlÃ³rez, Hugo S. Salinas & Heleno Bolfarine


4.2. The Observed and Expected Information Matrices
   Before computing the information matrix (Fisher), we present the elements of
the observed information matrix which, after extensive algebraic manipulations,
are given by
                    1 0
                       X In + (Î± âˆ’ 1)Î›2Î± + Î»2 Î›2Î» X,
                                                        
     jÎ²0 Î²    =      2
                   Ïƒ
                    1 0
     jÏƒÎ²      =        X [2Z + (Î± âˆ’ 1)Î›3Î± + Î»Î›3Î» ] ,
                   Ïƒ2
                    1 h          0
                                                 h
                                                           0        3 0
                                                                        
                                                                                 0        0
                                                                                              ii
      jÏƒÏƒ     =          âˆ’n + 3Z   Z +  (Î± âˆ’  1)   âˆ’2  |Z|   +    Z       Î›1Î± + Z  Î› 1Î± Î› 1Î± Z
                   Ïƒ2
                    Î» h      0     2 30
                                         
                                                     0         0
                                                                    i
              +           âˆ’2Z   +  Î» Z     Î› 1Î»  + Î»Z  Î›  1Î» Î› 1Î» Z   ,
                   Ïƒ2
     jÎ±Î±      =    n[Î±âˆ’2 âˆ’ 2Î± (2Î± âˆ’ 1)âˆ’2 ln2 (2)],
                   1h 0               0
                                                                i
      jÎ»Ïƒ     =        Z Î›1Î» âˆ’ Î»2 Z3 Î›1Î» âˆ’ Î»Z0 Î›1Î» Î›01Î» Z ,
                   Ïƒ
                        0
      jÎ»Î»     =    Î»Z3 Î›1Î» + Z0 Î›1Î» Î›01Î» Z,
                   1 0                   1     0                   1
     jÎ±Î²      =      X SÎ›1Î± , jÎ±Ïƒ = |Z| Î›1Î± , jÎ»Î² = âˆ’ X0 Î›3Î» and jÎ±Î» = 0.
                   Ïƒ                     Ïƒ                         Ïƒ
where
                  (              2                      )                      (                                2 )
                       Ï†(|z1 |)                Ï†(|zi |)                        Ï†(Î»zi )                   Ï†(Î»zi )
Î›2Î± = diag                             + |zi |                , Î›2Î» = diag Î»zi         +                                  ,
                       Î¦(|z1 |)                Î¦(|zi |)                        Î¦(Î»zi )                   Î¦(Î»zi )

Î›3Î± = (a1 , a2 , . . . , an )0 , with
                     (                                       2                     )
                                    Ï†(|zi |)        Ï†(|z i |)               Ï†(|zi |)
            ai = sgn(zi )zi2                 + zi                âˆ’ sgn(zi )            ,
                                    Î¦(|zi |)        Î¦(|zi |)                Î¦(|zi |)

Î›3Î» = (b1 , b2 , . . . , bn )0 , with
                             (                                             2               )
                                         Ï†(Î»zi )                  Ï†(Î»zi )          Ï†(Î»zi )
                        bi =      Î»2 zi2         + Î»zi                           âˆ’               ,
                                         Î¦(Î»zi )                  Î¦(Î»zi )          Î¦(Î»zi )

and i = 1, . . . , n.
   The above expressions can be computed numerically. The observed information
matrix is obtained after replacing unknown parameters with the corresponding
maximum likelihood estimators. The expected information matrix then follows by
taking expectations of the above components (multiplied by nâˆ’1 ).
     Considering: as in Bolfarine et al. (2012):

                  akj = E{z k (Ï†(z)/Î¦(|z|))j },                aâˆ—kj = E{|z|k (Ï†(z)/Î¦(|z|))j },

             aâˆ—âˆ—            k             j
              kj = E{sgn(z)z (Ï†(z)/Î¦(|z|)) },                      a1kj = E{z k (Ï†(Î»z)/Î¦(Î»z))j },
the elements of the expected information matrix are given by

                                                      Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

Bimodal Regression Model                                                                     73



              1 + (Î± âˆ’ 1)(a02 âˆ’ aâˆ—âˆ—      2
                                 11 ) + Î» (Î²a111 + a102 ) 0
   iÎ² 0 Î² =                        2
                                                         X X,           iÎ»Î» = Î»a131 + a122
                                 Ïƒ

               2a10 + (Î± âˆ’ 1) (aâˆ—âˆ—    âˆ—âˆ—              2
                                01 âˆ’ a21 âˆ’ a12 ) + Î»(Î» a121 + Î»a112 âˆ’ a101 ) 0
      iÏƒÎ² =                                                                 X 1,
                                            Ïƒ2

                 a101 âˆ’ Î»(Î»a121 + a112 ) 0                 a111   Î»(Î»a131 + Î»a122 )
        iÎ»Î² =                           X 1,       iÎ»Ïƒ =        âˆ’
                           Ïƒ                                Ïƒ            Ïƒ2

              1     3       Î±âˆ’1 âˆ—                       Î»
  iÏƒÏƒ = âˆ’       2
                  + 2 a20 +    2
                                 [a31 + a22 âˆ’ 2aâˆ—11 ] + 2 [Î»2 a131 + Î»a122 âˆ’ 2a111 ]
              Ïƒ    Ïƒ         Ïƒ                         Ïƒ
        1                        1 âˆ—
 iÎ±Î² = âˆ’ aâˆ—âˆ— X0 1,       iÎ±Ïƒ =    a ,       iÎ±Î» = 0,     iÎ±Î± = Î±âˆ’2 âˆ’ 2Î± (2Î± âˆ’ 1)âˆ’2 (ln 2)2 .
        Ïƒ 01                     Ïƒ 11


4.3. Bimodal Symmetric Case
   The bimodal symmetric regression model is the one where the errors follow the
probability density function and is given by

                                            Î±cÎ±  Îµi  n  Îµi oÎ±âˆ’1
                     Ï•(Îµi ; 0, Ïƒ, Î±, 0) =      Ï†        Î¦           ,
                                             Ïƒ    Ïƒ        Ïƒ

for i = 1, 2, . . . , n. We use the notation i âˆ¼ BP N (0, Ïƒ, Î±). Bolfarine et al. (2012)
demonstrate that this density is symmetric bimodal for Î± > 1 and unimodal
otherwise. We note that this model is a particular case of the ABPN model and
take the value Î» = 0. Therefore, the score functions and information matrix for
the parameter vector Î¸1 = (Î² 0 , Ïƒ, Î±)0 can be obtained from the previously obtained
ones for the asymmetric regression model, which is followed by making Î» = 0 in
the first derivatives with respect to the parameter Î², Ïƒ and Î± and similarly for
the second derivatives with respect to the vector Î¸1 .
    For Î± = 1, we have Ï•(Îµi ; 0, Ïƒ, 1) = Ï† (Îµi /Ïƒ) /Ïƒ, the density function of the
location-scale normal density, the information matrix is reduced to
                             ï£«   1    0                                     ï£¶
                                 Ïƒ2 X X         0p+1           0p+1
                    I(Î¸1 ) = ï£­      0              2           0.2063
                                  0p+1            Ïƒ2              Ïƒ
                                                                            ï£¸
                                  00p+1         0.2063
                                                   Ïƒ        1 âˆ’ 2(ln 2) 2



with determinant |I(Î¸1 )| > 0, so that the information matrix is nonsingular for
the special case of the symmetric normal distribution.
    The upper 2 Ã— 2 submatrix of I(Î¸1 )âˆ’1 corresponds to the information ma-
trix for the normal distribution. In the next section, we discuss consistency and
asymptotic normality for the maximum likelihood estimators. As is well-known,
the asymptotic variance is the inverse of the Fisher information above.

                                             Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

74                      Guillermo MartÃ­nez-FlÃ³rez, Hugo S. Salinas & Heleno Bolfarine


4.4. Large Sample Distribution of the MLE for the ABPN
     Model
     As mentioned above, the information matrix for the parameter vector

                              Î¸ = (Î² 0 , Ïƒ, Î±, Î»0 )0 = (Î¸10 , Î»)0

for the bimodal regression model is obtained by finding the expectations for the
observed information matrix. These expectations are not available in closed form
and have to be obtained numerically.
   In the particular case where Î± = 1, Î» = 0 so that Ï•(Îµi ; 0, Ïƒ, 1, 0) = Ï† (Îµi /Ïƒ) /Ïƒ,
the location-scale normal density function, the information matrix becomes
                                                         
                                      I(Î¸1 )    I(Î¸1 , Î»)
                         I(Î¸) =                            .
                                   I 0 (Î¸1 , Î»)    2
                                                   Ï€

                                   6 0, which is then nonsingular
The determinant is given by |I(Î¸)| =                     âˆš        at the vicinity of
symmetry, that is, for the normal case, so that the usual n-asymptotic behaviour
holds for the MLEs. Moreover, The upper 2 Ã— 2 submatrix of I(Î¸1 )âˆ’1 corresponds
to the information matrix for the normal distribution. For large n,
                                    A
                                Î¸b âˆ’â†’ Np+4 (Î¸, I(Î¸)âˆ’1 ),

and hence, Î¸b is consistent and asymptotically normal with asymptotic covariance
matrix I(Î¸)âˆ’1 . For this to be the case, regularity conditions must be satisfied.
    We have shown that the Fisher information matrix is not singular, and, more-
over, since second derivatives exist and are continuous with respect to each one of
the parameters Ïƒ, Î», Î±, and Î²j for j = 1, 2, Â· Â· Â· , p + 1 it is possible to differentiate
under the integral sign. This shows that part of the regularity conditions for large
sample normality of the maximum likelihood estimators are satisfied. To verify
the remaining conditions, following Lin & Stoyanov (2009), for y > 0 and Î» > 0,
                                     Ï†(Î»y)
lim inf yâ†’âˆž Î¦(Î»y) â‰¥ 1/2 so that 21 Î¦(Î»y)   â‰¤ Ï†(Î»y) â†’ 0 as y â†’ âˆž, and for Î» < 0,
                                                                                     1    2
log(Î¦(Î»y)) â‰ˆ âˆ’ 21 (Î»y)2 for y â†’ âˆž. From here it follows that Î¦(Î»y) â‰ˆ eâˆ’ 2 (Î»y) ,
           Ï†(Î»y)
leading to Î¦(Î»y) â‰ˆ (2Ï€)âˆ’1/2 as y â†’ âˆž. On the other hand, it is well known that
                                                                     Ï†(y)
the failure rate of the standard normal distribution h(y) satisfies 1âˆ’Î¦(y) > y, âˆ€y.
Therefore, the third derivatives with respect to the model parameters are bounded
by an integrable function. Finally, since the distribution support is independent
of model parameters, we have shown that the regularity conditions (see regular-
ity conditions in Lehmann & Casella., (1998) and Casella & Berger., 2002) are
satisfied. Thus, we have the following
Proposition 1. If Î¸Ì‚ is the MLE of Î¸, then
                                    A
                                 Î¸Ì‚ â†’ Np+4 (Î¸, I(Î¸)âˆ’1 ),

resulting that the asymptotic variance of the MLE Î¸Ì‚ is the inverse of the Fisher
information matrix I(Î¸), which can be denoted by Î£(Î¸) = I(Î¸)âˆ’1 .


                                           Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

Bimodal Regression Model                                                                                   75

5. Numerical Results
5.1. Simulation Study
    Results of two simulation studies, one for the BPN model and the other for
the ABPN model, report properties such as empirical bias and mean squared error
for the maximum likelihood estimators. We consider regression models with errors
BP N (0, Ïƒ, Î±) and ABP N (0, Ïƒ, Î±, Î»). For each model, 5000 samples of sizes n = 50,
150, 300 and 1000 were generated from the BPN and ABPN models, with parame-
ter values given by Î²0 = 0.75, Î²1 = 2.25, Ïƒ = 1 and Î± = 0.25, 0.75, 1.75 and 3.5.
For the ABPN model we took Î» = 1.0 and 3.0
    Estimators performances were evaluated by computing the absolute value of
empirical bias (|Bias|âˆš= empirical |bias| value) and the square root of the empirical
mean squared error ( M SE). Results are presented in Tables 1, 2, and 3. Results
show that the absolute value of the bias and the root of the mean square error of
the maximum verisimilitude estimators of the model parameters decrease as the
sample sizes are increased, ie estimators are approximately unbiased, (see Tables
1, 2, and 3). This indicates a good performance of the MLE for moderate sample
sizes. Small bias for large samples is expected given the asymptotic convergence
of the MLE discussed above. For small and moderate sample sizes, it can also be
depicted from the simulations that the bias Î²0 is small under the BPN model.

Table 1: Simulations for the BPN regression model with 5000 iterations to Î± = 0.25,
         0.75, 1.75, 3.5; Ïƒ = 1.0; Î²0 = 0.75 and Î²1 = 2.25, with sample sizes n = 50,
         150, 300, and 1000 respectively.
                         Î²Ì‚0                    Î²Ì‚1                     ÏƒÌ‚                       Î±Ì‚
                            âˆš                      âˆš                         âˆš                        âˆš
    Î±      n    |Bias|          M SE   |Bias|          M SE    |Bias|            M SE   |Bias|            M SE
           50   0.0074      0.3302     0.0134      0.5195      0.1294        0.1323     1.1796        1.0744
  0.25   150    0.0043      0.1583     0.0073      0.2673      0.0557        0.0831     0.4570        0.5542
          300   0.0015      0.1018     0.0023      0.1754      0.0315        0.0595     0.2629        0.3859
         1000   0.0013      0.0598     0.0019      0.1010      0.0095        0.0351     0.0758        0.2257
           50   0.0016      0.3600     0.0099      0.6388      0.1053        0.1263     1.0000        1.0886
  0.75   150    0.0010      0.1590     0.0014      0.2784      0.0374        0.0807     0.3200        0.5743
          300   0.0006      0.1141     0.0003      0.2055      0.0191        0.0593     0.1589        0.4130
         1000   0.0003      0.0608     0.0003      0.1045      0.0055        0.0347     0.0430        0.2346
           50   0.0067      0.3324     0.0128      0.5201      0.0669        0.1232     0.7070        1.1659
  1.75   150    0.0014      0.1738     0.0021      0.2891      0.0162        0.0753     0.1475        0.6376
          300   0.0012      0.1228     0.0019      0.2106      0.0073        0.0537     0.0617        0.4495
         1000   0.0003      0.0662     0.0007          0.116    0.002        0.0300     0.0161        0.2400
           50   0.0041      0.2574     0.0135      0.4795      0.0384        0.1041     0.5292        1.2747
   3.5   150    0.0027      0.1482     0.0075      0.2454      0.0124        0.0605     0.1737        0.6816
          300   0.0025      0.1020     0.0034      0.1795      0.0057        0.0430     0.0729        0.4601
         1000   0.0004      0.0563     0.0013      0.0987      0.0020        0.0238     0.0211        0.2473




                                            Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

76                        Guillermo MartÃ­nez-FlÃ³rez, Hugo S. Salinas & Heleno Bolfarine

Table 2: Simulations for the ABPN regression model with 5000 iterations for Î± = 0.25,
         0.75, 1.75, 3.5; Ïƒ = 1.0; Î²0 = 0.75, Î²1 = 2.25 and Î» = 1.0, with sample sizes
         n = 50, 150, 300, and 1000 respectively.
                     Î± = 0.25          Î± = 0.75            Î± = 1.75            Î± = 3.5
                          âˆš                 âˆš                   âˆš                  âˆš
     Î±      n    |Bias|     M SE   |Bias|     M SE     |Bias|     M SE     |Bias|    M SE
            50   0.0982   0.3843   0.0474     0.3932   0.0174    0.3567   0.0125     0.2776
 Î²Ì‚0       150   0.0941   0.3048   0.0459     0.3121   0.0123    0.2532   0.0020     0.1657
           300   0.0442    0.234   0.0477     0.2587   0.0074    0.2027   0.0012     0.1236
          1000   0.0394   0.1348   0.0436     0.1729   0.0074    0.2027   0.0011     0.0655
            50   0.0053   0.3807   0.0186     0.4251   0.0088    0.4308   0.0026     0.3776
 Î²Ì‚1       150   0.0042   0.2289   0.0021     0.2403   0.0049    0.2299   0.0015     0.2149
           300   0.0027   0.1631   0.0019     0.1670   0.0004    0.1691   0.0004     0.1570
          1000   0.0002   0.0815   0.0014     0.0894   0.0001    0.0929   0.0002     0.0848
            50   0.0862   0.2314   0.0906     0.2161   0.0629    0.1787   0.0492     0.1331
     ÏƒÌ‚    150   0.0149   0.1504   0.0236     0.1533   0.0209    0.1203   0.0165     0.0813
           300   0.0096   0.1038   0.0026     0.1175   0.0159    0.0915   0.0073     0.0576
          1000   0.0073   0.0538   0.0107     0.0700   0.0057    0.0555   0.0022     0.0308
            50   0.8782   3.3055   0.4588     2.4702   0.1659    1.3517   0.0139     0.4590
     Î»Ì‚    150   0.4562   1.2525   0.2456     1.1182   0.0451    0.4810   0.0047     0.2400
           300   0.3315   0.7103   0.1639     0.6036   0.0003    0.3563   0.0025     0.1680
          1000   0.1152   0.3535   0.1199     0.3844   0.0050    0.2320   0.0003     0.0893
            50   1.4899   1.1151   1.3681     1.0916   1.0502    1.1270   0.7574     1.3624
     Î±Ì‚    150   0.8874   0.7949   0.7259     0.7137   0.4250    0.6799   0.2532     0.7353
           300   0.5649   0.6921   0.4705     0.6037   0.2213    0.5051   0.1205     0.5119
          1000   0.1762   0.4440   0.2092     0.4476   0.0702    0.3069   0.0353     0.2707




5.2. Numerical Illustration
    The following illustration is based on a data set including 202 Australian ath-
letes, which can be downloaded at the following directory http://azzalini.stat.
unipd.it/SN/. The data set is related to certain body features such as height,
weight, and body mass index, among others, for all 202 athletes.
      The linear model considered is

                          Bfatk = Î²0 + Î²1 bmik + Î²2 lbmk + Îµk ,

for k = 1, 2, . . . , 202, where Bfatk is the body fat percentage for the k-th athlete,
and the covariates bmik and lbmk are the body mass index and lean body mass,
respectively, for the k-th athlete.
   We start by fitting the regression model under the assumption that the error
term follows the ordinary normal
                            âˆš      model. Summary statistics are shown seen in
Table 4, in which quantities b1 and b2 represent sample asymmetry and kurtosis
coefficients. As well as in Figure 1(a), there is in indication that the normal

                                            Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

Bimodal Regression Model                                                               77

Table 3: Simulations for the ABPN regression model with 5000 iterations for Î± = 0.25,
         0.75, 1.75, 3.5; Ïƒ = 1.0; Î²0 = 0.75, Î²1 = 2.25 and Î» = 3.0, with sample sizes
         n = 50, 150, 300, and 1000 respectively.
                  Î± = 0.25           Î± = 0.75           Î± = 1.75            Î± = 3.5
                       âˆš                  âˆš                  âˆš                  âˆš
  Î±       n   |Bias|     M SE    |Bias|     M SE    |Bias|     M SE    |Bias|      M SE
         50   0.1563    0.2428   0.0291    0.2262   0.0047    0.2321   0.0415     0.2289
 Î²Ì‚0    150   0.1131    0.1439   0.0848    0.1519   0.0293    0.1583   0.0061     0.1564
        300   0.1024     0.123   0.0861    0.1307   0.0279    0.1306   0.0056    0.1329
       1000   0.0533    0.0822    0.054    0.0959   0.0102    0.0964   0.0011    0.0839
         50   0.0014    0.3422   0.0037    0.2916   0.0035    0.3106   0.0016     0.3266
 Î²Ì‚1    150   0.0001    0.1604   0.0005    0.1527   0.0015    0.1790   0.0019     0.1871
        300   0.0002    0.1089   0.0039    0.1160   0.0038    0.1287   0.0010     0.1323
       1000   0.0001    0.0578   0.0001    0.0614   0.0003    0.0683   0.0012     0.0707
         50   0.0633    0.1732   0.0696    0.1632   0.0546    0.1471   0.0417     0.1263
  ÏƒÌ‚    150   0.0189    0.0890   0.0166    0.0891   0.0132    0.0845   0.0123     0.0722
        300   0.0127    0.0634   0.0089    0.0609   0.0071    0.0593   0.0079     0.0526
       1000   0.0062    0.0354   0.0034    0.0328   0.0013    0.0319   0.0029     0.0284
         50   1.9560    3.9919   1.8591    3.1235   0.5809    2.2944   0.3756     1.8389
  Î»Ì‚    150   2.0091    3.4448   1.5292    3.0733   0.6038    1.9650   0.2244     1.1408
        300   1.3391    2.2723   1.0631    2.1654   0.3976    1.5633   0.0730     0.7056
       1000   0.4955    0.7335   0.4657    0.7776   0.1183    0.6815   0.0132     0.3613
         50   1.0601    1.0039   0.9117    1.0110   0.6094    1.1338   0.2134     1.3831
  Î±Ì‚    150   1.0673    0.9535   0.8377    0.8987   0.4103    0.8430   0.1443     0.9856
        300   0.9233    0.9153   0.7451    0.8720   0.3270    0.7984   0.0795     0.8308
       1000   0.4613    0.6465   0.4533    0.7030   0.1156    0.6352   0.0356     0.5782



symmetric model may not be the most adequate and that an asymmetric model
such as the PN or its asymmetrical bimodal extension, namely the model ABPN,
can present a better fit.

                   Table 4: Descriptive statistics for the data set.
                                                    âˆš
                      n        e        se         b1         b2
                     202    0.0000    0.8559    âˆ’0.5920     2.5484

    Additionally, the Shapiro-Wilk test for normality, with p-values given in paren-
thesis, is given by SW = 0.9811(0.008), giving then indication that model errors
are not normally distributed. Thus, we fitted the regressions based on the PN
and ABPN models. In order to future investigate the model fit, we computed the
scaled residuals ek = (yk âˆ’ x0k Î²)/b
                                b Ïƒ for PN and ABPN linear Models.
   Figures 1(b) and (c) reveal the fit of regression models PN and ABPN. We
found that the PN model fits the data better than the ordinary normal regression
model. Moreover the ABPN model presents a better fit than the PN model. The
main idea is that the ABPN model is able to capture asymmetry and bimodality
and the others are not.
   The models above were fit using software R nonlinear function nlm program
from (see R Development Core Team 2015).


                                        Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

78                                                Guillermo MartÃ­nez-FlÃ³rez, Hugo S. Salinas & Heleno Bolfarine




          0.10




                                                                                                                                                  0.5
          0.08




                                                                                                                                                  0.4
                                                                          0.6
          0.06




                                                                                                                                                  0.3
                                                                                                                                    probability
                                                            probability
density




                                                                          0.4
          0.04




                                                                                                                                                  0.2
                                                                          0.2
          0.02




                                                                                                                                                  0.1
          0.00




                                                                          0.0




                                                                                                                                                  0.0
                 5   10   15     20     25       30    35                          0.5   1.0     1.5         2.0     2.5     3.0                        âˆ’3   âˆ’2   âˆ’1         0          1   2   3

                                Bfat                                                             scale residual                                                        scale residual




                           (a)                                                                  (b)                                                                    (c)

Figure 1: (a) Histogram for the body fat percentage. Histogram and model fitted resi-
          duals under: (b) PN model and (c) ABPN model.



   Moreover, the results in Table 6 present estimates, with standard errors in
parenthesis, for the model parameters. It also reveals that, according to the ABPN
regression model, the percentage (%) of body fat depends on the bmi and lbm
quantities the athletes in the sample.
    The model selection approaches considered are the BIC, written as BIC =
âˆ’2`(Â·) + k ln(n) and CAIC, written as CAIC = âˆ’2`(Â·) + k(1 + ln(n)), where k
is the number of unknown parameters for the model under study. According to
either the BIC or CAIC scores (see Table 6), the ABPN linear model presents
the best fit when compared to the normal and PN linear models. For the sake of
comparison, we also fitted the regression model with the error terms distributed as
a mixture of two normal distributions. Parameter estimates were obtained using
function mixreg in R. These are shown in Table 5.

                                        Table 5: Descriptive statistics for the data set.
                                             j         Î²Ì‚j0                         Î²Ì‚j1                Î²Ì‚j2                 ÏƒÌ‚j                     pÌ‚j
                                             1        âˆ’0.825                       âˆ’0.194              1.011               2.923                   0.344
                                             2        5.280                        âˆ’0.509              1.862               15.961                  0.656



          Table 6: Parameter estimates (SD) for Normal, Power Normal, and ABPN models.
                               Linear model                           Normal                                PN                                         ABPN
                                    Î²b0                            â€“0.546(2.417)                       â€“10.745(3.279)                               0.920(1.620)
                                    Î²b0âˆ—                                                                                                           -1.586(2.468)
                                    Î²b1                             1.965(0.148)                        1.902(0.143)                                1.789(0.096)
                                    Î²b2                            â€“0.479(0.032)                        â€“0.459(0.033)                              â€“0.400(0.021)
                                         Ïƒ
                                         b                          4.205(0.209)                        6.907(0.681)                                3.953(0.223)
                                         Î±
                                         b                                                              8.929(3.859)                                3.698(0.588)
                                         Î»
                                         b                                                                                                         â€“0.598(0.111)
                                        BIC                                     1174.69                            1173.55                            1170.05
                                       CAIC                                     1178.69                            1178.55                            1176.05



                                                                                               Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

Bimodal Regression Model                                                                    79

    We obtained BIC = 1170.17 and CAIC = 1179.17, meaning that, according
to the BIC and CAIC criteria, the regression model ABPN fits the data better
than the mixture of two normal distributions.
   Following Therneau, Grambsch & Fleming (1990), we can adapt the deviance
component residual for the ABPN model with no censored data by considering
                                        
                 rMi = 1 + ln SÌ‚ABP N (y) , i = 1, 2, . . . , n,         (7)

where SABP N (y) is the ML estimate of the reliability function of the ABPN model.
    Therneau et al. (1990) proposed the deviance component residual as a trans-
formation of the martingale type residual so that the deviance component residual
for noncensored data can be taken as

                                                            1/2
           rM Ti = sgn(rMi ) {âˆ’2 [rMi + ln(1 âˆ’ rMi )]}            , i = 1, 2, . . . , n.    (8)

    We use the residual rM Ti as a residual type martingale, given that they are sym-
metrically distributed around zero. Furthermore, to evaluate the global influence
of each observation on the parameter estimation, Cookâ€™s distance was computed
by removing one observation at a certain time and evaluating estimation changes
for parameters Î² = (Î², Ïƒ, Î±, Î») . This distance is computed as
                                              0                  
                          1                         âˆ’1
                                                       
       GDCi (Î²) =                    Î¸Ì‚ âˆ’ Î¸Ì‚ (i) Î£Ì‚      Î¸Ì‚ âˆ’ Î¸Ì‚ (i) , i = 1, . . . , n (9)
                     (p + 1) + 3                    Î¸Ì‚

where p + 1 is the number of regression coefficients in the regression model, Î£Ì‚
                                                                                   Î¸Ì‚
is an estimator for the variance-covariance matrix of Î¸Ì‚ and Î¸Ì‚ i is the maximum
likelihood estimator Î² after removing the i-th observation.
    Figure 2(a) and (b) presents Cookâ€™s distances values of the residuals versus
fitted values from which it can be depicted that (a) there are two influential obser-
vations, namely #56 and #133, and, moreover, (b) for some observations Cookâ€™s
distances fall a little outside the bands âˆ’2 and 2. This shows that no influential
observations are present in the data.
     The impact is measured by the relative changes in the estimates, represented
as
                                         Î¸Ì‚j âˆ’ Î¸Ì‚j(I)
                               RCÎ¸j =                   âˆ— 100,                             (10)
                                             Î¸Ì‚j

where Î¸Ì‚j denotes the maximum likelihood estimate for the parameter Î¸j includ-
ing all observations, and Î¸Ì‚j(I) denotes the estimate of the same parameter while
deleting the influential observations.




                                         Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

80                            Guillermo MartÃ­nez-FlÃ³rez, Hugo S. Salinas & Heleno Bolfarine


        Table 7 depicts the relative changes on the parameter estimates.

                   Table 7: Relative change, RC% for model parameters.
                  Index             Î²Ì‚0           Î²Ì‚1        Î²Ì‚2                                  ÏƒÌ‚     Î»Ì‚           Î±Ì‚
                   {56}       0.3560            0.0184     0.0204           0.0154                     0.0238   0.0331
                  {133}       0.1484            0.0292     0.0129           0.0032                     0.0448   0.0075
                 {56, 133}    0.5097            0.0112     0.0050           0.0177                     0.0162   0.0618


    Table 7 indicates that no observation exerts a great influence on the maximum
likelihood estimates. This corroborates the results that are presented above.
      0.08




                                          133




                                                                                                  2
                    56
      0.06




                                                                    deviance component residual

                                                                                                  1
GCD

      0.04




                                                                                                  0
                                                                                                  âˆ’1
      0.02




                                                                                                  âˆ’2
      0.00




             0     50         100               150        200                                          10      15               20     25

                              Index                                                                                  linear predictor



                             (a)                                                                                      (b)

Figure 2: Plot for the ABPN model (a) Index versus GCD and (b) deviance component
          residual versus predictor.


    Figures 3(a) and (b) present the QQ-plot with envelops for the deviance com-
ponent residual for the ABPN models. This also indicates a good fit for the ABPN
linear model and the empirical cdf for the scaled residuals under the ABPN model
(solid line). The dotted line corresponds to the cfd for the ABPN model.




                                                         Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

Bimodal Regression Model                                                                                                                                                             81



                           4




                                                                                                                1.0
                           3




                                                                                                                0.8
                           2




                                                                                                                0.6
                           1
           Residual




                                                                                                       Fn(ei)
                           0




                                                                                                                0.4
                          âˆ’1




                                                                                                                0.2
                          âˆ’2




                                                                                                                0.0
                          âˆ’3

                                     âˆ’2        âˆ’1          0          1          2       3                            âˆ’3    âˆ’2           âˆ’1        0             1       2       3

                                                      Standard normal quantile                                                                     ei




                                                            (a)                                                                               (b)

Figure 3: (a) QQ-plot with envelope under the ABPN model and (c) cdf for the ABPN
          model.




    It can be noticed that envelops for the ABPN model also indicates the presence
of outlying (influential) observations under the ABPN regression model. Figures
4(a) and (b) presents the QQ-plot with envelops for the deviance component resid-
ual for the normal and PN regression models.


                                                                                                                 4
                      4




                                                                                                                 2
                      2
                                                                                                    Residual
Residual




                      0                                                                                          0




                âˆ’2                                                                                              âˆ’2




                               âˆ’3         âˆ’2        âˆ’1         0          1          2       3                         âˆ’2        âˆ’1           0          1           2       3

                                                    Standard normal quantile                                                          Standard normal quantile



                                                         (a)                                                                              (b)
                                    Figure 4: QQ-plot with envelopes (a) normal model and (b) PN model.




                                                                                                 Revista Colombiana de EstadÃ­stica 40 (2017) 65â€“83

82                       Guillermo MartÃ­nez-FlÃ³rez, Hugo S. Salinas & Heleno Bolfarine


6. Final Discussion
    In this paper we extended the model in Bolfarine et al. (2012) for the case of
regression models. Emphasis was placed on the asymmetric bimodal power-normal
(ABPN) distribution. The skew-normal model (Azzalini 1985) is a special case.
Estimations were made by implementing the maximum likelihood approach. Large
sample estimates were obtained by using the observed information matrix (minus
the inverse of the estimated Hessian matrix). Results from a real data application
for the linear model situation illustrates the usefulness of the model developed.


Acknowledgments
   The authors acknowledge helpful comments and suggestions from the referee
which substantially improved the presentation.

                    Received: April 2015 â€” Accepted: February 2016
                                                                     




References
Arellano-Valle, R. B., Bolfarine, H. & Vilca-Labra, F. (1996), â€˜Ultrastructural elliptical modelsâ€™, The Canadian Journal of Statistics 24(2), 207â€“216.
Azzalini, A. (1985), â€˜A class of distributions which includes the normal onesâ€™, Scandinavian Journal of Statistics 12(2), 171â€“178.
Bolfarine, H., MartÃ­nez-FlÃ³rez, G. & Salinas, H. S. (2012), â€˜Bimodal symmetricasymmetric power-normal familiesâ€™, Communications in Statistics-Theory and Methods . DOI:10.1080/03610926.2013.765475.
Cancho, V. G., Lachos, V. H. & Ortega, E. M. M. (2010), â€˜A nonlinear regression model with skew-normal errorsâ€™, Statistical Papers 51(3), 547â€“558.
Casella, G. & Berger, R. (2002), Statistical Inference, 2 edn, Thomson, Singapore. 
Cordeiro, G. M., Ferrari, S. L. P., Uribe-Opazo, M. A. & Vasconcellos, K. L. P. (2000), â€˜Corrected maximum likelihood estimation in a class of symmetric nonlinear regression modelsâ€™, Statistics & Probability Letters 46(4), 317â€“328.
De Veaux, R. (1989), â€˜Mixtures of linear regressionsâ€™, Computational Statistics and Data Analysis 8, 227â€“245.
Durrans, S. R. (1992), â€˜Distributions of fractional order statistics in hydrologyâ€™, Water Resources Research 28(6), 1649â€“1655.
Galea, M., Paula, G. A. & Cysneiros, J. A. (2005), â€˜On diagnostic in symmetrical nonlinear modelsâ€™, Statistics & Probability Letters 73(4), 459â€“467.
Gupta, R. D. & Gupta, R. C. (2008), â€˜Analyzing skewed data by power normal modelâ€™, TEST 17(1), 197â€“210.
Lange, K. L., Little, R. J. A. & Taylor, J. M. G. (1989), â€˜Robust Statistical Modeling Using the t Distributionâ€™, Journal of the American Statistical Association 84(408), 881â€“896.
Lehman, E. L. & Casella, G. (1998), Theory of Point Estimation, 2 edn, Springer, New York.
Lin, G. & Stoyanov, J. (2009), â€˜The logarithmic Skew-Normal Distributions are Moment-Indeterminateâ€™, Journal of Applied Probability Trust 46, 909â€“916.
MartÃ­nez-FlÃ³rez, G., Bolfarine, H. & GÃ³mez, H. W. (2015), â€˜Likelihood-based inference for the power regression modelâ€™, SORT 39(2), 187â€“208.
Pewsey, A., GÃ³mez, H. W. & Bolfarine, H. (2012), â€˜Likelihood based inference for distributions of fractional order statisticsâ€™, TEST 21(4), 775â€“789.
Quandt, R. (1958), â€˜The estimation of the parameters of a linear regression system obeying two separate regimesâ€™, Journal of the American Statistical Association 53(284), 873â€“880.
R Core Team (2015), R: A Language and Environment for Statistical Computing, R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0. *http://www.R-project.org/
Rego, L. C., Cintra, R. J. & Cordeiro, G. M. (2012), â€˜On some properties of the beta normal distributionâ€™, Communications in Statistics-Theory and Methods 41, 3722â€“3738.
Therneau, T., Grambsch, P. & Fleming, T. (1990), â€˜Martingale-based residuals for survival modelsâ€™, Biometrika 77, 147â€“160.
Turner, T. (2000), â€˜Estimating the propagation rate of a viral infection of potato plants via mixtures of regressionsâ€™, Applied Statistics 49(3), 371â€“384.
Young, D. S. & Hunter, D. R. (2010), â€˜Mixtures of regressions with predictor dependent mixing proportionsâ€™, Computational Statistics and Data Analysis 54(10), 2253â€“2266.
