Double Generalized Beta-Binomial and Negative Binomial Regression Models. Modelos de regresión beta-binomial y binomial negativa doblemente generalizados
Universidad Nacional de Colombia, Bogotá, Colombia
Abstract
Overdispersion is a common phenomenon in count datasets, that can greatly affect inferences about the model. In this paper develop three joint mean and dispersion regression models in order to fit overdispersed data. These models are based on reparameterizations of the beta-binomial and negative binomial distributions. Finally, we propose a Bayesian approach to estimate the parameters of the overdispersion regression models and use it to fit a school absenteeism dataset.
Key words: Bayesian Approach, Beta-Binomial Distribution, Distribution, Gamma Distribution, Negative Binomial, Overdispersion, Poisson Distribution.
Resumen
La sobredispersión es un fenómeno común en conjuntos de datos de conteo, que puede afectar en alto grado las inferencias relacionadas con el modelo. En este artículo desarrollamos tres modelos de regresión conjunta de media y dispersión para ajustar datos sobredispersos. Estos modelos se basan en reparameterizaciones de las distribuciones beta-binomial y binomial negativa. Finalmente, proponemos un enfoque Bayesiano para la estimación de los parámetros de los modelos de regresión sobredispersos y lo utilizamos para ajustar un conjunto de datos de ausentismo escolar.
Palabras clave: distribución beta-binomial, distribución binomial negativa, distribución de Poisson, distribución gamma, enfoque bayesiano, sobredispersión.


1. Introduction
    The binomial and Poisson distributions are widely used to fit discrete count
data. However, a serious complication arises when the variance of the response
variable Y exceeds the nominal variance. This phenomenon, called overdispersion,
can lead to underestimation of standard errors (Cox 1983) in addition to misleading
inference of the regression parameters, such as confidence intervals (Breslow 1984).
In practice, the mentioned phenomenon is very common and even the theoretical
dispersion of a variable is sometimes considered an exception.
    In order to fit such data, for both binomial and Poisson overdispersed data,
many authors have proposed models and estimation methods. Regarding to bi-
nomial data, authors such as Williams (1982) and Collet (1991) have studied
the extra-binomial variation, which is another name for overdispersion in bino-
mial data, and have proposed methods such as the incorporation of an extra-
binomial variation components in the maximum likelihood estimation of the log-
linear model.
   Moreover, Breslow (1984) analyzed extra-Poisson variation data (or overdis-
persed Poisson data) by extending the results in Williams (1982) to the Poisson
case. Later, Lawless (1987) explored the robustness and efficiency of the methods
used to deal with extra-Poisson variation in regression models. In turn, McCullagh
& Nelder (1989) presented a more general discussion of overdispersion within the
framework of generalized linear models .
   Demétrio & Hinde (1998) stated that the different models for overdispersion
can be categorized by two general approaches:

  1. Those including additional parameters for the variance function.
  2. Those assuming a two-stage model, meaning the distribution parameter is
     itself a random variable.

    Demétrio & Hinde (1998) proposed the Hinde-Demétrio (HD) regression mod-
els in order to handle overdispersed count data and discussed the effect of the
dispersion parameters on this fit. For this topic, it is worth mentioning the work
of (Quintero-Sarmiento, Cepeda-Cuervo & Núñez-Antón 2012), who published a
review about overdispersion and the different methods to model it, such as GLMs
with random mean, the quasi-likelihood functions, and the double exponential
families.
    The aim of this paper is to introduce a new reparameterization of the beta bino-
mial and the negative binomial distribution to propose reparameterized overdis-
persed regression models and develop Bayesian and classic methods to fit the
proposed models. The paper is organized as follows: In Section 2, we define the
(µ, φ)- beta binomial distribution and analyze its main statistical properties. Sec-
tion 3 presents the (µ, α)- negative binomial distribution with its definition, mean,
variance, graphics, density and characteristic functions. Section 4 develops the
(µ, σ 2 )- negative binomial distribution. In Section 5, the definitions of the pro-
posed overdispersion models are established. In Section 6, a Bayesian approach


                                    Revista Colombiana de Estadística 40 (2017) 141–163

Double Generalized Beta-Binomial and Negative Binomial Regression Models             143

is proposed to fit the proposed models by applying the Metropolis Hastings algo-
rithm. As an extension, in Section 7, we discuss the application of the Newton
Rapson method to fit the proposed models. Finally, in Section 8 we present the
motivating data to analyze the factors affecting school absenteeism in rural New
South Wales.


2. (µ, φ)- Beta Binomial Distribution
    There are many day-to-day phenomena which show binomial behavior. There-
fore, improving the fit of binomial distribution models is important. However,
most of these models in real experiments exhibit a significant amount of trials
with estimated variance larger than the predicted by the theoretical binomial
model (Demétrio & Hinde 1998). In this case, the beta-binomial model is an
alternative to the binomial one since it captures the overdispersion and thereby
results in a better fit of the observed data (Williams 1975). In order to correct
estimation problems caused by overdispersion in binomial families, the p parame-
ter (proportion of successes) is assumed to be a random variable following a beta
distribution.
   To build a reparameterization of the beta-binomial distribution, we first refer to
the reparameterization of the beta distribution, which was proposed by Jørgensen
(1997) and later in Cepeda-Cuervo (2001). If X ∼ Beta(α, β), an appropriate
reparameterization of the beta distribution in terms of the mean and dispersion
parameter is given by:

                             Γ(φ)
        f (x, µ, φ) =                    xµφ−1 (1 − x)φ(1−µ)−1 I(0,1) (x)            (1)
                        Γ(µφ)Γ(φ(1 − µ))
           α
where µ = α+β and φ = α + β (Ferrari & Cribari-Neto 2004). In (1), Γ(.) denotes
the gamma function. Regarding the mean and variance of the beta-binomial, we
have:
                                 E(X) = µ                                   (2)
                                             µ(1 − µ)
                                 V ar(X) =
                                              φ+1
   In this paper we use p ∼ Beta(µ, φ) to denote that p follows a beta distribution
with mean µ and dispersion parameter φ. Thus, the (µ, φ)-beta binomial distri-
bution is the distribution of a random variable Y such that, conditional to p, has
a binomial distribution Y | p ∼ Bin(m, p), where p is a random variable with a
beta distribution p ∼ Beta(µ, φ). We use the notation Y ∼ BB(µ, φ) to denote
that Y has a beta-binomial distribution with mean µ and dispersion parameter φ.




                                      Revista Colombiana de Estadística 40 (2017) 141–163

144                              Edilberto Cepeda-Cuervo & María Victoria Cifuentes-Amado


      The (µ, φ)-beta binomial density functions is determined by:
                     !
                 m
                                             Z
                              Γ(φ)
       f (y) =                                   py+µφ−1 (1 − p)m−y+φ(1−µ)−1 I(0,1) (p) dp
                 y       Γ(µφ)Γ(φ(1 − µ))
                     !
                 m            Γ(φ)        Γ(y + µφ)Γ(m − y + φ(1 − µ))
            =
                 y       Γ(µφ)Γ(φ(1 − µ))          Γ(m + φ)
                     !                                                                       (3)
                 m            Γ(φ)
            =                             B(y + µφ, m − y + φ(1 − µ))
                 y       Γ(µφ)Γ(φ(1 − µ))
                     !
                 m       B(y + µφ, m − y + φ(1 − µ))
            =
                 y            B(µφ, φ(1 − µ))

where B(x, y) is defined in terms of the gamma function as B(x, y) := Γ(x)Γ(y)/Γ(x+
y). The mean and variance of Y are:
                                        E(Y ) = mE(p) = mµ                                   (4)
                                                                 
                             Y                   Y                   Y
                 V ar                = E V ar       |p + V ar E        |p
                             m                   m                  m
                                                           
                                             1
                                     =E          p(1 − p)|µ, φ + V ar (p|µ, φ)
                                            m
                                       1                 µ(1 − µ)
                                     = (µ − E(p2 )) +
                                       m                  φ+1
                                                              
                                       1         2    µ(1 − µ)     µ(1 − µ)
                                     =      µ−µ −                +
                                       m                φ+1         φ+1
                                                             
                                       1                m − 1 µ(1 − µ)
                                     = µ(1 − µ) +
                                       m                 m        φ+1
                                                             
                                       µ(1 − µ)        m−1
                                     =             1+
                                           m            φ+1
Therefore, the variance of a (µ, φ)-beta-binomial variable is:
                                                        
                                                   φ+m
                       V ar(Y ) = mµ(1 − µ)                                                  (5)
                                                   φ+1

    The behavior of the beta-binomial density function is illustrated in Figure 1.
Each of the four graphs represents the behavior of the beta-binomial distribution
for different mean and dispersion values, where m (number of trials) is assumed to
be constant and equal to 20. The title of each graph contains two numbers, which
represent, respectively, the mean and dispersion parameters. For instance, in the
first graph (top left box), µ = 0.3 and φ = 2, thus the mean of the beta-binomial
variable is mµ = 6, and since the dispersion parameter is small, the graph does
not show a noticeable accumulation around the mean 6. On the other hand, a
distribution like the third (bottom left box), in spite of having the same mean
as the first, shows greater accumulation around mean 6 because the dispersion
parameter is bigger (φ = 20). The other graphs can be similarly interpreted.


                                              Revista Colombiana de Estadística 40 (2017) 141–163

Double Generalized Beta-Binomial and Negative Binomial Regression Models                                                145

                                 (0.3,2)                                                                (0.7,2)



              0.14
Probability




                                                                  Probability
              0.10




                                                                                0.15
              0.06




                                                                                0.05
              0.02




                          5         10          15         20                                    5        10       15   20




                                 (0.3,20)                                                               (0.7,20)
              8e−07




                                                                                8e−07
Probability




                                                                  Probability
              4e−07




                                                                                4e−07
              0e+00




                                                                                0e+00
                          5         10          15         20                                    5        10       15   20


                      Figure 1: Reparameterized (µ,φ)-beta binomial probability function.



                By definition, the characteristic function of the binomial distribution is given
by:

                        E(eitY ) = E(E(eitY | p))
                                = E(((1 − p) + peit )n )
                                                                     
                                       m       
                                      X     m
                                =E               (1 − p)m−j (peit )j 
                                             j
                                      j=0
                                                                                     
                                       m        m−j                     !
                                      X     m        X     m−j                      j
                                =E                                   (−p)k pj eit 
                                             j               k
                                      j=0            k=0
                                                                                   
                                      Xm m−j
                                          X  m  m − j 
                                =E                                 (−1)k pk+j eitj 
                                                 j         k
                                            j=0 k=0
                                     m m−j
                                     X X m!
                                =                       (−1)k E(pk+j )eitj
                                     j=0 k=0
                                                 j!k!

Thus, given that p ∼ Beta(µ, φ), the E(pk+j ) is the (k + j)-moment of the beta
distribution. In fact, the characteristic function of the beta-binomial distribution
can be seen as:
                                               m m−j
                                                                                        dk+j
                                               X X m!                                               
                                    itY                                         k
                              E(e         )=                    (−1)                          φp (t)      eitj          (6)
                                               j=0 k=0
                                                         j!k!                           dtk+j         t=0


such that φp (t) is the characteristic beta distribution function.



                                                          Revista Colombiana de Estadística 40 (2017) 141–163

146                        Edilberto Cepeda-Cuervo & María Victoria Cifuentes-Amado


3. (µ, α)- Negative Binomial Distribution
    The negative binomial model, first proposed in Margolin, Kaplan & Zeiger
(1981), assumes that the random variable Xi , conditioned on λ, follows a Poisson
distribution X | λ ∼ P(λ), where λ is itself a random variable with a gamma
distribution λ ∼ G(α, β), where µ = α/β, α > 0 and β > 0.
   Cepeda-Cuervo (2001) and Cepeda-Cuervo & Gamerman (2005) propose a new
parameterization for the gamma distribution in terms of the mean µ and the shape
parameter α. This reparameterization, the gamma density function is given by:
                                             α
                                        1    αx       αx
                        f (x; µ, α) =              e− µ .
                                      xΓ (α) µ

Hereafter, we use λ ∼ G(µ, α) to mean that λ follows a gamma distribution with
mean µ and shape parameter α. With this reparameterization, the negative bino-
mial density function, denoted N B(µ, α), has a density function given by:
                                                 α−1 
                                                  α
                                        α − αµ λ µ λ
                          Z ∞  −λ x 
                               e λ
            f (x; α, µ) =               e                  dλ
                                                           
                           0     x!       µ        Γ(α)
                          α        Z ∞
                          α      1
                                         e−(1+ µ )λ λx+α−1 dλ
                                               α
                       =
                          µ   x!Γ(α) 0
                          α                     Z ∞                               (7)
                          α           1
                       =                    x+α       e−t tx+α−1 dt
                          µ
                                    
                                          α        0
                              x!Γ(α) 1 + µ
                          α
                          α        Γ(x + α)
                       =                    x+α
                          µ
                                    
                              x!Γ(α) 1 + αµ


      The mean and variance of the random variable X ∼ N B(µ, α) are:

                                  E(X) = E(λ) = µ                                   (8)


                    V ar(X) = E(V ar(X | λ)) + V ar(E(X | λ))
                              = E(λ) + V ar(λ)
                                    2
                                     µ                                              (9)
                              =µ+
                                      α
                                µ(µ + α)
                              =
                                   α

    It is possible to observe several phenomena which can be studied using nega-
tive binomial distributions. Some examples of the negative binomial distribution’s
applications are: the number of European red mites on apple leaves Demétrio,
Kokonendji & Zocchi (2007); the number of coin flips necessary to get a deter-
mined value (whose domain are integers valued between 2 and infinity), or the

                                     Revista Colombiana de Estadística 40 (2017) 141–163

   Double Generalized Beta-Binomial and Negative Binomial Regression Models                      147

   number of units for inspection until getting exactly a determined number of de-
   fective units from a production line. In sum, in the literature, this distribution is
   widely interpreted as the required number of independent Bernoulli experiments
   (success/failure events) to achieve k successes.
       To observe the behavior of the gamma-Poisson density function, we change
   one parameter at a time and we get Figure 2: each graph refers to the negative
   binomial density function for different values of its parameters (µ and α). For
   instance, in the first (left-top box) there are two numbers, 5 and 8, meaning that
   for this case the mean of this distribution is 5 and the shape parameter is 8. The
   graph shows bigger values for the density function near 5, and in comparison with
   the cases µ = 5, α = 4 (top-right box), this first graph has greater accumulation
   around the mean. The other graphs can be interpreted in the same way.

                                  (5,8)                                           (5,4)
              0.15




                                                                       0.12
Probability




                                                         Probability
              0.10




                                                                       0.08
              0.05




                                                                       0.04
              0.00




                                                                       0.00


                           5       10      15       20                        5    10      15    20




                                  (10,8)                                          (10,4)
                                                                       0.06
Probability




                                                         Probability
              0.06




                                                                       0.04
              0.02




                                                                       0.02




                           5       10      15       20                        5    10      15    20


                     Figure 2: Reparameterized (µ,α)-negative binomial probability function.


       From (7), the characteristic function of the reparameterized negative binomial
   distribution is given by:
                                                  µ
                                                        !α
                                           1 − µ+α
                              φX (t) =          µ
                                          1 − µ+α   eit
                                                                                 (10)
                                                          α
                                                 α
                                      =
                                          µ(1 − eit ) + α




                                                  Revista Colombiana de Estadística 40 (2017) 141–163

148                      Edilberto Cepeda-Cuervo & María Victoria Cifuentes-Amado


   Moreover, by making r = α and p = µ/(µ + α) in equation (7), we get a usual
parameterized density function for the negative binomial distribution:
                                     r
                                  1−p          Γ(x + r)
                  f (x; r, p) =                         x+r
                                   p
                                                
                                         x!Γ(r) 1 + 1−p
                                                      p
                                     r
                                  1−p       Γ(x + r)
                              =                  x+r                    (11)
                                   p              1
                                         x!Γ(r) p
                                 Γ(x + r) x        r
                             =           p (1 − p)
                                  x!Γ(r)

where x = 0, 1, 2, . . . It X ∼ N B(r, p) is used to denote a discrete random variable
X follows a negative binomial distribution with parameters r and p defined in (11),
where X can be intuitively considered as the number of successes in a sequence
of independent and identically distributed Bernoulli trials before r failures occur
and with a success probability equal to p in each trial.


4. (µ, σ 2 ) - Negative Binomial Distribution
   The (µ, σ 2 )-negative binomial model assumes that the random variable X,
conditioned on λ, follows a Poisson distribution, X | λ ∼ P(λ), where λ is itself a
random variable that has gamma distribution with mean µ > 0 and variance σ 2 .
Thus, E(X) = µ and V ar(X) = σ 2 . We use the notation N B(µ, σ 2 ) to denote
that X follows a negative binomial distribution with mean µ and variance σ 2 .
    Thus, given that α = µ2 /(σ 2 − µ), from equations (7) and (10) the (µ, σ 2 )-
negative binomial distribution and its characteristic function are given respectively
by:
                                                                      
                                         2µ2                      µ2
                              
                                   µ      σ −µ        Γ    x  +   2
                                                                 σ −µ
             f (x; µ, σ 2 ) =                                         x+ 2µ2
                                σ2 − µ                 2
                                                              
                                                                  2
                                               x!Γ( σ2µ−µ ) σ2σ−µ
                                                                         σ −µ


                                                                               (12)
                                       2               µ2
                              µ 2   µ
                                     σ −µ
                                              Γ x + σ2 −µ
                            =    2
                                                               x
                               σ                 2
                                          x!Γ( µ ) σ
                                                            2
                                                σ 2 −µ       σ 2 −µ



                                                               2µ2
                                                µ2                 σ −µ
                                              σ 2 −µ
                        φX (t) =                        2
                                                               
                                      µ(1 − eit ) + σ2µ−µ
                                                                                     (13)
                                                             2µ2
                                              µ                σ −µ
                              =
                                      σ 2 + (µ − σ 2 )eit



                                       Revista Colombiana de Estadística 40 (2017) 141–163

Double Generalized Beta-Binomial and Negative Binomial Regression Models                                   149

    In Figure (3), each graph represents the N B(µ, σ 2 ) distribution for different
values of mean and variance. The first number which appears in the top of each
box refers to the mean µ and the second one to the variance σ 2 . For instance
in the first (top-left box) the numbers 8 and 10 respectively, are the mean and
variance of the N B(µ, σ 2 ) distribution.

                                       (8,10)                                           (8,14)
                            0.12
              Probability




                                                           Probability
                                                                         0.06
                            0.06
                            0.00




                                                                         0.00
                                   5    10      15    20                            5    10      15   20




                                       (2,10)                                           (2,14)
              Probability




                                                           Probability
                                                                         0.10
                            0.10
                            0.00




                                                                         0.00


                                   5    10      15    20                            5    10      15   20

                                                                                2
                Figure 3: Reparameterized NB(µ,σ ) probability function.




5. A New Class of Overdispersed Regression Models
    Let Yi , i = 1 . . . n, a sample of a variable of interest; xi = (xi1 , . . . , xis )0
and zi = (zi1 , . . . , zik )0 , i = 1, 2, . . . , n, vector values of covariates, and β =
(β1 , . . . , βs )0 and γ = (γ1 , . . . , γk )0 , two vectors of the regression parameters. Using
this notation, we define the following overdispersed regression models:
Definition 1. The BB(µ, φ) regression model is defined by:

    • A random component: Let Yi ∼ BB(µi , φi ), i = 1 . . . n; a sample of n
      independent random variables.
    • The linear predictors η1i and η2i , such that η1i = x0i β and η2i = z0i γ.
    • Link functions h(.) and g(.), such that h(µi ) := η1i and g(φi ) = η2i .
      In this model, the usual links are the logit function for the mean and the
      logarithmic function for the dispersion parameter.
Definition 2. The N B(µ, α) regression model is defined by:

    • A random variable: Let Yi ∼ N B(µi , αi ), i = 1 . . . n, independent random
      variables.
    • Linear predictors: η1i and η2i such that: η1i = x0i β and η2i = z0i γ.

                                                     Revista Colombiana de Estadística 40 (2017) 141–163

150                          Edilberto Cepeda-Cuervo & María Victoria Cifuentes-Amado


   • Link functions: h(.) and g(.), such that h(µi ) := η1i and g(αi ) = η2i .
        For this model, the usual link functions are the logarithmic functions for
        both h and g.

      The N B(µ, σ 2 ) regression model is defined as the N B(µ, α) regression model.


6. Bayesian Estimation
    In following subsections we develop the Bayesian method for the three proposed
two-stage models: the (µ, φ)-beta-binomial, the (µ, α)-negative binomial and the
(µ, σ 2 )-negative binomial models. In these models, we have two vector parame-
ters to estimate: the mean regression parameters β, and the shape (or variance)
regression parameters γ. The Bayesian method used to fit the proposed models is
defined in the next nine points following Cepeda-Cuervo (2001).

  1. Let Yi , i = 1, 2, . . . , n, be n independent observed values obtained from one
     of the two-parameter distributions.

  2. The regression structures are defined as follows:

                                        h(µi ) = x0i β = η1i
                                                                                         (14)
                                         g(τi ) = z0i γ = η2i ,

        where β = (β1 , . . . , βs )0 , γ = (γ1 , . . . , γk )0 are vectors of unknown regres-
        sion parameters related to the mean and shape (or variance) parameters,
        respectively, such that s + k < m. The vectors xi = (xi1 , . . . , xis )0 , zi =
        (zi1 , . . . , zik )0 correspond to the i-th vector values of covariates and η1i , η2i
        are the linear predictors. We assume xi1 = 1, zi1 = 1, ∀i = 1, . . . , n.

  3. The link function should be strictly monotonic, twice differentiable in classic
     regression, and once in the Bayesian approach.

  4. Without loss of generality, the mean and shape (or variance) regression pa-
     rameters are assumed to have independent normal prior distributions:

                                           β ∼ N (b, B)
                                           γ ∼ N (g, G)

  5. Let L(β, γ | Y, X, Z) be the likelihood function and p(β, γ) the joint prior
     distribution. The likelihood function is L(θ | Y ) = Πf (yi | θ), where θ =
            0
     (β, γ) is the vector of the regression parameters.

  6. As the posterior distribution π(β, γ) ∝ L(β, γ)p(β, γ) is analytically in-
     tractable, we propose using the Metropolis Hastings algorithm to get samples
     of the posterior parameters from the conditional distributions πβ (β | γ, Y, X, Z)
     and πγ (γ | β, Y, X, Z).


                                         Revista Colombiana de Estadística 40 (2017) 141–163

Double Generalized Beta-Binomial and Negative Binomial Regression Models                151

  7. Since the posterior conditional distribution π(β | γ) is analytically intractable,
     we propose to use working variables to build a kernel transition function to
     propose posterior samples of the β parameter vector. This working variable
     is determined by the first order Taylor approximation of h(.) around the
     current parameter value of µi :

                           h(yi ) ' h(µi ) + h0 (µi )(yi − µi ) = ỹ1i ,
     where E(yi ) = µi , and

                                   E(ỹ1i ) = xi 0 β
                                                        2
                                V ar(ỹ1i ) = [h0 (µi )] V ar(yi ).

     Therefore, if β (c) and γ (c) are the current values of the parameters, then:
                                          h                 i
       y˜1i = x0i β (c) + h0 h−1 x0i β (c)     yi − h−1 x0i β (c) , ∀i = 1 . . . n (15)

  8. For this proposal, we assume the kernel transition function given by:

                                 q1 (β | γ) = N (b∗ , B∗ )                              (16)

     where

                                b∗ = B∗ (B−1 b + X0 Σ−1 Ỹ1 )
                                B∗ = (B−1 + X0 Σ−1 X)−1

     Ỹ2 = (ỹ11 , . . . , ỹ2n )0 and Σ = Diag (V ar (y˜1i )). With the kernel function
     (16), the values of β, which will appear in the sample of the posterior distri-
     bution π(β, γ), will be generated.
  9. Since the posterior conditional distribution π(γ | β) is analytically intractable,
     it is necessary to use working variables and a second kernel function to gen-
     erate posterior samples of the γ parameters. To do this, Cepeda-Cuervo
     (2001) assumes there are variables ti , such that E(ti ) = τi , where τi is the
     second parameter to be modeled, and that τi = g −1 (z0i γ). So, the working
     variables related to the second parameter, denoted by y˜2i are given by:

                            g(t) ' g(τ ) + g 0 (τ )(t − τ ) = y˜2i                      (17)

     The mean and variance of these working variables are given by:

                                    E(y˜2i ) = z0i γ
                                                        2
                                 V ar(y˜2i ) = [g 0 (τi )] V ar(ti )

     Then, the working variables are:
                                            h                  i
       y˜2i = z0i γ (c) + g 0 g −1 z0i γ (c)     ti − g −1 z0i γ (c) , ∀i = 1 . . . m   (18)


                                       Revista Colombiana de Estadística 40 (2017) 141–163

152                         Edilberto Cepeda-Cuervo & María Victoria Cifuentes-Amado


 10. The second kernel transition function, q2 , is

                                   q2 (γ | β) = N (g∗ , G∗ )                             (19)

      where
                                                           
                                 g∗ = G∗ G−1 g + Z0 Ψ−1 Ỹ2
                                                     −1
                                 G∗ = G−1 + Z0 Ψ−1 Z     ,

      Ỹ2 = (ỹ21 , . . . , ỹ2n )0 and Ψ = Diag(V ar (y˜2i )).

  Once the kernel transition functions and working variables are established, the
Metropolis Hastings algorithm is defined by the following steps:

  1. Begin the chain iteration counter at j = 1.

  2. Set initial chain values β (0) and γ (0) for β and γ, respectively.

  3. Propose a new value δ for β, generated from the kernel transition function
     q1 (β (j−1) , ·).

  4. Calculate the acceptance probability for the new value δ: α (β, δ). For large
     values of this probability, the new value is accepted. If the movement is
     accepted, then β (j) = δ, otherwise β (j) = β (j−1) .

  5. Propose a new value δ for γ, generated from the kernel transition function
     q2 (γ (j−1) , ·).

  6. Calculate the acceptance probability for the new value δ: α (γ, δ). For large
     values of this probability, the new value is accepted. If the movement is
     accepted, then γ (j) = δ, otherwise γ (j) = γ (j−1) .

  7. Return to 3 until convergence.


6.1. Working Variables in the BB(µ, φ) Regression Model
    We assume the (µ, φ)-beta-binomial regression model defined in Section 5 with
the logit link function for the mean and logarithm link function for the disper-
sion parameter. Thus it follows from equation (15) that the working variables to
define the kernel transition function to propose samples of the mean regression
parameters are:
                                                     (c)
                                           yi /ni − µi
                      ỹ1i = x0i β (c) +    (c)      (c)
                                                           ,∀ i = 1...m
                                           µi (1 − µi )

To propose a kernel transition function to obtain samples of the dispersion re-
gression parameter vector γ, we use the working variables established in equation
(18). These working variables were proposed in Cepeda-Cuervo, Migon, Garrido


                                           Revista Colombiana de Estadística 40 (2017) 141–163

Double Generalized Beta-Binomial and Negative Binomial Regression Models                             153

& Achcar (2014) in their framework of the generalized linear models with random
effects. In order to get these working variables, we have to ascertain the expression
                                               (c)    (c)
for ti such that E(ti ) = φ. Then, with ti = φi yi /µi ni , using equation (18), this
working variable is defined by:

                               (c)    (c)      (c)
                           φi yi /µi ni − φi                           yi
     ỹ2i = z0i γ (c) +                (c)
                                                     = z0i γ (c) +    (c)
                                                                           − 1,    ∀i = 1 . . . m,
                                      φi                             µi ni
                         (c)               (c)
                                              
               = exi β         / 1 + exi β       and φi = ezi γ .
         (c)       0                   0              (c)   0  (c)
where µi
   The variances of these working variables are, respectively:
                               h                             i−2
                                 yi         (c)           (c)
         V ar(ỹ1i ) = V ar              µi (1 − µi )
                                 ni
                                                               !
                         (c)          (c)         (c)                                   i−2
                       µi (1 − µi ) φi + ni h (c)                                  (c)
                     =                             (c)
                                                                    µ i  (1  −   µ i   )
                               ni                φi + 1
                                        !
                           (c)                                       i−1
                          φi + n i h               (c)          (c)
                     =       (c)
                                             n   µ
                                               i i     (1  −   µi   )       , ∀i = 1 . . . m
                          φi + 1
                                   h          i−2
                                       (c)
         V ar(ỹ2i ) = V ar(yi ) µi ni
                                                                    !
                                               φ(c) + n h                       i−2
                           (c)             (c)          i         i       (c)
                     = ni µi       1 − µi                (c)
                                                                        µ i   n i
                                                      φi + 1
                                        !"                 #
                           (c)                         (c)
                          φi + n i            1 − µi
                     =       (c)                     (c)
                                                              , ∀i = 1 . . . m.
                          φi + 1                ni µi

6.2. Working Variables in the NB(µ, α) Regression Model
    In the N B(µ, α) regression model defined in Section 5, assuming the logarith-
mic function for both the mean and the shape parameters from the first-order
Taylor approximation with ti = yi , the working variables used to define the kernel
transition function to propose samples of the mean regression parameters are given
by:
                         1        (c)
                                                      yi
      ỹ1i = x0i β (c) + (c) yi − µi     = x0i β (c) + (c) − 1, ∀i = 1 . . . m (20)
                        µi                            µi
    As developed in Cepeda-Cuervo (2001), Cepeda-Cuervo & Achcar (2009) Cepeda-
Cuervo & Gamerman (2005) and Cepeda-Cuervo et al. (2014), the working vari-
ables used to propose samples of the shape regression parameter require a variable
t such that E(t) = α, thus t = αy/µ. Then, from the first order Taylor approxi-
mation of the logarithmic function around α, the working variables are:
                                       !
                          (c)
            0 (c)   1   αi yi      (c)                 yi
    ỹ2i = zi γ + (c)      (c)
                               − αi      = z0i γ (c) + (c) − 1, ∀i = 1 . . . m (21)
                  αi     µi                           µi


                                              Revista Colombiana de Estadística 40 (2017) 141–163

154                           Edilberto Cepeda-Cuervo & María Victoria Cifuentes-Amado


      The variances of these working variables are, respectively:
                                                           
                                           (c)    (c)   (c)
                             h     i−2   µi      µi + αi      h      i−2
                               (c)                               (c)
      V ar(ỹ1i ) = V ar(yi ) µi       =            (c)
                                                               µ i       ,         ∀i = 1 . . . m
                                                  αi
                             h     i−2
                               (c)
      V ar(ỹ2i ) = V ar(yi ) µi       = V ar(ỹ1i ), ∀i = 1 . . . m.


6.3. Working Variables in the N B(µ, σ 2 ) Regression Model
   Assuming the logarithm link function for the mean and variance parameters,
                                                                                   2
the working variables ỹ1i , obtained using t = Y ; and ỹ2i , obtained using t = σ µY ,
remain as in equations (20) and (21), respectively. Therefore, the variances of the
working variables are:
                                                i−2    2(c)
                                          h
                                            (c)      σ
                   V ar(ỹ1i ) = V ar(yi ) µi       = i(c)2 ,    ∀i = 1 . . . m
                                                     µi
                                                i−2    2(c)
                                          h
                                            (c)      σ
                   V ar(ỹ2i ) = V ar(yi ) µi       = i(c)2 ,    ∀i = 1 . . . m
                                                     µi


7. Proposed Regression Models: A Classic Approach
   In order to obtain the parameter estimates of the proposed models using the
Newton Raphson algorithm, in this section, we develop the first-order and second-
order partial derivatives of the logarithm of the likelihood functions for each of
the proposed models. For each of the regression models defined in Section 5, the
logarithm of the likelihood function is given by:
                                             n
                                             X
                                    L(θ) =         li (θ),                                      (22)
                                             i=1

where li (θ) = log f (xi | θ) and θ = (β, γ) is the vector of the regression parame-
ters.
   Therefore, the first-order partial derivatives of the logarithm of the likelihood
function are given by:
                    ∂li   ∂li ∂µi ∂η1i   ∂li ∂µi
                        =              =          xji           j = 1, . . . , s                (23)
                    ∂βj   ∂µi ∂η1i ∂βj   ∂µi ∂η1i
                    ∂li   ∂li ∂αi ∂η2i   ∂li ∂αi
                        =              =          zji           j = 1, . . . , k                (24)
                    ∂γj   ∂αi ∂η2i ∂γj   ∂αi ∂η2i
      And the second-order partial derivatives are given by:
                  "            2               #
         ∂ 2 li     ∂ 2 li ∂µi       ∂li ∂ 2 µi
                                        
                =                 +                 xji xsi , j, s = 1, . . . , p               (25)
        ∂βs ∂βj     ∂µ2i ∂η1i       ∂µi ∂η1i  2



                                          Revista Colombiana de Estadística 40 (2017) 141–163

Double Generalized Beta-Binomial and Negative Binomial Regression Models                         155

       ∂ 2 li            ∂ 2 li ∂αi ∂µi
                                           
              =                                 xji zsi ,   j = 1, . . . , p, s = 1, . . . , r   (26)
      ∂γs ∂βj           ∂αi ∂µi ∂η2i ∂η1i
                 "            2              #
        ∂ 2 li     ∂ 2 li ∂αi      ∂li ∂ 2 αi
                                     
               =                 +               zji zsi ,                j, s = 1, . . . , r    (27)
       ∂γs ∂γj     ∂αi2 ∂η2i       ∂αi ∂η2i2



    Thus, denoting by H the Hessian matrix and by q the vector of first derivatives
of the logarithm of the likelihood function, the equation of the Newton-Rapson
algorithm,


                                    θ (k+1) = θ (k) − (H k )−1 q k ,                             (28)

is well defined. The parameter estimates are obtained by setting initial parameter
values and applying the Newton-Rapson equation until a convergence criterion is
satisfied.


7.1. Fitting the BB(µ, φ) Regression Model
    In order to fit the BB(µ, φ) regression model, if Yi ∼ BB(µi , φi ), i = 1, . . . , n;
is a sample of the variable of interest, the logarithm of the likelihood function is
given by:
                       
                   mi
li (θ) = log                +log(B(yi +µi φi , mi −yi +φi (1−µi )))−log(B(µi φi , φi (1−µi ))),
                   yi

where li is the i-th component of the likelihood function defined in (22).
    Thus, using the development in equations (23) and (24), the first order deriva-
tives of the logarithm of the likelihood function are given by:


    ∂li
        = [Ψ(yi + µi φi ) − Ψ(mi − yi + φi (1 − µi )) − Ψ(µi φi ) + Ψ(φi (1 − µi ))]
    ∂βj
                     
                φi
        ×               xji
            g 0 (µi )
    ∂li
        = [Ψ(yi + µi φi ) − Ψ(mi − yi + φi (1 − µi )) − Ψ(µi φi ) + Ψ(φi (1 − µi ))]
    ∂γj
                     
                µi
        ×               zji ,
            h0 (φi )

                                                    d
where Ψ(z) represents the digamma function, Ψ(z) := dz ln Γ(z).
   The second-order partial derivatives are given by equations (25), (26), and (27),
where:

                                             Revista Colombiana de Estadística 40 (2017) 141–163

156                            Edilberto Cepeda-Cuervo & María Victoria Cifuentes-Amado




      ∂li
          = φi [Ψ(yi + µi φi ) − Ψ(mi − yi + φi (1 − µi )) − Ψ(µi φi ) + Ψ(φi (1 − µi ))]
      ∂µi
   ∂ 2 li
          = φ2i Ψ0 (yi + µi φi ) + Ψ0 (mi − yi + φi (1 − µi )) − Ψ0 (µi φi ) − Ψ0 (φi (1 − µi ))
                                                                                                
   ∂µi 2

   ∂µi         1
          = 0
   ∂η1i     g (µi )
   ∂ 2 µi   −g 00 (µi ) ∂µi     −g 00 (µi )
       2
          =          2
                              =
   ∂η1i       0
            [g (µi )]   ∂η 1i   [g 0 (µi )]3
 ∂ 2 li
        = [Ψ(yi + µi φi ) − Ψ(mi − yi + φi (1 − µi )) − Ψ(µi φi ) + Ψ(φi (1 − µi ))]
∂φi ∂µi
          + µi φi Ψ0 (yi + µi φi ) + Ψ0 (mi − yi + φi (1 − µi )) − Ψ0 (µi φi ) − Ψ0 (φi (1 − µi ))
                                                                                                    

      ∂li
          = µi [Ψ(yi + µi φi ) − Ψ(mi − yi + φi (1 − µi )) − Ψ(µi φi ) + Ψ(φi (1 − µi ))]
      ∂φi
   ∂ 2 li
          = µ2i Ψ0 (yi + µi φi ) + Ψ0 (mi − yi + φi (1 − µi )) − Ψ0 (µi φi ) − Ψ0 (φi (1 − µi ))
                                                                                                
   ∂φ2i
   ∂φi         1
          = 0
   ∂η2i     h (φi )
   ∂ 2 φi   −h00 (φi )
       2
          =
   ∂η2i     [h0 (φi )]3


    With these derivatives, the vector of the first derivatives q and the Hessian
matrix H of equation (28) are well defined, and thus, the Newton-Rapson algo-
rithm can be applied in order to obtain the maximum likelihood estimation of the
model’s regression parameters.



7.2. Fitting NB(µ, α) Regression Model

    In the negative binomial regression model, if Yi ∼ N B(µi , αi ), i = 1, . . . , n;
is a sample of the negative binomial distribution, the logarithm of the likelihood
function is given by:
                                       
                                   αi
             li (θ) = αi log       + log(Γ(yi + αi )) − log(yi !) − log(Γ(αi ))
                                   µi
                                          
                                        αi
                   − (yi + αi ) log 1 +      ,
                                        µi

where li is the i-th component of the likelihood function.
    Thus, from equations (23) and (24), the first order derivatives of the logarithm
of the likelihood function are given by:

                                               Revista Colombiana de Estadística 40 (2017) 141–163

Double Generalized Beta-Binomial and Negative Binomial Regression Models                  157


                                               
                ∂li     µi − yi        −αi
                    =                               xji
                ∂βj     µi + αi      µi g 0 (µi )
                                                                      
                ∂li                                            yi + αi
                    = [1 + log(αi ) + Ψ(yi + αi ) − Ψ(αi ) −
                ∂γj                                            µi + αi
                                                 
                                             1
                    − log (µi + αi )]               zji .
                                        h0 (αi )

   Finally, from equations (25), (26) and (27), the second order derivatives of the
logarithm of the likelihood function are given by:
                                        
         ∂li        αi           yi + αi            αi (µi − yi )
               =−          1−                =−
         ∂µi        µi           µi + αi            µi (µi + αi )
           2
                                                               
        ∂ li     αi            yi + αi        αi        yi + αi
               =         1 −               −
        ∂µ2i     µ2i           µi + αi        µi (µi + αi )2
                                                            
                        αi          µi − yi        yi + αi
               =                               −
                 µi (µi + αi )         µi          µi + αi
                        2
                 αi (µi − 2yi µi − yi αi )
               =
                        µ2i (µi + αi )2
        ∂µi          1
               = 0
        ∂η1i     g (µi )
       ∂ 2 µi    −g 00 (µi ) ∂µi       −g 00 (µi )
            2  =            2       =             3
       ∂η1i      [g 0 (µi )] ∂η1i      [g 0 (µi )]
      ∂ 2 li
                                                                            
                     1           yi + αi         αi       yi + αi         1
               =−          1−                −                      −
    ∂αi ∂µi         µi          µi + αi          µi (µi + αi )2        µi + αi
                                                                           
         ∂li                                                       yi + αi
               = 1 + log(αi ) + Ψ(yi + αi ) − Ψ(αi ) −                        − log (µi + αi )
         ∂αi                                                       µi + αi
                                                                           
                 µi − yi                                              αi
               =              + Ψ(yi + αi ) − Ψ(αi ) + log
                 µi + αi                                           µi + αi
        ∂ 2 li
                                                                                  
                  1         0                 0                1         yi + αi           1
               =       + Ψ (yi + αi ) − Ψ (αi ) −                   −                −
        ∂αi2     αi                                        µi + αi     (µi + αi )2      µi + αi
                        µi                                           µi − yi
               =                  + Ψ0 (yi + αi ) − Ψ0 (αi ) −
                 αi (µi + αi )                                     (µi + αi )2
        ∂αi           1
               = 0
        ∂η2i     h (αi )
       ∂ 2 αi    −h00 (αi )
       ∂η2i 2 =             3.
                 [h0 (αi )]

   With these derivatives, the Newton Rapson equation (28) can be proposed, and
the maximum likelihood estimation of the regression parameters can be obtained
using the Newton Rapson algorithm.

                                         Revista Colombiana de Estadística 40 (2017) 141–163

158                         Edilberto Cepeda-Cuervo & María Victoria Cifuentes-Amado


7.3. Fitting the NB(µ, σ 2 ) Regression Model
    If Yi ∼ N B(µi , σi2 ), i = 1, . . . , n; is a sample of the variable of interest, then
the logarithm of the likelihood function is given by:

              µ2i                                                 µ2i
                                                                     
                                           2
                                             
   li (θ) = 2        log(µi ) − log(σi ) + log Γ yi + 2                     − log(yi !)
           σi − µi                                             σi − µi
                                                   µ2i
                                                      
                                                           − yi log(σi2 ) − log(σi2 − µi )
                                                                                           
                               − log Γ           2
                                               σi − µi

      The first derivatives are:
                                                               
                                   ∂li   ∂li             1
                                       =                            xji
                                   ∂βj   ∂µi        g 0 (µi )

                                                               
                                   ∂li   ∂li           1
                                       =                            zji ,
                                   ∂γj   ∂σi2        0
                                                    h (µi )

where:
                                                                             
                                                             µ2           µ2
                                                                                         
                                                               i            i
                                                    Ψ yi + σ2 −µ      Ψ σ2 −µ
∂li     µi    2σi2 − µi                                   i    i       i    i      yi
                         log(µi ) − log(σ 2 ) +
                                                                                          
    = 2      
                                          i                         −          −    + 1
     σi − µi  σi2 − µi 
                                                     
∂µi                                                           2
                                                             µi            2
                                                                          µi        µi    
                                                    Γ yi + σ2 −µ      Γ σ2 −µ
                                                                    i       i       i   i




                                                                µ2
                                                                                         
∂li      −µ2i                             σ 2
                                              −  µ
                                                                 i
                                                       Ψ yi + σ2 −µ      −1 y (σ 2 − µ )
                                                   i
  2
    =            2
                                    2
                   log(µi ) − log(σi ) +   i
                                               2
                                                     +        i
                                                                    2
                                                                     i
                                                                          − i i 2 i .
∂σi     2
      (σi − µi )                              σi                   µ
                                                        Γ yi + σ2 −µi          µi σi
                                                                  i    i



    The second derivatives can be obtained from equations (25), (26) and (27) as
in sections 7.1, and 7.2, and the maximum likelihood estimation of the regression
parameters can be obtained by applying the Newton Rapson algorithm.


8. School Absenteeism Data
    The data analyzed in this paper are presented in (Quine 1975) and come from
a sociological study of Australian Aboriginal and white children from Walgett,
New South Wales. There are nearly equal numbers of the two sexes and an equal
number from the two cultural groups. Children were classified by culture, age,
sex, and learner status, and the number of days absent from school in a particular
school year was recorded. The response variable of interest is the number of days
that a child was absent from school during the year; children who had suffered a
serious illness during the year were excluded from this analysis.




                                         Revista Colombiana de Estadística 40 (2017) 141–163

Double Generalized Beta-Binomial and Negative Binomial Regression Models                          159

    The possible values for each observed variable are:

    • Age: age group: Primary (0), First form (1), Second form (2), and Third
      form (3).
    • Gender: sex: factor with levels Female or Male. The number (0) represents
      “female” and (1) corresponds to “male”.
    • Cultural Background: ethnic background: Aboriginal or Not: number 0
      for Aboriginal and 1 for White.
    • Learning Ability: learner status: factor with levels Average (1) or Slow
      learner (0).
    • Days Absent: days absent from school in the year.

    Since the variable “days absent” counts the number of events that occurred
during a year, this variable can be modeled by a model associated with the Poisson
distribution. Thus, if X is the days of absenteeism of a student during a year, we
assume the negative binomial model N B(µ, α), where the mean and the shape
parameters follow linear regression structures given by:

                                       log(µi ) = η1i = x0i β                                     (29)
                                       log(αi ) = η2i = z0i γ,                                    (30)

where β = (β0 , β1 , β2 , β3 ), x0i = (intercepti , agei , cultural.backi , learning.abilityi ),
γ = (γ0 , γ1 ) and z0i = (intercepti , cultural.backi ), i = 1, . . . , 113.
   We fitted this model to the data by applying two Bayesian methods,one using
Open Bugs and the other using R software. In the second, the algorithm was built
based on the Metropolis Hastings algorithm, defined in Section 6. The posterior
parameter estimates, standard deviations and credibility intervals are given in
Table 1.
         Table 1: Parameter estimates of the beta-binomial regression model.
                                         OpenBugs                               RStudio
Parameter Description
                            Estimate    S.D.      Cred. Interval   Estimate   S.D.     Cred. Interval
    β0      Intercept        2.823     0.303        (2.24,3.42)     2.7844     0.3      (2.22,3.39)
    β1      Age              0.1734    0.115       (−0.054,0.4)     0.1866    0.113    (−0.04,0.41)
    β2      Cultur.backg. −0.6564 0.1884 (−1.024,−0.2807) −0.6269 0.1886 (−1.012,−0.268)
    β3      Learn.ability   −0.3632 0.1762       (−0.711,−0.021)   −0.3613 0.1805 (−0.706,−0.004)
    γ0      Intercept        0.593     0.2123      (0.171,0.999)    0.5643    0.1712   (0.205,0.863)
    γ1      Cultur.backg. −0.7276 0.3048          (−1.32,−0.128)    −0.74     0.2496   (−1.21,−0.22)


    We calculated the Bayesian information criterion (BIC) for the model in both
programs: the model in OpenBugs obtained a BIC equal to 826.97 while in R
software the BIC was 827.24. The parameter estimates, their standard deviations,
and the 95% credible intervals obtained from the two programs are given in Table
1. From this table, we can observe that Age can be removed from the mean
structure of the model, since zero (0) belongs to the 95% credible interval.


                                               Revista Colombiana de Estadística 40 (2017) 141–163

160                            Edilberto Cepeda-Cuervo & María Victoria Cifuentes-Amado


    Assuming, a mean structure (29) without the age variable, and if the shape
structure given by (30), the parameter estimates, standard deviations, and 95%
credible intervals are reported in Table 2.

           Table 2: Parameter estimates of the beta-binomial regression model.
                                            OpenBugs                             RStudio
Parameter Description          Estimate   S.D.      Cred. Interval   Estimate   S.D.     Cred. Interval
      β0     Intercept          3.235     0.136      (2.975,3.506)    3.1571    0.137     (2.89,3.43)
      β2     Cultural.backg.   −0.7253    0.1828   (−1.083,−0.3647) −0.6644     0.181    (−1.02,−0.31)
      β3     Learning.ability −0.3975 −0.3975 (−0.741,−0.0432)        −0.34      0.18    (−0.68,−0.02)
      γ0     Intercept          0.506     0.2014     (0.107,0.892)     0.43     0.204     (0.05,0.68)
      γ1     Cultural.backg.   −0.575     0.2895   (−1.14,−0.003)     −0.47     0.2524   (−0.89,0.02)


   Again we calculated the BIC with both programs: R software yielded BIC
equal to 829.83 while the BIC for the OpenBugs estimation was 833.92.
    Therefore, from the parameter estimates that are given in Table 2, it is possible
to conclude that the variable Days.abseent decreases while the Cultural.background
or the Learning.ability increases. For Slow learner status (0), the mean of Days
Absent is 2.4927 for Aboriginal background and 3.1571 for White cultural back-
ground. For average learner status (1), the mean of Days Absent is 8.61 for White
cultural background and 16.73 for Aboriginal cultural background.


Conclusions
    In this paper, new parameterizations of the beta-binomial and negative bino-
mial distributions have been proposed. These formulations, in terms of the mean
and shape parameters, are good options to explain the behavior of overdispersed
count data. From these reparameterizations, overdispersed regression models have
been defined assuming that the two parameters of these models follow regression
structures.
   We also propose Bayesian and classic methods to fit these models, which we ap-
ply to analyze school absenteeism data. In this application, the proposed methods
show suitable performance and posterior inferences; those obtained by applying
the Bayesian method in RStudio and in OpenBugs agree. The observed results
showed that the mean depends on the cultural background and the learning ability,
and the shape parameter depends on the cultural background.
                                                                 
                    Received: June 2016 — Accepted: November 2016
Appendix
    In the classic approach, there were some calculations related to the first and
second derivatives of the different likelihood functions that were dropped. In this
section, some of them appear.

   • Some details of the BB(µ, φ) and the negative binomial maximum likeli-
     hood functions’ derivatives that were used in the Newton Rapson algorithm
     (Section 7.1):
                                                                                          
             ∂B(yi +µi φi ,mi −yi +φi (1−µi ))         ∂B(yi +µi φi ,mi −yi +φi (1−µi ))
      ∂li                   ∂x
                                               φi −                  ∂x
                                                                                         φi  1 
                              1                                           2
          =                                                                                          xji
      ∂βj                      B(yi + µi φi , mi − yi + φi (1 − µi ))                        g 0 (µi )
                                                                
             ∂B(µi φi ,φi (1−µi ))       ∂B(µi φi ,φi (1−µi ))
                    ∂x
                                   φi −          ∂x
                                                               φi  1 
                        1                           2
          −                                                                   xji
                           B(µi φi , φi (1 − µi ))                    g 0 (µi )

           = [Ψ(yi + µi φi ) − Ψ(mi + φi ) − Ψ(mi − yi + φi (1 − µi )) + Ψ(mi + φi ) − Ψ(µi φi )
                                                         
                                                     φi
           + Ψ(φi ) + Ψ(φi (1 − µi )) − Ψ(φi )]    0
                                                            xji
                                                  g (µi )
                                                                                                   
                                                                                              φi
           = [Ψ(yi + µi φi ) − Ψ(mi − yi + φi (1 − µi )) − Ψ(µi φi ) + Ψ(φi (1 − µi ))]               xji
                                                                                          g 0 (µi )



                                                                                                        
      ∂li                                                                                       µi
          = [Ψ(yi + µi φi ) − Ψ(mi − yi + φi (1 − µi )) − Ψ(µi φi ) + Ψ(φi (1 − µi ))]                       zji
      ∂γj                                                                                     h0 (φi )


   • In Section (7.2), some details of the derivatives of the logarithm of the
     N B(µ, α) maximum likelihood function were dropped. These are:
                                                                         
                 ∂li     −αi                    µi           −αi           1
                     =        − (yi + αi )                                        xji
                 ∂βj      µi                 µi + αi         µ2       g 0 (µi )
                                                        i
                         1         yi + αi          −αi
                     =      −                                 xji
                         µi     µi (µi + αi )      g 0 (µi )


                                                                          
         ∂li                                                       yi + αi
             = [1 + log(αi ) − log(µi ) + Ψ(yi + αi ) − Ψ(αi ) −
         ∂γj                                                       µi + αi
                                       
                          αi         1
             − log 1 +        ]             zji
                          µi     h0 (αi )
                                                                
                                                         yi + αi
             = [1 + log(αi ) + Ψ(yi + αi ) − Ψ(αi ) −              − log (µi + αi )]
                                                         µi + αi
                      
                  1
                0
                         zji
               h (αi )


                                            Revista Colombiana de Estadística 40 (2017) 141–163

Double Generalized Beta-Binomial and Negative Binomial Regression Models                                             163

   • Finally, regarding Section (7.3), we find the following first derivatives for the
     (µ, σ 2 ) − N B log-likelihood function:

                  µ2i                                                       µ2i
                                                                                   
      ∂li                          2µi                         2
                                                                                       1
          =                  +                 log(µi ) − log(σi  )   +
      ∂µi    (σi2 − µi )2       σi2 − µi                                 σi2 − µi µi
                                                         
                         µ2              µ2           2µi              µ2
            Ψ yi + σ2 −µ   i
                                       2
                                          i
                                    (σi −µi )   2 +   2
                                                    σi −µi
                                                                        i
                                                                  (σi2 −µi )2
                                                                               + σ22µ   i        
                                                                                                       1
                                                                                                            
                                                                                    i −µi
                        i     i
          +                                                 −                         − yi      2
                                         2
                                        µi                                       µi2               σi − µi
                         Γ yi + σ2 −µ                              Γ yi + σ2 −µ
                                      i      i                                  i     i
                                                                                                                
                                                                                             µ2
                                                                                              i          µi
                                                                             Ψ yi + σ2 −µ             2
                                                                                                     (σi −µi )
                                                                                                               + 2
               µi             µi                                                            i   i
                                     + 2 log(µi ) − log(σi2 ) + 1 +
                                                                   
          = 2         {
                          σi2 − µi
                                                                                                         
            σi − µi                                                                                 µ2i
                                                                                          Γ yi + σ2 −µ
                                                                                                     i       i
                     µi
                  σi2 −µi
                            +2y
          −                − i}
                       µ2
                        i     µi
              Γ yi + σ2 −µ
                            i       i
                                                                                    
                                                                µ2
                                                                                                              
                                                                 i
                                                       Ψ yi + σ2 −µ    −1
              µi    2σi2 − µi                                i   i          yi
                               log(µi ) − log(σ 2 ) +
                                                                                  
          = 2                                                              − µ + 1
                                                                                
                       2                        i        
           σi − µi   σi − µi                                     2
                                                                  µi           i
                                                        Γ yi + σ2 −µ
                                                                 i   i




                                                                                                            
                                                                                          µ2
                                                                                           i            −µ2
                                                                                                          i
                                                                  Ψ               yi + σ2 −µ
                 −µ2i                                  µ2i                                             2 −µ )2
                                                            
      ∂li                                    2                −1                         i    i     (σ i   i
           =              [log(µ i ) − log(σ i )] +               +
      ∂σi2   (σi2 − µi )2                           σi2 − µi σi2
                                                                                                      
                                                                                                 µ2
                                                                                                  i
                                                                                      Γ yi + σ2 −µ
                                                                                                 i       i

                      −µ2i                                    
                   (σi2 −µi )2                   1      1
          −                 − yi                   − 2
                                                σi2
               
                       µ2
                        i                            σi − µi
              Γ yi + σ2 −µ
                            i       i
                                                                                  
                                                                          µ2i
                                "                           Ψ   y i  +
              −µ2i                       2     σi2 − µi                σi2 −µi                1
          =         2 log(µi ) − log(σi ) +         2
                                                          +                2
                                                                                    −                
             2
            σi − µi                                σ i                   µ  i                    µ2i
                                                            Γ yi + σ2 −µ               Γ yi + σ2 −µ
                                                                        i       i               i    i
                                             #
                                   yi (−µi )
                                −
                                     µi σi2
                                                                                   
                                                                           µ2
                                                                                                 
                                                                              i
                                                            Ψ    y i +                −1
              −µ2i                             σ  2−µ
                                                        i              σ  2 −µ
                                                                                  i       y (µ ) 
                                                                                       + i 2i 
                                         2       i                       i
          =
             2         log(µi ) − log(σi ) +
                    2 
                                                    σ 2
                                                          +      
                                                                                 2         µi σi 
            σi − µi                                   i       Γ yi + σ2 −µ
                                                                               µ i
                                                                                    i    i


                                                  Revista Colombiana de Estadística 40 (2017) 141–163

References
Breslow N. Extra-Poisson variation in Log-Linear models.(1984). Journal of Applied Statistics.
Cepeda Cuervo E. Modelagem da Variabilidade em Modelos Lineares Generalizados.(2001). Universidade Federal do Río de Janeiro.
Cepeda Cuervo E, Achcar J. Modelos de regresión heterocedásticos usando aproximación bayesiana.(2009). Revista Colombiana de Estadística.
Cepeda Cuervo E, Gamerman D. Bayesian methodology for modeling parameters in the two parameter exponential family.(2005). Revista Estadística.
Cepeda Cuervo E, Migon H, Garrido L, Achcar J. Generalized linear models with random effects in the two-parameter exponential family.(2014). Journal of Statistical Computation and Simulation.
Collet D. Modeling Binary Data.(1991). Chapman Hall.
Cox D. Some remarks on overdispersion.(1983). Biometrika.
Demétrio C, Hinde J. Overdipersion: Models and estimation.(1998). Computational Statistics and Data Analysis.
Demétrio C, Kokonendji C, Zocchi S. On Hinde-Demétrio regression models for overdispersed count data.(2007). Statistical Methodology.
Ferrari S, Cribari Neto F. Beta Regression for Modelling Rates and Proportions.(2004). Journal of Applied Statistics.
Jørgensen B. The Theory of Dispersion Models.(1997). Chapman and Hall.
Lawless J. Negative binomial regression model.(1987). Canadian Journal of Statistics.
Margolin B, Kaplan N, Zeiger E. Statistical analysis of the Ames Salmonella/ microsome test.(1981). Proceedings of the National Academy of Sciences.
McCullagh P, Nelder J. Generalized Linear Models.(1989). Chapman Hall.
Quine S. Achievement orientation of aboriginal and white Australian adolescents.(1975). Australian National University.
Quintero Sarmiento A, Cepeda Cuervo E, Núñez Antón V. Estimating infant mortality in Colombia: some overdispersion modeling approaches.(2012). Journal of Applied Statistics.
Williams D. The analysis of binary responses from toxicological experiments involving reproduction and teratogenicity.(1975). Biometrics.
Williams D. Extra-binomial Variation in Logistic linear Models.(1982). Journal of Applied Statistics.
