Decision Theory for the Variance Ratio in One-Way ANOVA with Random Effects. TeorÃ­a de la decisiÃ³n para la relaciÃ³n de las varianzas en el anÃ¡lisis de la varianza de un factor con efectos aleatorios
Universitat Pompeu Fabra, Barcelona, Spain. Universidad del Valle, Cali, Colombia
Abstract
Estimating a variance component in the model of analysis of variance with random effects and testing the hypothesis that the variance vanishes are important issues in many applications. Such inferences are beyond the confines of the standard (asymptotic) theory because a zero variance is on the boundary of the parameter space and the maximum likelihood or another reasonable estimator of variance has a non-trivial probability of zero in many settings. We derive decision rules regarding the variance ratio in balanced one-way analysis of variance, in both the frequentist and Bayesian perspectives. We argue that this approach is superior to hypothesis testing because it incorporates the consequences of the two kinds of error (incorrect choice) that may be committed. An application to a track athleteâ€™s training performance is presented.
Key words: Analysis of Variance with Random Effects, Decision, Equilibrium, Expected Loss, Variance Ratio.
Resumen
La estimaciÃ³n de una de las varianzas en el modelo de anÃ¡lisis de la varianza con efectos aleatorios y la prueba de hipÃ³tesis de que la varianza se anula, son temas importantes en muchas aplicaciones. Tales inferencias estÃ¡n fuera de los confines de la teorÃ­a asintÃ³tica estÃ¡ndar porque una varianza cero estÃ¡ en la frontera del espacio paramÃ©trico y la mÃ¡xima verosimilitud u otro estimador razonable de una varianza tiene una probabilidad no trivial de cero en muchos contextos. Nosotros derivamos una regla de decisiÃ³n sobre la razÃ³n de varianzas en un anÃ¡lisis de varianza de un factor balanceado tanto para la perspectiva frecuentista como la Bayesiana. Argumentamos que este enfoque es superior a la prueba de hipÃ³tesis porque incorpora las consecuencias de los dos tipos de error (elecciÃ³n incorrecta) que pueden cometerse. Se presenta una aplicaciÃ³n sobre los rendimientos de los entrenamientos de un atleta de pista.
Palabras clave: anÃ¡lisis de varianza con efectos aleatorios, decisiÃ³n, equilibrio, pÃ©rdida esperada, razÃ³n de la varianza.



1. Introduction
    Testing the hypothesis that between-cluster variance vanishes in the mixed
model of one-way analysis of variance (ANOVA) and its extensions has received
considerable attention recently (Crainiceanu & Ruppert 2004, Greven, Crainiceanu,
KÃ¼chenhoff & Peters 2008, Giampaoli & Singer 2009, Andrade, Longford & Tovar
2014). The principal findings in these references are that asymptotic theory, or
its adaptation for the non-standard nature of the inferential problem, provides a
poor approximation for small and moderately large samples, and that the likeli-
hood ratio test statistic has a distribution well approximated by a mixture of the
constant zero and one or several Ï‡2 distributions. The mixture probabilities are
specific to the setting (design).
    We regard hypothesis testing as problematic in general, because it has no means
of incorporating the consequences of the two kinds of error (bad choice) that may
be committed (Longford 2005, 2012b). We follow up on this criticism by solving
the decision-theoretical version of the problem, in which we choose whether to act
                                                                  2   2
as if the ratio of between- and within-cluster variances, Ï‰ = ÏƒB    /ÏƒW , were smaller
or greater than a given positive constant Ï‰0 , called the threshold. We specify a
loss function (and later a set of loss functions) which quantify the consequences
(ramifications) of two kinds of bad decision. We choose the course of action, that
is, one of the two verdicts, Ï‰ â‰¤ Ï‰0 and Ï‰ > Ï‰0 , for which the expected loss is
smaller. We use asymmetric loss functions that reflect the dependence of the loss
on both the size of the error | Ï‰ âˆ’Ï‰0 | and its sign, when the inappropriate action is
chosen. Choosing the appropriate action is associated with no loss. The outcome
of our analysis is the preferred action. In contrast, the outcome of a hypothesis
has to be interpreted, and the consequences of the two kinds of error considered
ad hoc.
    We address the uncertainty about the loss function by considering a range
of plausible loss functions and, in effect, solving the problem for every one of
them. Owing to some monotonicity properties, it suffices to solve the problem
for the loss functions that delimit the plausible set. This can be regarded as a
form of sensitivity analysis. The outcome may be an inferential impasse, but its
threat is an incentive for more detailed elicitation and declaration of a narrower
range of loss functions. Solutions are developed in the Bayesian paradigm, using
prior densities with analytically convenient functional forms. They have a natural
frequentist interpretation in terms of additional observations (degrees of freedom).


                                      Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

Decision Theory for One-Way ANOVA                                                      183

For background to decision theory we refer to DeGroot (1970, Part 3), Berger
(1985) and Lindley (1985) , and for a new perspective to Longford (2013). An
application of our approach to compare two normally distributed random samples
is presented in Longford (2012a).
    The next section presents a condensed background to one-way ANOVA with
random effects. For a comprehensive treatment of the topic, including numerous
generalisations, see Searle, Casella & McCulloch (2006). The following section
deals with decisions about the variance ratio, namely whether it is greater or
smaller than a given positive constant. The decision is based on the comparison
of the expected losses associated with the contemplated courses of action. Central
to our approach is the evaluation of configurations of values of estimates of the
variance ratio and of a parameter involved in the loss function for which the
two expected losses coincide. These configurations (equilibria), described by a
function, divide the space of loss functions to two subsets corresponding to the
preference for either action. Section 4 extends the results to unbalanced one-way
designs by approximations. An example from athletics training is presented in
Section 5. Apart from addressing a substantive issue, we use it to highlight the
inappropriateness of hypothesis testing as a basis for decision-making. Technical
derivations and related details are collected in a set of appendices.


2. Variance Ratio in One-Way ANOVA
    In this section, we derive the ML estimator of Ï‰ and show that its distribution
is a linear transformation of an F distribution. This simplifies the discussion of
the properties of Ï‰Ì‚ and prepares the main development in Section 3.
   We consider the balanced one-way ANOVA design
                                  yik = Âµ + Î´k + Îµik ,                                 (1)
with clusters k = 1, . . . , K of m observations each; Î´k and Îµik , i = 1, . . . , m, are
two mutually independent random samples from centred normal distributions with
                        2         2                                    2   2
respective variances ÏƒB     and ÏƒW  . Denote the variance ratio Ï‰ = ÏƒB   /ÏƒW     and the
overall sample size n = Km.
                                   P of
   By setting the partial derivatives Pthe loglikelihood for the model in (1) to
zero, we obtain the estimator ÂµÌ‚ = k i yik /n and the equations
                                                         K
                                                 m2 Ï‰Ì‚ X 2
                                 2
                              nÏƒÌ‚W = e> e âˆ’             eÌ„k
                                               1 + mÏ‰Ì‚
                                                        k=1
                                                         K
                              n       1     m2     X
                                   = 2           2
                                                     eÌ„2k ,                            (2)
                           1 + mÏ‰Ì‚  ÏƒÌ‚W (1 + mÏ‰Ì‚)
                                                        k=1

where e is the (n Ã— 1) vector of residuals eik = yik âˆ’ ÂµÌ‚, composed of the K within-
cluster subvectors ek = (e1k , . . . , emk )> ; eÌ„k is the within-cluster average residual,
      1 >
eÌ„k = m ek 1m , where 1m is the vector of unities of length m. See Appendix A for
the derivation of (2).

                                       Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

184                                                 Nicholas T. Longford & Mercedes Andrade


      We decompose e> e into the within- and between-cluster sum of squares,

                                        e> e = SW + SB ,
              PK                   >
                                                              PK 2
where SW =       k=1 (ek âˆ’ eÌ„k 1m ) (ek âˆ’ eÌ„k 1m ) and SB = m    k=1 eÌ„k . For the
balanced design, these two statistics are independent and have scaled Ï‡2 distribu-
                                                                            2
tions with n âˆ’ K and K âˆ’ 1 degrees of freedom and respective scaling by ÏƒW     and
    2    2
mÏƒB + ÏƒW :

                                              SW
                                               2     âˆ¼ Ï‡2nâˆ’K
                                              ÏƒW
                                     SB
                                    2 + Ïƒ2           âˆ¼ Ï‡2Kâˆ’1 .
                                  mÏƒB    W

By simple manipulation of (2) we obtain the identity

                                             m âˆ’ 1 SB   1
                                      Ï‰Ì‚ =            âˆ’   .                                 (3)
                                              m SW      m

Therefore Ï‰Ì‚ > 0 when SW < (m âˆ’ 1)SB . When SW > (m âˆ’ 1)SB , we truncate
Ï‰Ì‚ at zero, to conform with Ï‰ â‰¥ 0. To avoid any ambiguity, we denote by Ï‰Ì‚0 the
version of Ï‰Ì‚ given by (3), but truncated at zero. The random variable

                                                1   n âˆ’ K SB
                                 X =
                                             1 + mÏ‰ K âˆ’ 1 SW

has F distribution with K âˆ’ 1 and n âˆ’ K degrees of freedom, and (3) is equivalent
to
                                       X âˆ’u
                                 Ï‰Ì‚ =        ,                                (4)
                                        mu
where u = K/(K âˆ’ 1)/(1 + mÏ‰). Hence P(Ï‰Ì‚0 = 0) = P(Ï‰Ì‚ â‰¤ 0) = P(X â‰¤ u), but
this probability depends on Ï‰.
    Denote by fk1 ,k2 and Fk1 ,k2 the respective density and distribution function of
the F distribution with k1 and k2 degrees of freedom. To establish the properties
of the estimators Ï‰Ì‚ and Ï‰Ì‚0 , we use the identities

                                                k1
                     yfk1 ,k2 (y)      =              fk +2,k2 âˆ’2 (h1 y) ,
                                              k1 + 2 1
                                                   k1
                    y 2 fk1 ,k2 (y)    =                  fk +4,k2 âˆ’4 (h2 y) ,              (5)
                                              h1 (k1 + 4) 1

where
                                                k1 k2 âˆ’ 2j
                                        hj =               ,                                (6)
                                                k2 k1 + 2j
j = 1, 2. The identities are derived in Appendix B.

                                             Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

Decision Theory for One-Way ANOVA                                                     185

3. Decision Theory for Ï‰
    In this section, we derive the posterior distributions of Ï‰ for a range of prior
distributions and use them to formulate a decision rule for choosing between the ac-
tions (options) A and B. Although the derivations refer to the Bayesian paradigm,
they retain a frequentist interpretation by equating informative priors to specific
data alterations. The decision rule is based on the balance of the expected losses
for the two options; we prefer the option for which the expected loss is smaller.
    Suppose two courses of action, A and B, are contemplated. They are comple-
mentary (one of the actions has to be taken) and exclusive (both actions cannot
be taken). Suppose further that A is appropriate when Ï‰ â‰¤ Ï‰0 and B when
Ï‰ > Ï‰0 ; Ï‰0 is a given scalar, but Ï‰ is not known, and all the information about it
is contained in the vector of outcomes y and a prior distribution for Ï‰.
   The inverse of the identity in (4) is
                                        vÌ‚    1
                                     Ï‰ =   âˆ’    ,                               (7)
                                        X     m
where vÌ‚ = K(Ï‰Ì‚ + 1/m)/(K âˆ’ 1). This operation is associated with the so called
fiducial argument, originating from Fisher (1935, 1956). Its validity has been
extensively discussed (Lindley 1985, Seidenfeld 1992, Hannig 2009) . In fact, (7) is
an example of failure of the argument, because the solution Ï‰ as a random variable
has a positive probability of being negative.
    We consider the non-informative improper prior g(y) = 1, y â‰¥ 0, and a para-
metric class of proper priors for Ï‰, and base our choice of option (the course of
action) on the posterior distribution of Ï‰. The proper prior densities are
                                             m(q âˆ’ 1)
                                  g(y) =              ,                               (8)
                                            (1 + my)q
for y > 0 and a parameter q > 1. A selection of these densities is drawn in Figure 1
for m = 10, with the value of the parameter q printed at the left-hand margin. For
large q, the densities are highly informative and have large values in the vicinity
of Ï‰ = 0. We acknowledge that the presence of m in the prior densities may be
seen as problematic.
    The posterior densities of Ï‰ for these priors as well as the (improper) constant
prior are derived in Appendix C. They are given by the expression
                                 Hq m     fk1 (q),k2 (q) {Hq z(y; Ï‰Ì‚)}
                 gq (y; Ï‰Ì‚) =                                            ,            (9)
                                1 + mÏ‰Ì‚ 1 âˆ’ Fk1 (q),k2 (q) {Hq z(0; Ï‰Ì‚)}
where z(y; Ï‰Ì‚) = (1 + my)/(1 + mÏ‰Ì‚), k1 (q) = n âˆ’ K âˆ’ 2q + 2 and k2 (q) = K + 2q âˆ’ 3
and
                                        n âˆ’ K k2 (q)
                                Hq =                  .
                                           K k1 (q)
The non-informative prior corresponds to q = 0. For q = 1, when the prior is not
defined, the â€˜posteriorâ€™ density in (9) is well defined as
                               H1 m       fnâˆ’K,Kâˆ’1 {H1 z(y; Ï‰Ì‚)}
               g1 (y; Ï‰Ì‚) =                                        ,
                              1 + mÏ‰Ì‚ 1 âˆ’ Fnâˆ’K,Kâˆ’1 {H1 /(1 + mÏ‰Ì‚)}

                                       Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

186                                                   Nicholas T. Longford & Mercedes Andrade


and H1 = (K âˆ’ 1)/K. This is the conditional distribution of Ï‰ in (7) given that
Ï‰ > 0. The posterior distribution for q > 1 can be interpreted as an addition
of 2q clusters to the data, with the corresponding reduction of the cluster size m
to n/(K + 2q), to keep the overall sample size n unchanged. The estimator Ï‰Ì‚ is
also changed, multiplied by H1 /Hq . Thus, a prior in (8) is equivalent to altering
the dataset, and the posterior distribution of Ï‰ can be treated as the sampling
distribution of this altered dataset. Of course, we have to permit fractional within-
cluster sample sizes and numbers of clusters. In what follows, we drop q in the
arguments of k1 and k2 and in the subscript of H.

                    q
              20




                   3.0
              15
      g(Ï‰)

              10




                   2.0



                   1.5
              5




                   1.2
              0




                        0.0                     0.2                            0.4

                                                         Ï‰
             Figure 1: A set of prior densities given by (8) for cluster size m = 10.


    We assume that the appropriate course of action results in no loss. However,
when B is chosen (claiming that Ï‰ > Ï‰0 ), even though Ï‰ â‰¤ Ï‰0 , we incur unit loss,
and when A is chosen even though Ï‰ > Ï‰0 , the loss is equal to R. We refer to
R as the penalty ratio. We want to choose the action for which the expected loss
is smaller. The choice is based solely on the posterior distribution of Ï‰. Table 1
illustrates the loss function, with values of 0, 1 and R for the subsets of the space
of pairs (Ï‰Ì‚, Ï‰). It represents the clientâ€™s perspective, and is therefore subjective.
This perspective may be difficult to establish and quantify by specifying the value
of R. We address this issue in Section 3.2 by considering a range of plausible
values of R, and later we introduce classes of loss functions other than piecewise
constant.
                              Table 1: Piecewise constant loss function.
                                                                 Reality
                                                             Ï‰ â‰¤ Ï‰0   Ï‰ > Ï‰0
                                         A   (Ï‰ â‰¤ Ï‰0 )         0         R
                              Decision
                                         B   (Ï‰ > Ï‰0 )         1         0




                                             Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

Decision Theory for One-Way ANOVA                                                     187

   The expected loss associated with action B is
               Z Ï‰0
         LB =       gq (y; Ï‰Ì‚) dy
                   0
                                                    Z Ï‰0
                  Hm                1
              =                                            fk1 ,k2 {Hz(y; Ï‰Ì‚)} dy
                1 + mÏ‰Ì‚ 1 âˆ’ Fk1 ,k2 {Hz(0; Ï‰Ì‚)}       0

                  Fk1 ,k2 {Hz(Ï‰0 ; Ï‰Ì‚)} âˆ’ Fk1 ,k2 {Hz(0; Ï‰Ì‚)}
              =                                               .
                            1 âˆ’ Fk1 ,k2 {Hz(0; Ï‰Ì‚)}

Further, LA = R(1 âˆ’ LB ). We define the balance function as âˆ†L = LA âˆ’ LB ;

                           âˆ†L(Ï‰Ì‚; R, Ï‰0 ) = R âˆ’ (R + 1)LB .                          (10)

We choose action A when âˆ†L < 0 and action B when âˆ†L > 0. For greater values
of Ï‰Ì‚ we should be more inclined toward choosing action B, implying that âˆ†L, as
a function of Ï‰Ì‚, is increasing. We have no analytical proof of this conjecture. The
respective limits of âˆ†L as Ï‰Ì‚ â†’ âˆ’1/m and Ï‰Ì‚ â†’ +âˆ are âˆ’1 and R. Therefore
there is an equilibrium value Ï‰qâˆ— , for which âˆ†L(Ï‰qâˆ— ; R, Ï‰0 ) = 0. In an exhaustive
(empirical) search, we have found this equilibrium to be unique. When Ï‰Ì‚ < Ï‰qâˆ—
we choose A and when Ï‰Ì‚ > Ï‰qâˆ— we choose B. Note that the equilibrium may be
negative; in fact, Ï‰qâˆ— < 0 when

                          Fk1 ,k2 {H(1 + mÏ‰0 )} âˆ’ Fk1 ,k2 (H)    R
              âˆ†L(0) =                                         >     .
                                     1 âˆ’ Fk1 ,k2 (H)            R+1

This is equivalent to either of the following conditions

                            Fk1 ,k2 {H(1 + mÏ‰0 )} âˆ’ Fk1 ,k2 (H)
                       R>
                                 1 âˆ’ Fk1 ,k2 {H(1 + mÏ‰0 )}
                                                           
                             1       âˆ’1     R + Fk1 ,k2 (H)    1
                       Ï‰0 <       Fk1 ,k2                     âˆ’ ,
                            Hm                 R+1             m

that is, when the consequences of choosing action B incorrectly are sufficiently
serious (large R) or we are very strict about what we regard as small (small
Ï‰0 ). The equilibrium value is found by the Newton method or a similar algorithm.
Methods that use the derivatives âˆ‚âˆ†L/âˆ‚ Ï‰Ì‚ are not practical because the expressions
involved are quite lengthy.
   The decision rule based on the sign of âˆ†L is substantially different from the
outcome of a hypothesis test based on an F distribution. As an aside, we note
the complications with Ï‰ possibly being at the boundary of the parameter space.
The hypothesis in a test is often that Ï‰ = 0. However, in such a case we would
be satisfied if the hypothesis were not rejected even when Ï‰ is positive but small.
The value of Ï‰0 for the decision rule is comparable to the largest value of Ï‰ that
we would still call â€˜smallâ€™ (the largest unimportant value). In this perspective,
Ï‰0 = 0 is not appropriate. For a decision, we always choose Ï‰0 > 0. In decision
theory, the two alternatives, Ï‰ â‰¤ Ï‰0 and Ï‰ > Ï‰0 , have equal status, whereas in


                                       Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

188                                                                                  Nicholas T. Longford & Mercedes Andrade


hypothesis testing the hypothesis defines the default course of action, which is
overruled only when there is sufficient evidence against it. In hypothesis testing,
the semicontinuous prior for Ï‰, with a mass at zero, can be declared. Such a
distribution is a mixture of the mass at zero and a continuous distribution. For a
given continuous component, greater prior mass at zero leads to greater posterior
mass at Ï‰ = 0. Transparency of the declaration, a clear justification for the
declared mass, is therefore essential. In the approach we propose, we do not have
to resort to such a device.
    Instead of Ï‰ = 0, we may test the hypothesis that Ï‰ â‰¤ Ï‰0 . However, then the
power of the test for values slightly greater than Ï‰0 is very small, so Ï‰0 is not a
tangible quantity in hypothesis testing as it is in our approach, where it represents
a clear boundary between the two options. Note that tests (and decisions) for Ï‰
                                                  2    2
and the intraclass correlation coefficient, Ï = ÏƒB  /(ÏƒB + Ïƒ22 ), are equivalent because
Ï = 1/(1 + 1/Ï‰) is a monotone function of Ï‰. Although the scale of Ï is preferred
by some for interpretation, evaluations for Ï‰Ì‚ are easier for both hypothesis testing
and decision making because of its relation to F distribution.
    Having to set the penalty ratio R might seem as an additional burden to the
analyst. However, its importance is obvious from how Ï‰qâˆ— depends on R. Figure 2
displays the values of Ï‰0âˆ— as a function of R for the pairs (K, m) set to (4, 6), (7, 10)
and (10, 20), and thresholds Ï‰0 = 0.1 and Ï‰0 = 0.25 drawn by black and gray
colors, respectively. The noninformative prior (q = 0) is assumed. The functions
are drawn on the original (linear) and log scales, to obtain high resolution for both
large and small values of R.

                                                                                                                                  Log scale
                               0.4




                                                                                                              0.4




                                                                             Ï‰ 0
                                                                             Thr
                                                                             0.1
                                                                             0.25
      Equilibrium value (Ï‰*)




                                                                                     Equilibrium value (Ï‰*)
                               0.2




                                                                                                              0.2




                                                                              K,m
                                                                             10,20
                               0.0




                                                                                                              0.0




                                                                             10,20

                                            6,10   6,10
                                         4, 7
                                         4, 7


                                     0                    10            20                                          0.05   0.20    1.00       5.00   20.00
                                                    Penalty ratio (R)                                                        Penalty ratio (R)
Figure 2: The equilibrium values Ï‰ âˆ— as functions of the penalty ratio R for number of
          clusters K and cluster size m indicated in the left-hand panel; non-informative
          prior for Ï‰.


   The functions are decreasing with limits +âˆ as R â†’ 0 and âˆ’1/m as R â†’ +âˆ.
They are drawn either for R < 20 or up to the value of R at which the equilibrium
reaches the minimum of âˆ’1/m. For (K, m) = (10, 20), the equilibria are above
the minimum of âˆ’0.05 for both Ï‰0 = 0.1 and 0.25 even at R = 20, but for the
other settings the limits are reached for R < 20.


                                                                             Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

Decision Theory for One-Way ANOVA                                                   189

   For very small penalty ratio R, action B is selected only for extremely large
values of Ï‰Ì‚ because the error of incorrectly choosing action A has no serious reper-
cussions. When the equilibrium is at âˆ’1/m we choose action B for all values of Ï‰Ì‚.
The equilibrium functions Ï‰ âˆ— (R) have steep gradients for small values of R. The
equilibria have much less curvature on the log scale. Higher value of Ï‰0 is associ-
ated with uniformly higher equilibrium. The differences of the pairs of equilibrium
functions decrease with R.


3.1. Piecewise Linear Loss
    In some settings, the loss depends not only on the sign of the error made, but
also on its magnitude | Ï‰ âˆ’ Ï‰0 |. One such class of loss functions are the piecewise
linear, defined as Ï‰0 âˆ’ Ï‰ when we choose B inappropriately (when Ï‰ â‰¤ Ï‰0 ) and
R(Ï‰ âˆ’ Ï‰0 ) when we choose A inappropriately (when Ï‰ > Ï‰0 ).
   For a given value of Ï‰Ì‚, we compare the expected losses LA and LB , and choose
the action with the smaller expected loss. In Appendix D, we derive the following
expression for the balance function âˆ†L = LA âˆ’ LB ,

          1 + mÏ‰0 Fk1 ,k2 {G(Ï‰0 )} âˆ’ Fk1 ,k2 {G(0)}    K(n âˆ’ K + 2q âˆ’ 2)
   âˆ†L =                                             âˆ’
             m            1 âˆ’ Fk1 ,k2 {G(Ï‰0 )}        (K + 2q âˆ’ 5)(n âˆ’ K)
                                                                                   (11)
         1 + mÏ‰Ì‚ Fk1 +2,k2 âˆ’2 {h1 G(Ï‰0 )} âˆ’ Fk1 +2,k2 âˆ’2 {h1 G(0)}
       Ã—                                                           ,
            m                    1 âˆ’ Fk1 ,k2 {G(0)}

where G(y) = Hq z(y; Ï‰Ì‚) = Hq (1 + my)/(1 + mÏ‰Ì‚). This identity holds only when
K > 3 âˆ’ 2q. Otherwise LA is infinite, so action B is chosen, irrespective of Ï‰Ì‚, Ï‰0 or
R. When âˆ†L is well defined (finite) we choose A if âˆ†L(Ï‰Ì‚; R, Ï‰0 ) < 0, and choose
B otherwise.
    Figure 3 presents the equilibrium functions for a selection of designs (K, m,
indicated in the diagram), and Ï‰0 set to 0.1 and 0.25 (distinct colors), as functions
of the penalty ratio R. All six functions in the diagram are non-increasingâ€”
greater R makes the choice of B more attractive. Each equilibrium converges to
the minimum value of Ï‰Ì‚, equal to âˆ’1/m. For Ï‰0 = 0.1, K = 8 and m = 15, this
                       .
limit is reached at R = 5.92 < 20. For Ï‰0 = 0.1, K = 10 and m = 14, the limit
                 .
is reached at R = 23.26, off the horizontal scale. Smaller value of Ï‰0 is associated
with lower equilibrium, and therefore increased preference for action B.
    Earlier we conjectured that the balance function for the piecewise constant loss
is increasing. This is not the case for the piecewise linear loss in general. A set
of examples is drawn in Figure 4 for K = 8 and m = 15. All the functions in the
diagram converge to zero as Ï‰Ì‚ â†’ âˆ’1/m. However, for R smaller than a critical
value Râ€  = R(K, m), the balance has a dip before increasing. Without choosing
the initial values carefully, a search for the root of âˆ†L may converge to âˆ’1/m,
even when there is another root. For R < Râ€  , the balance in favor of action A is
narrower for Ï‰Ì‚ close to âˆ’1/m than for larger values of Ï‰Ì‚ because larger values of
Ï‰Ì‚ yield posterior distributions with greater dispersion. This adds strength to the
choice of B, although the decision rule remains reasonable:


                                     Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

190                                                                                 Nicholas T. Longford & Mercedes Andrade

                                                                                                                                          Log scale

                                                                     Ï‰Thr
                                                                      0
                                                                      0.1
                                                                      0.25

                           0.2




                                                                                                             0.2
  Equilibrium value (Ï‰*)




                                                                                    Equilibrium value (Ï‰*)
                           0.1




                                                                                                             0.1
                                                                       K, m


                                                                       12,20
                           0.0




                                                                                                             0.0
                                                                       10,14
                                                                       12,20
                                                                        8,15
                                            8,15                       10,14


                                 0                 10                20                                                0.05        0.20    1.00        5.00   20.00
                                            Penalty ratio (R)                                                                        Penalty ratio (R)
Figure 3: The equilibrium values Ï‰ âˆ— as functions of the penalty ratio R for number
          of clusters K and cluster size m indicated in the left-hand panel. Piecewise
          linear loss and non-informative prior for Ï‰.
                                      0.3
                                      0.2




                                                           R:   25     10                                                                                1
                                                                                    5                              3           2
                                 âˆ†L
                                      0.1




                                                                                                                                                        0.5
                                      0.0




                                                   âˆ’0.05                     0.00                                             0.05                    0.10

                                                                                                 ^
                                                                                                 Ï‰
Figure 4: The balance functions âˆ†L(Ï‰Ì‚) for the piecewise linear loss; K = 8, m = 15,
          Ï‰0 = 0.2 and non-informative prior for Ï‰. The values of R are indicated on
          the curves.


        â€¢ for R up to a certain value Râˆ— , choose A for Ï‰Ì‚ smaller than a critical value;
          otherwise, choose B;
        â€¢ for R greater than this value, choose B for all values of Ï‰Ì‚.

The borderline value Râˆ— depends on K, m and Ï‰0 .
    A more radical dependence of the loss on the magnitude of the error is rep-
resented by the quadratic loss function, given as (Ï‰ âˆ’ Ï‰0 )2 when we erroneously
choose action B, even though Ï‰ < Ï‰0 , and as R(Ï‰ âˆ’ Ï‰0 )2 when we choose action
A, even though Ï‰ > Ï‰0 . The balance function for this loss is derived in Appendix
E.



                                                                       Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

Decision Theory for One-Way ANOVA                                                    191

3.2. Plausible Loss Functions, Priors and Values of Ï‰0

   The values of the penalty ratio R and Ï‰0 are elicited from the subject-matter
expert (the client). Often no single pair of values is arrived at; instead, ranges of
plausible values R = (RL , RU ) and â„¦ = (Ï‰0L , Ï‰0U ) are declared. A range R (or
â„¦) is said to be plausible if any value R (or Ï‰0 ) outside the range can be ruled
out. Thus, if a range is plausible, then so is any (wider) range that contains it.
    With plausible ranges of R and Ï‰0 , we might solve the problem for a grid of
plausible values (R, Ï‰0 ) âˆˆ R Ã— â„¦ and choose action A or B if the expected loss
with that action is smaller for every plausible pair (R, Ï‰). If action A is preferred
for some plausible pairs (R, Ï‰0 ) but action B for some others, then an impasse
results, which can be resolved only by reviewing and reducing the ranges R and
â„¦. Uncertainty about the prior parameter q can be dealt with similarly.
    We assume that the plausible values of R and Ï‰0 form a rectangle, R Ã— â„¦.
In practice, it suffices to find the signs of the balance âˆ†L for the vertices of this
rectangle. If âˆ†L < 0 at all four vertices, then action A is selected; if âˆ†L > 0 at
all four vertices, then action B is selected. In these two cases, we have unequivocal
decisions; the same action is chosen for all plausible configurations of R and Ï‰0 .
Otherwise we reach an impasse. If the plausible ranges of R and Ï‰0 are reviewed
in further elicitation, the reduced ranges have to remain plausible.
    In an alternative approach, we split the space of all pairs (R, Ï‰0 ) according
to the sign of âˆ†L(Ï‰Ì‚; R, Ï‰0 ). For Ï‰0 close to zero, LB is very small, and so B
is the preferred action. For very large Ï‰0 , action A is preferred. For Ï‰Ì‚ and Ï‰0
fixed, the balance functions are linear in R. Therefore âˆ†L = 0 for a unique value
Râˆ— = R(Ï‰0 ; Ï‰Ì‚). This function of Ï‰0 and Ï‰Ì‚ splits the space (R, Ï‰0 ) to the regions
in which A or B has smaller expected loss. Figure 5 displays these functions for
piecewise constant loss and a selection of values Ï‰Ì‚ indicated at the right-hand
margin. The curves drawn by black color are for the design (K = 8, m = 11)
and those by gray color for (K = 8, m = 10), to explore the dependence of Râˆ—
on m. Less information in the data requires smaller penalty ratio R to reach the
balance of the two expected losses. This applies also when K is reduced; details
are omitted.
    If plausible ranges are declared for R and Ï‰0 and Râˆ— intersects the plausible
rectangle, we have an impasse, because for some plausible pairs (R, Ï‰0 ) one action,
and for other pairs the other action is preferred. The equilibrium value R is an
increasing function of Ï‰0 . Therefore, if the same action is preferred for both
extremes RL and RU , then it is preferred for all R âˆˆ (RL , RU ). If one action is
preferred for some plausible values of R, say, action A for R âˆˆ (RL , RQ ), RL <
RQ < RU , and action B for R âˆˆ (RQ , RU ), then we have an impasse that can be
resolved only by resuming elicitation to narrow down the plausible range (RL , RU ).
    Although the standard Bayesian analysis deals with a single prior, it is some-
times practical to consider a range of plausible priors, or prior parameters, (qL , qU ),
reflecting the lack of consensus in the elicitation or uncertainty admitted by the
expert. See Longford (2010) for an application and related discussion. An example
is given in Figure 6 for the design with K = 10 and m = 10. Piecewise quadratic


                                      Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

192                                           Nicholas T. Longford & Mercedes Andrade

                                                                           âˆ’0.05




               150
                                                                           âˆ’0.05




               100
          R*
                                                                           âˆ’0.02

                                                                           âˆ’0.02


               50
                                                                            0.00
                                                                            0.00
                                                                            0.02
                                                                               0.05

                                                                               0.20
               0




                     0.0                      0.5                       1.0

                                                    Ï‰0
Figure 5: The equilibrium functions Râˆ— = R(Ï‰0 ; Ï‰Ì‚) for the piecewise constant loss
          function and values of Ï‰Ì‚ indicated at the right-hand margin; K = 8 and
          m = 11 (black lines) and m = 10 (gray).



loss is assumed. The functions drawn in the diagram are the equilibrium functions
R(q) which satisfy the identity âˆ†Lq (Ï‰Ì‚; R(q), Ï‰0 ) = 0. The solutions for Ï‰Ì‚ = 0.05
are drawn by black lines for the values of Ï‰0 printed at the right-hand margin.
The solutions for Ï‰Ì‚ = 0.055 and the same values of Ï‰0 are drawn by gray lines.
For Ï‰0 = 0.10 the two curves are difficult to discern.

                             0.30            0.30
               10




                                                                              Ï‰0

                                                                           0.25

                                                                           0.25
          R

               5




                                                                           0.20
                                                                           0.20


                                                                           0.15
                                                                           0.10
               0




                              1.3                        1.4             1.5

                                                q
Figure 6: The equilibrium functions R(q) for the piecewise quadratic loss function and
          values of Ï‰0 indicated at the right-hand margin and at the top; Ï‰Ì‚ = 0.05
          (black lines) and Ï‰Ì‚ = 0.055 (gray), K = 10 and m = 10. The plausible
          rectangle is filled by gray color, and its reduced version is delimited by dots.


   Suppose the value Ï‰Ì‚ = 0.05 was realised and the plausible range (0.15, 0.20)
was declared for Ï‰0 . Further, suppose the respective plausible ranges for R and q
are (4, 6) and (1.3, 1.4); this rectangle is filled in the diagram by gray color. The


                                       Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

Decision Theory for One-Way ANOVA                                                  193

plausible rectangle is above the equilibrium curve R(q) for any plausible value of
Ï‰0 âˆˆ (0.15, 0.20), so action B is selected unequivocally.
    Suppose next that the plausible range of Ï‰0 is (0.20, 0.25). Then nearly the
entire plausible rectangle lies between the functions R(q) for Ï‰0 = 0.20 and Ï‰0 =
0.25. In this case, we have an impasseâ€”for some plausible values of Ï‰0 action A
and for other values action B is preferred. Finally, suppose the plausible range of
Ï‰0 is (0.25, 0.30). The plausible rectangle intersects the region delimited by the
functions R(q) for Ï‰0 = 0.25 and 0.30, but if the lower limit of plausible values
of q could be increased, or the upper limit for R reduced, the reviewed plausible
rectangle might be entirely under the curve R(q) for Ï‰0 = 0.25 and action A would
then be preferred unequivocally. Such a reviewed plausible rectangle is delimited
in the diagram by dots.
    These examples imply a strong incentive to declare as small a set of plausible
values as possible for all the parameters involved, to reduce the chances of an
equivocal decision. But the client has to be comfortable with the implication that
all pairs outside the declared set can be ruled out.


4. Designs Without Balance
    For designs without balance we do not have a closed-form expression for the
conditional distribution of Ï‰Ì‚ given Ï‰, nor a tractable posterior density of Ï‰. We
approximate this distribution by its match among the balanced designs.       In the
                                               of clusters K 0 = n2 / k n2k and the
                                                                     P
approximation, we use the synthetic numberP
harmonic mean of the cluster sizes m0 = K/ k 1/nk in the respective roles of K
and m. These proposals are based on Potthoff, Woodbury & Manton (1992) and
Longford (2000). Potthoff et al. (1992) derived a generalisation of the equation
for K 0 by matching the information in a sample with unequal sampling weights
with a sample that would have equal weights. The approximation for m0 is derived
directly from the information about the variance ratio in the likelihood maximi-
sation. We note that these approximations are poor for large Ï‰, such as Ï‰ > 0.5.
    Figure 7 presents the histograms of the ML estimates Ï‰Ì‚ obtained in 10,000
replications each for the values of Ï‰ listed in the titles for the design with two
clusters each of size 6, 7 and 8, for which K 0 = 5.92 and m0 = 6.90. The ap-
proximation is not perfect, but in the context of substantial dispersion of Ï‰Ì‚ it is
acceptable.
   An alternative is to assume that the statistic SB is associated with K âˆ’ 1
degrees of freedom and SW with degrees of freedom in the range (nl , nu ), where
nl = (K âˆ’ 1)ml and nu = (K âˆ’ 1)mu , and ml and mu are the sample sizes of the
smallest and largest clusters, respectively, 6 and 8 in the example above. Then
we solve the problem for K(ml âˆ’ 1) and K(mu âˆ’ 1) degrees of freedom associated
with SW . If we arrive at the same conclusion in both cases, then it applies also to
the original dataset. The method is poorly suited for extremely unbalanced data.




                                    Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

194                                                Nicholas T. Longford & Mercedes Andrade

                                0.00                                  0.05




                                                         4
                 6
           Density




                                                   Density
                4




                                                    2
                 2
                 0




                                                         0
                          0.0          0.5                   0.0    0.5       1.0

                                0.10                                  0.15




                                                         3
                 3




                                                         2
           Density




                                                   Density
              2




                                                         1
                 1
                 0




                                                         0




                         0.0     0.5         1.0             0            1             2

                                0.20                                  0.25
                                                         2
                 2
           Density




                                                   Density
                                                   1
                 1
                 0




                                                         0




                     0           1            2              0        1             2

Figure 7: The empirical distribution of Ï‰Ì‚ (histogram) for the design with sample sizes 6,
          6, 7, 7, 8 and 8 and the fitted distribution based on K 0 = 5.92 and m0 = 6.81
          (solid line), for Ï‰ = 0.0, 0.05, . . . , 0.025 (title). Histograms based on 10,000
          replications.



    Another alternative is motivated by methods for missing data (Little & Rubin
2002, Rubin 2002). We assume that the (unbalanced) observed dataset is incom-
plete, and its complete version is balanced, with mu observations in each cluster.
The EM algorithm (Dempster, Laird & Rubin 1977) is particularly easy to im-
plement for this setting, but it does not yield the sampling distributions of the
estimators. By multiple imputation, we generate a number of plausible comple-
tions of the observed data, and then analyse each dataset separately. If in every
case we prefer the same action, we have an unequivocal conclusion. Otherwise an

                                             Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

Decision Theory for One-Way ANOVA                                                          195

impasse results. The problem with this approach is that the chances of an impasse
increase substantially when the sample sizes nk are in a wide range, because the
missing data contains a large fraction of the complete information.
   We note that a similar approach to hypothesis testing requires an adjustment
because by augmenting the observed (incomplete) dataset to make it balanced
we bias the results toward the alternative. See Li, Meng, Raghunathan & Rubin
(1991) for a solution.



5. Application
    We illustrate the methods on the data from ten training sessions of a track
athlete. Each session comprised eight 400 metre runs (one lap of the track),
separated by jogging 400 metres for recovery. The athlete ran unaccompanied,
and was not informed about any intermediate times (e.g., at 200 metres), nor
when completing a lap; he could inspect the eight times only at the end of the
session. He aimed to run each lap in 55.0 seconds. The purpose of the sessions was
to develop a good judgement of speed, discounting any fatigue and any external
factors (weather). The collected data, times in seconds with precision to one
decimal place, are presented in Figure 8, with sessions marked as A â€“ J.
                      56.0




                                      J


                             B
                                                                         J
                                  F                            B    J    G
                      55.5




                                      G
                                      B          JI                 B
                                                                    D
                             G    I   EFI                      E         D   F
                                                                             G
                             C    A
                                  B
                                  E                            JI            H
               Time




                             JI   J
                                  D             D              H
                                                               C    H
                                                                    AI   B
                                                                         E
                                                                         C   E
                                                                             B
                             A    C             H
                                                A                   G
                                                                    E    A   C
                                                                             D
                      55.0




                             D    G   D         C
                                                E                   C        J
                             E    H   H
                                      A         G
                                                B              F
                                                               G         I
                                                 F                           A
                             F
                             H                                 A
                                                               D         F   I
                                                                         H
                      54.5




                                      C


                                                                    F


                             1    2   3         4              5    6    7   8
                                                      Repeat
Figure 8: The times for running 400 metres (a lap of the track) in sessions A â€“ J com-
          prising eight separate laps (repeats) each.


    We ignore the sequential order of the runs and sessions. Running a lap in 55.0
seconds does not require a full effort of the athlete, who could run the distance in
well below 50 seconds. Of course, fatigue accumulates over the laps, but there is an
equal threat of under- and over-compensating for it. Also, initial data exploration
suggests no presence of a trend over the eight laps nor any temporal dependence
across the sessions. The sufficient statistics for the random-effects ANOVA are
SW = 5.595 and SB = 1.797 and the sample mean is ÂµÌ‚ = 55.16. The maximum
                                                                   2
likelihood estimates (MLE) of the variance components are ÏƒÌ‚W         = 0.0799 and
  2
ÏƒÌ‚B = 0.0125.


                                            Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

196                                                             Nicholas T. Longford & Mercedes Andrade


    If the times achieved do not differ much across the sessions (and are sufficiently
close to 55.0 seconds on average), then the athlete can progress to the next stage
of training, for which the ability to maintain the particular speed is essential. One
aspect of the ability is a sufficiently small value of the variance ratio Ï‰; its MLE is
Ï‰Ì‚ = 0.156. Elicitation from the coach and his colleagues concluded that Ï‰ â‰¤ 0.20 is
necessary for progressing to the next stage and that piecewise linear loss functions
with R âˆˆ (1/5, 1/3) are plausible. Noninformative prior (q = 0) is assumed.

    Contrary to popular perceptions, (professional) athletes and their coaching and
management staff are well aware of uncertainty about future fitness and perfor-
mance, which they consider in preparing training schedules, planning attendance
in competitions and assessing the athleteâ€™s prospects. A lot of data is nowadays
collected in training, and statistical definitions of distributions and their dispersion
are relatively easy to introduce. In the elicitation process, we communicated with
the coaching staff mainly through graphs of large (simulated) sets of times, and
asked them whether the displayed variation was acceptable or not for progressing
to the next stage. We settled first on Ï‰ âˆˆ (0.17, 0.24) and later agreed on Ï‰ = 0.20.
Disagreement and uncertainty persisted about the value of R, which compares the
harm done by the two kinds of erroneous choice, and that is why we consider a
plausible range for it.

    Figure 9 summarises the results of the analysis by the plot of the equilibrium
function (marked q = 0). The plausible range of R is marked by shading and the
value of Ï‰Ì‚ by horizontal dots. Since Ï‰Ì‚ is above the equilibrium function throughout
R, the action with smaller expected loss is not to proceed to the next stage; the
estimated variance ratio is too large. The equilibrium values Ï‰ âˆ— are 0.101 and
0.074 for the respective values R = 31 and 15 . For R = 0.075, Ï‰ âˆ— = 0.157, so the
decision would not be affected if values of R much smaller than 51 were regarded
as plausible.


                                          Ï‰
                                          0=
                              Ï‰0
                      Ï‰0




                                              0.2
                                =0




                                                 72
                        =0




                                   .2
                        .20




                                      4




                                                                                           ^
                                                                                           Ï‰
               0.15
                      Ï‰0




                                                                                         q=1.01
                        =0
                        .17
          Ï‰*

               0.10




                                                                                          q=0



                                   0.1                    0.2              0.3            0.4

                                                                  R
Figure 9: The equilibrium values of Ï‰ for the study of the track athleteâ€™s times for 400
          metres.


                                                      Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

Decision Theory for One-Way ANOVA                                                   197

   If instead of a single value, Ï‰0 = 0.20, the plausible range Ï‰0 = (0.17, 0.24) were
agreed, as it was in an earlier round of elicitation, the equilibrium functions drawn
by black dashes and marked by the values of Ï‰0 would be obtained. Since both
curves are entirely below Ï‰Ì‚ = 0.156 for all R âˆˆ (0.20, 0.33), the conclusion â€˜Not to
proceedâ€™ would not be affected. By trial and error, we found that for Ï‰0 = 0.272
the equilibrium at R = 0.2 is equal to Ï‰Ì‚. Thus, if a standard more lenient than
Ï‰0 = 0.272 for the athleteâ€™s consistency were plausible, then the decision would
be equivocal (in doubt) because the equilibrium curve would then intersect the
horizontal line at Ï‰Ì‚ for a plausible value of R. For Ï‰0 = 0.32, the equilibrium
curve is equal to Ï‰Ì‚ at the upper limit of plausible values of R, 0.33. So, if Ï‰0 were
greater than 0.32 the appropriate decision would be â€˜To proceedâ€™.
    We add a word of caution at this point. Suppose the prior with q = 1.01 is
adopted. The proximity of q to unity might suggest that this prior is only mildly
informative. However, the equilibrium function, drawn by gray color, differs from
the equilibrium for q = 0 substantially. In fact, with q = 1.01, the decision
to proceed (that Ï‰ is small) would be preferred, because the â€˜grayâ€™ equilibrium
function is entirely above Ï‰Ì‚ throughout the plausible range of R.
    In an established approach, we would test the hypothesis that Ï‰ â‰¤ 0.20. This
hypothesis is not rejected; the p value, derived from (4), is 0.48. Commonly, one
would conclude that the action appropriate when the hypothesis applies should
be taken. Such a decision process is logically incorrect, confusing failure to reject
a hypothesis with its acceptance. That is, the analysis started by assuming that
the hypothesis is valid, and no contradiction with it was found. The appropriate
conclusion is that of ignorance, that we do not know whether Ï‰ â‰¤ 0.20 or Ï‰ > 0.20,
or more precisely, that the data yield sufficient evidence for neither the hypothesis
nor the alternative. We note that the hypothesis Ï‰ > 0.20 (exchanging the roles
of the original hypothesis and alternative) would not be rejected either, further
compounding the illogicality of a decision based on the result of a hypothesis test.


6. Discussion and Conclusion
    The outcome of a hypothesis test is often used to support a decision to continue
an analysis as if the hypothesis or the alternative were valid. This is widely
acknowledged to be inappropriate when we fail to reject the hypothesis, and yet
act as if it did apply, but this is often ignored in practice. In our perspective, such
use of a hypothesis test is inappropriate even when the hypothesis is rejected, and
thus evidence against it obtained, because the consequences of the type II error
are not taken into account. The pragmatic arguments for hypothesis testing, such
as relatively simple computational procedures and reference to asymptotic theory,
do not have a good foundation in the case of variance estimation, especially in
experiments with small or moderate sample sizes, in which the expenditure on the
study and the ramifications of the errors of the two kinds are important factors.
    In our approach based on decision theory, the consequences of the two kinds
of bad decision are represented by a loss function, and the uncertainty about
it by a set of plausible loss functions. The loss functions are elicited from the


                                     Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

198                                            Nicholas T. Longford & Mercedes Andrade


expert (client), together with the prior information about the variances. The prior
distribution of the variances is useful, but specifying it is not an imperative in our
analysis. In fact, it can be formulated as an alteration of the realised dataset, so
the analyst does not have to subscribe to the Bayesian paradigm. Plausible priors
(or prior data) and loss functions have an important role as conduits for sensitivity
analysis, exploring how the conclusion is changed as a result of (small) changes in
the input. This is greatly simplified by the choice of classes of loss functions, for
which the balance is a linear function of the penalty ratio R; see (10), (11) and
(16). The priors used in them involve the within-cluster sample size m. They are
chosen so that the posterior would have a closed form. An alternative to them is
the class of uniform priors on (0, Ï‰ â€  ), with a value or a plausible range specified
for Ï‰ â€  . The balance functions with these priors are obtained from (11) and (16)
by replacing each term 1 âˆ’ Fk10 ,k20 {hG(Ï‰0 )} with Fk10 ,k20 {hG(Ï‰0â€  ) âˆ’ Fk10 ,k20 {hG(Ï‰0 )},
where respectively k10 = k1 (q), k1 (q) + 2 or k1 (q) + 4 and similarly for k2 ; h = 1,
h1 or h2 .
    Restricted maximum likelihood (REML; Patterson and Thompson, 1971) is
often considered for random effects models. Our method has a simple adaptation
for REML. It amounts to setting u = 1/(1 + mÏ‰) in (4) and

                                           n âˆ’ K k1 (q)
                                    Hq =
                                           K âˆ’ 1 k2 (q)

in (9). No other changes are required in the subsequent equations.
    The task solved by our approach is to select the action that is appropriate when
the unknown value of the variance ratio Ï‰ is smaller than a set threshold Ï‰0 > 0,
or the complementary action; Ï‰0 can be interpreted as the smallest important
deviation from zero. Although a typical hypothesis test about Ï‰ is for Ï‰ = 0, failure
to reject it is regarded as appropriate when Ï‰ is positive but small. Our approach
requires a quantification of what â€˜smallâ€™ means, by Ï‰0 , and setting it to zero would
not be reasonable. Its magnitude should be informed by the methods (steps in
the analysis or in experimentation) contemplated after the decision. Uncertainty
about it and the contentious nature of having to specify a single value are dealt
with by using a plausible range for Ï‰0 .
    Our development is exact only for balanced (one-way) designs; the proposed
solutions for unbalanced designs make references to the results for similar bal-
anced designs. Extensions to more complex (multiway) designs are on our re-
search agenda; for such designs the reference to balanced designs may be rather
restricting.
    Decision theory for some elementary statistical problems, such as estimating
a mean (and fixed-effects ANOVA), a variance, and classifying units to two cat-
egories, is developed in Longford (2013). With fixed-effects ANOVA, the group-
level means Âµk , k = 1, . . . , K, are parameters, and Âµ1 = Â· Â· Â· = ÂµK is the commonly
tested hypothesis. A measure of the departure from this hypothesis, needed for
applying our approach,     can be defined through the (finite-sample) variance of the
          2
            = k (Âµk âˆ’ Âµ)2 /K, which is similar to, but not the same as ÏƒB       2
               P
means, ÏƒG                                                                         . The


                                        Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

Decision Theory for One-Way ANOVA                                                  199

            2                                                                      2
                                                                    P
variance ÏƒG   is estimated by the method of moments, adjusting         k (ÂµÌ‚k âˆ’ ÂµÌ‚) /K
for its bias.
    All computing was carried out in R, and the software developed is available from
the first author (NTL). All our evaluations are based on closed-form expressions
for the expected losses, and are executed instantly. When a prior is declared for
which the posterior density has to be evaluated numerically, Monte Carlo Markov
chain (MCMC) calculations can be employed (Robert & Cassella 2004) Large sam-
ples from the joint posterior distribution of the parameters are generated and the
integrals involved in the expected loss are evaluated from these samples empiri-
cally. Much of the calculus, similar to that presented in the Appendices, can be
dispensed with, in exchange for approximate results (due to the stochastic nature
of MCMC) and concerns related to the convergence of the chain(s). Although
MCMC evaluations are computationally much more demanding they should not
restrict the scope of sensitivity analysis, which we regard as an integral element of
our method.
                                                               
               Received: December 2013 â€” Accepted: October 2014



Appendix A. Maximum Likelihood Estimation
   We derive the score function for the model in (1). The joint distribution of the
n Ã— 1 vector of outcomes y is normal with block-diagonal matrix with (identical)
        2          2          2
blocks ÏƒW  Wk = ÏƒW    W1 = ÏƒW   (Im + Ï‰Jm ) corresponding to clusters k = 1, . . . , K;
Im is the m Ã— m identity matrix and Jm the m Ã— m matrix of unities. Denote by
1m the m Ã— 1 vector of unities, so that Jm = 1m 1>  m . Let ek = yk âˆ’ Âµ1m be the
vector of residuals in cluster k.
    The loglikelihood is l = l1 + Â· Â· Â· + lK , with cluster-level contributions
                                                                 
         1                                         1        âˆ’1
                        2
                             + log (det W1 ) + 2 e>
                           
  lk = âˆ’      m log 2Ï€ÏƒW                                W      ek
         2                                        ÏƒW k 1
                                                                                 
          1                                        1                Ï‰           2
                        2
                             + log(1 + mÏ‰) + 2 e>                          >
                           
     =âˆ’      m log 2Ï€ÏƒW                                   e
                                                        k k  âˆ’           e   1m       ,
          2                                      ÏƒW              1 + mÏ‰ k

using the identities det(W1 ) = 1 + mÏ‰ and Wâˆ’1
                                            1 = Im âˆ’ Ï‰/(1 + mÏ‰) Jm . The
                                2
respective score functions for ÏƒW and Ï‰ are
                            (                      K
                                                               )
               âˆ‚l       1       2    >        Ï‰   X
                                                       >
                                                            2
                2 = âˆ’ 2Ïƒ 4
              âˆ‚ÏƒW
                              nÏƒW âˆ’ e e +
                                           1 + mÏ‰
                                                      ek 1m
                         W                        k=1
                       (                           K
                                                               )
                âˆ‚l   1        n      1       1    X
                                                       >
                                                            2
                   =     âˆ’        + 2                 ek 1m      .
               âˆ‚Ï‰    2     1 + mÏ‰ ÏƒW    (1 + mÏ‰)2
                                                                      k=1

The expressions for the roots of these equations are given in (2).


Appendix B. A Link Among F Densities
    We prove the identities in (5). By Î“2 we denote the half-gamma function,
Î“2 (x) = Î“( 12 x). The density of the F distribution with k1 and k2 degrees of
freedom is
                                             k1 /2
                            Î“2 (k1 + k2 )    k1             y k1 /2âˆ’1
             fk1 ,k2 (y) =                                        (k1 +k2 )/2 . (12)
                           Î“2 (k1 ) Î“2 (k2 ) k2      
                                                       1 + kk12 y

Hence
                                                          k1 /2+1
                                                                              (h1 y)k1 /2
                                             
                       Î“2 (k1 + k2 )             k1 + 2
   yfk1 ,k2 (y) =                        ,
                  Î“2 (k1 + 2)Î“2 (k2 âˆ’ 2)         k2 âˆ’ 2
                                                                                      (k1 +k2 )/2
                                                                               +2
                                                                      1 + kk21 âˆ’2 h1 y
                                                                                                      (13)
                  k1
              =       fk +2,k2 âˆ’2 (h1 y),
                k1 + 2 1

where
                                                 k1 k2 âˆ’ 2
                                      h1 =                 .
                                                 k2 k1 + 2

                                         Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

202                                              Nicholas T. Longford & Mercedes Andrade


The identity in (13) is obtained by matching the expression for yfk1 ,k2 (y) with an-
other F density. First, the factor y k1 /2 implies k1 + 2 degrees of freedom instead of
k1 ; then the term 1 + yk1 /k2 in the denominator implies the argument h1 y instead
of y, and its exponent (k1 + k2 )/2 implies the change from k2 degrees of freedom
to k2 âˆ’ 2. The constant factors remaining from the match with fk1 +2,k2 âˆ’2 (h1 y)
reduce to k1 /(k1 + 2). By reusing the identity in (13) we obtain


                                           k1
                     y 2 fk1 ,k2 (y) =          yfk1 +2,k2 âˆ’2 (h1 y)
                                         k1 + 2
                                                                                        (14)
                                          k1
                                   =            fk +4,k2 âˆ’4 (h2 y),
                                     h1 (k1 + 4) 1
where
                                   k1 + 2 k2 âˆ’ 4   k1 k2 âˆ’ 4
                        h2 = h1                  =           .
                                   k2 âˆ’ 2 k1 + 4   k2 k1 + 4

Appendix C. Posterior Distribution of Ï‰
    Using the terminology associated with the Bayesâ€™ theorem, the conditional
distribution (Ï‰Ì‚ | Ï‰) is derived from (4). Its density is
                                                                          
                                     K       m                 K 1 + mx
     mufKâˆ’1,nâˆ’K (u + mux) =                        fKâˆ’1,nâˆ’K                  .
                                    K âˆ’ 1 1 + mÏ‰              K âˆ’ 1 1 + mÏ‰
For Ï‰ we choose the noninformative prior g(y) = I(y > 0), where I denotes the
indicator function; its result is unity when its argument as a statement is correct,
and is equal to zero otherwise. The posterior distribution of Ï‰ is the standardised
product of the densities of (Ï‰Ì‚ | Ï‰) and Ï‰:
                                                  (Kâˆ’1)/2          (Kâˆ’1)/2âˆ’1
                 Dm             1             K               1 + mÏ‰Ì‚
   g(y | Ï‰Ì‚) =
               1 + my B2 (K âˆ’ 1, n âˆ’ K) n âˆ’ K                 1 + my
                                   âˆ’(nâˆ’1)/2
                      K 1 + mÏ‰Ì‚
             Ã— 1+                             I(y > 0) ,
                    n âˆ’ K 1 + my
where B2 (k1 , k2 ) = Î“2 (k1 + k2 )/Î“2 (k1 )/Î“2 (k2 ) and D = D(Ï‰Ì‚; K, n) is the stan-
dardising function (the denominator in the Bayesâ€™ theorem). By rearranging the
penultimate factor as a power of a linear function of 1 + my, we obtain
                                                                  (nâˆ’K)/2
                       Dm          I(y > 0)           n âˆ’ K 1 + my
         g(y; Ï‰Ì‚) =
                     1 + mÏ‰Ì‚ B2 (n âˆ’ K, K âˆ’ 1)          K 1 + mÏ‰Ì‚
                                          âˆ’(nâˆ’1)/2
                          n âˆ’ K 1 + my
                   Ã— 1+                                .
                             K 1 + mÏ‰Ì‚
Except for the indicator I(y > 0) and a scalar D0 , this matches the density
                                                    âˆ’(nâˆ’1)/2
                    H0 m                     1 + my
                           fnâˆ’K+2,Kâˆ’3 H0                       ,             (15)
                  1 + mÏ‰Ì‚                   1 + mÏ‰Ì‚

                                          Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

Decision Theory for One-Way ANOVA                                                    203

where
                                    nâˆ’K K âˆ’3
                               H0 =                  .
                                  nâˆ’K +2 K
Let Y be a random variable with F distribution with n âˆ’ K + 2 and K âˆ’ 3 degrees
of freedom. Then the function in (15) is the density of
                                      1 + mÏ‰Ì‚    1
                                Z =           Y âˆ’ .
                                        H0       m
The posterior distribution of Ï‰ has the same distribution as Z, except for the
condition that Z > 0, that is, Y > H0 m/(1 + mÏ‰Ì‚). Therefore, the posterior
density of Ï‰ is
                              H0 m         fnâˆ’K+2,Kâˆ’3 (H0 z)
              g0 (y; Ï‰Ì‚) =                                          ,
                             1 + mÏ‰Ì‚ 1 âˆ’ Fnâˆ’K+2,Kâˆ’3 {H0 /(1 + mÏ‰Ì‚)}

where z = (1 + my)/(1 + mÏ‰Ì‚).

A Class of Informative Priors

    A class of proper densities for which the posterior density can be obtained in
a closed form is
                                     m(q âˆ’ 1)
                            g(y) =            I(y > 0)
                                    (1 + my)q
for q > 1. For orientation, a selection of densities is drawn in Figure 1. For q
close to unity, the prior is highly dispersed (the density for q = 1.2 is drawn by
a gray line). The densities are decreasing for y > 0 and g(0) = m(q âˆ’ 1). For
greater q, the density has greater mass around zero and has a steeper slope for
small y. Although this is quite a flexible class of functions, the dependence on m is
an obvious drawback. Note that q should not be set to a large value (say, q > 5),
because it corresponds to information comparable to that in a very large sample.
In the derivations below, we assume that q < (n âˆ’ K)/2 + 1.
    To reduce the typographical length of some subscripts, we introduce the nota-
tion k1 (q) = n âˆ’ K âˆ’ 2q + 2 and k2 (q) = K + 2q âˆ’ 3, and drop the argument q when
its value is not specified or is obvious from the context. Following the outline for
the non-informative prior, we obtain
                                                                   (nâˆ’Kâˆ’2q)/2
                       Dq m            I(y > 0)        n âˆ’ K 1 + my
     g(y; Ï‰Ì‚, q) =
                   (1 + mÏ‰Ì‚)q+1 B2 (n âˆ’ K, K âˆ’ 1)        K 1 + mÏ‰Ì‚
                                        âˆ’(nâˆ’1)/2
                         n âˆ’ K 1 + my
                 Ã— 1+
                           K 1 + mÏ‰Ì‚
                       0                             âˆ’(nâˆ’1)/2
                      Dq Hq m
                                       
                                             1 + my
                 =              f k ,k   H q                   ,
                   (1 + mÏ‰Ì‚)q+1 1 2          1 + mÏ‰Ì‚
where
                                    nâˆ’K        K + 2q âˆ’ 3
                        Hq =
                                n âˆ’ K âˆ’ 2q + 2    K

                                      Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

204                                                Nicholas T. Longford & Mercedes Andrade


generalises the definition of H0 . Hence, the posterior density of Ï‰ is

                                      Hq m            fk1 ,k2 (Hq z)
                      gq (y; Ï‰Ì‚) =                                        ,
                                     1 + mÏ‰Ì‚ 1 âˆ’ Fk1 ,k2 {Hq m/(1 + mÏ‰Ì‚)}


so long as q < (n âˆ’ K)/2 + 1. In view of Figure 1 this is not a restrictive condition.




Appendix D. The Balance Function for Piecewise Linear Loss

      Denote G(Ï‰) = Hq z(Ï‰; Ï‰Ì‚). The expected loss with action B is

              Z Ï‰0
       LB =          (Ï‰0 âˆ’ y) g(y; Ï‰Ì‚) dy
               0
                                         Z Ï‰0
              Hm             1
          =                                   (Ï‰0 âˆ’ y)fk1 ,k2 {G(y)} dy
            1 + mÏ‰Ì‚ 1 âˆ’ Fk1 ,k2 {G(0)} 0
                               Z G(Ï‰0 )                                
                     1                            1      u
          =                                 Ï‰0 +    âˆ’       (1 + mÏ‰Ì‚) fk1 ,k2 (u) du
            1 âˆ’ Fk1 ,k2 {G(0)} Hz(0,Ï‰Ì‚)          m Hm
                      
                    1 Fk1 ,k2 {G(Ï‰0 )} âˆ’ Fk1 ,k2 {G(0)}
          = Ï‰0 +
                   m            1 âˆ’ Fk1 ,k2 {G(0)}
                                                Z G(Ï‰0 )
              k1 1 + mÏ‰Ì‚             1
          âˆ’                                              fk1 +2,k2 âˆ’2 (h1 u) du
            k1 + 2 Hm 1 âˆ’ Fk1 ,k2 {G(0)} G(0)
                      
                    1 Fk1 ,k2 {G(Ï‰0 )} âˆ’ Fk1 ,k2 {G(0)}
          = Ï‰0 +
                   m           1 âˆ’ Fk1 ,k2 {G(Ï‰0 )}
                K n âˆ’ K + 2q âˆ’ 2 1 + mÏ‰Ì‚
          âˆ’
              n âˆ’ K K + 2q âˆ’ 5      m
              Fk1 +2,k2 âˆ’2 {h1 G(Ï‰0 )} âˆ’ Fk1 +2,k2 âˆ’2 {h1 G(0)}
          Ã—                                                     ,
                              1 âˆ’ Fk1 ,k2 {G(0)}

where we dropped the index q in G, H, k1 and k2 .
      The expected loss when action A is chosen is obtained by similar steps:

                                                   Z +âˆ
                    HmR             1
              LA =                                         (y âˆ’ Ï‰0 )fk1 ,k2 {G(y)} dy
                   1 + mÏ‰Ì‚ 1 âˆ’ Fk1 ,k2 {G(0)}        Ï‰0
                                                          Z +âˆ
                     R(1 + mÏ‰Ì‚)          1
                   =                                               ufk1 ,k2 (u) du
                        Hm      1 âˆ’ Fk1 ,k2 {G(0)}        G(Ï‰0 )



                                            Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

Decision Theory for One-Way ANOVA                                                       205



                 R(1 + mÏ‰0 ) 1 âˆ’ Fk1 ,k2 {G(Ï‰0 )}
         =âˆ’
                     m        1 âˆ’ Fk1 ,k2 {G(0)}
                                                    Z +âˆ
           R(1 + mÏ‰Ì‚) k1             1
         =                                                     fk1 +2,k2 âˆ’2 (h1 u) du
              Hm     k1 + 2 1 âˆ’ Fk1 ,k2 {G(0)}        G(Ï‰0 )

             R(1 + mÏ‰0 ) 1 âˆ’ Fk1 ,k2 {G(Ï‰0 )}
         âˆ’
                 m        1 âˆ’ Fk1 ,k2 {G(0)}
              RK n âˆ’ K + 2q âˆ’ 2 1 + mÏ‰Ì‚ 1 âˆ’ Fk1 +2,k2 âˆ’2 {h1 G(Ï‰0 )}
         =
             n âˆ’ K K + 2q âˆ’ 5      m        1 âˆ’ Fk1 ,k2 {G(0)}
             R(1 + mÏ‰0 ) 1 âˆ’ Fk1 ,k2 {G(Ï‰0 )}
         âˆ’                                    ,
                 m        1 âˆ’ Fk1 ,k2 {G(0)}

assuming that k2 > 2, that is, K > 5 âˆ’ 2q.
   The balance function is
                                 
                       1             K n âˆ’ K âˆ’ 2q + 2 1 + mÏ‰Ì‚
       âˆ†L =
              1 âˆ’ Fk1 ,k2 {G(0)} n âˆ’ K K + 2q âˆ’ 5                m
                                                                        
           Ã— R âˆ’ (R âˆ’ 1)Fk1 +2,k2 âˆ’2 {h1 G(Ï‰0 )} âˆ’ Fk1 +2,k2 âˆ’2 {h1 G(0)}
                                                                      
              1 + mÏ‰0                                               
           âˆ’              R âˆ’ (R âˆ’ 1)Fk1 ,k2 {G(Ï‰0 )} âˆ’ Fk1 ,k2 {G(0) .
                 m

The leading factor can be dropped in a search for the root of âˆ†L(Ï‰Ì‚).


Appendix E. The Balance Function for Piecewise Quadratic
Loss
   When action B is chosen,
         Z Ï‰0
    LB =      (y âˆ’ Ï‰0 )2 g(y; Ï‰Ì‚) dy
             0
             Hm             1
        =
           1 + mÏ‰Ì‚ 1 âˆ’ Fk1 ,k2 {G(0)}
        Z Ï‰0 n
                    1 2
                                  1
                                           1
                                                       o
                                                     1 2
               Ï‰0 + m   âˆ’2 y+ m        Ï‰0 + m   + y+ m    fk1 ,k2 {G(y)} dy
         0

          (1 + mÏ‰0 )2 Fk1 ,k2 {G(Ï‰0 )} âˆ’ Fk1 ,k2 {G(0)}
        =
              m2               1 âˆ’ Fk1 ,k2 {G(0)}
                                                    Z G(Ï‰0 )
          2(1 + mÏ‰0 )(1 + mÏ‰Ì‚)              1
        âˆ’                                                    ufk1 ,k2 (u) du
                 Hm2              1 âˆ’ Fk1 ,k2 {G(0)} G(0)




                                       Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

206                                                    Nicholas T. Longford & Mercedes Andrade



                            2                        Z G(Ï‰0 )
                   1 + mÏ‰Ì‚                 1
           +                                          u2 fk1 ,k2 (u) du
                     Hm    1 âˆ’ Fk1 ,k2 {G(0)} G(0)
                                 (1 + mÏ‰)2 
                                
                      1                                                       
           =                                 Fk1 ,k2 {G(Ï‰0 )} âˆ’ Fk1 ,k2 {G(0)}
             1 âˆ’ Fk1 ,k2 {G(0)}        m2
              2k2 (1 + mÏ‰0 )(1 + mÏ‰Ì‚)
           âˆ’
             k2 âˆ’ 2         Hm2
            h                                                 i
           Ã— Fk1 +2,k2 âˆ’2 {h1 G(Ï‰0 )} âˆ’ Fk1 +2,k2 âˆ’2 {h1 G(0)}
                                                 2
                       k1               1 + mÏ‰Ì‚
           +
               H 2 h1 h2 (k1 + 4)          m
                                                                
           Ã— [Fk1 +4,k2 âˆ’4 {h2 G(Ï‰0 )} âˆ’ Fk1 +4,k2 âˆ’4 {h2 G(0)}] .

      The constant factor of the concluding term is
                                k1           k1 + 2  k1 (k1 + 2)
                                           =                       .
                        H 2 h1 h2 (k1 + 4)     k1 (k2 âˆ’ 2)(k2 âˆ’ 4)

      By similar steps we obtain the expression
                                        (1 + mÏ‰)2h
                                     
                          R                                              i
           LA =                                    1 âˆ’ F k 1 ,k2
                                                                 {G(Ï‰0 )}
                  1 âˆ’ Fk1 ,k2 {G(0)}        m2
                    2k2 (1 + mÏ‰0 )(1 + mÏ‰Ì‚) h                               i
               âˆ’                      2
                                                1 âˆ’ Fk1 +2,k2 âˆ’2 {h1 G(Ï‰0 )}
                  k2 âˆ’ 2          Hm
                                            2
                        k1           1 + mÏ‰Ì‚                                  
              + 2                               [1 âˆ’ Fk1 +4,k2 âˆ’4 {h2 G(Ï‰0 )}] ,
                 H h1 h2 (k1 + 4)        m
and hence
                         1
        âˆ†L =
              1 âˆ’ Fk1 ,k2 {G(0)}
                (1 + mÏ‰0 )2 
              
                                                                           
           Ã—                  R âˆ’ (R + 1)Fk1 ,k2 {G(Ï‰0 )} + Fk1 ,k2 {G(0)}
                     m2
                2k2 (1 + mÏ‰0 )(1 + mÏ‰Ì‚)
           âˆ’
              k2 âˆ’ 2          Hm2                                                            (16)
                                                                          
            Ã— R âˆ’ (R + 1)Fk1 +2,k2 âˆ’2 {h1 G(Ï‰0 )} + Fk1 +2,k2 âˆ’2 {h1 G(0)}
                                     2
                    k1        1 + mÏ‰Ì‚
           +
             h1 h2 (k1 + 4)     Hm
                                                                          
            Ã— R âˆ’ (R + 1)Fk1 +4,k2 âˆ’4 {h2 G(Ï‰0 )} + Fk1 +4,k2 âˆ’4 {h2 G(0)} .

It holds only when K > 5 âˆ’ 2q; otherwise LA is infinite and B is the preferred
action. The factor [1 âˆ’ Fk1 ,k2 {G(0)}]âˆ’1 is not relevant for finding the root of âˆ†L,


                                               Revista Colombiana de EstadÃ­stica 38 (2015) 181â€“207

Decision Theory for One-Way ANOVA                                                                                                          207

and can be dropped. Figure 10 displays the equilibrium values for a selection of
designs (K, m) and Ï‰0 set to 0.1 and 0.25. The equilibrium functions are decreasing
and reach their minima of âˆ’1/m for finite R. In the diagram, the functions are
interrupted at that point. For K = 9, m = 7 and Ï‰0 = 0.1, no line is drawn
because the equilibrium is equal to âˆ’1/m even for very small Ï‰Ì‚.

                                                                                                                       Log scale

                                                                Ï‰Thr
                                                                  0
                         0.2




                                                                                                  0.2
                                                                0.1
                                                                0.25
Equilibrium value (Ï‰*)




                                                                         Equilibrium value (Ï‰*)
                         0.1




                                                                                                  0.1
                                                                  K, m
                                                                 15,20
                         0.0




                                                                                                  0.0
                                                                 15,20
                         âˆ’0.1




                                                                                                  âˆ’0.1
                                    11,9                       11,9
                                     9,7



                                0               10              20                                       0.05   0.20    1.00       5.00   20.00
                                           Penalty ratio (R)                                                      Penalty ratio (R)
Figure 10: The equilibrium values Ï‰ âˆ— as functions of the penalty ratio R for number
           of clusters K and cluster size m indicated in the left-hand panel. Piecewise
           quadratic loss and non-informative prior for Ï‰.

References
Andrade M, Longford N, Tovar D. Tests for spatial and temporal correlation in mixed models for climate data.(2014). Revista Brasileira de Biometria.
Berger J. Statistical Decision Theory and Bayesian Analysis.(1985). Springer-Verlag.
Crainiceanu C, Ruppert D. Likelihood ratio tests in linear mixed models with one variance component.(2004). Journal of the Royal Statistical Society.
DeGroot M. Optimal Statistical Decisions.(1970).McGraw-Hill.
Dempster A, Laird N, Rubin D. Maximum likelihood from incomplete data via the EM algorithm.(1977). Journal of the Royal Statistical Society.
Fisher R. The fiducial argument in statistical inference.(1935). Annals of Eugenics.
Fisher R. Statistical Methods and Scientific Inference.(1956). Oliver and Boyd.
Giampaoli V, Singer J. Likelihood ratio tests for variance components in linear mixed models.(2009). Journal of Statistical Planning and Inference.
Greven S, Crainiceanu C, KÃ¼chenhoff H, Peters H. Restricted likelihood ratio testing for zero variance components in linear mixed models.(2008). Journal of Computational and Graphical Statistics.
Hannig J. On generalized fiducial inference.(2009). Statistica Sinica.
Li K, Meng X L, Raghunathan T, Rubin D. Significance levels from repeated p values with multiply-imputed data.(1991). Statistica Sinica.
Lindley D. Making Decisions.(1985). Wiley.
Little R, Rubin D. Statistical Analysis with Missing Data.(2002). Wiley.
Longford N. On estimating standard errors in multilevel analysis.(2000). The Statistician.
Longford N. Editorial: Model selection and efficiency Is â€˜Which model? the right question?.(2005). Journal of the Royal Statistical Society.
Longford N. Bayesian decision making about small binomial rates with uncertainty about the prior.(2010). The American Statistician.
Longford N. Comparing normal random samples, with uncertainty about the priors and utilities.(2012). Scandinavian Journal of Statistics.
Longford N. Which model? is the wrong question.(2012). Statistica Neerlandica.
Longford N. Statistical Decision Theory.(2013). Springer-Verlag.
Patterson D, Thompson R. Recovery of inter-block information when block sizes are unequal.(1971). Biometrika.
Potthoff R, Woodbury M, Manton K. Equivalent sample size and equivalent degrees of freedom refinements for inference using survey weights under superpopulation models.(1992). Journal of American Statistical Association.
Robert C, Cassella G. Monte Carlo Statistical Methods.(2004). Springer Verlag.
Rubin D. Multiple Imputation for Nonresponse in Surveys.(2002). Wiley.
Searle S, Casella G, McCulloch C. Variance Components.(2006). Wiley.
Seidenfeld T. R-A Fisherâ€™s fiducial argument and Bayes theorem.(1992). Statistical Science.