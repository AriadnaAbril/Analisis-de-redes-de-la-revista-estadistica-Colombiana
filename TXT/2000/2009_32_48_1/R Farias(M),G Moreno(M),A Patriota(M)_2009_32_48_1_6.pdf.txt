ReducciÃ³n de modelos en la presencia de parÃ¡metros de perturbaciÃ³n
Universidad de SÃ£o Paulo;Universidad Industrial de Santander (UIS)
Resumen
En muchos problemas de inferencia estadÃ­stica existe interÃ©s en estimar solamente algunos elementos del vector de parÃ¡metros que definen el modelo adoptado. Generalmente, esos elementos estÃ¡n asociados a las medidas de localizaciÃ³n, y los parÃ¡metros adicionales -que en la mayorÃ­a de las veces estÃ¡n en el modelo solo para controlar la dispersiÃ³n o la asimetrÃ­a- son conocidos como parÃ¡metros de perturbaciÃ³n o de incomodidad (nuisance parameters) de las distribuciones subyacentes. Es comÃºn estimar todos los parÃ¡metros del modelo y hacer inferencias exclusivamente para los parÃ¡metros de interÃ©s. Dependiendo del modelo adoptado, este procedimiento puede ser muy costoso, tanto algebraica como computacionalmente, por lo cual conviene reducirlo para que dependa Ãºnicamente de los parÃ¡metros de interÃ©s. En este artÃ­culo, hacemos una revisiÃ³n de los mÃ©todos de estimaciÃ³n en la presencia de parÃ¡metros de perturbaciÃ³n y consideramos algunas aplicaciones en modelos recientemente discutidos en la literatura.
Palabras clave: estimaciÃ³n, parÃ¡metro de perturbaciÃ³n, funciÃ³n de verosimilitud, suficiencia, informaciÃ³n auxiliar.
IntroducciÃ³n
Uno de los principales objetivos de la estadÃ­stica es inferir sobre determinada poblaciÃ³n apoyada solamente en la informaciÃ³n de una parte de ella (muestra).Usualmente, estamos interesados en determinada cantidad como la media, mediana, varianza, asimetrÃ­a, curtosis, coeficiente de correlaciÃ³n, entre otras. Algunas
veces, deseamos encontrar y explicar relaciones entre variables y hacer previsiones
sobre los valores futuros de la variable estudiada.
    En cualquier situaciÃ³n prÃ¡ctica, inicialmente debemos identificar quÃ© cantida-
des de la poblaciÃ³n son de principal interÃ©s. DespuÃ©s de definidas estas cantidades,
es natural suponer un modelo estadÃ­stico que se adecue al problema. Por ejemplo,
supÃ³ngase que el investigador estÃ¡ interesado en los parÃ¡metros de localizaciÃ³n
                                                                            >
y de escala. En este caso especÃ­fico, el vector de interÃ©s es Î¸ = Âµ, Ïƒ 2 , y su-
poniendo el modelo estadÃ­stico F = N Âµ, Ïƒ 2 : Âµ âˆˆ IR y Ïƒ 2 âˆˆ IR+ , siendo IR
el conjunto de los nÃºmeros reales y IR+ el conjunto de los nÃºmeros reales posi-
tivos, tenemos que el vector de interÃ©s es el vector que define la familia F ; por
tanto, no existen parÃ¡metros de perturbaciÃ³n. Si X1 , . . . , Xn es una muestra alea-
toria de la poblaciÃ³n objetivo, entonces, para estimar el vector Î¸ basta encontrar
                                                                          
un estadÃ­stico suficiente y completo que sea no sesgado; Î¸     b = X, S 2 > , siendo
       P            2
                         P            2
X =      i Xi /n y S =     i Xi âˆ’ X /(n âˆ’ 1), cumple estas condiciones (vÃ©ase
Lehmann & Casella 1998); entonces, el problema inferencial se resuelve, dado que
toda la informaciÃ³n de la muestra estÃ¡ concentrada en el estadÃ­stico Î¸. b
    Si el vector de interÃ©s define por completo el modelo estadÃ­stico adoptado,
estamos en el problema de la inferencia usual. Se deben encontrar estimadores
Ã³ptimos segÃºn algÃºn criterio de optimizaciÃ³n. Por ejemplo, estimadores no ses-
gados de varianza uniformemente mÃ­nima (obtenidos minimizando una funciÃ³n
de pÃ©rdida cuadrÃ¡tica), estimadores invariantes segÃºn algÃºn grupo de transfor-
maciones (de escala, de origen, de permutaciones, entre otras), estimadores que
minimicen el riesgo mÃ¡ximo generado por un subespacio paramÃ©trico (estimador
minimax), estimadores que minimicen el riesgo segÃºn alguna distribuciÃ³n a priori
(estimadores de Bayes). Todos esos estimadores dependen de estadÃ­sticos suficien-
tes minimales o completos (si existen) que, a su vez, se relacionen con estadÃ­sticos
auxiliares. Las propiedades de estos estimadores pueden ser vistas con detalles en
Lehmann & Casella (1998) y Lindsey (1996). Si el vector de interÃ©s no define por
completo el modelo estadÃ­stico, entonces existen parÃ¡metros de perturbaciÃ³n y es

                                        Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

ReducciÃ³n de modelos en la presencia de parÃ¡metros de perturbaciÃ³n                           101

preciso encontrar estimadores Ã³ptimos siguiendo otros criterios, como suficiencia
e informaciÃ³n parcial.
    Para ilustrar la idea de parÃ¡metros de perturbaciÃ³n, suponga que X1 , . . . , Xn es
una muestra aleatoria de la poblaciÃ³n objeto de estudio. Considere que el modelo
estadÃ­stico propuesto para describir el comportamiento de los datos observados es
               n                      >                                 o
         F = SN (Î¸) : Î¸ = Âµ, Ïƒ 2 , Î» , con Âµ, Î» âˆˆ IR y Ïƒ 2 âˆˆ IR+                    (1)

siendo SN (Âµ, Ïƒ 2 , Î») una distribuciÃ³n normal-asimÃ©trica (Skew-Normal ), con Âµ, Ïƒ 2
y Î» los parÃ¡metros de localizaciÃ³n, escala y asimetrÃ­a, respectivamente. La funciÃ³n
de densidad de la normal-asimÃ©trica definida por Azzalini (1985) es dada por
                                                    
                           2      2    xâˆ’Âµ         xâˆ’Âµ
               f x | Âµ, Ïƒ = Ï†                Î¦ Î»           , x âˆˆ IR             (2)
                                  Ïƒ      Ïƒ           Ïƒ
siendo Ï†(Â·) y Î¦(Â·) la funciÃ³n de densidad y la distribuciÃ³n acumulada de la dis-
tribuciÃ³n normal estÃ¡ndar, respectivamente. Las propiedades de esta distribuciÃ³n
pueden ser encontradas en Azzalini (1985). Considerando que estamos interesados
solamente en los parÃ¡metros de localizaciÃ³n y escala, podemos escribir el vec-
tor de parÃ¡metros para la distribuciÃ³n definida en (2) como Î¸ = (Î¸ 1 , Î¸2 ), donde
                 >
Î¸ 1 = Âµ, Ïƒ 2          y Î¸2 = Î». En este caso, el vector de interÃ©s Î¸1 no coincide con el
vector de parÃ¡metros que indexa la familia de distribuciones F y Î» es un parÃ¡metro
de perturbaciÃ³n para la estimaciÃ³n de Î¸1 . ObsÃ©rvese que, cuando Î» = 0, el modelo
(2) se reduce al modelo normal y, por tanto, no existe parÃ¡metro de perturbaciÃ³n.
    En ciertas ocasiones, la dimensiÃ³n del vector de parÃ¡metros de perturbaciÃ³n
crece con el tamaÃ±o de la muestra. Neyman & Scott (1948) definen estos pa-
rÃ¡metros como parÃ¡metros incidentales. Para ilustrar esta definiciÃ³n, considere
(Y1 , X1 ), . . . , (Yn , Xn ) una muestra aleatoria, cuya relaciÃ³n entre Yi y Xi estÃ¡ dada
por Yi = g(Î¸1 , xi ) + ei y Xi = xi + ui , siendo ei y ui variables aleatorias indepen-
dientes para todo i = 1, . . . , n y g(Î¸1 , xi ) una funciÃ³n conocida. AsÃ­, el vector de
                                                             >
                                                         (n)>            (n)
parÃ¡metros que define el modelo es Î¸(n) = Î¸ >       1 , Î¸2       , con Î¸ 2 = (x1 , . . . , xn )> ,
el vector de parÃ¡metros incidentales que generalmente no es de interÃ©s del investi-
gador. Este modelo es conocido en la literatura como modelo funcional con errores
en las variables y puede ser estudiado con mÃ¡s detalles en Fuller (1987). En este
caso, es comÃºn hacer inferencias sobre los parÃ¡metros de interÃ©s usando la funciÃ³n
de verosimilitud perfilada, definida en la secciÃ³n 4.2.
    A pesar de que existen diversas formas de tratar modelos que poseen parÃ¡me-
tros de perturbaciÃ³n, el enfoque principal de este trabajo se basa en la reducciÃ³n
de modelos. La forma mÃ¡s simple y directa es encontrar una funciÃ³n de verosimi-
litud ortogonal para el parÃ¡metro de interÃ©s. AsÃ­, en la secciÃ³n 2.2, introducimos
el concepto de verosimilitud ortogonal con algunos ejemplos en modelos asimÃ©-
tricos. En la secciÃ³n 3, presentamos algunas tÃ©cnicas de reducciÃ³n de modelos a
travÃ©s de estadÃ­sticos e ilustramos la teorÃ­a con algunos ejemplos. En la secciÃ³n 4,
exhibimos dos funciones de verosimilitudes aproximadas que son utilizadas para
construir funciones de verosimilitudes ortogonales para los parÃ¡metros de interÃ©s.
Finalizamos el artÃ­culo con algunos comentarios de las tÃ©cnicas presentadas.

                                           Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

102                                Rafael Farias, GermÃ¡n Moreno & Alexandre Patriota

   El principal objetivo de este artÃ­culo es motivar el uso de las tÃ©cnicas de re-
ducciÃ³n de modelos ilustrÃ¡ndolas con ejemplos recientemente discutidos en la lite-
ratura.


2. FunciÃ³n de verosimilitud
    Asumimos en este artÃ­culo que Î¸1 (la particiÃ³n de interÃ©s) y Î¸ 2 (el vector
de parÃ¡metros de perturbaciÃ³n) tienen dimensiones p1 y p âˆ’ p1 , respectivamente.
Consideramos tambiÃ©n que toda la informaciÃ³n de la muestra estÃ¡ contenida en la
funciÃ³n de verosimilitud, que estÃ¡ correctamente especificada. El problema consiste
en estimar Î¸ 1 minimizando la pÃ©rdida de informaciÃ³n que puede ocurrir en la
estimaciÃ³n de Î¸2 . La pÃ©rdida de informaciÃ³n serÃ¡ definida con mÃ¡s detalles en el
transcurso del texto.


2.1. FunciÃ³n de verosimilitud genuina
    Sea X una variable aleatoria en un espacio de probabilidad (â„¦, A, Î½), siendo â„¦
el espacio de posibilidades del experimento, A = Ïƒ(X) la Ïƒ-Ã¡lgebra asociada a â„¦
tal que X es medible y Î½ una medida de probabilidad aplicada a los elementos de
A. Sea X âŠ‚ IR el espacio de valores posibles que X puede asumir. Considere que
la distribuciÃ³n de probabilidad de X pertenece a la familia
                        n                        >           o
                                                             p
                   F = F (Â· | Î¸) : Î¸ = Î¸ >1 , Î¸ >
                                                2    âˆˆ Î˜ âŠ† R
                                                           I                    (3)

siendo F (Â· | Î¸) una funciÃ³n de distribuciÃ³n. Sea X = (X1 , . . . , Xn )> una muestra
aleatoria de X; denotaremos por L(Î¸ | x) la funciÃ³n de verosimilitud genuina
asociada a F (Â· | Î¸). Si X es una variable continua, entonces
                                   n
                                   Y                    n
                                                        Y
                                     dF (xi | Î¸)
                     L(Î¸ | x) =                     =         f (xi | Î¸)               (4)
                                   i=1
                                           dxi          i=1

   Si X es una variable discreta, entonces
                           n h
                           Y                            i Yn
                                          
              L(Î¸ | x) =         F x+
                                    i | Î¸   âˆ’ F (xâˆ’
                                                  i | Î¸) =   f (xi | Î¸)                (5)
                           i=1                                    i=1
                                      
siendo lÄ±Ìmyâ†“x F (y | Î¸) = F x+                                  âˆ’
                                i | Î¸ y lÄ±Ìmyâ†‘x F (y | Î¸) = F (xi | Î¸). La fun-
ciÃ³n f (xi | Î¸) denota la funciÃ³n de densidad en el caso continuo y la funciÃ³n de
probabilidad en el caso discreto.
    En el enfoque clÃ¡sico es comÃºn maximizar la funciÃ³n de verosimilitud L(Î¸ | x)
en relaciÃ³n con los parÃ¡metros del modelo para obtener sus estimadores. Los
estimadores de mÃ¡xima verosimilitud (EMV) son ampliamente usados debido a
sus buenas propiedades como invarianza, consistencia, eficiencia y normalidad
asintÃ³tica, si se satisfacen algunas condiciones de regularidad (ver Lehmann &
Casella 1998).

                                         Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

ReducciÃ³n de modelos en la presencia de parÃ¡metros de perturbaciÃ³n                   103

2.2. FunciÃ³n de verosimilitud ortogonal
   Suponiendo que X es un vector aleatorio con distribuciÃ³n de probabilidad
perteneciente a F , decimos que la funciÃ³n de verosimilitud L(Î¸ | x) es ortogonal
en relaciÃ³n con la particiÃ³n de interÃ©s si

                          L(Î¸ | x) = L1 (Î¸ 1 | x)L2 (Î¸2 | x)                         (6)
y los vectores Î¸1 y Î¸ 2 tienen variaciones independientes, o sea,

                                >
                         Î¸>    >
                          1 , Î¸2    âˆˆ Î˜1 Ã— Î˜2 = Î˜ âŠ‚ IRp                              (7)

donde Î˜k es el espacio paramÃ©trico en que Î¸k puede asumir valores, con k = 1, 2.
Denotaremos Lk (Î¸k | x) simplemente por Lk (Î¸ k ) para k = 1, 2.
      A partir de la ecuaciÃ³n (6) tenemos que el EMV para Î¸ 1 depende de la funciÃ³n
de verosimilitud genuina solamente a travÃ©s de L1 (Î¸1 ). En este caso, el EMV de
Î¸ 1 no depende de Î¸2 ; luego podemos ignorar la estimaciÃ³n de Î¸2 , sin que esto
interfiera la estimaciÃ³n de los parÃ¡metros de interÃ©s. Por tanto, podemos definir
un nuevo modelo reducido, F1 = {L1 (Î¸ 1 ); Î¸ 1 âˆˆ Î˜1 }, para hacer inferencias sobre
Î¸ 1 . Es importante notar que, en este caso, la informaciÃ³n dada por la estimaciÃ³n
de Î¸2 es irrelevante en la estimaciÃ³n de Î¸1 .

Ejemplo 1. AnÃ¡lisis de supervivencia. El principal interÃ©s en anÃ¡lisis de super-
vivencia es estudiar el tiempo hasta la ocurrencia de determinado evento. En esta
Ã¡rea de la estadÃ­stica es comÃºn encontrar la presencia de censuras antes de la ocu-
rrencia del evento de interÃ©s. En algunas situaciones, es razonable asumir que las
censuras no son informativas, o sea, su distribuciÃ³n no comparte parÃ¡metros con
la funciÃ³n de distribuciÃ³n del tiempo de ocurrencia del evento. AdemÃ¡s, se asume
tambiÃ©n independencia entre las censuras y el evento de interÃ©s. Sea T el tiempo
hasta la ocurrencia del evento y C el tiempo hasta la censura.
    (*) Suponga que T âˆ¼ f (t | Î¸1 ) es independiente de C âˆ¼ g(c | Î¸2 ), de modo que
Î¸ 2 no comparte parÃ¡metros con Î¸1 .
   En la prÃ¡ctica se observa el tiempo hasta la ocurrencia del evento o el tiempo
hasta la censura, o sea, Z = mÄ±Ìn{T, C} y Î´ = I(C â‰¥ T ). La distribuciÃ³n conjunta
de (Z, Î´) se obtiene asÃ­:

                    f (z, Î´ = 1 | Î¸) = P (Î´ = 1 | Î¸)f (z | Î´ = 1, Î¸)
                            = P (C â‰¥ T | Î¸)f (z | Î¸ 1 )                              (8)
                          = G(z | Î¸2 )f (z | Î¸1 )

pues, si Î´ = 1, entonces Z = T .

                    f (z, Î´ = 0 | Î¸) = P (Î´ = 0 | Î¸)f (z | Î´ = 0, Î¸)
                                    = P (C â‰¤ T | Î¸)g(z | Î¸2 )                        (9)
                                    = S(z | Î¸ 1 )g(z | Î¸2 )

                                       Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

104                                Rafael Farias, GermÃ¡n Moreno & Alexandre Patriota

y si Î´ = 0, tendremos Z = C. AsÃ­, la funciÃ³n de verosimilitud serÃ¡

           L(Î¸1 , Î¸2 ) = f (z, Î´ | Î¸)
                                               Î´                    1âˆ’Î´
                       = G(z | Î¸ 2 )f (z | Î¸ 1 ) S(z | Î¸ 1 )g(z | Î¸ 2 )               (10)
                         h                           ih                         i
                       = S(z | Î¸ 1 )1âˆ’Î´ f (z | Î¸1 )Î´ G(z | Î¸ 2 )Î´ g(z | Î¸2 )1âˆ’Î´

por tanto, la funciÃ³n de verosimilitud puede ser separada en una parte que solo
depende del parÃ¡metro de interÃ©s Î¸1 y otra que solo depende del parÃ¡metro de
perturbaciÃ³n Î¸2 . Si las censuras no son informativas, podemos usar Ãºnicamente
L1 (Î¸ 1 ) = S(z | Î¸ 1 )1âˆ’Î´ f (z | Î¸1 )Î´ para hacer inferencias sobre Î¸1 , sin tener pÃ©rdida
de informaciÃ³n.

    En la mayorÃ­a de las situaciones no es posible tener una funciÃ³n de verosimilitud
ortogonal. En algunos modelos, podemos encontrar una reparametrizaciÃ³n adecua-
da, tal que la funciÃ³n de verosimilitud sea ortogonal para el nuevo vector de parÃ¡-
                                                                                  >
metros. Esto es, podemos definir un nuevo vector de parÃ¡metros, Î» = Î»>      1 , Î»>
                                                                                 2
con Î»1 = Î»1 (Î¸ 1 ) y Î»2 = Î»2 (Î¸) de forma que

                                 L(Î») = Lâˆ—1 (Î»1 )Lâˆ—2 (Î»2 )                            (11)

    Asumiendo que Î»1 es una funciÃ³n biyectiva del vector de interÃ©s, podemos usar
Lâˆ—1 para estimar Î»1 y, en consecuencia, estimar Î¸ 1 . Solo en algunos casos especÃ­ficos
la reparametrizaciÃ³n existe y tiene interpretaciÃ³n para el problema analizado.
    Lindsey (1996) define varios tipos de reparametrizaciones ortogonales, entre los
cuales se pueden citar estimaciÃ³n ortogonal (el EMV de Î¸ 1 no depende del EMV
de Î¸ 2 ), diseÃ±o ortogonal (cuando las columnas de la matriz de diseÃ±o del modelo
de regresiÃ³n son linealmente independientes), informaciÃ³n ortogonal (la matriz de
informaciÃ³n de Fisher esperada es bloque diagonal en relaciÃ³n a Î¸1 y Î¸2 ) y la
funciÃ³n de verosimilitud ortogonal.
   Cuando la funciÃ³n de verosimilitud no es ortogonal y las reparametrizaciones
no son viables, se puede escribir la funciÃ³n de verosimilitud de la forma

                                  L(Î¸) = L1 (Î¸1 )L2 (Î¸)                               (12)

o sea, siempre serÃ¡ posible factorizar la funciÃ³n de verosimilitud de modo que uno
de los factores dependa solamente de Î¸1 y otro dependa de una funciÃ³n del vector
completo Î¸. En el caso mÃ¡s extremo, L1 (Î¸ 1 ) = 1 y L2 (Î¸) = L(Î¸).

Ejemplo 2. AnÃ¡lisis de supervivencia (continuaciÃ³n). ConsidÃ©rese el ejemplo 1
alterando la condiciÃ³n (*) para (**), siendo esta nueva condiciÃ³n definida por:
   (**) Suponga que T âˆ¼ f (t | Î¸1 ) es independiente de C âˆ¼ g(c | Î¸), tal que
             >
Î¸ = Î¸>      >
       1 , Î¸2    .


                                        Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

ReducciÃ³n de modelos en la presencia de parÃ¡metros de perturbaciÃ³n                     105

   Con la suposiciÃ³n (**), la funciÃ³n de verosimilitud estÃ¡ dada por

            L(Î¸1 , Î¸ 2 ) = f (z, Î´)
                                              Î´                  1âˆ’Î´
                         = G(z | Î¸)f (z | Î¸ 1 ) S(z | Î¸ 1 )g(z | Î¸)
                           h                          ih                     i        (13)
                         = S(z | Î¸ 1 )1âˆ’Î´ f (z | Î¸1 )Î´ G(z | Î¸)Î´ g(z | Î¸)1âˆ’Î´
                       = L1 (Î¸1 )L2 (Î¸)

por tanto, si se ignora L2 (Î¸), se puede perder mucha informaciÃ³n en la estimaciÃ³n
de Î¸1 , si usamos Ãºnicamente el tÃ©rmino L1 (Î¸ 1 ).

    Existen algunos criterios para escoger la funciÃ³n L1 (Î¸1 ) tal que conserve toda
la informaciÃ³n sobre Î¸1 contenida en la funciÃ³n de verosimilitud L(Î¸); por con-
siguiente, serÃ­a razonable despreciar la funciÃ³n L2 (Î¸) en el proceso de estimaciÃ³n
de Î¸1 . Esto genera la necesidad de definir mÃ¡s precisamente un concepto para
pÃ©rdida de informaciÃ³n, pues serÃ­a interesante encontrar L1 (Î¸ 1 ) y L2 (Î¸) tal que la
informaciÃ³n que L2 (Î¸) cargue sobre Î¸1 sea mÃ­nima (o nula). En la prÃ³xima secciÃ³n
introducimos algunos conceptos esenciales para determinar tales funciones.



3. ReducciÃ³n de modelos a travÃ©s de estadÃ­sticos
    Sea X un vector aleatorio con distribuciÃ³n de probabilidad perteneciente a F ,
            n                          >            o
donde F = F (Â· | Î¸) : Î¸ = Î¸>          >
                                 1 , Î¸2    âˆˆ Î˜ âŠ† IRp . La reducciÃ³n de modelos
se basa en estadÃ­sticos, funciones de X, que concentren la mayor parte de la
informaciÃ³n relevante sobre el vector de interÃ©s Î¸1 disponible en X.
   Considere T = T (X) y U = U (X), estadÃ­sticos que dependen Ãºnicamente de
X. La funciÃ³n de densidad conjunta de (T, U, X) es dada por

                   f (t, u, x | Î¸) = f (t | Î¸)f (u | t, Î¸)f (x | t, u, Î¸)             (14)

   Factorizando el lado izquierdo de esta ecuaciÃ³n, obtenemos

               f (t, u | x, Î¸)f (x | Î¸) = f (t | Î¸)f (u | t, Î¸)f (x | t, u, Î¸)        (15)

    Como los estadÃ­sticos T y U son determinados por X, sus distribuciones con-
dicionales en X son degeneradas. Se sigue que


                  f (x | Î¸) = f (t | Î¸)f (u | t, Î¸)f (x | t, u, Î¸) c.s. Î½             (16)

siendo que â€œc.s. Î½â€ significa â€œcasi segura Î½â€, o sea, la relaciÃ³n (16) vale para todo
x âˆˆ (X n âˆ’ A) tal que Î½ (A) = 0, donde Î½ es la medida de probabilidad aplicada a
los elementos de A.

                                         Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

106                                 Rafael Farias, GermÃ¡n Moreno & Alexandre Patriota

3.1. FunciÃ³n de verosimilitud marginal y condicional
    En la teorÃ­a de la verosimilitud introducida por Fisher, la funciÃ³n de verosimili-
tud ordinaria es la funciÃ³n de densidad conjunta (o probabilidad) de la muestra X
en funciÃ³n del vector de parÃ¡metros que define por completo la familia. Siguiendo
la idea de la factorizaciÃ³n dada antes, podemos definir dos nuevas funciones de
verosimilitud.
DefiniciÃ³n 1. Sea T un estadÃ­stico cuya distribuciÃ³n solo depende de Î¸ 1 . La
funciÃ³n de verosimilitud marginal estÃ¡ dada por
                             LM (Î¸ 1 ; t) = f (t | Î¸1 )    c.s. Î½                      (17)

   Suponga que (U , T ) sea un estadÃ­stico tal que sea posible obtener la factoriza-
ciÃ³n
                    f (t, u | Î¸1 , Î¸2 ) = f (t | Î¸1 )f (u | t, Î¸1 , Î¸2 )       (18)
  Despreciando el tÃ©rmino f (u | t, Î¸1 , Î¸2 ), tenemos la funciÃ³n de verosimilitud
marginal LM (Î¸1 ; t) basada en T = t.
DefiniciÃ³n 2. Sean U y T dos estadÃ­sticos tales que la distribuciÃ³n de T |U no
dependa de Î¸2 . La funciÃ³n de verosimilitud condicional estÃ¡ dada por

                         LC (Î¸ 1 ; t | u) = f (t | u, Î¸ 1 ) c.s. Î½                     (19)

   Suponga que (U , T ) sea un estadÃ­stico tal que es posible obtener la factorizaciÃ³n
                      f (t, u | Î¸1 , Î¸2 ) = f (u | Î¸1 , Î¸2 )f (t | u, Î¸1 )             (20)

   Despreciando el tÃ©rmino f (u | Î¸1 , Î¸2 ), tenemos la funciÃ³n de verosimilitud
condicional LC (Î¸ 1 ; t | u) basada en T | U = u.
   Las funciones de verosimilitudes marginales y condicionales tambiÃ©n pueden
usarse para hacer inferencias sobre Î¸1 , pero el precio es la pÃ©rdida de informaciÃ³n,
dado que en los dos casos dejamos de considerar una parte de la funciÃ³n de verosi-
militud original. Se pierde el mÃ­nimo de informaciÃ³n si son utilizados estadÃ­sticos
con propiedades Ã³ptimas como I-suficiencia, I-auxiliar y ausencia de informaciÃ³n
parcial en el sentido extendido, conceptos definidos en las siguientes secciones.


3.2. EstadÃ­stico suficiente y auxiliar
    Fisher definiÃ³ el concepto de estadÃ­stico suficiente y auxiliar (ancillary statistic)
para una familia de distribuciones, esto es, cuando el parÃ¡metro de interÃ©s coincide
con el parÃ¡metro que determina por completo la familia. Lindsey (1996) llama a
estas clases de estadÃ­sticos F-suficientes y F-auxiliares (F por Full, total, pues
definen totalmente la familia). En el transcurso del texto hablaremos simplemente
de estadÃ­sticos suficientes y auxiliares, y se definen asÃ­:
DefiniciÃ³n 3. Un estadÃ­stico T = T (X) es suficiente para el vector de parÃ¡metros
Î¸ si f (x | t, Î¸) = f (x | t) no depende de Î¸ c.s. Î½.

                                          Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

ReducciÃ³n de modelos en la presencia de parÃ¡metros de perturbaciÃ³n                       107

    Para encontrar estadÃ­sticos suficientes para una familia se puede utilizar el
criterio de la factorizaciÃ³n1 (Halmos & Savage 1949) definido por:
DefiniciÃ³n 4. Un estadÃ­stico T es suficiente para el vector de parÃ¡metros Î¸ si la
funciÃ³n de verosimilitud puede ser factorizada de la forma L(Î¸) = g(t | Î¸)h(x).

   Un ejemplo bÃ¡sico de aplicaciÃ³n de este criterio es el siguiente.
Ejemplo 3. DistribuciÃ³n Poisson. Sea X1 , . . . , Xn una muestra aleatoria de X âˆ¼
P (Î»), distribuciÃ³n de Poisson de parÃ¡metro Î». La funciÃ³n de verosimilitud estÃ¡
dada por


              L(Î» | X1 , . . . , Xn ) = P (X1 = x1 | Î») . . . P (Xn = xn | Î»)
                                     Î»x1 expâˆ’Î»      Î»xn expâˆ’Î»
                                   =            Â·Â·Â·
                                        x1 !           xn !
                                      Pn
                                     Î» i=1 xi expâˆ’nÎ»                                     (21)
                                   =     Qn
                                           i=1 xi !
                                      Pn                  1
                                   = Î» i=1 xi expâˆ’nÎ» Qn
                                                               i=1 xi !
                                                                Pn
    Por el criterio de la factorizaciÃ³n, tenemos que T =           i=1 xi es un estadÃ­stico
suficiente para Î».
DefiniciÃ³n 5. Un estadÃ­stico U = U (X) es auxiliar para Î¸ si la distribuciÃ³n de
U no depende de Î¸, o sea, f (u | Î¸) = f (u) c.s. Î½.

   Asumiendo que T y U son estadÃ­sticos suficiente y auxiliar para Î¸, respectiva-
mente, una consecuencia de las definiciones 3 y 5 es que la funciÃ³n de verosimilitud
para Î¸ puede factorizarse como
         L(Î¸ | x) = f (t | Î¸)f (x | t) y L(Î¸ | x) = f (x | u, Î¸)f (u) c.s. Î½             (22)

           dependiendo del estadÃ­stico
   Por tanto,                          usado, podemos reducir el modelo F ,
para F1 = F (t | Î¸) : Î¸ âˆˆ Î˜ o F1âˆ— = F (x | u, Î¸) : Î¸ âˆˆ Î˜ .
Ejemplo 4. DistribuciÃ³n alfa-normal. Sea X1 , . . . , Xn una muestra aleatoria de
X âˆ¼ Î±N (Î±), alfa-normal estÃ¡ndar definida inicialmente por Durrans (1992) y
estudiada recientemente por Jones (2004), cuya densidad es dada por
                          f (x | Î±) = Î±Ï†(x)Î¦(x)Î±âˆ’1 , x âˆˆ IR                              (23)
estando Ï†(Â·) y Î¦(Â·) definidas en (2). La funciÃ³n de verosimilitud estÃ¡ dada por
                                    "n        #" n        #Î±âˆ’1
                                     Y          Y
                                  n
                    L(Î± | x) = Î±        Ï†(xi )      Î¦(xi )                    (24)
                                        i=1           i=1
                                                              Q
    Por el criterio de la factorizaciÃ³n, tenemos que T =         i Î¦(Xi ) es un estadÃ­stico
suficiente para Î±.
  1 TambiÃ©n conocido en la literatura como criterio de factorizaciÃ³n de Neyman-Fisher.




                                         Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

108                                  Rafael Farias, GermÃ¡n Moreno & Alexandre Patriota

Ejemplo 5. DistribuciÃ³n normal   asimÃ©trica. Sea Y1 , . . . , Yn una muestra aleatoria
de la variable Y âˆ¼ SN 0, Ïƒ 2 , Î» definida en (2), con Ïƒ 2 = 1. Usando las propieda-
          distribuciÃ³n Normal-AsimÃ©trica derivadas por Azzalini (1985), tenemos
des de la P
             n
que U = i=1 Yi2 âˆ¼ Ï‡2 (n), distribuciÃ³n chi-cuadrado con n grados de libertad.
Entonces, por la definiciÃ³n 5, el estadÃ­stico U es auxiliar para Î».

    Si optamos por un estadÃ­stico suficiente T , es deseable que este sea minimal
(funciÃ³n de todos los estadÃ­sticos suficientes), pues asÃ­ tendremos la mayor re-
ducciÃ³n posible en los datos (Pace & Salvan 1997, Lehmann & Casella 1998). Si
optamos por un estadÃ­stico auxiliar U , es conveniente que la misma sea maximal,
o sea, no existe otro estadÃ­stico auxiliar que sea funciÃ³n de este.
    Como el objetivo de este trabajo es estimar solo una parte del vector Î¸, es
conveniente definir estadÃ­sticos que contengan informaciÃ³n solo sobre una particiÃ³n
del vector que define la familia o modelo en cuestiÃ³n, es decir, estadÃ­sticos que
generalicen los conceptos de suficiencia e informaciÃ³n auxiliar introducidos por
Fisher. A continuaciÃ³n definimos los conceptos de informaciÃ³n parcial y ausencia
parcial de informaciÃ³n.


3.3. Suficiencia y ausencia parcial de informaciÃ³n
DefiniciÃ³n 6. Si (T , U ) es suficiente para Î¸ y, en (16), f (u | t, Î¸) = f (u | t, Î¸2 ),
o sea, la densidad de U | T solo depende de Î¸ 2 , entonces decimos que T es
parcialmente suficiente para Î¸ 1 . AdemÃ¡s, si los campos de variaciÃ³n de Î¸1 y Î¸2
son independientes entre sÃ­, entonces T es llamada S-suficiente para Î¸1 .

                                                                           alea-
Ejemplo 6. DistribuciÃ³n exponencial truncada. Sea X1 , . . . , Xn una muestra
toria de X con distribuciÃ³n exponencial truncada perteneciente a F = E(Î¸) :
Î¸ = (Î±, Î²)> âˆˆ Î˜ = IR Ã— (0, âˆž) , cuya densidad es dada por
                                  (           )
                             1        (x âˆ’ Î±)
                  f (x | Î±) = exp âˆ’             , x âˆˆ (Î±, âˆž)                  (25)
                             Î²           Î²

y su funciÃ³n de verosimilitud por
                                                        ( P     )
                               âˆ’n             nÎ±             i xi                
             L(Î±, Î² | x) = Î²        exp                exp âˆ’        I(Î±)                     (26)
                                               Î²             Î²           âˆ’âˆž,x(1)


donde x(1)
          = mÄ±Ìn{x     . . . , xn }. Utilizando el criterio de la factorizaciÃ³n, tenemos
                     1, 
                 P
que V = X(1) , i Xi es suficiente para Î¸ = (Î±, Î²)> . Al mismo tiempo, el vector
                                            P              
V âˆ— = (U, T ), con U = X(1) y T = 2n i Xi âˆ’ X(1) , tambiÃ©n es suficiente, pues
es funciÃ³n 1 : 1 de V . El estadÃ­stico V âˆ— tambiÃ©n es completo2 , pues satisface la
condiciÃ³n                  
                  IE g(V âˆ— ) = 0 â‡â‡’ g(V âˆ— ) = 0, âˆ€Î¸ âˆˆ Î˜ c.s. Î½                 (27)
  2 Si X es una variable aleatoria con distribuciÃ³n perteneciente a una familia F , Î¸ âˆˆ Î˜, se dice
                                                                                 Î¸    Ë†     Ëœ
que un estadÃ­stico T es completo si para cualquier funciÃ³n medible g se verifica IEÎ¸ g(T ) = 0,
si y solo si âˆ€Î¸ âˆˆ Î˜, g(T ) = 0, c.s. Î½.


                                              Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

ReducciÃ³n de modelos en la presencia de parÃ¡metros de perturbaciÃ³n                        109

    Dado que U es estadÃ­stico suficiente y completo, T es estadÃ­stico auxiliar para
Î², y esto vale para todo Î² âˆˆ (0, âˆž), por el Teorema de Basu3 , U y T son indepen-
dientes y la distribuciÃ³n de T | U es igual a la distribuciÃ³n de T , y esta Ãºltima no
depende de Î±, pues U âˆ¼ E(Î±, n/Î²) y T âˆ¼ Î² Ï‡2 (2n). Entonces U es un estadÃ­stico
parcialmente suficiente para Î± y tambiÃ©n es S-suficiente, pues (Î±, Î²) âˆˆ IR Ã— (0, âˆž).
DefiniciÃ³n 7. Si T es degenerada y, en (16), f (u | t, Î¸) = f (u | Î¸2 ), o sea, la
densidad de U solo depende de Î¸2 , decimos que U es parcialmente auxiliar para
Î¸ 1 . AdemÃ¡s, si los campos de variaciÃ³n de Î¸ 1 y Î¸2 son independientes entre sÃ­,
entonces se dice que U es S-auxiliar para Î¸1 .
Ejemplo 7. DistribuciÃ³n normal asimÃ©trica (continuaciÃ³n). Considere el ejemplo
5, SN (0, Ïƒ 2 , Î»), con Ïƒ 2 desconocido. El estadÃ­stico U âˆ¼ Ïƒ 2 Ï‡2 (n) es parcialmente
auxiliar para Î», y como los parÃ¡metros varÃ­an independientemente, entonces U
tambiÃ©n es S-auxiliar.

    En las definiciones 6 y 7 establecemos los conceptos de suficiencia e informaciÃ³n
auxiliar parcial para particiones de un vector. Con tales definiciones es posible
retirar de la funciÃ³n de verosimilitud parte de la informaciÃ³n que no es relevante
en el proceso de estimaciÃ³n del parÃ¡metro de interÃ©s. Por ejemplo, si el vector
(U , T ) es suficiente para el vector completo Î¸ y T es un estadÃ­stico parcialmente
suficiente para Î¸1 , entonces la funciÃ³n de verosimilitud puede ser factorizada de la
forma
                       L(Î¸) = f (t|Î¸)f (u | t, Î¸2 )f (x | t, u) c.s. Î½           (28)
    AsÃ­, se puede proponer un modelo reducido usando Ãºnicamente f (t | Î¸). Si U
es parcialmente auxiliar para Î¸ 1 , entonces
                      L(Î¸) = f (t | u, Î¸)f (u | Î¸ 2 )f (x | t, u) c.s. Î½                 (29)

   Por tanto, el modelo reducido puede usar solo f (t | u, Î¸).
    A pesar de reducir la funciÃ³n de verosimilitud, esta no se torna ortogonal y, por
tanto, el parÃ¡metro de perturbaciÃ³n continÃºa presente. La funciÃ³n de verosimilitud
serÃ¡ ortogonal, usando las definiciones 6 y 7, solo cuando exista un estadÃ­stico
T âˆ— parcialmente suficiente para Î¸ 1 y parcialmente auxiliar para Î¸ 2 , o exista un
estadÃ­stico U âˆ— parcialmente suficiente para Î¸ 2 y parcialmente auxiliar para Î¸ 1 .
AdemÃ¡s, los vectores de parÃ¡metros Î¸ 1 y Î¸2 deben variar independientemente, o
sea, el campo de variaciÃ³n de Î¸1 debe ser igual para cada Î¸2 fijo, y viceversa.
Esta propiedad puede encontrarse en la familia exponencial de rango completo
(ver Lindsey 1996).
    Por tanto, si las anteriores condiciones se satisfacen, el estadÃ­stico T âˆ— separa
la funciÃ³n de verosimilitud de la forma
                    L(Î¸) = f (tâˆ— | Î¸ 1 )f (x | tâˆ— , Î¸2 ) = L1 (Î¸1 )L2 (Î¸2 )              (30)
y usando el estadÃ­stico U âˆ— , obtenemos
                   L(Î¸) = f (x | uâˆ— , Î¸1 )f (uâˆ— | Î¸2 ) = L1 (Î¸1 )L2 (Î¸2 )                (31)
  3 El Teorema de Basu dice que dos estadÃ­sticos U y T son independientes si U es suficiente y

completo para Î¸ y T es auxiliar para Î¸.


                                          Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

110                                Rafael Farias, GermÃ¡n Moreno & Alexandre Patriota

Ejemplo 8. AnÃ¡lisis de supervivencia (continuaciÃ³n). Considere el ejemplo 2.
SupÃ³ngase tambiÃ©n que T âˆ¼ exp(Î») y C âˆ¼ exp(ÎºÎ»). En este caso, Î¸ = (Î», Îº),Psiendo
Î» el parÃ¡metro
      P          de interÃ©s y ÎºPel parÃ¡metro de perturbaciÃ³n. Haciendo A = i Î´i zi ,
B =      i (1 âˆ’ Î´i )zi y d =     i Î´i , se puede mostrar que Î»A | d âˆ¼ gamma(d, 1),
Î»B | d âˆ¼ gamma(d, Îº) y d âˆ¼ Bin (n, 1/(1 + Îº)). Por consiguiente, la distribuciÃ³n
conjunta de W = A/B y d no depende de Î». La funciÃ³n de verosimilitud estÃ¡ dada
por
                                              
                       L(Î», Îº) = Î»n Îºnâˆ’d exp Î»(1 + Îº)Î£i zi
                                              
                               = Î»n Îºnâˆ’d exp Î»(1 + Îº)(A + B)                 (32)
                                   n nâˆ’d
                                              
                               =Î» Îº        exp Î»(1 + Îº)B(1 + W )

    Por el criterio de la factorizaciÃ³n, se nota que (B, W, d) es suficiente para (Î», Îº).
Haciendo U âˆ— = (W, d) tenemos que B | U âˆ— âˆ¼ gamma(d, Î»W ). AsÃ­, se pueden hacer
inferencias sobre Î» usando solo la distribuciÃ³n de B | U âˆ— . El estimador de mÃ¡xima
verosimilitud de Î» usando esta distribuciÃ³n estÃ¡ dado por Î»     b = d/(BW ) = d/A.

DefiniciÃ³n 8. Un estadÃ­stico T âˆ— que sea parcialmente suficiente para Î¸1 , y par-
cialmente auxiliar para Î¸2 y cuyos parÃ¡metros sean ortogonales, es llamado â€œcorte
propioâ€ (proper cut) por Lindsey (1996); tambiÃ©n se denomina estadÃ­stico que de-
fine un corte de Bardorff-Nielsen en el modelo F .

   Si T âˆ— define un corte de Bardorff-Nielsen para Î¸ = (Î¸1 , Î¸ 2 ), entonces T âˆ— es
un estadÃ­stico S-suficiente para Î¸1 y S-auxiliar para Î¸ 2 . AdemÃ¡s, la funciÃ³n de
verosimilitud es ortogonal y siempre puede ser escrita de la forma
                            L(Î¸) = f (tâˆ— | Î¸1 )f (x | tâˆ— , Î¸2 )                      (33)

   En este caso no tendremos pÃ©rdida de informaciÃ³n al usar el modelo L1 (Î¸ 1 )
dado en (30) o (31).
    Es raro encontrar estadÃ­sticos T âˆ— y U âˆ— con estas propiedades. Jorgensen (1993)
usÃ³ la definiciÃ³n de modelo saturado para introducir nuevos conceptos de suficien-
cia e informaciÃ³n auxiliar, con el objetivo de reducir al mÃ¡ximo el modelo. El
concepto de modelo saturado corresponde a la idea de un parÃ¡metro para cada
observaciÃ³n, y se define a continuaciÃ³n.
DefiniciÃ³n 9. Se dice que un modelo estadÃ­stico F = {F (Â· | Î¸) : Î¸ âˆˆ Î˜} es
                                                                        b
                                                                    b = Î¸(X)
saturado si, para todo X âˆˆ X , el estimador de mÃ¡xima verosimilitud Î¸        es
Ãºnico y funciÃ³n 1:1 de X.

   En las definiciones 10 y 11 considere que el vector (T , U ) es suficiente para
Î¸ = (Î¸ 1 , Î¸2 ).
DefiniciÃ³n 10. Sea T un estadÃ­stico S-auxiliar para Î¸2 ; entonces
                   L(Î¸) = f (t | Î¸1 )f (u | t, Î¸) = L1 (Î¸1 )L2 (Î¸)                   (34)

    Para Î¸1 fijo, si f (u | t, Î¸) es un modelo saturado, entonces se dice que el
estadÃ­stico T es I-suficiente para Î¸1 .

                                        Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

ReducciÃ³n de modelos en la presencia de parÃ¡metros de perturbaciÃ³n                   111

DefiniciÃ³n 11. Sea U un estadÃ­stico S-suficiente para Î¸2 ; entonces

                  L(Î¸) = f (t | u, Î¸1 )f (u | Î¸) = L1 (Î¸1 )L2 (Î¸)                   (35)

para Î¸1 fijo, si f (u | Î¸) es un modelo saturado, entonces se dice que el estadÃ­stico
U es I-auxiliar para Î¸1 .

    En la definiciÃ³n 10, toda la informaciÃ³n relevante sobre Î¸1 estÃ¡ contenida en
el primer tÃ©rmino f (t | Î¸ 1 ). En la definiciÃ³n 11, la idea es contraria: no existe
informaciÃ³n relevante sobre Î¸1 en el segundo tÃ©rmino f (u | Î¸). AdemÃ¡s, en la
definiciÃ³n 10, cuando Î¸ 1 estÃ¡ fijo, la saturaciÃ³n del modelo L2 (Î¸) = f (u | t, Î¸)
no garantiza que el estadÃ­stico U sea totalmente no informativo para diferentes
valores de Î¸1 .
                       
                      b2 la funciÃ³n de verosimilitud f (u | t, Î¸1 , Î¸2 ) cuando substi-
    Sea f u | t, Î¸1 , Î¸
                                                                                     
tuimos Î¸2 por su EMV Î¸                                                             b2
                         b2 . Pace & Salvan (1997) argumentan que si f u | t, Î¸1 , Î¸
fuera no identificable o no existiera el EMV para Î¸1 , entonces L2 (Î¸) podrÃ­a ser ig-
norado en la estimaciÃ³n de Î¸1 . Este concepto de falta de informaciÃ³n se denomina
ausencia de informaciÃ³n parcial en el sentido extendido.

Ejemplo 9. DistribuciÃ³n exponencial truncada (continuaciÃ³n). Considere el ejem-
plo 6, donde X1 , . . . , Xn es una muestra aleatoria de una distribuciÃ³n E(Î±, Î²). El
parÃ¡metro de escala Î² es el parÃ¡metro de interÃ©s y Î± es el parÃ¡metro de perturba-
ciÃ³n.
   Por el ejemplo 6, tenemosque el vector de estadÃ­sticos V âˆ— = (U, T ), con U =
              P
X(1) y T = 2n i Xi âˆ’ X(1) , es suficiente para (Î±, Î²); ademÃ¡s, U âˆ¼ E(Î±, n/Î²)
y T âˆ¼ Î²Ï‡2 (2n) son independientes. El estadÃ­stico T es S-auxiliar para Î±, pues la
distribuciÃ³n de T no depende de Î± y la distribuciÃ³n U | T = t es igual a la de
la distribuciÃ³n marginal de U por la independencia. Fijando el valor de Î² en la
distribuciÃ³n de U | T = t, el EMV de Î± es Î±  b = U ; luego el modelo es saturado, y
consecuentemente T es I-suficiente para Î². AsÃ­, toda la informaciÃ³n relevante que
la muestra tiene sobre Î² estÃ¡ contenida en la distribuciÃ³n marginal de T . Entonces,
el factor ignorado en la funciÃ³n de verosimilitud serÃ¡ L2 (Î¸) = f (u | t, Î¸).
   Si sustituimos Î± por su estimador de mÃ¡xima verosimilitud en L2 , tenemos la
nueva funciÃ³n de verosimilitud dada por
                                         (          )
                                     n      (u âˆ’ u)     n
                                 b) = exp âˆ’
                    f (u | t, Î², Î±                    =                   (36)
                                     b         Î²        Î²

   Como L2 es una funciÃ³n decreciente en Î², tenemos que su EMV no estÃ¡ definido,
y dado T = t, la distribuciÃ³n de U no es informativa en la estimaciÃ³n de Î² en el
sentido extendido.

Ejemplo 10. El test exacto de Fisher es una de las pruebas mÃ¡s famosos para
verificar si existe asociaciÃ³n entre variables categÃ³ricas, este test se deriva de la
distribuciÃ³n binomial como veremos a continuaciÃ³n.

                                       Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

112                                Rafael Farias, GermÃ¡n Moreno & Alexandre Patriota

    En la tabla 1 presentamos una tabla de contingencia 2 Ã— 2, bÃ¡sicamente, una
tabla de contingencia es la representaciÃ³n de las frecuencias conjuntas entre dos
o mÃ¡s caracterÃ­sticas que deseamos estudiar. Sea A y B la representaciÃ³n de dos
eventos independientes de interÃ©s, A0 y B 0 sus respectivos eventos complementarios;
tal que a es el nÃºmero de ocurrencias del evento Aâˆ©B, b es el nÃºmero de ocurrencias
del evento A0 âˆ© B, c es el nÃºmero de ocurrencias del evento A âˆ© B 0 y d es el nÃºmero
de ocurrencias del evento A0 âˆ© B 0 .
                        Tabla 1: Tabla de Contingencia 2 Ã— 2.
                                       A     A0     Total
                                B      a     b       m
                                B0     c     d      mâˆ’n
                               Total   t    nâˆ’t      n


   Si n y m son fijos, entonces a âˆ¼ Bin(m, p1 ) y c âˆ¼ Bin(m âˆ’ n, p2 ), donde
Bin(n, p) denota la distribuciÃ³n binomial con parÃ¡metros n y p. La funciÃ³n de
verosimilitud puede ser escrita como
                                        
                                     m mâˆ’n a
              f (a, c | p1 , p2 ) =         p1 (1 âˆ’ p1 )b pc2 (1 âˆ’ p2 )d (37)
                                     a   c

    Suponga que estamos interesados en estimar la razÃ³n de ventajas (odds ratio)
       p1 (1 âˆ’ p2 )
Î¸1 =                . AsÃ­, haciendo Î¸2 = p2 , la funciÃ³n de verosimilitud puede ser
       (1 âˆ’ p1 )p2
reescrita de la forma
                                                 
                                           m m âˆ’ n Î¸1a Î¸2> (1 âˆ’ Î¸2 )nâˆ’t
                    f (a, t | Î¸1 , Î¸2 ) =                                      (38)
                                           a   t âˆ’ a (1 âˆ’ Î¸2 + Î¸1 Î¸2 )m

   La distribuciÃ³n condicional de a dado t estÃ¡ dada por
                                              
                                            m mâˆ’n a
                                                    
                                            a   tâˆ’a Î¸1
                         f (a | t, Î¸1 ) = P m mâˆ’n k                               (39)
                                           k k    tâˆ’k Î¸1


   La distribuciÃ³n de a | t se utiliza para hacer pruebas de asociaciÃ³n entre A y
B. La distribuciÃ³n de t estÃ¡ dada por
                                                             
                                 Î¸1a Î¸2> (1 âˆ’ Î¸2 )nâˆ’t X m m âˆ’ n k
              f (t | Î¸1 , Î¸2 ) =                                 Î¸1           (40)
                                 (1 âˆ’ Î¸2 + Î¸1 Î¸2 )m     k   tâˆ’k
                                                    k


   Si Î¸1 = 1, la derivada del logaritmo de f (t | Î¸1 , Î¸2 ) es

                           âˆ‚ log f (t | Î¸1 , Î¸2 )    t   nâˆ’t
                                                  =    âˆ’                            (41)
                                   âˆ‚Î¸2              Î¸2   1 âˆ’ Î¸2

y el estimador de mÃ¡xima verosimilitud de Î¸2 es t/n. AsÃ­, f (t | Î¸1 , Î¸2 ) es un modelo
saturado y, por tanto, el estadÃ­stico T es I-auxiliar para Î¸1 .


                                       Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

ReducciÃ³n de modelos en la presencia de parÃ¡metros de perturbaciÃ³n                  113

4. Funciones de seudoverosimilitudes
    Cuando existen modelos donde no es posible aplicar las tÃ©cnicas vistas en las
secciones anteriores o la informaciÃ³n contenida en L2 no puede ser ignorada debido
a su importancia en la inferencia, es indispensable definir otras alternativas. Las
funciones de seudoverosimilitudes pueden utilizarse como una aproximaciÃ³n a la
clase de funciones de verosimilitudes genuinas. Las funciones de verosimilitudes ca-
nÃ³nica, perfilada, perfilada corregida, perfilada modificada, predictivas bayesianas
(no bayesianas) y cuasiverosimilitud son algunos ejemplos de funciones de seudo-
verosimilitudes. Para ilustrar este tipo de tÃ©cnicas de reducciÃ³n de modelos, en
esta secciÃ³n presentamos ejemplos de las funciones de verosimilitudes canÃ³nica y
perfilada.


4.1. FunciÃ³n de verosimilitud canÃ³nica

    Sea L(Î¸) una funciÃ³n de verosimilitud de dos parÃ¡metros, con Î¸ = (Î¸1 , Î¸2 ) âˆˆ
Î˜1 Ã— Î˜2 âŠ‚ IR2 , es decir, los parÃ¡metros de interÃ©s Î¸1 y de perturbaciÃ³n Î¸2 son
escalares. Ya vimos que si L(Î¸) = L1 (Î¸1 )L2 (Î¸2 ), entonces L(Î¸) serÃ¡ ortogonal en
relaciÃ³n con la particiÃ³n de interÃ©s y la inferencia sobre Î¸1 estarÃ¡ basada inte-
gralmente en L1 (Î¸1 ). Dado que no siempre es posible obtener con exactitud esta
separaciÃ³n, Hinde & Aitkin (1987) propusieron realizar una aproximaciÃ³n a esta
factorizaciÃ³n. La idea central es considerar una factorizaciÃ³n aproximada para la
funciÃ³n de verosimilitud original, esto es,

                             L(Î¸1 , Î¸2 ) â‰ˆ L1 (Î¸1 )L2 (Î¸2 )                        (42)

donde la distancia entre las funciones de verosimilitudes original y aproximada es la
menor posible. Las funciones L1 (Î¸1 ) y L2 (Î¸2 ) se obtienen por una descomposiciÃ³n
de autofunciones de L(Î¸). Estas funciones se llaman verosimilitudes canÃ³nicas para
los parÃ¡metros Î¸1 y Î¸2 , respectivamente.
    Para determinar las funciones L1 (Î¸1 ) y L2 (Î¸2 ), Hinde y Aitkin consideraron
tres casos, dependiendo de la naturaleza del espacio paramÃ©trico: i) ambos discre-
tos; ii) uno discreto y el otro continuo y iii) ambos continuos. La idea principal
de los autores es integrar (o sumar) L(Î¸1 , Î¸2 )L2 (Î¸2 ) con respecto al parÃ¡metro de
perturbaciÃ³n Î¸2 ; el resultado es la funciÃ³n de verosimilitud canÃ³nica para el pa-
rÃ¡metro de interÃ©s Î¸1 . A continuaciÃ³n se presenta un ejemplo clÃ¡sico para ilustrar
esta tÃ©cnica.

Ejemplo 11. DistribuciÃ³n normal. Sea X una variable aleatoria con distribuciÃ³n
N (Âµ, 1). Defina Î¸1 = |Âµ| y Î¸2 = signo(Âµ), esto es, Î¸1 âˆˆ IR+ y Î¸2 âˆˆ {âˆ’1, 1}.
Suponga que estamos interesados en hacer inferencias sobre Î¸1 = |Âµ|. La funciÃ³n
de verosimilitud genuina es
                                                           
                                      1     1             2
                    L(Î¸1 , Î¸2 | x) = âˆš exp âˆ’ (x âˆ’ Î¸1 Î¸2 )                          (43)
                                      2Ï€    2

                                      Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

114                                Rafael Farias, GermÃ¡n Moreno & Alexandre Patriota

   Sean T = |X| y S = signo(X), entonces
                                                       
                                  1      1            2
             L(Î¸1 , Î¸2 | t, s) = âˆš exp âˆ’ (ts âˆ’ Î¸1 Î¸2 )
                                  2Ï€     2
                                                                                    (44)
                                  1      1
                               = âˆš exp âˆ’ t2 âˆ’ 2tsÎ¸1 Î¸2 + Î¸22
                                  2Ï€     2
luego, T y S son conjuntamente suficientes para Î¸1 y Î¸2 . Note que la funciÃ³n de
verosimilitud no es ortogonal.
   Siguiendo la idea de Hinde y Aitkin, se debe minimizar
                    2 Z        h
                    X                                      i2
                                L Î¸1 , Î¸2j âˆ’ L1 (Î¸1 )L2 Î¸2j     dÎ¸1                    (45)
                    j=1   Î˜1

cuyas soluciones son

           L(Î¸1 , âˆ’1 | x)L2 (âˆ’1 | x) + L(Î¸1 , 1 | x)L2 (1 | x) = Î»L1 (Î¸1 |x)           (46)
                  Z
                       L(Î¸1 , 1 | x)L1 (Î¸1 | x) dÎ¸1 = Î»L2 (1 | x) y                    (47)
                   Î˜1
                  Z
                       L(Î¸1 , âˆ’1 | x)L1 (Î¸1 | x) dÎ¸1 = Î»L2 (âˆ’1 | x)                    (48)
                    Î˜1
    En la expresiÃ³n (46), L1 (Î¸1 | x) depende de las cantidades desconocidas L2 (1 |
x) y L2 (âˆ’1 | x). En la expresiÃ³n (48) las cantidades L2 (1 | x) y L2 (âˆ’1 | x)
dependen de L1 (Î¸1 | x). Con el fin de simplificar la notaciÃ³n en este problema,
considere M1 = L(Î¸1 , 1 | x), M2 = L(Î¸1 , âˆ’1 | x), N1 = L2 (1 | x) y N2 = L2 (âˆ’1 |
x). Como N1 y N2 no dependen de los parÃ¡metros, (46) y (48) pueden reescribirse
matricialmente de la forma:
                                              
                                     N1         N1
                                 Î»2       =M                                  (49)
                                     N2         N2

donde M es la matriz de dimensiÃ³n 2 Ã— 2 cuyo elemento en la posiciÃ³n (j, j 0 ) es
dado por                         Z
                                mjj 0 =         Mj Mj 0 dÎ¸1                            (50)
                                           Î˜1
   Resolviendo las integrales para cada elemento de la matriz M , tenemos que
                          "          âˆš                  #
                              1               1
                             âˆš
                            2 Ï€
                                Î¦ âˆ’ 2x      4
                                             âˆš
                                                Ï€
                                                  exp âˆ’x2
                    M=       1
                                               1
                                                     âˆš                    (51)
                             âˆš
                            4 Ï€
                                exp âˆ’x2        âˆš
                                              2 Ï€
                                                   Î¦ 2x

donde Î¦(Â·) es la funciÃ³n de distribuciÃ³n acumulada de la distribuciÃ³n normal
estÃ¡ndar. Los autovalores de la matriz M estÃ¡n dados por
                             q     âˆš       2     
                        1+      2Î¦ 2x âˆ’ 1 + exp âˆ’2x2
                   Î·1 =                                                  (52)
                                         2

                                          Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

ReducciÃ³n de modelos en la presencia de parÃ¡metros de perturbaciÃ³n                     115

y
                                q        âˆš     2    
                           1âˆ’       2Î¦    2x âˆ’ 1 + exp âˆ’2x2
                    Î·2 =                                                              (53)
                                                2
    Se comprueba fÃ¡cilmente que la suma de los autovalores Î·1 y Î·2 es 1. Ahora,
dado que la soluciÃ³n de la ecuaciÃ³n (46) es Î»L1 (Î¸1 ), y en la ecuaciÃ³n (49) tenemos la
             âˆš
relaciÃ³n Î» = Î·, entonces la funciÃ³n de verosimilitud canÃ³nica serÃ¡ completamente
informativa cuando Î·maÌx = 1 (siendo Î·maÌx el mayor autovalor de M ).
                                                                    2        1/2
   El autovector asociado
                             âˆš es b =(r(x), 1), donde r(x) = (v (x) + 1) âˆ’
                          a Î·maÌx
v(x), con v(x) = exp x2 2Î¦ 2x âˆ’ 1 . Remplazando en la ecuaciÃ³n (46) con
        âˆš               âˆš
Î»maÌx = Î·maÌx , tenemos, Î·maÌx Â· L1 (Î¸1 ) = M1 Â· r(x) + M2 Â· 1, y por consiguiente

                       "                                        #
                  1          1                         1
     L1 (Î¸1 ) = âˆš       exp âˆ’ (x + Î¸1 )2 Â· r(x) + exp âˆ’ (x âˆ’ Î¸1 )2 Â· 1
                 Î·maÌx       2                         2

   Para cualquier valor que tome el parÃ¡metro de perturbaciÃ³n Î¸2 , la funciÃ³n de
verosimilitud canÃ³nica L1 (Î¸1 ) serÃ¡ siempre igual.

    Las principales ventajas de la funciÃ³n de verosimilitud canÃ³nica son fundamen-
talmente que la inferencia sobre Î¸1 se basa integralmente en L1 (Î¸1 ); y la funciÃ³n de
verosimilitud canÃ³nica siempre existe para modelos con dos parÃ¡metros, en con-
traste con las funciones verosimilitudes marginal y condicional, que generalmente
no existen. Dos de las principales desventajas de este mÃ©todo son: tiene Ã¡lgebra pe-
sada, aun para espacios paramÃ©tricos de baja dimensiÃ³n y para cada configuraciÃ³n
de la funciÃ³n de verosimilitud existe una soluciÃ³n particular.


4.2. FunciÃ³n de verosimilitud perfilada
    Inferir sobre el parÃ¡metro de interÃ©s a partir de la funciÃ³n de verosimilitud mar-
ginal o condicional es muy adecuado, porque estas son verosimilitudes genuinas; el
problema es que no siempre es posible su construcciÃ³n. Una soluciÃ³n es sustituir
en la verosimilitud original el vector de parÃ¡metros de perturbaciÃ³n por una esti-
mativa consistente; la funciÃ³n resultante se conoce como funciÃ³n de verosimilitud
perfilada.
    Formalmente, sea (X1 , . . . , Xn ) una muestra aleatoria de la variable X con
                                                   n                       >     o
distribuciÃ³n de probabilidad en la familia F = F (Â· | Î¸) : Î¸ = Î¸>   1 , Î¸ >
                                                                          2    âˆˆ Î˜  ,
siendo Î¸1 el vector de parÃ¡metros de interÃ©s y Î¸ 2 el vector de parÃ¡metros de
                       > > >
                  b= Î¸
perturbaciÃ³n. Sea Î¸     b ,Î¸b        el estimador de mÃ¡xima verosimilitud del vector
                           1    2

Î¸ completo, y Î¸  bi (Î¸ j ) el estimador de mÃ¡xima verosimilitud de Î¸i cuando Î¸j estÃ¡
fijo, para i, j = 1, 2. La funciÃ³n de verosimilitud perfilada es definida por
                                                           
                                                    b2 (Î¸1 )
                                  Lp (Î¸1 ) = L Î¸1 , Î¸                           (54)


                                         Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

116                                Rafael Farias, GermÃ¡n Moreno & Alexandre Patriota

                                                                b2 (Î¸1 ) denota el
donde L(Î¸ 1 , Î¸2 ) denota la funciÃ³n de verosimilitud genuina y Î¸
estimador de mÃ¡xima verosimilitud de Î¸2 para Î¸1 fijo.
   La expresiÃ³n (54) sugiere un procedimiento de maximizaciÃ³n en dos etapas.
                                               b2 (Î¸1 ) que maximice L(Î¸1 , Î¸2 ) con
La primera etapa consiste en calcular el valor Î¸
respecto a Î¸2 , suponiendo Î¸ 1 constante. La segunda etapa busca el valor Î¸1 que
maximice Lp (Î¸1 ).
   La inferencia aproximada sobre Î¸ 1 se hace tratando Lp (Î¸ 1 ) como una funciÃ³n
de verosimilitud genuina basada en un modelo solamente con el parÃ¡metro Î¸ 1 .
Usar la funciÃ³n de verosimilitud perfilada es semejante a tratar el parÃ¡metro de
perturbaciÃ³n como si fuese conocido. Tal procedimiento puede conducir a algu-
nos problemas; por ejemplo, inconsistencia e ineficiencia de los estimadores de los
parÃ¡metros de interÃ©s.
   Veamos dos ejemplos.

Ejemplo 12. DistribuciÃ³n normal. Suponga    que X1 , . . . , Xn es una muestra alea-
toria de una distribuciÃ³n normal, N Âµ, Ïƒ 2 . Luego, su funciÃ³n de verosimilitud
genuina es
                                             (                        )
                                âˆ’n/2          1 X
                                                       n
                     2           2                                  2
             L Âµ, Ïƒ ; x = 2Ï€Ïƒ            exp âˆ’ 2            (xi âˆ’ Âµ)             (55)
                                                 2Ïƒ i=1

                                 Î£i (xi âˆ’ Âµ)2
   Dado Âµ, el EMV de Ïƒ 2 es                   . Y dada Ïƒ 2 , el EMV de Âµ es x. Por
                                       n
tanto, la funciÃ³n de verosimilitud perfilada de Âµ es
                                                             âˆ’n/2
                                           Î£i (xi âˆ’ Âµ)2
                         Lp (Âµ; x) =                    2eÏ€                             (56)
                                                 n

y la funciÃ³n de verosimilitud perfilada de Ïƒ 2 es
                                                 (               )
                                   âˆ’n/2        Î£   (x  âˆ’ x)2
                                                     i   i
                  Lp Ïƒ 2 ; x = 2Ï€Ïƒ 2        exp âˆ’                                       (57)
                                                        2Ïƒ 2

    En este caso, considerando las funciones de verosimilitudes perfiladas; los EMV
coinciden con los estimadores usuales cÃ¡lculados a partir de la funciÃ³n de verosi-
militud genuina.

   La funciÃ³n de verosimilitud perfilada tambiÃ©n se utiliza bastante en modelos
con errores en las variables, donde el nÃºmero de parÃ¡metros de perturbaciÃ³n crece
con el tamaÃ±o de la muestra (parÃ¡metros incidentales). Presentamos un ejemplo
de este modelo.

Ejemplo 13. Modelo con errores en las variables. Considere (Y1 , X1 ), . . . , (Yn , Xn )
una muestra aleatoria cuya relaciÃ³n entre Yi y Xi es dada por Yi = Î± + Î²xi + ei y
Xi = xi +ui , siendo ei âˆ¼ N (0, Î») y ui âˆ¼ N (0, Îº) variables aleatorias independientes
para todo i = 1, . . . , n. El logaritmo de la funciÃ³n de verosimilitud (`) para este

                                           Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

ReducciÃ³n de modelos en la presencia de parÃ¡metros de perturbaciÃ³n                        117

modelo es ilimitado, y es necesario hacer algunas suposiciones extras para limitarla.
Suponiendo que la razÃ³n Ïƒ = Î»/Îº es conocida, el logaritmo de la funciÃ³n de
verosimilitud se torna limitado y dado por

                                                           n
                                                           X
                       `(Î¸1 , Î¸2 ) = log L(Î¸1 , Î¸ 2 ) =          `i (Î¸1 , Î¸2 )           (58)
                                                           i=1


siendo

                       1         1         (yi âˆ’ Î± âˆ’ Î²xi )2   (Xi âˆ’ xi )2
     `i (Î¸ 1 , Î¸2 ) âˆ âˆ’ log(ÏƒÎº) âˆ’ log(Îº) âˆ’                  âˆ’                            (59)
                       2         2               2ÏƒÎº              2Îº


    AquÃ­, Î¸1 = (Î±, Î², Îº)> es el vector de parÃ¡metros de interÃ©s y Î¸ 2 = (x1 , . . . , xn )>
es el vector de parÃ¡metros incidentales (de perturbaciÃ³n). El estimador de mÃ¡xima
verosimilitud para xi estÃ¡ dado por


                            b2 (Î¸1 ) = x    ÎºÎ²(Yi âˆ’ Î±) + ÏƒÎºXi
                            Î¸          bi =                                              (60)
                                                Î² 2 Îº + ÏƒÎº


   Sustituyendo (60) en la log-verosimilitud genuina (58), tenemos

                                            n
                                            X                     
                              `p (Î¸ 1 ) =                  b2 (Î¸1 )
                                                  `pi Î¸1 , Î¸                             (61)
                                            i=1


siendo

                                                            2            2
                                                           xi
            b2 (Î¸ 1 ) âˆ âˆ’ 1 log(ÏƒÎº) âˆ’ 1 log(Îº) âˆ’ yi âˆ’ Î± âˆ’ Î²b
  `pi Î¸ 1 , Î¸                                                    âˆ’
                                                                   Xi âˆ’ xbi
                                                                                         (62)
                          2           2               2ÏƒÎº             2Îº


    Los EMV para Î±, Î² y Îº, cuando Ïƒ es conocida, se obtienen igualando a cero
las derivadas de `p en relaciÃ³n con los parÃ¡metros de interÃ©s. Los estimadores son
dados por

                    Î±       b
                    b = Y âˆ’ Î²X
                                            q                 2
                                   2
                            SY âˆ’ ÏƒSX +                    2
                                                  SY2 âˆ’ ÏƒSX        âˆ’ 4ÏƒSY2 X
                     Î²b =
                                        2SY X                                            (63)
                                          2
                        Xn   Yi âˆ’ Î±   b i
                                  b âˆ’ Î²X
                     b=
                     Îº                  
                        i=1   2n Î²b2 + Ïƒ


                                            Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

118                                  Rafael Farias, GermÃ¡n Moreno & Alexandre Patriota

siendo,
                                           n
                                     1X
                            Y =            Yi
                                     n i=1
                                           n
                                     1X
                            X=             Xi
                                     n i=1
                                     1 X         2
                                           n
                            2
                           SX =            Xi âˆ’ X
                                     n i=1
                                     1 X         2
                                        n
                           SY2 =           Yi âˆ’ Y
                                     n i=1
                                     1 X              
                                        n
                         SY X =            Yi âˆ’ Y Xi âˆ’ X
                                     n i=1

    Patefield (1978) mostrÃ³ que el EMV Îº     b converge en probabilidad para Îº/2.
En este caso, el estimador consistente es dado por 2b   Îº. Mak (1982) estudiÃ³ las
propiedades de los estimadores en presencia de parÃ¡metros incidentales. El autor
demostrÃ³ que el estimador del vector de parÃ¡metros de interÃ©s existe y converge
para una distribuciÃ³n normal multivariada con media igual al vector de parÃ¡metros
de interÃ©s, si se satisfacen las condiciones
                    n                                                n
              1X                                            1X              2
          lÄ±Ìm      xi = lÄ±Ìm xn < âˆž,                    lÄ±Ìm     (xi âˆ’ xn ) < âˆž
          nâ†’âˆž n         nâ†’âˆž                             nâ†’âˆž n
                i=1                                           i=1

y
                                               n
                                               X
                                           1
                             lÄ±Ìm                     |xi |2+Î´ = 0
                            nâ†’âˆž n1+Î´/2
                                                i=1

para todo Î´ > 0.
   Mak (1982) tambiÃ©n demostrÃ³ que la matriz de covarianzas asintÃ³tica de los pa-
rÃ¡metros de interÃ©s no es la inversa de la matriz de informaciÃ³n de Fisher esperada
y debe ser sustituida por la siguiente matriz
                            
                      Cov Î¸  b1 = 1 A(Î¸1 )âˆ’1 V (Î¸ 1 )A(Î¸1 )âˆ’1
                                    n
siendo,                                                                      !
                                          
                          1          âˆ‚`p                       1   âˆ‚ 2 `p
                 V (Î¸1 ) = Var                 y      A(Î¸ 1 ) = E
                          n          âˆ‚Î¸1                       n  âˆ‚Î¸1 âˆ‚Î¸t1

   Para terminar, resaltamos que las principales ventajas de usar la funciÃ³n de
verosimilitud perfilada cuando el nÃºmero de parÃ¡metros de perturbaciÃ³n no crece
con el tamaÃ±o de la muestra son:

    â€¢ La funciÃ³n de verosimilitud perfilada siempre existe.

                                           Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

ReducciÃ³n de modelos en la presencia de parÃ¡metros de perturbaciÃ³n                 119

   â€¢ La funciÃ³n de verosimilitud perfilada no depende del parÃ¡metro de pertur-
     baciÃ³n.

   â€¢ La matriz de informaciÃ³n observada perfilada de Î¸1 se define de forma anÃ¡lo-
     ga a la informaciÃ³n observada de (Î¸1 , Î¸2 ).

   â€¢ El estadÃ­stico de la razÃ³n de verosimilitudes tiene distribuciÃ³n asintÃ³tica
     igual a la basada en la razÃ³n de verosimilitudes genuinas, esto es,
                                                  
                                       c               D
                       Wp (Î¸1 ) = 2 Lp Î¸1 âˆ’ Lp (Î¸ 1 ) âˆ’â†’ Ï‡2 (p1 )

                   D
     siendo que âˆ’â†’ significa convergencia en distribuciÃ³n y p1 la dimensiÃ³n de
     Î¸1 .

Las demostraciones de estas propiedades estÃ¡n en Cordeiro (1992).
    La principal desventaja es que la funciÃ³n de verosimilitud perfilada, general-
mente, no presenta todas las propiedades de una funciÃ³n de verosimilitud genuina.
Por ejemplo, la esperanza de la funciÃ³n escore perfilada generalmente es diferente
de cero. Por tanto, los estimadores obtenidos vÃ­a funciÃ³n de verosimilitud perfilada
pueden no ser consistentes. Por tanto, es necesario hacer ajustes en la verosimilitud
perfilada para minimizar estos problemas. En la literatura, existen varias modifica-
ciones para la funciÃ³n de verosimilitud perfilada propuestas por diversos autores;
ver Barndorff-Nielsen (1983), Barndorff-Nielsen (1991), Cox & Reid (1987), Cox
& Reid (1992) y McCullagh & Tibshirani (1990). Estas modificaciones consisten
en la incorporaciÃ³n de un tÃ©rmino en la verosimilitud perfilada anterior al proceso
de estimaciÃ³n que tiene por efecto disminuir el sesgo de la funciÃ³n escore y de la
informaciÃ³n de Fisher esperada.


5. Conclusiones
    En este trabajo presentamos y discutimos algunos mÃ©todos de estimaciÃ³n en
presencia de parÃ¡metros de perturbaciÃ³n. Como existen diversas metodologÃ­as en
la literatura para tratar tales modelos, enfocamos nuestra atenciÃ³n en tÃ©cnicas de
reducciÃ³n de modelos a travÃ©s de estadÃ­sticos con propiedades Ã³ptimas o a travÃ©s de
funciones de verosimilitudes canÃ³nicas y perfiladas. Ilustramos y analizamos algu-
nos conceptos sobre ausencia de informaciÃ³n presente en la muestra con relaciÃ³n a
los parÃ¡metros de perturbaciÃ³n en ejemplos simples y recientemente discutidos en
la literatura. A los interesados, dejamos las referencias para que sean consultadas
posteriormente.


Agradecimentos
    Durante el desarrollo de este trabajo los autores recibieron apoyo financiero
del Conselho Nacional de Desenvolvimento CientÃ­fico e TecnolÃ³gico (CNPq), de la

                                     Revista Colombiana de EstadÃ­stica 32 (2009) 99â€“121

120                             Rafael Farias, GermÃ¡n Moreno & Alexandre Patriota

FundaÃ§Ã£o de Amparo Ã  Pesquisa do Estado de SÃ£o Paulo (FAPESP), Brasil, y de la
Universidad Industrial de Santander, Colombia. Los autores tambiÃ©n expresan sus
agradecimientos al profesor Dr. Heleno Bolfarine (IME-USP) por las sugerencias
metodolÃ³gicas, a la profesora Dra. Silvia Ferrari (IME-USP) por la motivaciÃ³n
para escribir este trabajo, al profesor Dr. Bernardo Mayorga (UIS) por la revisiÃ³n
de estilo y a los dos Ã¡rbitros por las valiosas sugerencias dadas para mejorar el
presente documento.
Referencias
Azzalini A.A Class of Distributions which Includes the Normal Ones.(1985).Scandinavian Journal of Statistics.
Barndorff Nielsen O.On a Formula for the Distribution of the Maximum Likelihood Estimator.(1983).Biometrika.
Barndorff Nielsen O.Likelihood Theory.(1991).Chapman and Hall.London.
Cordeiro G.IntroduÃ§Ã£o Ã  Teoria de VerossimilhanÃ§a.(1992).SimpÃ³sio Nacional de Probabilidade e EstatÃ­stica.Rio de Janeiro.
Cox D R,Reid N.Parameter Orthogonality and Approximate Conditional Inference.(1987).Journal The Royal Statistical Society.
Cox D R,Reid N.A Note on the Difference Between Profile and Modified Profile Likelihood.(1992).Biometrika.
Durrans S R.Distributions of Fractional Order Statistics in Hydrology.(1992).Water Resources Research.
Fuller W A.Measurement Error Models.(1987).Wiley.New York.
Halmos P R,Savage L J.Application of the Radonâ€“Nikodym Theorem to the Theory of Sufficient Statistics.(1949).Annals of Mathematics Statistics.
Hinde J,Aitkin M.Canonical Likelihoods: A New Likelihood Treatment of Nuisance Parameters.(1987).Biometrika.
Jones M C.Families of Distributions Arising from Distributions of Order Statistics.(2004).Test.
Jorgensen B.A Review of Conditional Inference: Is there a Universal Definition of Noinformation?.(1993).Bulletin of International Statistical Institute.
Lehmann E L,Casella G.Theory of Point Estimation.(1998).Springer-Verlag.New York.
Lindsey J K.Parametric Statistical Inference.(1996).Clarendon Press.Oxford.
Mak T K.Estimation in the Presence of Incidental Parameters.(1982).The Canadian Journal of Statistics, La Revue Canadienne de Statistique.
McCullagh P,Tibshirani R.A Simple Method for the Adjustment of Profile Likelihoods.(1990).Journal The Royal Statistical Society.
Neyman J,Scott E L.Consistent Estimates Based on Partially Consistent Observations.(1948).Econometrica.
Pace L,Salvan A.Principles of Statistical Inference.(1997).World Scientific.Singapore.
Patefield W M.The Unreplicated Ultrastructural Relation: Large Sample Properties.(1978).Biometrika.