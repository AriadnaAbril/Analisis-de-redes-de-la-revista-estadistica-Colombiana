Modelado de parejas aleatorias usando cÃ³pulas
Universidad AutÃ³noma Metropolitana Unidad Iztapalapa
Resumen
Las cÃ³pulas se han convertido en una herramienta Ãºtil para el modelado multivariado tanto estocÃ¡stico como estadÃ­stico. En este artÃ­culo se revisan propiedades fundamentales de las cÃ³pulas que permitan caracterizar la estructura de dependencia de familias de distribuciÃ³n bivariadas definidas por la cÃ³pula. TambiÃ©n se describen algunas clases de cÃ³pulas, enfatizando en la importancia de la cÃ³pula Gaussiana y la familia Arquimediana. Se resalta la utilidad de las cÃ³pulas para el modelado de parejas de variables aleatorias continuas y el de las discretas. La aplicaciÃ³n de la cÃ³pula se ilustra con la construcciÃ³n de modelos de regresiÃ³n de Markov de primer orden para respuestas no Gaussianas.
Palabras clave: dependencia, cÃ³pula, medida de asociaciÃ³n,estadÃ­stica aplicada, Ï„ de Kendall, Ï de Spearman, correlaciÃ³n serial.
IntroducciÃ³n
    Las cÃ³pulas bidimensionales son funciones bivariadas que juntan o bien â€œcopu-
lanâ€ dos funciones de distribuciÃ³n univariadas para construir funciones de distribu-
ciÃ³n bivariadas continuas. La cÃ³pula representa una forma paramÃ©trica conveniente
para modelar la estructura de dependencia en distribuciones conjuntas de varia-
bles aleatorias, en particular para parejas de variables aleatorias. Varias cÃ³pulas
con diversas formas estÃ¡n disponibles para representar a familias de distribuciones
bivariadas.
    El uso de la cÃ³pula es atractivo, pues permite una gran flexibilidad para mo-
delar la distribuciÃ³n conjunta de una pareja aleatoria que pueda surgir de prÃ¡c-
ticamente cualquier disciplina, y lo hace de forma sencilla ya que solo se necesita
especificar la funciÃ³n que copula y las marginales. Las cÃ³pulas pueden extraer la
estructura de dependencia de la funciÃ³n de distribuciÃ³n conjunta de un vector de
variables aleatorias y, al mismo tiempo, permiten separar la estructura de depen-
dencia del comportamiento marginal. Al igual que en el caso univariado, es posible
usar transformaciones que permitan crear funciones de distribuciÃ³n bivariadas dis-
cretas a partir de las distribuciones continuas; de esta forma, puede aprovecharse
la cÃ³pula cuando el objetivo es modelar parejas aleatorias discretas.
    Las cÃ³pulas fueron presentadas originalmente por Sklar (1959), quien resolviÃ³
algunos problemas formulados por M. FrÃ©chet sobre la relaciÃ³n entre una funciÃ³n de
distribuciÃ³n de probabilidad multidimensional y sus marginales de menor dimen-
siÃ³n. En la actualidad, las cÃ³pulas se han convertido en una poderosa herramienta
de modelado multivariado en muchos campos de la investigaciÃ³n donde la depen-
dencia entre varias variables aleatorias, continuas o discretas, es de gran interÃ©s, y
para las cuales la suposiciÃ³n de normalidad multivariada puede ser cuestionable.
    Algunos ejemplos del modelado de parejas aleatorias continuas se pueden en-
contrar en aplicaciones biomÃ©dicas donde el interÃ©s puede centrarse en los tiempos
de ocurrencia de una enfermedad en Ã³rganos pares (e.g. Wang & Wells 2000),
o en los tiempos de ocurrencia de un evento cuando este se clasifica en dos ti-
pos de causas mutuamente excluyentes, i.e. datos de riesgos concurrentes (e.g.
Carriere 1995, Escarela & Carriere 2003). En ambos casos, cuando se construye la
funciÃ³n de distribuciÃ³n bivariada correspondiente, es importante asignar funcio-
nes de distribuciÃ³n marginales del tipo de supervivencia como la Exponencial, la
Weibull o la Burr, que permitan hacer comparaciones entre sus ajustes; ademÃ¡s,
es preponderante entender el mecanismo de dependencia de las parejas aleatorias.
    Otras disciplinas que se favorecen de la utilizaciÃ³n de las cÃ³pulas son la hi-
drologÃ­a y los cÃ¡lculos actuariales. La primera porque generalmente los fenÃ³menos
hidrolÃ³gicos son multidimensionales; por tanto, se requiere modelar conjuntamente
diferentes procesos (e.g. Genest & Favre 2007), mientras que la segunda lo hace en
el anÃ¡lisis de portafolios de seguros -por mencionar un ejemplo-, donde el interÃ©s
puede centrarse en la estimaciÃ³n de la distribuciÃ³n conjunta de los montos corres-
pondientes a dos tipos de indemnizaciÃ³n (e.g. Klugman & Parsa 1999); a este tipo
de datos se les conoce en la literatura anglosajona como loss data.

                                       Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

Modelado de parejas aleatorias con cÃ³pulas                                               35

    Una ilustraciÃ³n para parejas aleatorias discretas, la cual tambiÃ©n se puede be-
neficiar del modelo de cÃ³pula, proviene del anÃ¡lisis de series de tiempo discretas,
como el nÃºmero de casos mensuales de una enfermedad casi erradicada: la polio-
mielitis (e.g. Escarela et al. 2006). Cuando se tienen variables aleatorias discretas
correlacionadas en serie de la forma AR(1), las funciones de distribuciÃ³n margi-
nales en la distribuciÃ³n conjunta de las variables aleatorias adyacentes deben de
tener la misma forma y -al igual que su contraparte continua- es crucial usar una
estructura de dependencia. En la literatura existen muy pocas distribuciones bi-
variadas para parejas aleatorias discretas con estas propiedades; las distribuciones
construidas con la cÃ³pula discretizada son unas de ellas.
    Los lectores que buscan mÃ¡s aplicaciones encontrarÃ¡n en los artÃ­culos de Frees
& Valdez (1998) y de Clemen & Reilly (1999) una revisiÃ³n mÃ¡s detallada. En
cuanto al asunto de la implementaciÃ³n, el trabajo de Jan (2007) y las rutinas en el
lenguaje de distribuciÃ³n gratuita R expuestas ahÃ­ pueden facilitar la programaciÃ³n
de los modelos.
    El propÃ³sito de este artÃ­culo es exponer algunas propiedades importantes de
las cÃ³pulas y algunos detalles para su aplicaciÃ³n. En la segunda secciÃ³n se define la
cÃ³pula y se revisan propiedades fundamentales de las cÃ³pulas, las cuales permiten
caracterizar la estructura de dependencia de familias de distribuciÃ³n bivariadas
definidas por la cÃ³pula. En la tercera secciÃ³n se revisan los conceptos de correlaciÃ³n,
concordancia y dependencia. En la cuarta secciÃ³n se describen tres familias de
cÃ³pula relevantes en la literatura y se dan algunas consideraciones sobre la selecciÃ³n
de la cÃ³pula; ademÃ¡s, se muestran algunos contornos de funciones de densidad
bivariadas generadas a travÃ©s de varias clases de cÃ³pulas. La quinta secciÃ³n muestra
dos ejemplos, los cuales se enfocan en el modelado de series de tiempo AR(1)
para respuestas diferentes a las Gaussianas. El primero consiste en comparar el
ajuste de varios modelos de cÃ³pula a respuestas de valor extremo, mientras que
el segundo presenta la representaciÃ³n de una matriz de transiciÃ³n para series de
tiempo binarias en presencia de informaciÃ³n concomitante; en ambas ilustraciones
se trata de emular al modelo AR(1) para respuestas Gaussianas.


2. La cÃ³pula
2.1. DefiniciÃ³n estadÃ­stica de cÃ³pula
    Una cÃ³pula bidimensional es una funciÃ³n de distribuciÃ³n bivariada de un vector
aleatorio V = (V1 , V2 ) cuyas marginales V1 y V2 son uniformes en el intervalo
I = (0, 1). Es decir, una cÃ³pula es una funciÃ³n C : I2 â†’ I que satisface las
siguientes condiciones:

1) de acotamiento
                                   lÄ±Ìm C(v1 , v2 ) = v3âˆ’j                              (1)
                                  vj â†’1âˆ’

                                     lÄ±Ìm C(v1 , v2 ) = 0                               (2)
                                    vj â†’0

   donde j = 1, 2 y (v1 , v2 )T âˆˆ I2 , y

                                           Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

36                                                  Gabriel Escarela & AngÃ©lica HernÃ¡ndez

2) de incremento
                     C(u2 , v2 ) âˆ’ C(u2 , v1 ) âˆ’ C(u1 , v2 ) + C(u1 , v1 ) â‰¥ 0
     para toda u1 , u2 , v1 , v2 âˆˆ I tal que u1 â‰¤ u2 y v1 â‰¤ v2 .

   La importancia de las cÃ³pulas en estadÃ­stica matemÃ¡tica se describe en el si-
guiente teorema (e.g. Nelsen 1999).
Teorema 1. (Teorema Sklar) Sean Y1 , Y2 variables aleatorias con funciÃ³n de dis-
tribuciÃ³n conjunta F , con marginales F1 y F2 respectivamente. Entonces existe
una cÃ³pula C tal que satisface
                                                                             (3)
                                                            
                          F (y1 , y2 ) = C F1 (y1 ), F2 (y2 )
para toda y1 , y2 âˆˆ R. Si F1 y F2 son continuas, entonces C es Ãºnica; de otra forma
C estÃ¡ determinada en forma Ãºnica sobre el rango F1 Ã— rango F2 . Inversamente,
si C es una cÃ³pula y F1 , F2 son funciones de distribuciÃ³n, entonces F definida en
la ecuaciÃ³n (3) es una funciÃ³n de distribuciÃ³n conjunta con marginales F1 y F2 .

    Este teorema establece que, en el contexto de parejas aleatorias continuas, es
posible construir una funciÃ³n de distribuciÃ³n bivariada en tÃ©rminos de dos fun-
ciones de distribuciÃ³n continuas univariadas y una cÃ³pula que permite relaciones
de dependencia entre dos variables aleatorias individuales. Una demostraciÃ³n del
teorema de Sklar puede encontrarse en Schweizer & Sklar (1983). Las cÃ³pulas
pueden emplearse para definir distribuciones bivariadas con marginales discretas,
de manera que satisfagan la ecuaciÃ³n (3); sin embargo, en contraste con el caso
continuo, no hay una forma Ãºnica para expresar la distribuciÃ³n conjunta de dos
variables aleatorias discretas como una funciÃ³n de sus distribuciones marginales
(ver Denuit & Lambert 2005). Cuando las variables son discretas, la unicidad solo
se encuentra en rango F1 Ã— rango F2 .
Corolario 1. Dada una funciÃ³n de distribuciÃ³n conjunta F con marginales con-
tinuas F1 y F2 , como estÃ¡ indicado en el teorema de Sklar, es fÃ¡cil construir la
cÃ³pula correspondiente como se muestra a continuaciÃ³n:
                                                       
                                       (âˆ’1)      (âˆ’1)
                      C(v1 , v2 ) = F F1 (v1 ), F2 (v2 )
                                                                h         i
          (âˆ’1)                                                    (âˆ’1)
donde Fj         es la funciÃ³n cuasi-inversa de Fj , dada por Fj Fj    (u) = u si u âˆˆ
                      (âˆ’1)
rango Fj , o por Fj    (u) = sup{z | Fj (z) â‰¤ u} si u âˆˆ / rango Fj , para j = 1, 2;
aquÃ­ las cuasi-inversas se usan para funciones de distribuciÃ³n no estrictamente
crecientes. NÃ³tese que si Y1 y Y2 son variables aleatorias continuas con funciones
de distribuciÃ³n F1 y F2 , respectivamente, entonces C es la funciÃ³n de distribuciÃ³n
conjunta de V1 = F1 (Y1 ) y V2 = F2 (Y2 ) ya que F1 (Y1 ) y F2 (Y2 ) se distribuyen
uniformemente en I.

   La desigualdad de las cotas FrÃ©chet-Hoeffding indica que si F es una funciÃ³n
de distribuciÃ³n bivariada con marginales F1 y F2 , entonces
           maÌx{F1 (y1 ) + F2 (y2 ) âˆ’ 1, 0} â‰¤ F (y1 , y2 ) â‰¤ mÄ±Ìn{F1 (y1 ), F2 (y2 )}

                                           Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

Modelado de parejas aleatorias con cÃ³pulas                                               37

    Este resultado es consecuencia del teorema de Sklar. En tÃ©rminos de cÃ³pulas,
la desigualdad puede expresarse como (FrÃ©chet 1951)

     W (v1 , v2 ) = maÌx{v1 + v2 âˆ’ 1, 0} â‰¤ C(v1 , v2 ) â‰¤ mÄ±Ìn{v1 , v2 } = M (v1 , v2 )

la cual se conoce como desigualdad de las cotas de FrÃ©chet e indica que cualquier
cÃ³pula C representa un modelo de dependencia que se encuentra entre los extremos
W y M . Las funciones W y M son conocidas como las cotas inferior y superior,
respectivamente; de hecho, FrÃ©chet (1951) demuestra que W y M son tambiÃ©n
cÃ³pulas.


2.2. La funciÃ³n de densidad de la cÃ³pula y la cÃ³pula de
     supervivencia
    Si F1 (y1 ), F2 (y2 ) y la cÃ³pula C(v1 , v2 ) son diferenciables, la densidad conjunta
de (Y1 , Y2 ), correspondiente a la funciÃ³n de distribuciÃ³n conjunta en la ecuaciÃ³n
(3), puede expresarse como
                                                                            
                       f (y1 , y2 ) = f1 (y1 )f2 (y2 ) Ã— c F1 (y1 ), F2 (y2 )

donde f1 (y1 ) y f2 (y2 ) son las funciones de densidad marginales correspondientes,
y
                                   âˆ‚ 2 C(v1 , v2 )
                     c(v1 , v2 ) =                    (v1 , v2 )T âˆˆ (0, 1)2      (4)
                                      âˆ‚v1 âˆ‚v2
es la funciÃ³n de densidad de la cÃ³pula. Como consecuencia, se tiene que la funciÃ³n
de densidad condicional de Y2 dada Y1 puede expresarse convenientemente de la
siguiente forma:
                                                                                 (5)
                                                                       
                       f2|1 (y2 | y1 ) = f2 (y2 ) Ã— c F1 (y1 ), F2 (y2 )

    En varias situaciones donde el objetivo es el modelado, es mÃ¡s conveniente ha-
blar de la funciÃ³n de supervivencia conjunta, la cual se define como S(y1 , y2 ) =
Pr{Y1 > y1 , Y2 > y2 }, en lugar de la funciÃ³n de distribuciÃ³n conjunta F (y1 , y2 );
esta representaciÃ³n es particularmente relevante cuando se tienen parejas de va-
riables aleatorias positivas. En forma anÃ¡loga a como se construye la funciÃ³n
              conjunta, si se dan dos funciones de supervivencia marginales
de distribuciÃ³n
Sj (yj ) = Pr Yj > yj , con j = 1, 2, estas pueden ser â€œcopuladasâ€ para formar una
funciÃ³n de supervivencia conjunta, como se muestra a continuaciÃ³n (e.g. Wang &
Wells 2000)
                                                                                 (6)
                                                            
                           S(y1 , y2 ) â‰¡ C S1 (y1 ), S2 (y2 )

    La funciÃ³n de densidad conjunta correspondiente a la funciÃ³n de supervivencia
definida en la ecuaciÃ³n (6) es:
                                                                        
                   f (y1 , y2 ) = f1 (y1 )f2 (y2 ) Ã— c S1 (y1 ), S2 (y2 )

donde f1 (y1 ) y f2 (y2 ) son las funciones de densidad marginales correspondientes
a S1 (y1 ) y S2 (y2 ), respectivamente, y c(Â·, Â·) es la cÃ³pula de densidad definida en la
ecuaciÃ³n (4).

                                         Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

38                                               Gabriel Escarela & AngÃ©lica HernÃ¡ndez

2.3. El caso discreto
    El potencial de las cÃ³pulas no se reduce al caso continuo. Cuando el modelado
es para parejas de variables aleatorias discretas, se puede obtener una funciÃ³n
de probabilidad bivariada al tomar la derivada Radon-Nikodym de F (y1 , y2 ) en
la ecuaciÃ³n (3) con respecto a la medida contable. De esta forma, la funciÃ³n de
probabilidad conjunta de una pareja de variables aleatorias discretas (Y1 , Y2 ) puede
representarse en tÃ©rminos de la versiÃ³n discretizada de la cÃ³pulaPy de las funciones
de distribuciÃ³n marginales, las cuales tienen la forma Fj (yj ) = zâ‰¤yj fj (z), donde
fj (yj ) = Pr Yj = yj representa la funciÃ³n de probabilidad marginal de Yj para
             

j = 1, 2, como se muestra a continuaciÃ³n (Song 2000):

      Pr{Y1 = y1 , Y2 = y2 } = C(u1 , u2 ) âˆ’ C(u1 , v2 ) âˆ’ C(v1 , u2 ) + C(v1 , v2 )

aquÃ­ uj = Fj (yj ) y vj = Fi (yj âˆ’ 1) para j = 1, 2.
    Cuando se trata de representar la familia de distribuciones condicionales de
Y2 | Y1 , esta toma la forma
                        n                                           o.
       F2|1 (y2 | y1 ) = C F1 (y1 ), F2 (y2 ) âˆ’ C F1 (y1 âˆ’ 1), F2 (y2 )    f1 (y1 )


3. Medidas de correlaciÃ³n, concordancia y depen-
   dencia
    Cuando los modelos de cÃ³pula son usados para construir una distribuciÃ³n con-
junta de una pareja aleatoria continua, pueden verse como versiones de funciones
de distribuciÃ³n conjuntas libres de marginales que tienen la poderosa habilidad
de capturar propiedades de dependencia invariante al reescalamiento de las pare-
jas aleatorias (ver e.g. RodrÃ­guez-Lallena & Ãšbeda Flores 2004); aquÃ­, invariante
al reescalamiento significa que las propiedades y las medidas se quedan sin cam-
biar cuando se realizan transformaciones estrictamente crecientes a las variables
aleatorias. De esta forma, las medidas de asociaciÃ³n invariantes bajo reescalamien-
to, como las de concordancia, pueden estudiarse sin necesidad de especificar las
distribuciones marginales.
    AdemÃ¡s de la concordancia, existen varios conceptos de correlaciÃ³n, asociaciÃ³n
y dependencia, importantes para entender el modelado de parejas aleatorias usando
las cÃ³pulas (ver e.g. Lehmann 1966, Barlow & Proschan 1975, Nelsen 1991). En
esta secciÃ³n se describen algunos de los conceptos y medidas correspondientes, los
cuales son relevantes en la literatura.


3.1. El coeficiente de correlaciÃ³n
    El coeficiente de correlaciÃ³n es la forma mÃ¡s tradicional para cuantificar la
relaciÃ³n de dos variables aleatorias. Este mide la fuerza y direcciÃ³n de una relaciÃ³n
lineal entre dos variables aleatorias.

                                         Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

Modelado de parejas aleatorias con cÃ³pulas                                             39

Axioma 1. Sean Y1 , Y2 variables aleatorias con varianzas finitas. Entonces el
coeficiente de correlaciÃ³n de Pearson, se define como
                                Cov(Y1 , Y2 )
             Cor(Y1 , Y2 ) = p        p
                              Var[Y1 ] Var[Y2 ]
                                                           
                                  E Y1 âˆ’ E[Y1 ] Y2 âˆ’ E[Y2 ]
                           =n
                                           2 o1/2 n             2 o1/2
                              E Y1 âˆ’ E[Y1 ]         E Y2 âˆ’ E[Y2 ]

   Entre las propiedades del coeficiente de correlaciÃ³n de Pearson se encuentra
que su rango estÃ¡ en el intervalo [âˆ’1, 1], con valores Â±1 si y solo si Y1 = a + bY2 .
Este coeficiente es simÃ©trico, es decir, Cor(Y1 , Y2 ) = Cor(Y2 , Y1 ), y no cambia bajo
transformaciones lineales; esto es, Cor (Y1 , f (Y2 )) = Cor(Y1 , Y2 ) cuando f (y) =
a + b y, donde b > 0.
    Como las cÃ³pulas permiten un camino fÃ¡cil en el estudio de la dependencia en-
tre variables aleatorias y son invariantes al reescalamiento, entonces el coeficiente
de correlaciÃ³n de Pearson se usa con mÃ¡s frecuencia como medida de dependencia,
pues es mÃ¡s fÃ¡cil calcular y es un parÃ¡metro importante en distribuciones elÃ­p-
ticas; de hecho, a menudo se emplea en la familia normal multivariada y en la
distribuciÃ³n t-Student multivariada. Sin embargo, para algunos casos de parejas
aleatorias continuas cuyas distribuciones no son elÃ­pticas, como es el caso de dis-
tribuciones construidas con ciertas cÃ³pulas. La utilidad de este coeficiente es poca,
pues su valor depende no solo de la cÃ³pula, sino tambiÃ©n de las marginales, ya que
Cor(f (Y1 ), f (Y2 )) 6= Cor(Y1 , Y2 ) cuando f (y) no es lineal; es decir, esta medida no
siempre es invariante al reescalamiento.


3.2. Medidas de concordancia
    Cuando se considera una pareja de variables aleatorias, es Ãºtil saber quÃ© tanto
tienden a estar asociados valores grandes de una de las variables aleatorias con va-
lores grandes de la otra, y que tanto estÃ¡n asociados valores pequeÃ±os de una con
valores pequeÃ±os de la otra. Una formalizaciÃ³n de la idea intuitiva de este grado
de asociaciÃ³n fue propuesta por Yanagimoto & Okamoto (1969), quienes propo-
nen el uso del orden de concordancia de distribuciones bivariadas con marginales
univariadas dadas de acuerdo con la fuerza de su asociaciÃ³n positiva, el cual se
denota por â‰º. Este orden estocÃ¡stico se define a continuaciÃ³n.
Axioma 2. Dadas dos parejas aleatorias (X1 , Y1 ) y (X2 , Y2 ) con marginales idÃ©n-
ticas, se dice que (X2 , Y2 ) es mÃ¡s concordante que (X1 , Y1 ), y se denota (X1 , Y1 ) â‰º
(X2 , Y2 ), si
                     Pr{X1 â‰¤ s, Y1 â‰¤ t} â‰¤ Pr{X2 â‰¤ s, Y2 â‰¤ t}
para toda s, t âˆˆ R.

   Es importante seÃ±alar que el uso de las medidas de concordancia permiten
construir estimaciones fiables cuando se asume que la cÃ³pula pertenece a una
familia paramÃ©trica especÃ­fica.

                                         Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

40                                                   Gabriel Escarela & AngÃ©lica HernÃ¡ndez

    El tÃ©rmino medida de asociaciÃ³n se refiere a una medida de concordancia, un
concepto desarrollado por Scarsini (1984) y presentado por Nelsen (1999) como se
define a continuaciÃ³n.
Axioma 3. Una medida numÃ©rica Îº de asociaciÃ³n entre dos variables aleatorias
continuas Y1 y Y2 , cuya cÃ³pula es C, es una medida de concordancia si:

     1. Îº esta definida para cualquier pareja de variables aleatorias continuas
     2. Îº âˆˆ [âˆ’1, 1] con Îº(Y, Y ) = 1 y Îº(Y, âˆ’Y ) = âˆ’1
     3. Îº(Y1 , Y2 ) = Îº(Y2 , Y1 )
     4. si Y1 y Y2 son independientes entonces Îº(Y1 , Y2 ) = 0
     5. Îº(âˆ’Y1 , Y2 ) = Îº(Y1 , âˆ’Y2 ) = âˆ’Îº(Y1 , Y2 )
     6. si dos parejas aleatorias estÃ¡n representadas por las cÃ³pulas C1 y C2 de
        manera tal que C1 â‰º C2 , y si Îºi denota la mediciÃ³n de concordancia corres-
        pondiente a la cÃ³pula Ci , donde i = 1, 2, entonces Îº1 â‰¤ Îº2
     7. si {Yn } es una sucesiÃ³n de parejas aleatorias continuas con cÃ³pula Cn y
        medida de concordancia Îºn y si {Cn } converge a C cuya medida de concor-
        dancia es Îº, entonces lÄ±Ìmnâ†’âˆ Îºn = Îº.

   A continuaciÃ³n se describen dos medidas de asociaciÃ³n importantes en la lite-
ratura estadÃ­stica, las cuales satisfacen la definiciÃ³n de concordancia.

3.2.1. La Ï„ de Kendall

Axioma 4. Sean (X1 , Y1 ) y (X2 , Y2 ) vectores aleatorios independientes e idÃ©nti-
camente distribuidos, tales que (Xi , Yi ) âˆ¼ F , i = 1, 2. Entonces, la Ï„ de Kendall
se define como
               h                              i
       Ï„ = E signo (X1 âˆ’ X2 ) (Y1 âˆ’ Y2 )
                                                  
           = Pr (X1 âˆ’ X2 )(Y1 âˆ’ Y2 ) > 0 âˆ’ Pr (X1 âˆ’ X2 )(Y1 âˆ’ Y2 ) < 0
             =    Pr {concordancia} âˆ’ Pr {discordancia}
                      
             =    2 Pr (X1 âˆ’ X2 ) (Y1 âˆ’ Y2 ) > 0 âˆ’ 1
             =    2 Pr {concordancia} âˆ’ 1

    La versiÃ³n de la Ï„ de Kendall de las entradas de una pareja aleatoria continua
(Y1 , Y2 ), dada en tÃ©rminos de la cÃ³pula C, puede expresarse como
                               Z Z
                         Ï„ =4        C(v1 , v2 ) dC(v1 , v2 ) âˆ’ 1               (7)
                                      I2


   La ecuaciÃ³n (7) indica que la Ï„ de Kendall estÃ¡ completamente determinada
por la cÃ³pula y no estÃ¡ relacionada con las distribuciones marginales de (Y1 , Y2 ).

                                           Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

Modelado de parejas aleatorias con cÃ³pulas                                              41

3.2.2. La Ï de Spearman

    La Ï de Spearman, al igual que la Ï„ de Kendall, es una medida de asociaciÃ³n
que satisface la definiciÃ³n de concordancia. Esta medida de asociaciÃ³n puede ser
definida como el coeficiente de correlaciÃ³n de Pearson, pero no aplicado a las
variables aleatorias Y1 , Y2 , sino a sus rangos V1 = F1 (Y1 ) y V2 = F2 (Y2 ).
Axioma 5. Sean (X1 , Y1 ), (X2 , Y2 ) y (X3 , Y3 ) vectores aleatorios independientes
e idÃ©nticamente distribuidos, tales que (Xi , Yi ) âˆ¼ H, i = 1, 2, 3. La Ï de Spearman
se define como
            h                                                                i
       Ï = 3 Pr (X1 âˆ’ X2 )(Y1 âˆ’ Y3 ) â‰¥ 0 âˆ’ Pr (X1 âˆ’ X2 )(Y1 âˆ’ Y3 ) < 0

   Las variables aleatorias V1 y V2 son uniformes en I = [0, 1]; ademÃ¡s E(V1 ) =
E(V2 ) = 1/2 y Var(V1 ) = Var(V2 ) = 1/12. Si C es la funciÃ³n de distribuciÃ³n
conjunta de U y V , como se especifica en la cÃ³pula, entonces se tiene que
                        E [V1 V2 ] âˆ’ E [V1 ] E [V2 ]     E [V1 V2 ] âˆ’ 1/4
                    Ï=    p          p               =
                            Var[V1 ] Var[V2 ]                  1/12
                                                 Z Z
                      = 12E (V1 V2 ) âˆ’ 3 = 12            v1 v2 dC âˆ’ 3                  (8)
                                                      I2
                           Z Z
                                                       
                      = 12         C (v1 , v2 ) âˆ’ v1 v2 dv1 dv2
                                 I2


3.3. Medidas de dependencia
    Un inconveniente de la definiciÃ³n dada en la secciÃ³n 3 sobre la concordancia es
que la cuarta propiedad indica que si las dos variables aleatorias son independien-
tes, entonces la medida es igual a cero, pero no viceversa. RÃ©nyi (1959) estableciÃ³
un marco axiomÃ¡tico que, ademÃ¡s de considerar la situaciÃ³n mencionada, formali-
za el concepto de medida de dependencia. A continuaciÃ³n se define dicha medida
usando la colecciÃ³n de condiciones relevantes de los axiomas de RÃ©nyi.
Axioma 6. Una medida numÃ©rica Î´ de dos variables aleatorias continuas Y1 y Y2
es una medida de dependencia si satisface las siguientes condiciones:

  1. Î´ estÃ¡ definida para cualquier pareja aleatoria Y = (Y1 , Y2 )
  2. Î´(Y1 , Y2 ) = Î´(Y2 , Y1 )
  3. Î´ âˆˆ [0, 1]
  4. Î´ = 0 si y solo si Y1 y Y2 son independientes
  5. Î´ = 1 si y solo si la variable aleatoria Y3âˆ’i es una funciÃ³n estrictamente
     monÃ³tona de Yi casi seguramente, para i = 1, 2

                                                   sobre el rango
  6. Si f y g son funciones estrictamente monÃ³tonas                Y1 y rango Y2 ,
     respectivamente, casi seguramente, entonces Î´ f (Y1 ), g(Y2 ) = Î´(Y1 , Y2 )

                                          Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

42                                               Gabriel Escarela & AngÃ©lica HernÃ¡ndez

     7. Si las parejas aleatorias Y y Yn , n = 1, . . ., tienen funciones de distribu-
        ciones conjuntas H y Hn respectivamente, y si la sucesiÃ³n {Hn } converge
        dÃ©bilmente a H, entonces lÄ±Ìmnâ†’âˆ Î´(Yn ) = Î´(Y).

    Es posible demostrar que el valor absoluto de la medida de concordancia de
Speraman, |Ï|, satisface las condiciones 1 a 7 mencionadas arriba con la importan-
te excepciÃ³n del punto 4. El mismo resultado es aplicable para el valor absoluto
de la medida de concordancia de Kendall, |Ï„ |. El valor absoluto del coeficiente de
correlaciÃ³n de Pearson satisface las condiciones 1, 2 y 3; las condiciones 5 y 6 las
satisface si y solo si las funciones f y g son lineales; y no satisface las condiciones
4 y 7. Existen varias medidas de dependencia que satisfacen las condiciones da-
das. Estas medidas estÃ¡n principalmente basadas en la distancia de la distribuciÃ³n
conjunta de la pareja aleatoria en cuestiÃ³n y el producto de las distribuciones mar-
ginales correspondientes. Una ilustraciÃ³n de una medida de dependencia bivariada
basada en dicha distancia se presenta a continuaciÃ³n.

3.3.1. La Ïƒ de Schweizer y Wolff

    El integrando de la ecuaciÃ³n (8), que define a la Ï de Spearman, representa el
volumen con signo entre las superficies v3 = C(v1 , v2 ) y v3 = v1 v2 . Schweizer &
Wolff (1981) notaron que las variables aleatorias Y1 y Y2 son independientes si y
sÃ³lo si C(v1 , v2 ) = v1 v2 ; entonces una medida adecuada de distancia normalizada
entre v3 = C(v1 , v2 ) y v3 = v1 v2 , como la norma L1 , podrÃ­a resultar una medida de
dependencia que satisface las siete condiciones dadas en la definiciÃ³n de medida de
dependencia. Esta medida, se conoce como la Ïƒ de Schweizer y Wolff, y se define
como                              Z Z
                          Ïƒ = 12        |C(v1 , v2 ) âˆ’ v1 v2 | dv1 dv2             (9)
                                    I2


3.4. Dependencia en las colas
    Diversas cÃ³pulas pueden caracterizar de manera diferente la dependencia en el
centro de una distribuciÃ³n. De igual forma, hay muchas situaciones en las que se
requiere cuantificar la estructura de la dependencia asintÃ³tica de datos bivariados
para estimar las probabilidades de eventos raros. La siguiente definiciÃ³n es un
ejemplo de estas medidas (ver e.g. Embrechts et al. 2002, Charpentier & Juri 2006),
y las referencias citadas allÃ­.
Axioma 7. Sean Y1 y Y2 variables aleatorias continuas con funciÃ³n de distribuciÃ³n
conjunta F , cÃ³pula C y marginales F1 y F2 . El coeficiente de dependencia de cola
superior de Y1 y Y2 estÃ¡ dado por

                   Î»u = lÄ±Ìmâˆ’ Pr Y2 > F2âˆ’1 (u) | Y1 > F1âˆ’1 (u)
                                
                        uâ†’1
                      = lÄ±Ìm Pr Y1 > F1âˆ’1 (u) | Y2 > F2âˆ’1 (u)
                                
                           uâ†’1âˆ’
                                  1 âˆ’ 2u + C(u, u)
                        = lÄ±Ìm
                           uâ†’1âˆ’        1âˆ’u


                                         Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

Modelado de parejas aleatorias con cÃ³pulas                                             43

siempre y cuando el lÃ­mite Î»u exista.

   El coeficiente Î»u mide la dependencia entre valores extremos de Y1 y Y2 ; en
particular, la dependencia de cola superior contesta la pregunta: â€œsuponiendo que
un valor extremo impacta a Y1 , Â¿cuÃ¡l es la probabilidad de que un valor extremo
impacte a Y2 ?â€. Si Î»u = 0, entonces entre Y1 y Y2 no existe dependencia de cola
superior; de otra forma, Y1 y Y2 son dependientes en la cola superior. En forma
similar, el coeficiente de la cola inferior se define como Î»l = lÄ±Ìmvâ†’0+ C(v, v)/v.



4. Familias de cÃ³pulas
    Si se tiene una colecciÃ³n de cÃ³pulas, entonces puede emplearse el Teorema
de Sklar para construir distribuciones bivariadas con marginales arbitrarias. Una
cantidad importante de familias de cÃ³pulas puede ser encontrada en la literatura.
Estas familias no son equivalentes en tÃ©rminos del tipo de dependencia estocÃ¡stica
que ellas representan o el grado de dependencia que ellas pueden capturar. Como
resultado, un problema esencial es la elecciÃ³n de la familia de cÃ³pulas para cons-
truir una distribuciÃ³n bivariada particular. Aunque se puede usar gran variedad
de familias de cÃ³pulas para modelar dependencia, en este artÃ­culo se adoptan la
familia Morgenstern por su simplicidad, la familia Gaussiana por su semejanza
con la distribuciÃ³n Gaussiana bivariada, y la familia Arquimediana por ser con-
venientemente representada a travÃ©s de una funciÃ³n univariada. A continuaciÃ³n
se describen estas tres familias de cÃ³pulas. Una revisiÃ³n mÃ¡s completa de estas y
otras familias puede encontrarse en los textos de Joe (1997) y de Nelsen (1999).


4.1. La cÃ³pula de Morgenstern

   La cÃ³pula de Morgenstern es representada por
                                                          
            CÎ± (v1 , v2 ) = v1 v2 1 + Î± (1 âˆ’ v1 ) (1 âˆ’ v2 ) ,    Î± âˆˆ [âˆ’1, 1]

   La funciÃ³n de densidad de la cÃ³pula correspondiente es
                                                                
                       cÎ± (v1 , v2 ) = 1 + Î± (1 âˆ’ 2v1 )(1 âˆ’ 2v2 )

    La Ï„ de Kendall correspondiente estÃ¡ dada por Ï„ = 2Î±/9, por lo que Ï„ âˆˆ
[âˆ’2/9, 2/9], lo cual indica que esta cÃ³pula tiene un rango de concordancia muy
limitado y entonces su aplicabilidad es Ãºtil para parejas aleatorias las cuales es-
tÃ¡n asociadas modestamente. De hecho, esta cÃ³pula se obtiene a partir de una
perturbaciÃ³n de la cÃ³pula de independencia dada por C(v1 , v2 ) = v1 v2 (vere.g.
Joe 1997). Esta cÃ³pula no exhibe dependencia de cola superior y tampoco inferior,
i.e. Î»u = Î»l = 0.

                                         Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

44                                             Gabriel Escarela & AngÃ©lica HernÃ¡ndez

4.2. La familia Gaussiana
    La funciÃ³n de distribuciÃ³n bivariada que pertenece a la cÃ³pula Gaussiana tiene
la forma

            Cr (v1 , v2 ) = Î¦2 Î¦âˆ’1 (v1 ), Î¦âˆ’1 (v2 ) , (v1 , v2 )T âˆˆ (0, 1)2   (10)
                                                  

donde Î¦2 (Â·, Â·) es la funciÃ³n de distribuciÃ³n conjunta de una Gaussiana bivariada
con media (0, 0)T y matriz de covarianza R, igual a una matriz no singular de 2 Ã— 2
cuyos elementos fuera de la diagonal son cada uno iguales a r, con r âˆˆ (âˆ’1, 1), y
los elementos en la diagonal son iguales a uno, y Î¦âˆ’1 (Â·) es la funciÃ³n inversa de la
distribuciÃ³n acumulada Gaussiana estÃ¡ndar. La funciÃ³n de densidad de la cÃ³pula
estÃ¡ dada por:
                                         Ï†2 Î¦âˆ’1 (v1 ), Î¦âˆ’1 (v2 )
                                                               
                        cr (v1 , v2 ) =  âˆ’1                                  (11)
                                       Ï† Î¦ (v1 ) Ã— Ï† Î¦âˆ’1 (v2 )
donde Î¦ y Ï† denotan, respectivamente, las funciones de distribuciÃ³n y densidad
de la Gaussiana estÃ¡ndar univariada, y Ï†2 denota la densidad bivariada de la
Gaussiana definida por:
                                                 
                                         1
           Ï†2 (z) = (2Ï€)âˆ’1 |R|âˆ’1/2 exp âˆ’ zT Râˆ’1 z ,       zT âˆˆ R2
                                         2

   Note que si Y1 y Y2 estÃ¡n normalmente distribuidas, la funciÃ³n de densidad
conjunta resultante generada con la cÃ³pula Gaussiana se reduce a la usual funciÃ³n
de densidad normal bivariada.
   La Ï„ de Kendall y la Ï de Spearman para la cÃ³pula Normal estÃ¡n dadas res-
pectivamente por:
                                                       
                      2                        6        r
                 Ï„r = arcsen(r)     y    Ïr = arcsen
                      Ï€                        Ï€        2

   Conforme a la ecuaciÃ³n (5), se tiene que la funciÃ³n de densidad de Y2 | Y1
correspondiente a la cÃ³pula Gaussiana estÃ¡ dada por

                                        Ï†2 Î¦âˆ’1 (F1 (y1 )) , Î¦âˆ’1 (F2 (y2 ))
                                                                         
           f2|1 (y2 | y1 ) = f2 (y2 ) Ã—  âˆ’1                              
                                       Ï† Î¦ (F1 (y1 )) Ã— Ï† Î¦âˆ’1 (F2 (y2 ))

     DespuÃ©s de algo de Ã¡lgebra laboriosa, se puede demostrar que
                                              (                       )
                                                 1 (s2 âˆ’ r s1 )2
                                                   
                                f2 (y2 )                            2
             f2|1 (y2 | y1 ) = âˆš         Ã— exp âˆ’                 âˆ’ s2
                                 1 âˆ’ r2          2    1 âˆ’ r2

donde si = Î¦âˆ’1 (Fi (yi ) para i = 1, 2.
                       

    El uso de la cÃ³pula Gaussiana bivariada es atractivo ya que codifica la depen-
dencia en la misma forma en que la distribuciÃ³n normal bivariada lo hace usando
el parÃ¡metro de dependencia r, con la diferencia de que se calcula para variables
aleatorias con cualesquiera marginales arbitrarias. Esta cÃ³pula tiene la capacidad

                                       Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

Modelado de parejas aleatorias con cÃ³pulas                                                   45

de capturar el rango completo de dependencia, ya que incluye tanto las cÃ³pulas de
cota superior e inferior de FrÃ©chet como el modelo de independencia. Este Ãºltimo
caso se obtiene cuando r = 0 y define la cÃ³pula de independencia. Una desven-
taja de esta cÃ³pula es que para Ï < 1, la dependencia en las colas es nula, i.e.
Î»u = Î»l = 0; cuando Ï = 1, entonces Î»u = Î»l = 1. Para mayor informaciÃ³n so-
bre la cÃ³pula Gaussiana multivariada, el lector puede referirse a los artÃ­culos de
Clemen & Reilly (1999) y de Song (2000).


4.3. CÃ³pulas Arquimedianas
   Como se ha mencionado, las cÃ³pulas proveen una estructura general para mo-
delar distribuciones bivariadas. Una familia de cÃ³pulas que permite este modelado
a travÃ©s de una sola funciÃ³n univariada es la Arquimediana. A continuaciÃ³n se
enuncia la definiciÃ³n de cÃ³pula Arquimediana dada por Genest & Rivest (1993).
Axioma 8. Una cÃ³pula es llamada Arquimediana si esta puede ser expresada en
la forma
           C(v1 , v2 ) = z âˆ’1 z(v1 ) + z(v2 ) , (v1 , v2 )T âˆˆ (0, 1)2  (12)
                                            

para alguna funciÃ³n convexa decreciente z definida en (0, 1] que satisface
z âˆ’1 (1) = 0; por convenciÃ³n z âˆ’1 (v) = 0 cuando v â‰¥ z(0).

    Las condiciones dadas en la definiciÃ³n de cÃ³pula arquimediana son necesarias
y suficientes para que la cÃ³pula en la ecuaciÃ³n (12) sea una funciÃ³n de distribuciÃ³n
bivariada (Schweizer & Sklar 1983). Estas condiciones equivalen a que 1 âˆ’ z âˆ’1 (v)
debe ser una funciÃ³n de distribuciÃ³n unimodal en [0, âˆ) con moda en 0. AquÃ­, la
funciÃ³n z es conocida como el generador.
    La familia de cÃ³pulas Arquimediana permite    la definiciÃ³n
                                                              Râˆde modelos generados
por la transformada de Laplace z(s) = E exp(âˆ’sW ) = 0 eâˆ’Î¸t dH(Î¸), corres-
                                                        

pondiente a una variable aleatoria W , la tan citada variable frailty, que es una
variable aleatoria no negativa cuya funciÃ³n de distribuciÃ³n es H. En el contexto
de variables aleatorias positivas continuas, Oakes (1989) demostrÃ³ que para la clase
de funciones de distribuciÃ³n conjunta definidas por la cÃ³pula en la ecuaciÃ³n (12),
las variables aleatorias Y1 y Y2 son condicionalmente independientes dada la varia-
ble aleatoria W , por lo que F (y1 , y2 | W = w) = F1 (y1 | W = w)F2 (y2 | W = w).
   Para encontrar la funciÃ³n de densidad de la cÃ³pula c(v1 , v2 ) definida en la
ecuaciÃ³n (4), defÃ­nase z(C) = z(v1 ) + z(v2 ), y derivando respecto a v1 ,
                                               âˆ‚C
                                     z 0 (C)       = z 0 (v1 )
                                               âˆ‚v1
   Derivando esta expresiÃ³n respecto a v2 , se tiene que:
                                    âˆ‚C âˆ‚C              âˆ‚2C
                         z 00 (C)           + z 0 (C)         =0
                                    âˆ‚v1 âˆ‚v2           âˆ‚v1 âˆ‚v2
   Por tanto
                                                z 00 (C)z 0 (v1 )z 0 (v2 )
                           c(v1 , v2 ) = âˆ’                     3
                                                        z 0 (C)

                                               Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

46                                             Gabriel Escarela & AngÃ©lica HernÃ¡ndez

   Dado (Y1 , Y2 ) un par de variables aleatorias con distribuciÃ³n F definida en
(12), se puede demostrar que la Ï„ de Kendall correspondiente tiene la siguiente
forma (ver e.g. Genest & MacKay 1986b):
                                   Z 1
                                        z(t)
                              Ï„ =4        0 (t)
                                                dt + 1
                                     0  z

    De esta forma, el valor de Ï„ estÃ¡ relacionado en forma lineal con el Ã¡rea bajo la
curva de z(t)/z 0 (t) entre 0 y 1. Note que la cota inferior de FrÃ©chet correspondiente
es z(t)/z 0 (t) = t âˆ’ 1; cuando z(t)/z 0 (t) tiende a cero, se obtiene la cota superior
de FrÃ©chet. De hecho, Genest & MacKay (1986a) notan que la convergencia de
una sucesiÃ³n de distribuciones bivariadas construidas con cÃ³pulas arquimedianas
puede determinarse al observar la grÃ¡fica de z(t)/z 0 (t) de la sucesiÃ³n.
    La familia de cÃ³pulas Arquimediana ha recibido la atenciÃ³n de la comunidad
estadÃ­stica debido a que su representaciÃ³n y otras cantidades importantes estÃ¡n
dadas en tÃ©rminos de la funciÃ³n z(s). En aplicaciones a finanzas, por mencionar
un ejemplo, esta familia es ampliamente usada, pues varios modelos de cÃ³pula
pueden ser implementados y comparados a travÃ©s de funciones dadas en tÃ©rminos
de z(s) (ver e.g. Whelan 2004). A continuaciÃ³n se describen dos cÃ³pulas de la
familia Arquimediana que sobresalen en la literatura.

4.3.1. La cÃ³pula de Frank

    La cÃ³pula de Frank, cuyo generador es z(t) = âˆ’ log (eâˆ’Î¸t âˆ’ 1)/(eâˆ’Î¸ âˆ’ 1) , estÃ¡
                                                                          

definida por
                                                      !
                                 eâˆ’Î¸v1 âˆ’ 1 eâˆ’Î¸v2 âˆ’ 1
                                          
                        1
       CÎ¸ (v1 , v2 ) = âˆ’ log 1 +                         ,     Î¸ âˆˆ R âˆ’ {0}
                        Î¸              (eâˆ’Î¸ âˆ’ 1)

    El uso de la cÃ³pula de Frank es atractivo ya que puede capturar el rango
completo de dependencia; esto es, al igual que la cÃ³pula Gaussiana, la cÃ³pula de
Frank incluye las cÃ³pulas de cota superior de FrÃ©chet cuando Î¸ â†’ âˆ’âˆ, de cota
inferior de FrÃ©chet cuando Î¸ â†’ âˆ, y de independencia cuando Î¸ â†’ 0. De hecho,
cuando se trata de inferencia, algunos estadÃ­sticos prefieren usar la cÃ³pula de
Frank a la Gaussiana, ya que mientras ambas cÃ³pulas tienen propiedades similares
(ver e.g. Carriere 1995), la cÃ³pula de Frank proporciona cantidades cerradas y,
por tanto, mÃ¡s fÃ¡ciles de programar (ver e.g. Escarela & Carriere 2003). El uso
de la cÃ³pula de Frank no es recomendable para modelar dependencia de eventos
extremos pues no es dependiente en las colas superior ni inferior.
    Para evaluar el grado de asociaciÃ³n entre las marginales en el modelo generado
por la cÃ³pula de Frank, la Ï„ de Kendall correspondiente estÃ¡ dada por:
                                                          !
                                  4       1 Î¸ t
                                            Z
                         Ï„Î¸ = 1 âˆ’     1âˆ’               dt
                                  Î¸       Î¸ 0 et âˆ’ 1

   La integral en esta expresiÃ³n no tiene soluciÃ³n analÃ­tica; sin embargo, es posible
usar mÃ©todos numÃ©ricos, como el de cuadraturas Gauss-Kronrod, que pueden dar

                                       Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

Modelado de parejas aleatorias con cÃ³pulas                                                    47

buenas aproximaciones. La Ï„ de Kendall de la cÃ³pula de Frank toma valores en el
rango completo de concordancia. Observando los casos especiales de la cÃ³pula de
Frank, se puede comprobar que lÄ±ÌmÎ¸â†’âˆ’âˆ Ï„Î¸ = âˆ’1, lÄ±ÌmÎ¸â†’âˆ Ï„Î¸ = 1 y lÄ±ÌmÎ¸â†’0 Ï„Î¸ = 0.

4.3.2. La cÃ³pula positiva estable

   La funciÃ³n de distribuciÃ³n bivariada presentada por Hougaard (1986) toma la
forma
                        h                             iÎ¸ 
                                     1/Î¸           1/Î¸
    CÎ¸ (v1 , v2 ) = exp âˆ’ (âˆ’ log v1 ) + (âˆ’ log v2 )         , Î¸ âˆˆ (0, 1)  (13)

El generador de esta cÃ³pula es z(t) = (âˆ’ log t)1/Î¸ y la densidad de cÃ³pula corres-
pondiente estÃ¡ dada por
                                 1 âˆ‚CÎ¸ âˆ‚CÎ¸  âˆ’1
                                            (Î¸ âˆ’ 1)(âˆ’ log CÎ¸ )âˆ’1 + 1
                                                                     
               cÎ¸ (v1 , v2 ) =
                                 CÎ¸ âˆ‚v1 âˆ‚v2
donde                                           Î¸1 âˆ’1
                         âˆ‚CÎ¸           log vj             CÎ¸
                             =                               ,   j = 1, 2
                         âˆ‚vj           log CÎ¸             vj
     La cÃ³pula positiva estable exhibe una asimetrÃ­a porque hay un cluster de va-
lores hacia la cola derecha pero con colas poco pesadas. Esta cÃ³pula es Ãºtil para
modelar variables aleatorias asociadas en forma positiva (ver e.g. Nelsen 1999,
Joe 1997); valores pequeÃ±os de Î¸ proveen dependencia positiva alta entre Y1 y
Y2 , i.e. lÄ±ÌmÎ¸â†’1 CÎ¸ (v1 , v2 ) = M (v1 , v2 ), mientras que valores grandes de Î¸ proveen
asociaciones cercanas a independencia, i.e. lÄ±ÌmÎ¸â†’1 CÎ¸ (v1 , v2 ) = v1 v2 .
    En lo que respecta a las medidas de asociaciÃ³n para esta familia de cÃ³pulas,
la Ï„ de Kendall es definida por Ï„Î¸ = 1 âˆ’ Î¸, mientras que la Ï de Sperman no
tiene forma cerrada. NÃ³tese que el rango de la Ï„ de Kendall se encuentra en el
intervalo (0, 1), por lo que la cÃ³pula positiva estable solo considera concordancias
positivas. Esta cÃ³pula exhibe dependencia asimÃ©trica en las colas con dependencia
nula en la cola inferior Î»l = 0, y dependencia en la cola superior Î»u = 2 âˆ’ 2Î¸ . A
la cÃ³pula positiva estable tambiÃ©n se le conoce como la cÃ³pula de valor extremo,
ya que provee modelos flexibles para parejas aleatorias cuyas entradas representan
mÃ¡ximos de series de tiempo estacionarias (ver e.g. Dupuis 2005).


4.4. SelecciÃ³n y comparaciÃ³n de cÃ³pulas
4.4.1. Sobre la selecciÃ³n de cÃ³pulas

    Escoger una cÃ³pula para ajustar un conjunto de datos dado es un problema
importante pero difÃ­cil; de hecho, no hay un mÃ©todo que la comunidad estadÃ­stica
use rutinariamente. En los Ãºltimos aÃ±os se han propuesto varios mÃ©todos para
seleccionar una cÃ³pula particular. AnÃ© & Kharoubi (2003) muestran un mÃ©todo de
selecciÃ³n basado en comparaciones paramÃ©tricas y no paramÃ©tricas a travÃ©s de un
estimador de distancia. Huard et al. (2006) proponen un mÃ©todo bayesiano basado

                                                Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

48                                            Gabriel Escarela & AngÃ©lica HernÃ¡ndez

en la Ï„ de Kendall para seleccionar la cÃ³pula mÃ¡s probable de las dadas en un con-
junto. Dobric & Schmid (2007) presentan una prueba de bondad de ajuste basada
en la transformaciÃ³n de Rosenblatt; cuando las marginales son especificadas, la
prueba funciona bien, pero cuando Ã©stas son estimadas empÃ­ricamente, la prueba
no es tan Ãºtil. Genest & Rivest (1993) propusieron un procedimiento no paramÃ©-
trico que sin tomar en cuenta a las marginales estima la funciÃ³n que determina
a una cÃ³pula arquimediana. Este procedimiento representa una estrategia para
seleccionar la familia paramÃ©trica de cÃ³pulas arquimedianas que provee el mejor
ajuste posible para un conjunto de datos apareados. Una tÃ©cnica de selecciÃ³n de
cÃ³pulas arquimedianas similar, pero para modelar funciones de supervivencia en
presencia de datos con censura, fue propuesta por Wang & Wells (2000).
    En general, la selecciÃ³n de una cÃ³pula y de las marginales, en particular, de-
pende de la aplicaciÃ³n que se quiera dar. Un juicio informado que incluye rangos
de asociaciÃ³n, distribuciones marginales Ãºtiles para el problema en cuestiÃ³n y gra-
dos de dependencia en colas puede mejorar el modelado. Otra forma de evaluar si
conviene un modelo particular es usar un anÃ¡lisis de residuales. Por ejemplo, para
verificar las suposiciones de los modelos AR(1) de la ilustraciÃ³n en la siguiente
secciÃ³n, donde se usa un marco de mÃ¡xima verosimilitud, se pueden usar los resi-
duales propuestos por Dunn & Smyth (1996), los cuales pueden ser graficados de
varias formas, y decidir visualmente si el ajuste es adecuado. La selecciÃ³n de la
cÃ³pula y la evaluaciÃ³n de la bondad de ajuste son temas actuales de investigaciÃ³n.

4.4.2. RepresentaciÃ³n grÃ¡fica de las cÃ³pulas

    Las densidades de las cÃ³pulas pueden ser graficadas con diagramas de superficie
para ver la diferencia entre las diversas cÃ³pulas; sin embargo, para tener una mejor
visiÃ³n de lo que hacen las cÃ³pulas, en la prÃ¡ctica es Ãºtil observar las funciones de
densidad correspondientes a una funciÃ³n de distribuciÃ³n o funciÃ³n de superviven-
cia construida por una cÃ³pula con un grado de asociaciÃ³n predeterminado y dos
marginales dadas en la literatura.
    La figura 1 muestra los contornos de las funciones de densidad correspondientes
a la funciÃ³n de supervivencia bivariada definida por la ecuaciÃ³n (6), usando las
cÃ³pulas Gaussiana, de Frank y positiva estable para asociaciones dÃ©bil (Ï„ = 0.1),
moderada (Ï„ = 0.4) y fuerte (Ï„ = 0.7), con funciones de supervivencia marginales
Weibull parametrizadas como
                                    n       a o
                      Sj (yj ) = exp âˆ’ bj yj j ,       j = 1, 2

donde b1 = 0.028, a1 = 2, b2 = 0.039 y a2 = 1.5.
    Observando los contornos en la figura 1, es evidente que los parÃ¡metros aj y bj
solo controlan las transformaciones de escala y potencia. El efecto del parÃ¡metro
de dependencia es mÃ¡s influyente y puede resumirse de igual forma para las tres
familias de cÃ³pulas. Cuando el grado de asociaciÃ³n es bajo, los tres contornos
tienen formas similares. En general, cuando el grado de asociaciÃ³n se incrementa,
los contornos son atraÃ­dos al origen y concentrados alrededor de la lÃ­nea y1 âˆ’
y2 = constante. La diferencia principal entre los contornos de las distintas cÃ³pulas

graficadas en la figura 1 es la forma del achatamiento de la densidad cuando
el valor de Ï„ se incrementa. En particular, es posible notar que el contorno de
la cÃ³pula positiva estable para dependencia alta tiende a ser mÃ¡s angosto para
valores grandes de y1 y y2 ; esta caracterÃ­stica puede atribuirse a la propiedad de
dependencia en la cola superior que posee esta cÃ³pula.


5. IlustraciÃ³n: modelado de cadenas de Markov
5.1. El contexto de una cadena de Markov de orden 1
    Un proceso de Markov estacionario de primer orden a tiempo discreto cuyo
espacio de estados es continuo puede construirse a partir de una distribuciÃ³n bi-
variada dada F (y1 , y2 ) = Pr{Y1 â‰¤ y1 , Y2 â‰¤ y2 }, la cual corresponde a un vector
aleatorio conjuntamente continuo (Y1 , Y2 ) con ambas distribuciones marginales
univariadas iguales a la distribuciÃ³n estacionaria. La distribuciÃ³n de transiciÃ³n,
definida como F2|1 (y2 | y1 ) = Pr{Y2 â‰¤ y2 | Y1 = y1 }, puede calcularse como (ver
e.g. SchÃ¤be 1997):
                                                     
                                        âˆ‚F (y1 , y2 ) âˆ‚F (y1 , âˆ)
                      F2|1 (y2 | y1 ) =
                                           âˆ‚y1           âˆ‚y1

donde F (y1 , âˆ) denota la funciÃ³n de distribuciÃ³n marginal de Y1 . Si el espacio de
estados del proceso es finito o un conjunto contable, la distribuciÃ³n de transiciÃ³n
estÃ¡ dada por (ver e.g. Joe 1997):
                                                  X f (y1 , z)
                              F2|1 (y2 | y1 ) =
                                                         f1 (y1 )
                                                  zâ‰¤y2

donde f (y1 , y2 ) es una funciÃ³n de probabilidad conjunta con ambas funciones de
probabilidad marginales igual a f1 (Â·).
    Un ejemplo importante es el caso de una sucesiÃ³n de variables aleatorias nor-
males cuya respuesta actual depende del valor de la variable aleatoria inmediata
anterior, como se describe a continuaciÃ³n. Considere la serie detiempo estacionaria
{Yt , t = 1, 2, . . .} con respuestas marginales Yt âˆ¼ N Î²T xt , Ïƒ 2 , para t = 1, 2, . . .,
entonces Î²T xt es la esperanza marginal de Yt , xt es un vector de variables explica-
tivas en el tiempo t, Î² es el vector de coeficientes de regresiÃ³n y Ïƒ 2 es la varianza
marginal de las respuestas. Si la correlaciÃ³n entre las respuestas adyacentes Ytâˆ’1
y Yt es r, el modelo de transiciÃ³n tiene la siguiente especificaciÃ³n:
                                                             
                   Yt | Ytâˆ’1 âˆ¼ N Î² T xt + r Ytâˆ’1 âˆ’ Î²T xtâˆ’1 , Î½ 2                  (14)
                                            


donde Î½ 2 = Ïƒ 2 1 âˆ’ r2 , y |r| < 1.
                      

    Usando la funciÃ³n de densidad condicional de la ecuaciÃ³n (5) en tÃ©rminos de
una funciÃ³n de densidad de cÃ³pula y una marginal dada, Yt âˆ¼ f , se puede cons-
truir un modelo de transiciÃ³n para respuestas continuas en una forma similar al

                                                   CÃ³pula positiva estable

         Figura 1: Contornos de la funciÃ³n de densidad conjunta resultante cuando se utilizan
                   las cÃ³pulas Gaussiana, de Frank y positiva estable para marginales Weibull
                   con parÃ¡metros b1 = 0.028, a1 = 2, b2 =0.039, a2 = 1.5 para asociaciones
                   baja (Ï„ = 0.1), moderada (Ï„ = 0.4) y alta (Ï„ = 0.7).




         modelo de transiciÃ³n Gaussiano dado por la ecuaciÃ³n (14). Por ejemplo, si se
         desea obtener el modelo autoregresivo de primer orden en tÃ©rminos del modelo
         de transiciÃ³n de cÃ³pula, simplemente se necesita proveer la funciÃ³n de densidad

                                                           Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

Modelado de parejas aleatorias con cÃ³pulas                                           51

                                                       (11),i la funciÃ³n de distribu-
de la cÃ³pula Gaussiana cÏ (Â·, Â·) dada enh la ecuaciÃ³n.
ciÃ³n marginal estandarizada F (yt ) = Î¦ yt âˆ’ Î²T xt         Ïƒ y la densidad marginal
                âˆ’1/2      n             2 .     o
f (yt ) = 2Ï€Ïƒ 2       Ã— exp âˆ’ yt âˆ’ Î²T xt      2Ïƒ 2 .
    Cuando los modelos de transiciÃ³n son empleados para respuestas discretas,
se puede definir la distribuciÃ³n de transiciÃ³n de Yt | Ytâˆ’1 al definir una funciÃ³n
de probabilidad condicional cuando la distribuciÃ³n de Yt estÃ¡ dada. Si f (yt ) =
Pr{Yt = yt } representa la funciÃ³n de probabilidad marginal de Yt , la familia de
distribuciones de transiciÃ³n de {Yt } se puede caracterizar usando    P la cÃ³pula biva-
riada discretizada y la funciÃ³n de distribuciÃ³n discreta F (yt ) = zâ‰¤yt f (z), como
se muestra a continuaciÃ³n:
                          n                                         o.
                                                                                   (15)
                                                
         F2|1 (yt | y2 ) = C F (y2 ), F (yt ) âˆ’ C F (y2 âˆ’ 1), F (yt )    f (y2 )

    Se tiene, en consecuencia, que la funciÃ³n de densidad de transiciÃ³n, la cual se
define como f2|1 (yt | y2 ) = Pr{Yt = yt | Y2 = y2 }, estÃ¡ dada por
                   n                                      
  f2|1 (yt | y2 ) = C F (yt ), F (y2 ) âˆ’C F (yt ), F (y2 âˆ’ 1) âˆ’
                                                                     o.
                                                                                   (16)
                                               
                     C F (yt âˆ’ 1), F (y2 ) + C F (yt âˆ’ 1), F (y2 âˆ’ 1)   f (y2 )


5.2. Cadenas de Markov de valor extremo de orden 1
    En disciplinas como la ciencia ambiental, el propÃ³sito principal de un estu-
dio es analizar datos que corresponden a extremos de algÃºn fenÃ³meno durante
varios periodos (e.g. Smith 1989). Muchas veces resulta poco verosÃ­mil suponer
independencia entre las observaciones de la serie de tiempo resultante. Un ejer-
cicio importante en este tipo de aplicaciones es determinar quÃ© tan robusta es la
elecciÃ³n de una cÃ³pula para modelar cadenas de Markov de primer orden {Yt }tâ‰¥1 ,
cuya distribuciÃ³n marginal pertenece a alguna distribuciÃ³n de valor extremo con
varios grados de dependencia entre los valores adyacentes Ytâˆ’1 y Yt . En esta ilus-
traciÃ³n se desea realizar una comparaciÃ³n cuantitativa de varios modelos de la
distribuciÃ³n condicional de Yt | Ytâˆ’1 generados con las cÃ³pulas, como se estableciÃ³
en la ecuaciÃ³n (5).
    Intuitivamente, la forma mÃ¡s adecuada de modelar una serie de tiempo de va-
lores extremos de primer orden serÃ­a escogiendo una cÃ³pula cuyos coeficientes de
cola inferior o superior sean diferentes a cero, dependiendo si se trata de mÃ­ni-
mos o mÃ¡ximos. Si los grados de asociaciÃ³n de las observaciones adyacentes son
altos, entonces es imperativo seleccionar una familia de cÃ³pulas con coeficientes
de cola positivos; sin embargo, si los grados de asociaciÃ³n de las observaciones
adyacentes no son muy altos, es posible que la elecciÃ³n de la cÃ³pula no sea tan
crucial. Para realizar la evaluaciÃ³n correspondiente, se simularon cadenas de Mar-
kov estacionarias de primer orden cuya estructura de dependencia entre Ytâˆ’1 y
Yt estÃ¡ caracterizada por la cÃ³pula positiva estable dada en la ecuaciÃ³n (13) y
cuya distribuciÃ³n marginal es Gumbel, representada por la siguiente funciÃ³n de

                                       Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

52                                                Gabriel Escarela & AngÃ©lica HernÃ¡ndez

distribuciÃ³n:

donde âˆ’âˆ < Âµ < âˆ y Ïƒ > 0.
    Para la simulaciÃ³n de datos con las caracterÃ­sticas que se acaban de describir
se usÃ³ la funciÃ³n evmc del paquete evd (Stephenson 2002) del lenguaje R (Ihaka
& Gentleman 1996). Por conveniencia, se fijaron Âµ = 0 y Ïƒ = 1; ademÃ¡s, se
seleccionaron tres grados de asociaciÃ³n entre las respuestas adyacentes: baja (Ï„Î¸ =
0.1), moderada (Ï„Î¸ = 0.4) y alta (Ï„Î¸ = 0.7). Se generaron muestras de tamaÃ±o
n = 400 para cada una de las estructuras de dependencia. Las tres series de tiempo
resultantes se muestran en la figura 2. Es posible observar cÃ³mo se va perdiendo la
aleatoriedad cuando la concordancia entre los datos adyacentes va aumentando.

Figura 2: Cadenas de Markov de primer orden simuladas con marginal Gumbel y cÃ³pula
          positiva estable para grados de asociaciÃ³n adyacentes (a) bajo (Ï„Î¸ = 0.1),
          (b) moderado (Ï„Î¸ = 0.4) y (c) alto (Ï„Î¸ = 0.7).

    Para construir los modelos de transiciÃ³n, caracterizados por la funciÃ³n de densi-
dad condicional f2|1 (yt | ytâˆ’1 ) dada por la ecuaciÃ³n (5), se seleccionaron las cÃ³pulas
positiva estable, de Frank y Gaussiana, dejando al parÃ¡metro de dependencia co-
mo desconocido; tambiÃ©n se seleccionÃ³ la distribuciÃ³n Gumbel caracterizada por
la ecuaciÃ³n (17), dejando como desconocidos los parÃ¡metros Âµ y Ïƒ. Para ajustar
los modelos resultantes, se usÃ³ la tÃ©cnica Q   de mÃ¡xima verosimilitud; en este caso
                                                 n
la funciÃ³n de verosimilitud es L = f (y1 ) k=2 f2|1 (yk | ykâˆ’1 ), donde f (y) es la
funciÃ³n de densidad correspondiente a la distribuciÃ³n marginal. La programaciÃ³n
correspondiente se realizÃ³ en el lenguaje R; para definir cada modelo de transiciÃ³n
se usÃ³ el paquete copula (Jan 2007), y para obtener los estimadores de mÃ¡xima
verosimilitud se usÃ³ la funciÃ³n optim de R para optimizar la funciÃ³n log verosimi-
litud.
    El ajuste de cada modelo puede ser evaluado mediante la comparaciÃ³n de las
funciones de densidad condicionales correspondientes a Yt | Ytâˆ’1 = yp , donde
yp = âˆ’ log(âˆ’ log p) es el p-Ã©simo percentil de la distribuciÃ³n marginal F ; i.e. yp
satisface F (yp ) = p para 0 â‰¤ p â‰¤ 1. En este estudio se seleccionaron densidades
condicionales correspondientes a p =0.05, p =0.5 y p =0.95. La figura 3 muestra
las densidades condicionales resultantes. Es posible observar que cuando los datos
muestran un grado de dependencia adyacente modesto, no hay mucha diferencia
entre la densidad condicional verdadera y el ajuste de los modelos de las cÃ³pulas
positiva estable y de Frank, lo cual sugiere que la elecciÃ³n entre estas dos cÃ³pulas
es robusta. Cuando se trata de datos con dependencia moderada, la cÃ³pula positiva
estable siempre observa un buen ajuste, mientras que la cÃ³pula Gaussiana tiene
un ajuste razonable Ãºnicamente cuando p = 0.05; para valores de p mÃ¡s grandes,
las cÃ³pulas de Frank y Gaussiana ofrecen un ajuste malo. En presencia de una
dependencia alta, solo la cÃ³pula positiva estable tiene un ajuste bueno, lo cual
sugiere que la elecciÃ³n de la cÃ³pula es bastante crucial.
    Note que el ajuste de la cÃ³pula Gaussiana no aparece para los datos de depen-
dencia baja y alta. Esto se debe a que la funciÃ³n de cÃ³pula que se usÃ³ presentaba
varios problemas numÃ©ricos cuando se trataba de optimizar la funciÃ³n log verosi-
militud. Estos problemas son formulados por Jan (2007) y no parecen tener una
soluciÃ³n trivial.

5.3. Un modelo AR(1) para una serie binaria

    Un modelo de transiciÃ³n de orden uno para una cadena de Markov de dos
estados puede ser construido fÃ¡cilmente al predeterminar la marginal F en las
ecuaciones (15) y (16) como la distribuciÃ³n Bernoulli con probabilidad de Ã©xito p,
y al escoger la funciÃ³n cÃ³pula Gaussiana Cr (u, v) con parÃ¡metro de dependencia
r, como se define en la ecuaciÃ³n (10).
    En el planteamiento de regresiÃ³n, es posible modelar las funciones de probabi-
lidad condicional de Yt , dada Ytâˆ’1 con una funciÃ³n de la variable explicativa xt .
AsÃ­, un modelo de transiciÃ³n mÃ¡s generalizado supone que la marginal F tiene la
siguiente funciÃ³n de distribuciÃ³n Bernoulli, la cual incluye la variable explicativa

                        Datos de asociaciÃ³n moderada y p=0.05                        Datos de asociaciÃ³n moderada y p=0.5                                                        Datos de asociaciÃ³n moderada y p=0.95

Figura 3: Densidades condicionadas a Ytâˆ’1 = yp de modelos de transiciÃ³n verdadero
          y ajustados a datos con dependencia adyacente baja, moderada y alta para
          distribuciones construidas con marginal Gumbel y cÃ³pulas positiva estable,
          de Frank y Gaussiana con, p =0.05, p =0.5 y p =0.95.



xt :
                                           F (yt ; xt ) = q(xt ) I[0,1) (yt ) + I[1,âˆ) (yt ),                                                                                         yt âˆˆ R
donde q(xt ) = 1 âˆ’ p(xt ), p(xt ) es la probabilidad de Ã©xito dada en tÃ©rminos del
vector de variables explicativas en el tiempo t, y IA (y) es la funciÃ³n indicadora de
A, la cual es igual a 1 si y âˆˆ A y es 0 de otra forma. Empleando las propiedades
de la cÃ³pula en las ecuaciones (1) y (2), la funciÃ³n de probabilidad de transiciÃ³n

                                                                                               Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

Modelado de parejas aleatorias con cÃ³pulas                                           55

Ã©sta dada por,

           pl|m = f2|1 (l | m) = Pr{Yt = l | Ytâˆ’1 = m},       l, m âˆˆ {0, 1}

y, usando la parametrizaciÃ³n escogida aquÃ­, esta puede representarse como
                                  
           p0|0 = Cr q(xt )q(xtâˆ’1 ) q(xtâˆ’1 )
                  n                        o.
           p0|1 = q(xt ) âˆ’ Cr q(xt )q(xtâˆ’1 )      p(xtâˆ’1 )
                  n                             o.
           p1|0 = q(xtâˆ’1 ) âˆ’ Cr q(xt ), q(xtâˆ’1 )     q(xtâˆ’1 )
                  n                                       o.
           p1|1 = 1 âˆ’ q(xtâˆ’1 ) âˆ’ q(xt ) + Cr q(xt )q(xtâˆ’1 )    p(xtâˆ’1 )

    Una forma conveniente de tomar en cuenta informaciÃ³n concomitante en este
modelo es usar la liga probiten el modelo de probabilidad marginal. Esta liga
implica que p(xt ) = Î¦ Î² T xt , donde Î¦ es la funciÃ³n de distribuciÃ³n acumula-
da normal estÃ¡ndar. Usando la propiedad  de simetrÃ­a de la distribuciÃ³n normal
                                      T
estÃ¡ndar, se tiene que q(xt ) = Î¦ âˆ’Î² xt , y por tanto, usando la cÃ³pula Gaussia-
                                                               
na, se obtiene que Cr q(xt ), q(xtâˆ’1 ) = Î¦2 âˆ’Î² T xt , âˆ’Î² T xtâˆ’1 . AquÃ­, Î¦2 denota
                                     

la funciÃ³n de distribuciÃ³n correspondiente a Ï†2 cuyo parÃ¡metro de dependencia
es igual a r. De esta forma, la elecciÃ³n de la cÃ³pula Gaussiana y de la marginal
Bernoulli con liga probit provee probabilidades de transiciÃ³n de la serie binaria
relativamente fÃ¡ciles de calcular; para hacerlo, es necesario contar con una rutina
que evalÃºe a la funciÃ³n de distribuciÃ³n Gaussiana bivariada Î¦2 . Una aplicaciÃ³n de
este modelo de transiciÃ³n puede encontrarse en Escarela et al. (2009).


6. Conclusiones
    En la actualidad en muchos campos se ha incrementado el interÃ©s en modelar
problemas con respuestas multivariadas. En este artÃ­culo se ha propuesto el uso de
la funciÃ³n cÃ³pula para el estudio de respuestas bivariadas el cual puede extenderse
al caso multivariado, ya sea para respuestas continuas o discretas. En particular,
este trabajo se ha enfocado a la relaciÃ³n de las cÃ³pulas y problemas estadÃ­sticos
aplicados. Debido a que las cÃ³pulas son familias paramÃ©tricas, el uso de la tÃ©cnica
de mÃ¡xima verosimilitud resulta bastante atractivo para las inferencias, particu-
larmente cuando se desea incluir variables explicativas. Desde luego, tambiÃ©n otras
herramientas estadÃ­sticas tales como las provenientes de la estadÃ­stica bayesiana
han sido desarrolladas para ajustar a los modelos resultantes.
    Como se ha expuesto en este trabajo, las cÃ³pulas ofrecen una estructura atrac-
tiva para el modelado de parejas aleatorias debido a que permite la investigaciÃ³n
del comportamiento marginal y de la estructura de dependencia en forma simul-
tÃ¡nea. Los autores de este artÃ­culo desean que las ideas y conceptos descritos aquÃ­
sean de utilidad para los investigadores que cuenten con problemas de dependencia
y estÃ©n interesados en aplicar la tÃ©cnica de la cÃ³pula, y que los paquetes junto con

                                       Revista Colombiana de EstadÃ­stica 32 (2009) 33â€“58

56                                             Gabriel Escarela & AngÃ©lica HernÃ¡ndez

las rutinas del lenguaje de acceso gratuito R puedan facilitar significativamente la
programaciÃ³n de los modelos que se generen.
Agradecimientos
Los autores agradecen a dos Ã¡rbitros y al editor por los comentarios tan Ãºtiles
que ayudaron a mejorar la presentaciÃ³n de este trabajo. Esta investigaciÃ³n ha sido
auspiciada por el programa ECOS-ANUIES-CONACYT. Los autores agradecen a
CONACYT y PROMEP, MÃ©xico, por su financiamiento.
Referencias
AnÃ©, T. & Kharoubi, C. (2003), â€˜Dependence Structure and Risk Measureâ€™, The Journal of Business 76, 411â€“438.
Barlow, R. E. & Proschan, F. (1975), Statistical Theory of Reliability and Life Testing, Holt and Rinehart and Winston.
Carriere, J. F. (1995), â€˜Removing Cancer when it is Correlated with other Causes of Deathâ€™, Biometrical Journal 37, 339â€“350.
Charpentier, A. & Juri, A. (2006), â€˜Limiting Dependence Structures for Tail Events, with Applications to Credit Derivativesâ€™, Journal of Applied Probability 43, 563â€“586.
Clemen, R. T. & Reilly, T. (1999), â€˜Correlations and Copulas for Decision and Risk Analysisâ€™, Management Science 45, 208â€“224.
Denuit, M. & Lambert, P. (2005), â€˜Constraints on Concordance Measures in Bivariate Discrete Dataâ€™, Journal of Multivariate Analysis 93, 40â€“57.
Dobric, J. & Schmid, F. (2007), â€˜A goodness of Fit Test for Copulas Based on Rosenblattâ€™s Transformationâ€™, Computational Statistics & Data Analysis 51, 4633â€“4642.
Dunn, P. K. & Smyth, G. K. (1996), â€˜Randomized Quantile Residualsâ€™, Journal of Computational and Graphical Statistics 5, 236â€“244.
Dupuis, D. J. (2005), â€˜Ozone Concentrations: A Robust Analysis of Multivariate Extremesâ€™, Technometrics 47, 191â€“201.
Embrechts, P., McNeil, A. J. & Straumann, D. (2002), Correlation and dependence in risk management: Properties and pitfalls, in M. A. H. Dempster, ed., â€˜Risk Management: Value at Risk and Beyondâ€™, pp. 176â€“223.
Escarela, G. & Carriere, J. F. (2003), â€˜Fitting Competing Risks with an Assumed Copulaâ€™, Statistical Methods in Medical Research 12, 333â€“349.
Escarela, G., Mena, R. H. & Castillo-Morales, A. (2006), â€˜A Flexible Class of Parametric Transition Regression Models Based on Copulas: Application to Poliomyelitis Incidenceâ€™, Statistical Methods in Medical Research 15, 593â€“609.
Escarela, G., PÃ©rez-Ruiz, L. C. & Bowater, R. (2009), â€˜A Copula-Based Markov Chain Model for the Analysis of Binary Longitudinal Dataâ€™, Journal of Applied Statistics . En prensa.
FrÃ©chet, M. (1951), â€˜Sur les Tableaux de CorrÃ©lation dont les Marges sont DonnÃ©sâ€™,Annales de lâ€™UniversitÃ© de Lyon (14), 53â€“77.
Frees, E. W. & Valdez, E. A. (1998), â€˜Understanding Relationships Using Copulasâ€™,North American Actuarial Journal 2, 1â€“25.
Genest, C. & Favre, A. C. (2007), â€˜Everything you always wanted to know about Copula Modeling but were afraid to askâ€™, Journal of Hydrologic Engineering 12, 347â€“368.
Genest, C. & MacKay, R. J. (1986a), â€˜Copules ArchimÃ©diennes et Familles de Lois Bidimensionnelles dont les Marges sont DonnÃ©sâ€™, The Canadian Journal of Statistics 14, 145â€“159.
Genest, C. & MacKay, R. J. (1986b), â€˜The Joy of Copulas: Bivariate Distributions with Uniform Marginalsâ€™, The American Statistician 40, 280â€“283.
Genest, C. & Rivest, L. (1993), â€˜Statistical Inference Procedures for Bivariate Archimedian Copulas.â€™, Journal of the American Statistical Association 88, 1034â€“1043.
Hougaard, P. (1986), â€˜A Class of Multivariate Failure Time Distributionsâ€™, Biometrika 73, 671â€“678.
Huard, D., Ã‰vin, G. & Favre, A. C. (2006), â€˜Bayesian Copula Selectionâ€™, Computational Statistics & Data Analysis 51, 809â€“822.
Ihaka, R. & Gentleman, R. (1996), â€˜R: A Language for Data Analysis and Graphicsâ€™, Journal of Computational and Graphical Statistics 5, 299â€“314.
Jan, Y. (2007), â€˜Enjoy the Joy of Copulas with a Package Copulaâ€™, Journal of Statistical Software 21, 1â€“21.
Joe, H. (1997), Multivariate Models and Dependence Concepts, Chapman & Hall,New York, United States.
Klugman, S. & Parsa, R. (1999), â€˜Fitting Bivariate loss Distributions with Copulasâ€™, Insurance: Mathematics and Economics 24, 139â€“148.
Lehmann, E. L. (1966), â€˜Some Concepts of Dependenceâ€™, Annals of Mathematical Statistics 37, 1137â€“1153.
Nelsen, R. (1991), Copulas and association, in Dallâ€™Aglio, ed., â€˜Advances in Probability Distributions with Given Marginals: Beyond the Copulasâ€™, Kluwer,Dordrecht, Netherlands, pp. 51â€“74.
Nelsen, R. B. (1999), An Introduction to Copulas, Springer, New York, United States.
Oakes, D. (1989), â€˜Bivariate Survival Models Induced by Frailtiesâ€™, Journal of the American Statistical Association 84, 487â€“493.
RÃ©nyi, A. (1959), â€˜On Measures of Dependenceâ€™, Acta Mathematica Academiae Scientiarum Hungaricae 10, 441â€“451.
RodrÃ­guez-Lallena, J. A. & Ãšbeda Flores, M. (2004), â€˜A New Class of Bivariate Copulasâ€™, Statistics & Probability Letters 66, 315â€“325.
Scarsini, M. (1984), â€˜On Measures of Concordanceâ€™, Stochastica 8, 201â€“218.
SchÃ¤be, H. (1997), â€˜Parameter Estimation for a Special Class of Markov Chainsâ€™,Statistical Papers 38, 303â€“327.
Schweizer, B. & Sklar, A. (1983), Probabilistic Metric Spaces, Dover Publications,New York, United States.
Schweizer, B. & Wolff, E. F. (1981), â€˜On Nonparametric Measures of Dependence for Random Variablesâ€™, The Annals of Statistics 9, 879â€“885.
Sklar, A. (1959), â€˜Fonctions de RÃ©partition a n Dimensions et Leurs Margesâ€™,Publications de lâ€™Institut Statistique de lâ€™UniversitÃ© de Paris 8, 229â€“231.
Smith, R. L. (1989), â€˜Extreme Value Analysis of Environmental Time Series: An Application to Trend Detection in Ground-Level Ozoneâ€™, Statistical Science 4, 367â€“377.
Song, P. X. K. (2000), â€˜Multivariate Dispersion Models Generated from Gaussian Copulaâ€™, Scandinavian Journal of Statistics 27, 305â€“320.
Stephenson, A. (2002), â€˜Evd: Extreme Value Distributionsâ€™, R News 2(2), 31â€“32.
Wang, W. & Wells, M. (2000), â€˜Model Selection and Semiparametric Inference for Bivariate Failure-Time Dataâ€™, Journal of the American Statistical Association 95, 62â€“76.
Whelan, N. (2004), â€˜Sampling from Archimedian Copulasâ€™, Quantitative FinanceÂ 4, 339â€“352.
Yanagimoto, T. & Okamoto, M. (1969), â€˜Partial Orderings of Permutations and Monoticity of a Rank Correlation Statisticâ€™, Annals of the Institute of Statistical Mathematics 21, 489â€“506.