Multiple Factor Analysis: Main Features and Application to Sensory Data
ENSA
Abstract
Data table in which a single set of individuals is described by several groups of variables are frequently encountered. In the factor analysis framework, taking into account different groups of variables in a unique analysis firstly raises the problem of balancing the different group. This problem being solved, beyond classical outputs from factor analysis, it is necessary to have at one’s disposal specific tools in order to compare the structure upon individuals induced by the different groups of variables. That is the aim of Multiple Factor Analysis (MFA), factor analysis devoted to such data table. This paper presents the method, its main properties and an application to sensory data.
Keywords: Factor analysis, Principal components analysis, Canonical analysis.
Data table: denotations, examples
Multiple Factor Analysis (MFA) (Escofier & Pagès (1988-1998), Pagès (2002))
deals with data table in which a set of individuals is described by several sets
of variables. Within one set, variables must belong to the same type (quantita-
tive or categorical) but, even for active ones, groups of variables can belong to
different types. We focus the hereafter presentation on quantitative variables,
with only some comments about qualitative ones.


                              Figure 1: Data table
xik value of variable k for individual i. If k is a continue variable, xik is a real
number; if k is a categorical variable, xik is a number of category. The j th set
is denoted by j or Kj .


   If we consider the whole table: individuals are denoted i (i =1, I); they
constitute the cloud NI lying in the K-dimensional space RK ; the K variables
constitute the cloud NK lying in the I-dimensional space RI .
   In we consider the only (sub-)table j: individuals are denoted ij (i =1, I);
they constitute the cloud NIj lying in the Kj -dimensional space RKj ; the Kj
                                j
variables constitute the cloud NK  lying in the I-dimensional space RI .


1.1.    Examples

    Survey. An individual is a person; a variable is a question. Questions are
gathered according to the different themes of the questionnaire. Each theme
defines one set.
    Sensory analysis. An individual is a food product. A first set of variables
includes sensory variables (sweetness, bitterness, etc.); a second one includes
chemical variables (pH, glucose rate, etc.).
    Ecology. An individual is a observation place. A first set of variables des-
cribes soil characteristics; a second one describes flora.
   Times series. Several individuals are observed at different dates. In such
a case, there is often two ways of defining sets of variables: generally, each set
gathers variables observed at one date; but, when variables are the same from
one date to the other, each set can gather the different dates for one variable.

Multiple Factor Analysis: Main Features and Application to Sensory Data         3


2.    General factor analysis: denotations and main
      relationships
   General Factor Analysis (GFA) is here understood according to Lebart et al
(1997). Main features and denotations of GFA can be summed up by 6 points.

  1. Given a table X having I rows and K columns, two clouds are considered:
     the one of rows (NI lying in RK ); the one of columns (NK lying in RI );
     to simplify this short synthesis, weights of individuals and weights of
     variables are supposed uniform.
  2. The maximum inertia directions of NI and NK are highlighted: let us
     (resp. zs ) a unit vector along the rank-s principal direction of NI (resp.
     NK ) in RK (resp. RI ). These vectors satisfy (λs being the rank s eigen-
     value of X 0 X):

            X 0 Xus = λs us ,   kus k = 1 ,   XX 0 zs = λs zs ,   kzs k = 1.

  3. NI and NK are projected onto their maximum inertia directions us and
     zs ; co-ordinates of NI (resp. NK ) onto axis s constitute the I-factor of
     rank s (resp. K-factor) denoted Fs (resp. Gs ):

                           Fs = Xus ,          Gs = X 0 zs .

     In PCA, Fs is named principal component.
  4. The inertia directions of NI and NK are related (that is named duality)
     by:
                           Fs     Fs              Gs     Gs
                    zs =         =√ ,     us =          =√ ,
                         ||Fs ||   λs           ||Gs ||    λs
     zs is often named standardised I-factor (in PCA: standardised principal
     component).
  5. The projection of row i (resp. column k) onto rank s axis in RK (resp.
     RI ) can be calculated from the co-ordinates of NK (resp. NI ) onto rank
     s axis in RI (resp. RK ) by the way of the transition formulae:
                           1                          1
                     Fs = √ XGs ,               Gs = √ XFs0 .
                           λs                         λs
                    1 X                                  1 X1
          Fs (i) = √      xik Gs (k)           Gs (k) = √        xik Fs (i) .
                     λs k                                 λs i I

4                                                                     Jérôme Pagès


       Since Benzecri (1973), these formulae are especially known in the case of
       correspondence analysis (often under the name of barycentric properties);
       they are seldom quoted in the case of PCA but are implicit during the
       interpretation.

    6. Principal Components Analysis (PCA), Correspondence analysis (CA)
       and Multiple Correspondences Analysis (MCA) can be viewed as parti-
       cular cases of GFA.



3.      Usual methods or MFA?
    In the context of factor analysis (PCA or MCA according to the type of
variables), to study the kind of data table described figure 1, usual practice
consists in introducing only one set of variables as active, the others being il-
lustrative. This ensures homogeneity of active variables, characteristic which
goes hand in hand with a clear two-steps problematic: 1. looking for main
factors describing data variability according to one theme (the one correspond-
ing to the active variables) 2. relating each of the illustrative variables to the
previous factors.
    This basic methodology is excellent. But it should be noted that, in this
strategy, the only multidimensional structure really handled is the one of the
active variables; the illustrative variables intervene independently one to the
other. According to this point of view, one can want to introduce several
sets of variables simultaneously as active elements, in order to take them si-
multaneously into account in the definition of distance between individuals.
Introducing, as active elements, several sets of variables (or, according to an
other point of view, distinguishing sets among active variables) firstly implies
to balance these sets and, secondly, enriches problematic, that is to say induces
new questions about data.
     MFA brings solutions to these problems in a way described hereafter.



4.      Balancing the sets of variables
    If all the sets of variables are introduced, as active elements, without balanc-
ing their influence, a single set can contribute quite by itself to the construction
of the first axes. In such a case, the user want to analyse all the sets and, in
fact, analyses only one of them.

Multiple Factor Analysis: Main Features and Application to Sensory Data           5


    Thus, the global analysis, in which several sets of variables are simultane-
ously introduced as active ones, requires balancing the influences of these sets.
The influence of one set j derives from its structure, in the sense of its inertia
distribution (of the two clouds NIj and NK   j
                                                it induces) in the different space
directions. For example, if a set presents a high inertia in one direction, this
direction will strongly influence the first axis of the global analysis.
     This suggests to normalise the highest axial inertia of each set. Technically,
it is done by weighting each variable of the set j by 1/λj1 , denoting λj1 the first
eigenvalue of factor analysis applied to set j.
    This weighting can be easily interpreted: considering the two clouds (NIj
      j
et NK   ) induced by the set j of variables, MFA weighting normalises each of
these two clouds by making its highest axial inertia equal to 1. This weighting
does not balance total inertia of the different sets. Thus, a set having a high
dimensionality will have a high global influence in that sense that this set
will contribute to numerous axes. But such a set has no reason to contribute
particularly to the first axes. Correlatively, a one-dimensional set can strongly
contribute to only one axis, but this axis can be the first one.



5.     MFA as a general factor analysis

    The core of MFA is a general factor analysis applied to all active sets of
variables (global analysis). MFA works with continuous variables as princi-
pal component analysis does, the variables being weighted; MFA works with
categorical variables as multiple correspondences analysis does, the variables
being weighted. Weighting, which balances highest axial inertia of sets, allows
to work simultaneously with continuous and categorical variables as active ele-
ments. Likewise, it is possible to introduce simultaneously as active elements,
set(s) of standardized variables and set(s) of un-standardized variables.
    The aim is to bring out main factors of data variability, individuals being
described, in a balanced manner, by several sets of variables (those introduced
as active).
    According to this point of view, MFA provides classical outputs of general
factor analysis, that is to say, for each axis:

      Co-ordinates, contributions and squared cosines of individuals.

      Correlation coefficient between factors and continuous variables.

6                                                                     Jérôme Pagès


         For each category, co-ordinate (and test-value in the sense of Spad soft-
         ware 2002) of the centre of gravity of individuals belonging to this cate-
         gory.


5.1.       Remark about categories inside MFA

    In MFA, categories are represented by exact centers of gravity (as in PCA
and differently from MCA). For each category, we can calculate the inertia of
the corresponding center of gravity in per cent of the total inertia; this ratio
is named contribution. It is proportional to the contribution usually defined
in MCA for the active variables and possesses the following property: its sum,
for all the categories of the variable k and for axis s, equals to the correlation
ratio between the variable k and the factor s. This ratio can be calculated for
all categorical variables (active and illustrative).



6.       Superimposed representation of the J clouds
         of individuals.

6.1.       Questions

    To each set j, we associate the cloud NIj of individuals lying in the space
    Kj
R . This cloud, named “partial”, is the one analyzed in the factor analysis
restricted to set j; it contains “partials” individuals, denoted ij (individual i
according to the set j).
   A classical question is: are there structures common to these cloudsNIj j=1,
J? That is to say: are there some resemblances, from one cloud to the other,
among distances between homologous points?
    To answer these questions, we are looking for a superimposed representation
of clouds NIj which:

         Fits well each of the clouds NIj .

         Highlights the resemblances between the different NIj , that is to say dis-
         plays homologous points as close one to the other as possible (homologous
         i.e. referring to the same individual).

     This problematic is similar to the one of Generalized Procured Analysis

Multiple Factor Analysis: Main Features and Application to Sensory Data           7


(GPA ; Gower, 1975). The way used by MFA to obtain such a superimposed
representation is described hereafter.




     Figure 2: Representation of the J partial clouds NIj in the space RK
i: individual described by all the variables; ij : individual described by the vari-
ables of the group Kj . NIj can be viewed as a projection of NI onto RKj , the
subspace of RK spanned by the variables of Kj .



6.2.    Principle
                                                                L Kj
    RK can be viewed as the direct sum of the RKj : RK =           R . Using
                                                j
this property, it is possible to put all the NI in the same space (cf. figure
2). In MFA, the clouds NIj are projected upon the axes of the global analysis,
as illustrative elements (cf. figure 3). In fact these elements are not exactly
illustrative since their data contribute to axes construction; moreover, this
representation is possible only for the clouds NIj corresponding to active sets.
   The co-ordinate of ij along axis s is denoted: Fs (ij ). These co-ordinates
can be gathered in the vector Fsj such as: Fsj (i) = Fs (ij ).


6.3.    Restricted transition formula

    The superimposed representation of MFA is not optimal, in the sense that
the Fsj do not satisfy a global criterion. But it possesses the very useful follow-
ing property.

8                                                                   Jérôme Pagès




 Figure 3: Principle of the superimposed representations provided by MFA
Each partial cloud N jI is projected onto main axes of the mean cloud N I


    It can be easily shown that the co-ordinate Fs (ij ) can be calculated from
the coordinates of the variables Gs (k), k ∈ Kj , by the way of the following
relationship:
                                         1  1 X
                   Fsj (i) = Fs (ij ) = √ q        xik Gs (k).
                                         λs λj k∈K
                                              1    j


We recognize here the usual transition formula (see § 2) but restricted to the
variables of the group Kj .


6.3.1.   Ratio to measure the global similarity between axial
         representations of the clouds NjI

   When the different sets induce similar structures on individuals, homologous
points {ij , j=1, J} are close one to the other. This global property is measured,
per axis, through the ratio described below.
    Let’s consider all the points of all the clouds Nlj (j = 1, J) and a partition
of these I × J points in I classes, such as the J homologous points {ij , j=1,
J} corresponding to the same individual i belong to the same class. When
axis s brings out a structure common to the different sets of variables, the
homologous points ij , corresponding to the same individual i, are close one to
the other and this partition has a low within-inertia (along axis s). The ratio
(between-inertia) / (total-inertia) can be calculated for each axis. This ratio is

Multiple Factor Analysis: Main Features and Application to Sensory Data         9


close to 1 when the axis represents a structure common to the different sets.
   Be careful: 1. this ratio does not decrease with axis rank order since it is
not the criterion optimized by MFA; 2. it cannot be summarized for several
axes.


6.3.2.   Detailed examination of axial representations of NjI

    The distance between each point ij and the corresponding mean point i
gives an idea about the position of i (among I) in the cloud NIj compared
to the one in the cloud NI . These distances can be examined visually, or by
selecting the projections of ij having the highest contributions to the within
inertia. This allows to detect:

     Individuals having their homologous points close one to the other (low
     within inertia); they illustrate the common structure represented by axis
     s.

     Individuals having their homologous points far one from the other (high
     within inertia); they constitute exceptions to the common structure repre-
     sented by axis s.


6.3.3.   Case of categories

   In factor analysis, when the individuals are numerous, it is the case in
surveys for example, they aren’t studied directly but by means of categorical
variables, active and/or illustrative (students, old people, etc.). Thus:

     In PCA, each category k is represented by the center of gravity of indi-
     viduals that belong to this category k.

     In MCA, the co-ordinates of points representing the categories are only
     proportional to those of the corresponding centers of gravity (application
     of the correspondence analysis centroid property to indicator matrix).

In MFA, the categories are represented by their associated centers of gravity.
This allows to work with categories as with individuals. Particularly, each
category (e.g. student in a survey) can be represented by a global point (center
of gravity of the students) and by one partial point for each set of variables
(e.g. the center of gravity of partial points representing the students according
to set j ).

10                                                                    Jérôme Pagès


7.      MFA as a multicanonical analysis

7.1.     Principle of multicanonical analysis

   In the simultaneous study of several sets of variables, the main question is:
are there factors common to the different sets of variables?
   In the simple case of two sets, this question refers to canonical analysis
(Hotelling 1936). When there are more than two groups, the reference method
is multicanonical analysis. There are several multicanonical analyzes. The
most used is the one of Carroll (1968), that works in two steps:

       Looking for a sequence of variables {zs ; s=1,S} (named general variables),
       normalized and not correlated one to the other, related to the sets of
       variables as strongly as possible.

       For each general variablezs and for each set j, looking for the linear combi-
       nation of the variables of set j (combinations named canonical variables)
       related to general variable zs as strongly as possible.

MFA can be interpreted in this framework.


7.2.     Measure of relationship between one variable and a
         group of variable

    To do this, it is necessary to firstly define a measure of relationship (denoted
Lg ) between one continuous variable z and a set of variables Kj = {vk , k =
1, Kj }.
       ¡                     ¢       ¡       ¢
    Lg z, {vk , k = 1, Kj } = Lg z, Kj
                               = inertia of all variables vk projected upon z.

     When the vk are reduced continuous variables, weighted by mk :
                                     X    £         ¤2
                       Lg (z, Kj ) =   mk r(z, vk ) .
                                        k


    This measure is implicitly used in PCA: the first principal component is the
linear combination (of variables) the most related to the whole set of variables
(it maximizes Lg (z, K)).

Multiple Factor Analysis: Main Features and Application to Sensory Data            11


   If Lg (z, Kj )=0, variable z is not correlated to any variable of the set Kj .
   Due to MFA weighting, Lg (z, Kj ) ≤1 ; Lg (z, Kj )=1 when z is the first
principal component of Kj .


7.3.    General variables

   The first factor of MFA (as defined in §5) maximizes projected inertia of all
the sets of variables, that is:
                           X
                                 Lg (z, Kj )   maximum.
                             j


In that sense, MFA factors (denoted Fs §6) can be considered as general vari-
ables of a multicanonical analysis (in Carroll’s method, relationship between
one variable and one set of variables is measured by means of multiple correla-
tion coefficient).


7.4.    Canonical variables

    The coherence between the multicanonical point of view and the superim-
posed representation point of view suggests to use the previously defined Fsj
as canonical variables. It can be shown (Pagès & Tenenhaus, 2001) that Fsj is
the first component in the PLS regression between the general variable zs and
the data table Xj . This result reinforces the superimposed representation: it
induces that the Fsj j = 1, J must be correlated one to the other since each Fsj
expresses the same structure Fs in the group Kj .


7.5.    Canonical correlation coefficients in MFA

    In MFA, factors of global analysis (denoted Fs ) are the common factors and
factors of partial points (denoted Fsj ) represent common factors in each set j
of variables. In order to judge if factors of global analysis really are common to
the different sets, it is possible to calculate, for each set j and each factor s, the
correlation coefficient between general variable Fs and canonical variable Fsj .
If this coefficient (named canonical correlation coefficient and always positive)
is high, then the structure brought out by variable Fs does “exist” in the set
j. If not, it does not. The synthesis of all these correlation coefficients shows
factors common to all the sets, common to some sets, specific to only one set.

12                                                                    Jérôme Pagès


8.     Global study of sets of variables
    It is often interesting to globally study the sets of variables, the question
being: do these sets define similar structures upon individuals (i.e. similar
distances between individuals from one cloud Nij to the other)? We find again
the problem of superimposed representations and the one of common factors
but now the investigation about the similarities between sets is global.
   Here, we are looking for a display in which each set of variables is represented
by a unique point. In such a display, two sets must be close one another if they
induce similar structures on the individuals.
    To each set of variables Kj , we associate the I ×I matrix Wj of scalar prod-
ucts between individuals (Wj = Xj Xj0 ). Each scalar product matrix Wj can be
                                                                               2
represented by one point in the I 2 -dimensional euclidian space (denoted RI ).
Thus, in this space, one set is represented by one point: the J points constitute
the set cloud, denoted NJ . In this cloud NJ , the distance between two points
Wj and Wl decreases as the similarity between the structures (defined upon
individuals) induced by the sets Kj and Kl increases. For this reason, it is
interesting to get a representation of the cloud NJ .
   The Statis method (Lavit, 1988) is based on such a representation, obtained
by projecting NJ onto its main inertia directions. But these directions cannot
be interpreted (they are linear combinations of couples of individuals) (Pagès,
1996).
    The representation provided by MFA is obtained by projecting NJ upon
               2
vectors (in RI ) induced by I-factors of global analysis (one factor may be
considered as a set including a single variable; it is possible to associate to this
                                                        2
set a scalar product matrix and thus a vector in RI ).
     The normalized factor of rank s in RK , previously denoted zs , induces
                 2
ws = zs zs0 in RI . Some properties of zs induce corresponding properties for
ws :

                            zs0 zt = 0 ⇒ hws , wt i = 0,
                            kzs k = 1 ⇒ kws k = 1.

    The main interest of this projection space is that its axes (upon which NJ
is projected) are interpretable and, above all, possess the same interpretation
that axes of global analysis (in the same manner, due to factor analysis duality,
axis of rank s upon which individuals are projected and axis of rank order s
upon which variables are projected possess the same interpretation).

Multiple Factor Analysis: Main Features and Application to Sensory Data         13




Figure 4: The representation of the groups and its links with the one of variables


   This representation has the following property: it can be shown (Escofier
& Pagès 1998, pag. 167) that co-ordinate of set j upon axe of rank s is equal
to Lg (zs , Kj ). Thus:


      Set co-ordinates are always comprised between 0 and 1.


      A small distance between two sets along axe s means that these two
      sets include the structure expressed by factor s each one with the same
      intensity. In other words, set representations shows which ones are similar
      (or different) from the point of view of global analysis factors.


    This representation has been introduced as an aid to the interpretation of
representations of individuals and variables. But it possesses its own optimality:
axes upon which NJ is projected, taking into account the usual orthogonality
constraint but also the constraint to be of order 1 (i.e. induced by one direction
in RI ), make the sum (and not the sum of squares) of co-ordinates maximum
(for axis s, this sum is equal to the sth eigenvalue of the global analysis). Thus,
              2
from the RI point of view, the contribution of the set j to axis s is equal to
the set j co-ordinate divided by the sum of co-ordinates (this contribution is
equal to the sum of contributions to axis s, in RI , of variables belonging to the
set j).
                                                                           2
   The set study can be completed by squared cosines computed in RI .

14                                                                   Jérôme Pagès


9.      Relationship between global analysis and
        separated analysis of each set
   It is always important and interesting to relate MFA results to those of
separated analysis of each set. To do this, I-factors of separated analysis (called
“partial” factors) are projected as illustrative quantitative variables.
   It is equivalent to perform MFA from variables or from all the partial factors
(each one being, within its set, “pre-weighted” proportionally to its eigenvalue).
Thus:

      For each partial factor, the ratio between its projected inertia along axis
      s and the axis s eigenvalue may be interpreted, in case of active sets,
      actually as a contribution to MFA axis s.

      MFA may be considered as a method providing an optimal representation
      of separated analyzes axes.

    This last point is useful for applications: MFA is a convenient tool in order
to compare several factor analyzes having the same individuals. Example: in
order to compare, for the same variables, normalized PCA an un-normalized
PCA, variables must appear twice in data base, firstly within a normalized set
and secondly within a un–normalized set; MFA is there an optimal tool in order
to display factors of the two analysis (a direct PCA is not desirable because
set weighting is necessary).


10.      The orange juice example

10.1.     Data

    Six pure orange juices (P1 to P6) were selected from the main brands on
the French market. These juices were pasteurized in two ways: thus, three
of them must be stored in refrigerated conditions (R) while the others can
be stored at ambient temperature (A). Here is the list of the six orange juices:
Pampryl at ambient temperature (P1), Tropicana at ambient temperature (P2),
refrigerated Fruivita (P3), Joker at ambient temperature (P4), refrigerated
Tropicana (P5), refrigerated Pampryl (P6).
   Ninety-six students in food science, both trained to evaluate foodstuffs and
consumers of orange juice, described each of these six products on the basis

Multiple Factor Analysis: Main Features and Application to Sensory Data       15


of seven attributes: intensity and typical nature of its smell, intensity of the
taste, pulp content, sweetness, sourness and bitterness.
   The serving order design was a juxtaposition of Latin squares balanced for
carry-over effects (MacFie, Bratchell, Greenhoff & Vallis, 1989).
    In addition to the sensory investigation, chemical measurements (pH, citric
acid, overall acidity, saccharose, fructose, glucose, vitamin C and sweetening
power, defined as: saccharose + 0.6 glucose + 1.4 fructose), were carried out.
   The data are gathered in a table using the format shown in figure 3. The
complete data table is in the appendix A.
   The outputs described below come from Spad 2002 software.




                      Figure 5: The orange juices data table
For juice i : xik is the panel average of the sensory variable k or the chemical
measurement k


10.2.    Separate analyses

    Table 1 gathers inertia from separate and global analysis. The first eigen-
value of the separate PCA of chemical variables is slightly higher than the one
of PCA of sensory variables. The balancing is useful to avoid the domination
of chemical variables in the construction of the first axis.
    The sequence of eigenvalues is similar from one analysis to the other: the
two groups of variables have a strong first direction of inertia. Moreover, the
homologous factors of the two separate PCA are correlated one another (table
2). This data set is interesting from a methodological point of view: the simi-
larity between the two groups of variables justifies their simultaneous analysis;
the differences between the two groups are sufficiently important to justify the

16                                                                   Jérôme Pagès


use of a specific method to highlight common and specific features.


     Table 1: Eigenvalues (= inertia) from separate PCA and from MFA
          PCA chemical var.         PCA sensory var.               MFA
  Axes    Eigenvalue        %       Eigenvalue      %         Eigenvalue     %
     1       6.2135        69.04      4.7437       67.77        1.7907     61.24
     2       1.4102        15.67      1.3333       19.05        0.4764     16.29
     3       1.0457        11.62      0.8198       11.71        0.2938     10.05
     4       0.3173        03.53      0.0840       01.20        0.2009     6.87
     5       0.0133        00.15      0.0192       00.27        0.1623     5.55



             Table 2: Correlations between separate PCA factors
                                         PCA chemical var.
                                          F1            F2
                 PCA sensory        F1   -0.78        -0.25
                      Variables     F2    0.08        -0.74



10.3.    Representation of individuals (= products)
         and variables (Fig. 6 and 7)

    This MFA builds a product spacestarting from factors common to the sen-
sory and instrumental data, in which the influences of these two groups of
variables are balanced. These MFA representations (of products and variables)
can be read like those from a PCA: the co-ordinates of a product are its values
for the common factors; the co-ordinates of a variable are its correlations with
these factors.
    The first axis is highly correlated to variables belonging to the two groups;
that was awaited due to the group balancing. It opposes the juices 1, 4 and 6
to the juices 2, 3 and 5.
    According to the usual transition formulae, the juices 1, 4 and 6 have a high
level of acidity (and a low pH), and a high [(glucose + fructose)/saccharose]
ratio; they were perceived as sour, bitter and not very sweet. Symmetrically,
the juices 2, 3 and 5 have opposite characteristics: a low level of acidity (and a

Multiple Factor Analysis: Main Features and Application to Sensory Data         17


high pH), and a low [(glucose + fructose)/saccharose] ratio; they were perceived
as being not sour or bitter, but sweet.
    The juices 2, 3 and 5 come from Florida; the first axis is linked to geographic
origin. The second bisector is more interesting than the second axis. This
bisector correspond to pulpy. It opposes the refrigerated juices to the others.
   Let us also point out:

      The opposition between fructose and glucose on the one hand and sac-
      charose and pH on the other hand, connected with the hydrolysis of
      saccharose, facilitated in an acid medium.

      The correlation between acidity, pH and sourness.

      The absence of correlation between sweetening power and sweetness: a
      high level of sweetness is associated with a low level of sourness (this
      refers to the concept of gustatory balance). Thus the strong correlation
      between saccharose and sweetness is not due to the direct influence of
      saccharose but to a high pH.




          Figure 6: First factorial map from MFA: mean individuals
Refr.: refrigerated; amb.: stored at ambient temperature. Tropicana and Fru-
vita come from Florida

18                                                               Jérôme Pagès




     Figure 7: First factorial map from MFA: chemical and sensory variables

Multiple Factor Analysis: Main Features and Application to Sensory Data      19


10.4.    Factors from separate analyses (Fig. 8)

    Factors from separate analyses can be represented by the way of their cor-
relations with factors of MFA. Figure 8 shows that the first factor of MFA is
highly correlated with the first factor of each separate analysis. These factors
of separate analyzes are not so highly correlated one another (cf. table 2) but,
in this analysis, the first MFA factor, being a kind of compromise between
them, is highly correlated to the two. The same observation can be made for
the second factor. Thus, the first MFA map is roughly similar to the one of
each separate analysis (figure 8 gives an idea of the slight rotation to obtain
one map from an other).

       Figure 8: Representation of the factors from separate analyses
GjFs: rank-s factor of separate PCA of group j

         Figure 9: Superimposed representation of partial individuals
The mean points of figure 6 are here joined to corresponding partial points. S:
sensory; C: chemical

             Figure 10: Separate PCA of each group of variables

Multiple Factor Analysis: Main Features and Application to Sensory Data        21

10.5.    Superimposed representations (Fig. 9)

    Figure 9 derives from figure 6 by adding partial points (C and S). Whatever
the set of variables considered, the first axis opposes the products 1, 4 and 6
to the product 2, 3 and 5. This is a other way to highlight common factor.
   The resemblance between the two partial clouds can be globally evaluated
by two series of measures (cf. table 3 and 4). The two first factors from MFA
can be considered as factors common to the two groups of variables.


                   Table 3: Canonical correlation coefficient
           Table 4: Ratio [(between-inertia) / (total-inertia)]; cf. §6
   This representation allows a precise comparison of the clouds NIj . Thus,
the figure 9 suggests that the product 5 is highly characteristic from a sensory
point of view though it is not the case from a chemical point of view; conversely,
product 2 is characteristic from the chemical point of view but that does not
induce a particular sensory evaluation.
    This can be directly verified in the data (cf. appendix A), preferably in the
standardized data that can be compared from one variable to the other. Thus,
the standardized data table shows that product 5 has absolute values higher
for the sensory attributes than for the chemical variables. It is the reverse for
he product 2.
    In these data, in which the two first factors from MFA are highly corre-
lated to the corresponding ones from each separate analysis, the superimposed
representation gives a good idea of the representation from separate analysis.
This can be illustrated by the comparison between the representation of partial
individuals in MFA and the representation of individuals from separate PCA

22                                                                  Jérôme Pagès


(cf. Fig. 10). Thus, for example, the opposition between the products P2 and
P4 is much bigger from a chemical point of view than for a sensory point of
view.




10.6.    Representation of categories (Fig. 11)

   In this data table, the individuals are very few: here, the interest of the
representation of categories is mostly technical. But, when the individuals are
numerous, as in a survey for example, this representation is essential. Each cat-
egory lies in the center of gravity of the individuals which posses this category.
This is applied to mean and partial point.
    In Figure 11, the mean points show immediately that the factor 1 corre-
sponds to origin (Florida/elswhere) and that the second bisector corresponds
to way of storing (refrigerated or ambient). The partial points shows that the
opposition of the two origins along axis 1 is equally clear from the two points
of view (chemical/sensory). Along the second bisector, the opposition of the
two ways of storing mostly appears from the sensory point of view (pulpyness).




             Figure 11: Display of mean and partial categories
 Each category lies in the center of gravity of the individuals belonging to it

Multiple Factor Analysis: Main Features and Application to Sensory Data         23




               Figure 12: Representation of groups of variables
In this display, a group of variables is represented by a single point. Here,
appears a third group, obtained from the chemical one, by giving up two
variables (pH2 and vitamin C)



10.7.     Representation of groups of variables (Fig. 12)

     These data being composed of two groups only, the representation of groups
                 2
as points of RI (cf. § 8) has not practical interest. To enrich the technical
comments, here a third group is added (as a supplementary one): it derives
from the chemical group, in which we have removed the variables pH2 (because
it is quite equivalent to pH1 ) and vitamin C (because it is not related to sensory
evaluation). Thus we get an empiric idea about the stability of the chemical
group.
   The three groups are strongly related to Factor 1 (in the sense of Lg mea-
surement; cf. § 7): this factor correspond to a direction having a strong inertia

24                                                                   Jérôme Pagès


for the three clouds of variables (in other words, many variables of each groups
are related to this factor).
   Regarding the first two MFA factors, these three groups are very similar:
the clouds of individuals they induce (previously denoted NIj ) are very similar.
In particular, removing pH2 and vitamin C has had a weak influence.


Conclusion
MFA allows to take into account several sets of variables as active elements in a unique factor analysis. Its main features are The balancing of the sets of variables. Outputs specific of the partition of the variables in different sets; mainly the superimposed representations of individuals and of categories, the groups representation.
References
Carroll, J. D. (1968), ‘A generalization of canonical correlation analysis to three or more sets of variables’.
Decisia (2002), ‘Spad rel. 5.5: Système pour l’analyse des données. logiciel diffusé par Decisia’, Levallois-Perret 92532, France.
Escofier, B. & Pagès, J. (1988-1998), Analyses factorielles simples et multiples; objectifs, méthodes et interprétation, Dunod.
Escofier, B. & Pagès, J. (1994), ‘Multiple factor analysis (afmult package)’, Computational statistics & data analysis 18, 121–140.
Hotelling, H. (1936), ‘Relations between two sets of variables’, Biometrika 28, 129–149.
Macfie, H. J., Greenhoff, N. B. & Vallis, L. V. (1989), ‘Designs to balance the effect of order of presentation and first-order carry-over effects in hall tests’, Journal of Sensory Studies 4, 129–148.
Pagès, J. (1996), ‘Eléments de comparaison entre l’analyse factorielle multiple et la méthode statis’, Revue de Statistique appliquée XLIV 4, 81–95.
Pagès, J. (2002), ‘Analyse factorielle multiple appliquée aux variables qualitatives et aux données mixtes’, Rev Statistique appliquée 4, 5–37.