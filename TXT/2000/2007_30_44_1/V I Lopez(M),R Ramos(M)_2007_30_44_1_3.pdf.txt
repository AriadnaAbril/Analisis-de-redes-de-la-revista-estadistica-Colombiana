Una introducciÃ³n a los diseÃ±os Ã³ptimos
Universidad Nacional de Colombia;Centro de InvestigaciÃ³n en MatemÃ¡ticas
Resumen
Introducimos varios conceptos utilizados en la teorÃ­a de diseÃ±os de experimentos Ã³ptimos. Definimos criterios de optimalidad utilizados en esta Ã¡rea y exploramos sus propiedades. Se listan algunos resultados importantes para encontrar diseÃ±os Ã³ptimos para modelos lineales y no lineales, entre ellos teoremas de equivalencia. Finalmente se presentan algunos ejemplos tÃ­picos donde se aplica la teorÃ­a vista anteriormente.
Palabras clave: funciÃ³n de informaciÃ³n, matriz de informaciÃ³n, criterios de optimalidad, teoremas de equivalencia, modelos de regresiÃ³n no lineal.
1.     IntroducciÃ³n
    En muchas Ã¡reas de investigaciÃ³n interesa explicar una variable respuesta, Y ,
a travÃ©s de kâˆ’variables explicativas, xT = [x1 , x2 , . . . , xk ], mediante un modelo de
la forma:
siendo Î·(x, Î¸) una funciÃ³n lineal o no lineal en el vector de parÃ¡metros desconocido
Î¸ âˆˆ Rm ; y el tÃ©rmino de error se asume que tiene media cero y varianza constante
Ïƒ 2 . Una vez se especifica el modelo, la siguiente etapa consiste en determinar en
quÃ© condiciones experimentales, niveles de los xj â€™s, se debe medir la respuesta para
obtener una mejorÃ­a en la calidad de la inferencia estadÃ­stica a un menor costo.
Esto se logra construyendo un diseÃ±o donde la elecciÃ³n de los niveles de los xj â€™s
y la frecuencia de mediciÃ³n de la respuesta estÃ¡n regidas por algÃºn criterio de
optimalidad (con significado estadÃ­stico). Hay varios ejemplos prÃ¡cticos que han
hecho uso de los diseÃ±os Ã³ptimos (vÃ©ase Atkinson (1996)) y existe un gran nÃºmero
de contribuciones sobre este tema; por ejemplo, entre otros autores, Smith (1918)
encontrÃ³ diseÃ±os para los modelos polinomiales, Kiefer (1959) introdujo explÃ­ci-
tamente la nociÃ³n de diseÃ±o Ã³ptimo y sus propiedades; y posteriormente realizÃ³
muchos trabajos en el Ã¡rea (vÃ©ase Brown et al. (1985)). TambiÃ©n, recientemente
en los libros de Atkinson & Donev, A. N. (1992) y Pukelsheim (1993), los auto-
res hicieron un tratamiento estadÃ­stico y formal, respectivamente, de los diseÃ±os
Ã³ptimos.
   Este trabajo tiene como objetivo presentar los conceptos bÃ¡sicos de los diseÃ±os
Ã³ptimos y, en forma general, los criterios de optimalidad, tanto en modelos lineales
como no lineales, dando mayor Ã©nfasis y extensiÃ³n a los primeros, ya que son
una alternativa de soluciÃ³n para los modelos no lineales, por ejemplo los diseÃ±os
Ã³ptimos locales mencionados en la secciÃ³n 3.1.
    Este artÃ­culo se divide en cuatro secciones. En la siguiente secciÃ³n se darÃ¡n los
aspectos sobresalientes de los diseÃ±os Ã³ptimos para el modelo lineal, se definen los
criterios de optimalidad en general y se mencionan varios resultados, principalmen-
te teoremas de equivalencia para determinar optimalidad. En la tercera secciÃ³n
se estudia el caso no lineal y se definen algunos de los criterios de optimalidad
usados en la literatura. En la Ãºltima secciÃ³n se construyen diseÃ±os Ã³ptimos para
dos posibles escenarios: cuando el experimentador conoce de antemano los puntos
de soporte del diseÃ±o, caso usual en diseÃ±os de experimentos (vÃ©ase la secciÃ³n 4.1);
y cuando no se conocen ni los puntos de soporte ni los pesos del diseÃ±o (vÃ©ase la
secciÃ³n 4.3).



2.    DiseÃ±os Ã³ptimos para modelos lineales
   Para los modelos lineales se considera que la relaciÃ³n entre las N âˆ’observaciones
Yi y xi estÃ¡ dada por:

                    Y (xi ) = Î¸T f (xi ) + ,   xi âˆˆ Rk ,   Î¸ âˆˆ Rm

donde f = [f1 , . . . , fm ]T es un vector de mâˆ’funciones continuas linealmente in-
dependientes definidas en un conjunto compacto Ï‡, rango de regresiÃ³n, Ï‡ âŠ† Rk ,
Î¸ âˆˆ Rm es un vector de mâˆ’parÃ¡metros desconocidos,  es una variable alea-
toria con media cero y varianza constante Ïƒ 2 y se asume incorrelaciÃ³n en las
N âˆ’observaciones.

                                        Revista Colombiana de EstadÃ­stica 30 (2007) 37â€“51

Una introducciÃ³n a los diseÃ±os Ã³ptimos                                                 39

   Aunado al modelo anterior, se define un diseÃ±o aproximado,
                                                
                                    x   . . . xn
                             Î¾= 1
                                    w1 . . . wn
con wi = Î¾(xi ), como una medida de probabilidad definida en B, conjunto de Borel
de Ï‡ que incluye los conjuntos unitarios; tal que Î¾ tiene soporte finito. El soporte
de Î¾ es Supp(Î¾) = [x1 , . . . , xn ], n: nÃºmero de puntos de soporte de Î¾, y las obser-
vaciones Y (x) se hacen en x1 , . . . , xn con frecuencias (o pesos) aproximadamente
proporcionales a w1 , . . . , wn .
   Para cada diseÃ±o Î¾ se define la matriz de momentos:
                              Z                      n
                                                     X
                  M (Î¾) â‰¡        f (x)f T (x)dÎ¾(x) =   f (xi )f T (xi )wi
                            Ï‡                      i=1

    La forma de cuantificar la informaciÃ³n suministrada por la matriz de momen-
tos depende de los criterios de optimalidad, definidos como aquellos que maxi-
mizan algÃºn funcional real (con un significado estadÃ­stico) de la matriz de mo-
mentos sobre Î; clase de todos los diseÃ±os aproximados definidos en B. Es-
tos criterios de optimalidad se presentan a continuaciÃ³n, siguiendo el enfoque de
Pukelsheim (1993), quien introduce la matriz de informaciÃ³n CK , funciÃ³n puente
que da cuenta de la â€œinformaciÃ³nâ€ contenida en combinaciones lineales de Î¸; CK
es una funciÃ³n del conjunto de las matrices definidas no negativas de orden m,
N N D(m), en el conjunto de las matrices simÃ©tricas de orden q, Sim(q). Con-
cluyendo con la nociÃ³n de funciÃ³n de informaciÃ³n Ï†. La matriz de informaciÃ³n
intuitivamente mide la informaciÃ³n que aporta el sistema de parÃ¡metros K T Î˜,
mientras que la funciÃ³n de informaciÃ³n la cuantifica por medio de un nÃºmero real.
   En las observaciones 1 y 2 se presenta lo anterior esquemÃ¡ticamente, y en la
observaciÃ³n 3 se da la formulaciÃ³n del problema de diseÃ±o.

ObservaciÃ³n 1. Se considera el caso general, cuando el investigador estÃ¡ intere-
sado en la estimaciÃ³n de qâˆ’combinaciones lineales de Î¸. Es decir, la estimaciÃ³n
del subsistema K T Î¸, donde K âˆˆ RmÃ—q es conocida y r(K) = q.

      Sea Î¾ un diseÃ±o factible para K T Î¸, es decir, C(K) âŠ† C(M (Î¾)), C(A) es el
      espacio generado por las columnas de la matriz A. Se define la matriz de
      informaciÃ³n como la funciÃ³n:
                                CK : N N D(m) â†’ Sim(q)
      tal que: CK (M (Î¾)) = (K T M âˆ’ K)âˆ’1 , Aâˆ’ denota una inversa generalizada de
      A.
      Por notaciÃ³n, A â‰¥ 0 si y sÃ³lo si A âˆˆ N N D(m); A â‰¥ B si y sÃ³lo si A âˆ’ B â‰¥ 0.
      La matriz de informaciÃ³n es homogÃ©neamente positiva (CK (Î´A) = Î´CK (A),
      A â‰¥ 0, Î´ > 0), superaditiva (CK (A + B) â‰¥ CK (A) + CK (B), A, B â‰¥ 0),
      Rango(CK ) âŠ† N N D(q), cÃ³ncava (CK ((1 âˆ’ Î±)A + Î±B) â‰¥ (1 âˆ’ Î±)CK (A) +
      Î±CK (B), A, B âˆˆ N N D(m), 0 < Î± < 1) e isotÃ³nica (A â‰¥ B â‡’ CK (A) â‰¥
      CK (B)).

                                         Revista Colombiana de EstadÃ­stica 30 (2007) 37â€“51

40                                           VÃ­ctor Ignacio LÃ³pez & Rogelio Ramos

     Si K = Im , interesa estimar Î¸, y M (Î¾) es no singular, entonces CI (M (Î¾)) =
     M (Î¾). Es decir, la matriz de informaciÃ³n coincide con la matriz de momentos;
     por esta razÃ³n en la literatura M tambiÃ©n se llama matriz de informaciÃ³n.

ObservaciÃ³n 2. CuantificaciÃ³n de la informaciÃ³n suministrada para cada diseÃ±o,
ya sea por la matriz de momentos o la matriz de informaciÃ³n, es definida a partir
de una funciÃ³n de valor real Ï†.
    Sea Ï† un funcional de valor real, Ï† : N N D(q) â†’ R. Ï† es una funciÃ³n de
informaciÃ³n si es: homogÃ©neamente positiva (Ï†(Î´C) = Î´Ï†(C), Î´ > 0, C â‰¥ 0),
superaditiva: Ï†(C + D) â‰¥ Ï†(C) + Ï†(D), no negativa: (Ï†(C) â‰¥ 0, C â‰¥ 0) y
semicontinua superiormente (los conjuntos de nivel {Ï† â‰¥ Î±} = {C âˆˆ N N D(q) :
Ï†(C) â‰¥ Î±} son cerrados para todo Î± âˆˆ R).
    Para lo que sigue Ï†, denotarÃ¡ una funciÃ³n de informaciÃ³n.

ObservaciÃ³n 3. FormulaciÃ³n del problema de diseÃ±o.

     El problema de diseÃ±o para el sistema parametral K T Î¸ consiste en encontrar
     un diseÃ±o Î¾ âˆ— que sea factible y que maximice, sobre todos los diseÃ±os Î¾
     factibles para K T Î¸, la funciÃ³n de informaciÃ³n:

                         Ï†(CK (M (Î¾))) = Ï†((K T M (Î¾)âˆ’ K)âˆ’1 )

     Por las propiedades de Ï† y CK , principalmente la semicontinuidad superior
     y la compacidad de Ï‡, el mÃ¡ximo anterior se alcanza para algÃºn diseÃ±o Î¾.
     câˆ’optimalidad. Si K = c, c âˆˆ RmÃ—1 entonces el criterio asociado se deno-
     mina câˆ’optimalidad; se puede mostrar que la Ãºnica funciÃ³n de informaciÃ³n
     es la identidad: Ï†(Î´) = Î´ y el problema de diseÃ±o se reduce a encontrar un
     diseÃ±o Î¾ âˆ— que sea factible para cT Î¸ y maximice la funciÃ³n de informaciÃ³n:

                     Ï†(Cc (M (Î¾))) = Cc (M (Î¾)) = (cT M (Î¾)âˆ’ c)âˆ’1

     observe que el lado derecho representa el inverso de la varianza asociada al
     estimador Ã³ptimo para cT Î¸; luego los diseÃ±os câˆ’Ã³ptimos son aquellos que
                                 b
     minimizan la varianza de cT Î¸.

  A continuaciÃ³n se exhibe una clase de funciones de informaciÃ³n, denominada
matriz de medias (matrix means), la cual contiene los criterios de optimalidad de
mayor popularidad.
  Sea C âˆˆ N N D(q), para C > 0:
                           ï£±
                           ï£´
                           ï£´ Î»max (C),        p = âˆ;
                           ï£´h
                           ï£´           i1/p
                           ï£² 1      p
                  Ï†p (C) =    q tr(C )      , p 6= 0, p 6= Â±âˆ;
                                                                              (2)
                           ï£´(det(C))1/q ,
                           ï£´                  p =  0;
                           ï£´
                           ï£´
                           ï£³
                             Î»min (C),        p = âˆ’âˆ.




                                     Revista Colombiana de EstadÃ­stica 30 (2007) 37â€“51

Una introducciÃ³n a los diseÃ±os Ã³ptimos                                                 41

ObservaciÃ³n 4. Anotaciones sobre los criterios Ï†p âˆ’Ã³ptimos.

     Ï†p es funciÃ³n de informaciÃ³n para p âˆˆ [âˆ’âˆ, 1].

     Si un diseÃ±o Î¾ maximiza el criterio anterior, se dice que el diseÃ±o es Ï†p âˆ’Ã³ptimo
     (p âˆˆ [âˆ’âˆ, 1]).

     Si C = CK (M (Î¾)) = (K T M (Î¾)âˆ’ K)âˆ’1 y p âˆˆ {0, âˆ’1, âˆ’âˆ} se tienen los crite-
     rios de optimalidad mÃ¡s populares, versiÃ³n generalizada, que dependen de la
     maximizaciÃ³n del respectivo funcional evaluado en la matriz informaciÃ³n (o
     en algunos casos evaluado en la matriz de diseÃ±o); ellos son, respectivamente,

        â€¢ Dâˆ’optimalidad, criterio del determinante, equivale a minimizar el vo-
          lumen del elipsoide asociado a la estimaciÃ³n del sistema K T Î¸, cuando
          los errores son normales.
        â€¢ Aâˆ’optimalidad, criterio promedio, recÃ­proco del promedio de las va-
          rianzas asociado a las qâˆ’combinaciones lineales de Î¸, y
        â€¢ Eâˆ’optimalidad, criterio del valor propio, minimizaciÃ³n del valor propio
          mÃ¡s pequeÃ±o.

    El problema de optimizaciÃ³n planteado en la observaciÃ³n 3 es muy complejo;
en la prÃ¡ctica se hace uso de teoremas de equivalencia para verificar si un diseÃ±o
dado es Ï†âˆ’Ã³ptimo (Pukelsheim 1993, Atkinson & Donev, A. N. 1992). El primer
teorema de equivalencia lo demostraron Kiefer & Wolfowitz (1960); allÃ­ estable-
cieron la equivalencia entre Dâˆ’optimalidad y Gâˆ’optimalidad âˆ’ Î¾ es un diseÃ±o
Gâˆ’Ã³ptimo si minimiza: âˆ€Î¾ âˆˆ Î,
                            (
                              supxâˆˆÏ‡ d(x, M (Î¾)), C(M (Î¾)) âŠ‡ Ï‡;
                 d(M (Î¾)) =
                              âˆ,                  en otro caso.

siendo, d(x, M (Î¾)) = f T (x)M (Î¾)âˆ’ f (x). Es decir, si Î¾ minimiza la varianza mÃ¡s
grande posible sobre Ï‡, rango de regresiÃ³n.

Teorema 1. Teorema de Equivalencia de Kiefer-Wolfowitz.
Sea Ï‡ âŠ† Rk con mâˆ’vectores linealmente independientes. Un diseÃ±o Î¾ con matriz
de momentos M (Î¾), definida positiva, es Dâˆ’Ã³ptimo si y sÃ³lo si Î¾ es Gâˆ’Ã³ptimo si
y sÃ³lo si f T (x)M (Î¾)âˆ’1 f (x) â‰¤ m, âˆ€x âˆˆ Ï‡ si y sÃ³lo si d(M (Î¾)) = m.
                                                              1
En caso de optimalidad, f T (xi )M âˆ’1 f (xi ) = m, Î¾(xi ) â‰¤ m   , âˆ€xi âˆˆ Supp(Î¾).

    Por lo popular de los criterios Ï†p (p âˆˆ [âˆ’âˆ, 1]), se enuncia el siguiente teorema
de equivalencia, da condiciones necesarias y suficientes para garantizar que un
diseÃ±o dado es Ï†p âˆ’Ã³ptimo.

Teorema 2. Sea Ï†p , p âˆˆ (âˆ’âˆ, 1], M un subconjunto convexo y compacto de
N N D(m) y M (Î¾) âˆˆ M, con Î¾ factible para K T Î¸ y matriz de informaciÃ³n C =
CK (M (Î¾)). Entonces:

                                         Revista Colombiana de EstadÃ­stica 30 (2007) 37â€“51

42                                              VÃ­ctor Ignacio LÃ³pez & Rogelio Ramos

       Î¾ es Ï†p âˆ’Ã³ptimo para K T Î¸ en M siÃ­: âˆƒG âˆˆ M âˆ’ tal que:
       Tr(AGKC p+1 K T GT ) â‰¤ Tr(C p ),     âˆ€A âˆˆ M      (desigualdad de normalidad).

       En caso de optimalidad, la igualdad se obtiene si en vez de A se coloca M u
                   f âˆˆ M Ï†p âˆ’Ã³ptima para K T Î¸ en M.
       otra matriz M
       Si 0 < M (Î¾) âˆˆ M, entonces Î¾ es Ï†p âˆ’Ã³ptimo para Î¸ en M siÃ­: Tr(AM pâˆ’1 ) â‰¤
       Tr(M p ), âˆ€A âˆˆ M.
    Para p = 0 y M > 0, la condiciÃ³n requerida se traduce en: Tr(AM âˆ’1 ) â‰¤
m, âˆ€A âˆˆ M, pero M es generado por las matrices de rango uno: A = f (x)f T (x);
es suficiente verificar la condiciÃ³n para A, y el lado izquierdo de la desigualdad es:
     Tr(AM âˆ’1 ) = Tr(f (x)f T (x)M âˆ’1 ) = Tr(f T (x)M âˆ’1 f (x)) = f T (x)M âˆ’1 f (x)
lo cual muestra un caso particular de una de las equivalencias del Teorema 1. Existe
la versiÃ³n del teorema de equivalencia para Eâˆ’optimalidad (p = âˆ’âˆ) (vÃ©ase
Pukelsheim 1993). Para p = âˆ’1 (Aâˆ’optimalidad), M > 0 y C = (K T M âˆ’1 K)âˆ’1 ,
la condiciÃ³n a verificar serÃ¡:
             f T (x)M âˆ’1 KK T M âˆ’1 f (x) âˆ’ Tr(K T M âˆ’1 K) â‰¤ 0,      âˆ€x âˆˆ Ï‡            (3)


3.     DiseÃ±os Ã³ptimos para los modelos no lineales
     Los modelos no lineales se pueden representar por:
                                  Y (x) = Î·(x, Î¸) +                                  (4)
donde, como en el modelo lineal, las variables explicativas xT = [x1 , x2 , . . . , xk ]
varÃ­an en un espacio de diseÃ±o compacto, Ï‡ âŠ† Rk , dotado de una Ïƒâˆ’Ã¡lgebra, B,
(Borelianos en Ï‡, agregÃ¡ndole los conjuntos unitarios), Î¸ âˆˆ Î˜ âŠ† Rm , los errores
con media cero y varianza constante y Î·(x, Î¸) es una funciÃ³n no lineal en Î¸.
   En el modelo 4, dado un diseÃ±o Î¾ definido en B, se sabe que el estimador de
mÃ­nimos cuadrados para Î¸, bajo ciertas condiciones de regularidad, es asintÃ³tica-
mente insesgado y su matriz de varianzasâˆ’covarianzas asintÃ³tica es la inversa de
la matriz:
                                               Z
                                           
             M (Î¾, Î¸) = EÎ¾ g(x, Î¸)g T (x, Î¸) =   g(x, Î¸)g T (x, Î¸) dÎ¾(x)
                                                  Ï‡

donde:   g(x, Î¸) = âˆ‚Î·(x,Î¸)
                        Lo cual motiva el anÃ¡lisis de M (Î¾, Î¸). En la literatura
                     âˆ‚Î¸ .
a M se le conoce como matriz de informaciÃ³n, y juega el papel de la matriz de
momentos del modelo lineal, si se considerara el modelo linealizado.
    La dependencia de M de Î¸ hace que la bÃºsqueda de diseÃ±os Ã³ptimos dependa
de este parÃ¡metro. En forma anÃ¡loga al caso lineal, se cuantifica la magnitud
de la informaciÃ³n suministrada por M (Î¾, Î¸) a partir de funcionales de Ã©sta, y
consecuentemente la maximizaciÃ³n de alguna funciÃ³n de informaciÃ³n Ï†, de valor
real. Para la construcciÃ³n de los diseÃ±os Ã³ptimos existen varios enfoques; en este
trabajo se exploran los siguientes:

                                        Revista Colombiana de EstadÃ­stica 30 (2007) 37â€“51

Una introducciÃ³n a los diseÃ±os Ã³ptimos                                                 43

3.1.     DiseÃ±os Ã³ptimos locales
    Introducidos por Chernoff (1953), son los primeros diseÃ±os que aparecieron
para el caso no lineal. Consisten en dar inicialmente un valor a priori para Î¸, Î¸0 ,
que estÃ© cercano al valor verdadero del parÃ¡metro, luego utilizar la aproximaciÃ³n
lineal de Taylor para Î·(x, Î¸) alrededor de Î¸0 y construir diseÃ±os Ã³ptimos para el
modelo linealizado: Y âˆ— (x) = Î² T g(x, Î¸0 ) + âˆ— . Los diseÃ±os resultantes son diseÃ±os
Ã³ptimos locales. Varios autores han construido diseÃ±os con este enfoque; vÃ©ase por
ejemplo: Ford et al. (1992), Dette et al. (2004), Dette et al. (2005), entre otros.
La construcciÃ³n de diseÃ±os Dâˆ’Ã³ptimos locales y Aâˆ’Ã³ptimos locales se explora en
los ejemplos de la secciÃ³n 4.3.


3.2.     DiseÃ±os Ã³ptimos promediados por una distribuciÃ³n
         a priori Ï€âˆ’enfoque Bayesiano
    Este criterio hace uso del conocimiento que se tiene acerca de Î¸ por una dis-
tribuciÃ³n a priori Ï€, resultando un criterio de optimalidad denominado Bayesiano.
En particular, un diseÃ±o Î¾ es Dâˆ’Ã³ptimo Bayesiano (con respecto a la distribuciÃ³n
a priori Ï€), para abreviar DÏ€ âˆ’Ã³ptimo, si maximiza:
                                           Z
                                      
                      EÎ¸ log |M (Î¾, Î¸)| =     log |M (Î¾, Î¸)|dÏ€(Î¸)
                                             Î˜


    En general, un diseÃ±o es Ï†âˆ’Ã³ptimo Bayesiano con respecto a la distribuciÃ³n a
priori Ï€, abreviado por Ï†Ï€ âˆ’Ã³ptimo, si maximiza: EÎ¸ Ï†(M (Î¾)) (Dette et al. 2003).
Ejemplos de este tipo de diseÃ±os se muestran en la secciÃ³n 4.3.
   Para DÏ€ optimalidad, se obtiene la siguiente equivalencia, generalizaciÃ³n del
teorema de Kiefer y Wolfowitz:

          Î¾ es DÏ€ âˆ’ Ã³ptimo siÃ­ E[g T (x, Î¸)M âˆ’1 (Î¾, Î¸)g(x, Î¸)] â‰¤ m,     âˆ€x âˆˆ Ï‡        (5)

La respectiva equivalencia se obtiene para AÏ€ âˆ’optimalidad al calcular la esperan-
za, con respecto a Î¸, de la expresiÃ³n 3:

  Î¾ es AÏ€ âˆ’ Ã³ptimo siÃ­
       E[g T (x, Î¸)M âˆ’1 (Î¾)KK T M âˆ’1 g(x, Î¸) âˆ’ Tr(K T M âˆ’1 (Î¾)K)] â‰¤ 0,      âˆ€x âˆˆ Ï‡    (6)

donde K, M âˆ’1 , son funciones que dependen de Î¸.


4.     Ejemplos
    En esta secciÃ³n se presentan varios ejemplos de modelos (lineales y no linea-
les) donde el interÃ©s estÃ¡ en encontrar diseÃ±os Ã³ptimos, ya sea que se conozcan
los puntos de soporte o no. Inicialmente se considera el caso lineal, criterios Ï†p
optimales y por Ãºltimo el caso no lineal.

                                         Revista Colombiana de EstadÃ­stica 30 (2007) 37â€“51

44                                                   VÃ­ctor Ignacio LÃ³pez & Rogelio Ramos

4.1.    Ejemplo 1. DeterminaciÃ³n de los pesos Ã³ptimos para
        un diseÃ±o dado
    Este ejemplo muestra los resultados reportados por Pukelsheim & Torsney
(1991) cuando los puntos de soporte del diseÃ±o son conocidos, y luego se da una
aplicaciÃ³n.
    Por conveniencia se reescribe el modelo lineal de la siguiente forma:
                  E[Yij ] = xTi Î¸,   j = 1, 2, . . . , ni ,   i = 1, 2, . . . , l       (7)
con observaciones Yij incorrelacionadas, varianza constante Ïƒ 2 , y los l vectores de
regresiÃ³n {x1 , x2 , . . . , xl } linealmente independientes y conocidos.
    El objetivo es encontrar un diseÃ±o experimental Î¾ que indique, en forma Ã³ptima,
el nÃºmero de rÃ©plicas ni que se harÃ¡n en el vector de regresiÃ³n xi , con el fin de esti-
mar K T Î¸. En tÃ©rminos generales, hallar un vector de pesos wT = [w1 , w2 , . . . , wl ]
que maximice la funciÃ³n de informaciÃ³n:
                                     Ï†p [CK (M (w))]
donde M (w), la matriz de momentos asociada al modelo (7), es expresada como:
                                     l
                                     X
                           M (w) =         xi xTi wi = X T âˆ†w X
                                     i=1

  T
                         
X = x1       x2    Â· Â· Â· xl y âˆ†w = diag(w), con la siguiente inversa generalizada
para M :
                       M (w)âˆ’ = X T (XX T )âˆ’1 âˆ†âˆ’    T âˆ’1
                                               w (XX )   X
V = (XX T )âˆ’1 XK, entonces la matriz de informaciÃ³n es:
                              CK (M (w)) = (V T âˆ†âˆ’
                                                 wV )
                                                     âˆ’1


âˆ†âˆ’w inversa generalizada para âˆ†w , si todos los pesos de w no son positivos.
   En el siguiente resultado se obtiene una expresiÃ³n cerrada para los pesos
Aâˆ’Ã³ptimos (p = âˆ’1) y una forma de encontrarlos recursivamente para los otros
valores de p.
Teorema 3. Sea p âˆˆ (âˆ’âˆ, 1], el vector de pesos w es Ï†p âˆ’Ã³ptimo para K T Î¸ si y
sÃ³lo si:                    âˆš
                              bii
                   wi = Pl p , para i = 1, . . . , l                       (8)
                           j=1    bjj
donde b11 , . . . , bll son los elementos de la diagonal de la matriz definida no negativa
l Ã— l: B = V C p+1 V T , con C = CK (M (w)).
ObservaciÃ³n 5. Si p = âˆ’1, Aâˆ’optimalidad, y el sistema de interÃ©s es el vector
                                                                   1             âˆ’1
de parÃ¡metros Î¸, entonces la funciÃ³n objetivo: Ï†âˆ’1 (M (w)) = m        Tr(M âˆ’1 (w))    ,
el inverso del promedio de las varianzas de los estimadores de mÃ­nimos cuadrados
Î¸b1 , . . . , Î¸bm , estandarizados relativo a su tamaÃ±o muestral N y a la varianza del
modelo Ïƒ 2 .



                                           Revista Colombiana de EstadÃ­stica 30 (2007) 37â€“51

Una introducciÃ³n a los diseÃ±os Ã³ptimos                                                     45

AplicaciÃ³n: Modelo de anÃ¡lisis de varianza de un factor con tres niveles:
                     Yij = Âµi + ij ,   j = 1, 2, . . . ,   ni ,   i = 1, 2, 3
En este caso, segÃºn el modelo (7), Î¸T = [Âµ1 , Âµ2 , Âµ3 ], xT1 = [1, 0, 0], xT2 = [0, 1, 0],
xT3 = [0, 0, 1]. El interÃ©s estÃ¡ en conocer el nÃºmero de rÃ©plicas en cada nivel del
factor con el fin de estimar en forma Ã³ptima: C1.âˆ’ Los tres efectos promedio, y
C2.âˆ’ El contraste: Âµ3 âˆ’ Âµ1 y Âµ2 . Aplicando el teorema 3 se obtienen los pesos
Ï†p âˆ’Ã³ptimos para p = âˆ’1, 0, ver tabla 1. Note que:
       Para el caso C1 los diseÃ±os Ã³ptimos coinciden para los dos criterios conside-
       rados.
       En ambos casos, el diseÃ±o Aâˆ’Ã³ptimo requiere la misma proporciÃ³n de ob-
       servaciones en cada uno de los tres niveles del factor. Difiere con respecto
       al criterio Dâˆ’Ã³ptimo ya que en el caso C2, el diseÃ±o Dâˆ’Ã³ptimo requiere
       alrededor de la mitad de las observaciones para el segundo nivel, y el resto
       se reparte igualmente para los otros dos niveles.

           Tabla 1: Resultados para los pesos Ã³ptimos diseÃ±o de un factor.
                                              p       criterio                 w
                                                                         Ë†            Ëœ
    Caso 1. EstimaciÃ³n de Âµ                  -1    A-optimalidad          1/3 1/3  1/3Ëœ
                                                                         Ë†
                                              0    D-optimalidad          1/3 1/3 1/3
                                                                         Ë†            Ëœ
    Caso 2. EstimaciÃ³n de Âµ3 âˆ’ Âµ1 y Âµ2       -1    A-optimalidad
                                                                      Ë† 1/3 1/3 1/3 Ëœ
                                              0    D-optimalidad       0.251 0.498 0.251



4.2.      Ejemplo 2. DiseÃ±os Ã³ptimos para modelos
         polinomiales
   Considere inicialmente el modelo polinomial de grado 2 en el intervalo [âˆ’1, 1],
                             Y (x) = f T (x)Î¸ + 
                                           
donde f T (x) = 1 x x2 y Î¸T = Î¸0 Î¸1 Î¸2 , x âˆˆ [âˆ’1, 1].
   En el caso Dâˆ’Ã³ptimo, se verificarÃ¡ a continuaciÃ³n que el diseÃ±o
                                                 
                                   âˆ’1    0     1
                            Î¾=
                                   1/3 1/3 1/3
es un diseÃ±o Dâˆ’Ã³ptimo para estimar Î¸ (tomando K = I).
   En efecto, bastarÃ¡ con mostrar que el diseÃ±o Î¾ verifica las condiciones del
teorema 1. Primero note que su matriz de momentos es:

   En la figura 1 se muestra que esta funciÃ³n tiene todos sus valores por debajo
de m = 3, y en los puntos de soporte alcanza su mÃ¡ximo, luego Î¾ es Dâˆ’Ã³ptimo
para estimar el vector de parÃ¡metros Î¸.
                          Figura 1: GrÃ¡fico de la funciÃ³n d(x, Î¾).
    Suponga que el interÃ©s del investigador estÃ¡ en estimar la diferencia entre el
             la potencia
coeficiente de          cuadrÃ¡tica y la lineal. En este caso el sistema de interÃ©s
es: K T Î¸ = 0 âˆ’1 1 Î¸.
    En la tabla 2 aparecen los resultados que se obtuvieron con los criterios D
y A optimalidad para los casos C1 y C2, y con los mismos puntos de soporte.
Observe que los diseÃ±os dados por ambos criterios, para estimar Î¸2 âˆ’ Î¸1 , reparten
en forma equitativa el nÃºmero de observaciones en los puntos x = âˆ’1 y en x = 0 y
ninguna observaciÃ³n para x = 1. Se presentan diferencias en los diseÃ±os Ã³ptimos
para la estimaciÃ³n del vector de parÃ¡metros; para Aâˆ’optimalidad el 50% de las
observaciones se deberÃ¡n tomar en x = 0, y el resto se reparte equitativamente
en los otros dos puntos, mientras que con Dâˆ’optimalidad el mismo nÃºmero de
observaciones se deberÃ¡ tomar en los tres puntos.
   En la literatura (Pukelsheim 1993) existe la soluciÃ³n para el caso general, poli-
nomios de grado d, para los diseÃ±os Dâˆ’Ã³ptimos en el intervalo [âˆ’1, 1]; los autores
usan como argumento el teorema 1, y muestran que los diseÃ±os Dâˆ’Ã³ptimos tienen

                                             Revista Colombiana de EstadÃ­stica 30 (2007) 37â€“51

Una introducciÃ³n a los diseÃ±os Ã³ptimos                                                      47

igual peso 1/(d + 1) en los puntos de soporte que son soluciÃ³n a la ecuaciÃ³n:

                                      (1 âˆ’ x2 )PÌ‡d (x) = 0

donde PÌ‡d (x) es la derivada del polinomio de Legendre de grado d.

        Tabla 2: Resultados para los pesos Ã³ptimos para el modelo cuadrÃ¡tico.
                                             p     Criterio                    w
                                                                   Ë†                    Ëœ
         Caso 1. EstimaciÃ³n de Î¸            -1     A-optimalidad       0.25   0.50 0.25Ëœ
                                                                       Ë†
                                             0     D-optimalidad        1/3 1/3 1/3
                                                                        Ë†            Ëœ
         Caso 2. EstimaciÃ³n de Î¸2 âˆ’ Î¸1      -1     A-optimalidad
                                                                        Ë†0.5 0.5 0.0Ëœ
                                             0     D-optimalidad          0.5 0.5 0.0




4.3.     Ejemplo 3. Modelos no lineales
   Como ilustraciÃ³n se consideran dos modelos no lineales (Atkinson & Donev, A.
N. 1992), y se construyen diseÃ±os Ã³ptimos locales y usando un enfoque Bayesiano.

  1. El modelo de decaimiento exponencial estÃ¡ dado por:

                                   Î·(x, Î¸) = exp(âˆ’Î¸x),         x>0

       Si Î¸0 es una buena asignaciÃ³n para Î¸, suR matriz de informaciÃ³n, la cual es un
       escalar, es: M (Î¾, Î¸0 ) = M (x0 , Î¸0 ) = x>0 f 2 (x, Î¸0 ) dÎ¾(x), donde f (x, Î¸0 ) =
        d
       dÎ¸ Î·(x, Î¸)|Î¸=Î¸0 = âˆ’x exp(âˆ’Î¸0 x).
       El modelo linealizado consta de un parÃ¡metro, y el diseÃ±o Dâˆ’Ã³ptimo local
       concentra toda su masa en un punto. Se verÃ¡ a continuaciÃ³n que el punto es:
       x0 = 1/Î¸0 . Sea Î¾0 el diseÃ±o que tiene como punto de soporte a x0 , entonces:

                                   M (Î¾0 , Î¸0 ) = x20 exp(âˆ’2Î¸0 x0 )                         (9)

       No es difÃ­cil mostrar que el mÃ¡ximo de la ecuaciÃ³n (9) se alcanza en
       x0 = 1/Î¸0 , y

         d(x, Î¾0 ) = f T (x, Î¸0 )M âˆ’1 (Î¾0 , Î¸0 )f (x, Î¸0 ) =
                                                   f 2 (x, Î¸0 )
                                              R                  = (xÎ¸0 )2 exp(âˆ’2(xÎ¸0 âˆ’ 1))
                                                  f 2 (x)dÎ¾0 (x)

       observe que d(x, Î¾0 ) â‰¤ 1, âˆ€x > 0 y d(x, Î¾0 ) = 1 en x = 1/Î¸0 , luego el diseÃ±o
       que concentra su masa en 1/Î¸0 es Dâˆ’Ã³ptimo local. Este diseÃ±o no permite
       realizar pruebas de bondad de ajuste para el modelo en cuestiÃ³n. El diseÃ±o
       depende de la especificaciÃ³n de Î¸0 , y puede llegar a ser ineficiente si Î¸0 estÃ¡
       muy lejos del valor verdadero Î¸. Otra forma de hallar un diseÃ±o Ã³ptimo es a
       partir de un enfoque Bayesiano, donde se incorpora el conocimiento acerca
       de Î¸ por medio de una distribuciÃ³n a priori. Como ilustraciÃ³n se consideran
       6 distribuciones a priori discretas, uniformes en 5 puntos, y se hallaron los

                                             Revista Colombiana de EstadÃ­stica 30 (2007) 37â€“51

48                                                  VÃ­ctor Ignacio LÃ³pez & Rogelio Ramos

       respectivos diseÃ±os DÏ€ âˆ’Ã³ptimos para estimar Î¸, con las diferentes a prioris;
       ver tabla 3. Lo anterior se hizo numÃ©ricamente con ayuda de algoritmos
       computacionales programados en el lenguaje R (R Development Core Team
       2006) usando la equivalencia 5. Los diseÃ±os DÏ€ âˆ’Ã³ptimos obtenidos estÃ¡n
       formados por tres puntos de soporte, observÃ¡ndose variaciÃ³n en las distintas
       a prioris consideradas, tanto en los puntos de soporte como en sus pesos.
       Atkinson & Donev, A. N. (1992, pÃ¡g. 230) muestran cÃ³mo los puntos de
       soporte del diseÃ±o aumentan a medida que la distribuciÃ³n a priori que ellos
       consideran es mÃ¡s dispersa.

Tabla 3: Ejemplo de decaimiento exponencial con diferentes distribuciones a prio-
         ris uniformes para Î¸.
           Puntos de la a priori j                x                        wâˆ—
         Ë†                         Ëœ   Ë†                    Ëœ    Ë†                      Ëœ
        Ë† 0.09  0.49 1 4.9 9 Ëœ          0.156
                                        Ë†      1.503 10.998Ëœ     Ë†0.438   0.403   0.158Ëœ
        Ë†0.10 0.50 1 5.0 10Ëœ            Ë†0.143 1.517 9.812Ëœ      Ë†0.432   0.420   0.148Ëœ
        Ë†0.11 0.51 1 5.1 11Ëœ            Ë†0.132 1.536 8.812Ëœ      Ë†0.428   0.437   0.135Ëœ
        Ë†0.12 0.52 1 5.2 12Ëœ            Ë†0.123 1.558 7.952Ëœ      Ë†0.424   0.455   0.121Ëœ
        Ë†0.14 0.54 1 5.4 14Ëœ            Ë†0.107 1.617 6.547Ëœ      Ë†0.418   0.496   0.085Ëœ
          0.15 0.55 1 5.5 15             0.101 1.649 5.965        0.416   0.521   0.063



     2. Modelo de compartimientos.

       Los modelos de compartimientos son de gran utilidad en farmacocinÃ©tica,
       utilizados, entre otras aplicaciones, para modelar el nivel de concentraciÃ³n
       de un medicamento en la sangre de un individuo a lo largo del tiempo. Se
       considera el siguiente modelo:

                         Î¸1
          Î·(x, Î¸) =           {exp(âˆ’Î¸2 x) âˆ’ exp(âˆ’Î¸1 x)},        x â‰¥ 0,    Î¸1 > Î¸2 > 0       (10)
                      Î¸1 âˆ’ Î¸2

       Asociado al trabajo biolÃ³gico es de interÃ©s, ademÃ¡s de estimar el vector de
       parÃ¡metros Î¸, estimar tres cantidades que ayudan al estudio de la cinÃ©tica
       del medicamento en un individuo. Estas cantidades son:
                                                        Râˆ
         a) El Ã¡rea bajo la curva (AUC): g1 (Î¸) =         0   Î·(x, Î¸)dÎ¸ = Î¸12 .
                                                                          âˆ’log Î¸2
         b) Tiempo para la concentraciÃ³n mÃ¡xima: g2 (Î¸) = xmax = log Î¸Î¸11 âˆ’Î¸ 2
                                                                                  .
         c) La concentraciÃ³n mÃ¡xima: g3 (Î¸) = Î·(xmax , Î¸).

       La construcciÃ³n de diseÃ±os Ã³ptimos para la estimaciÃ³n de estas funciones
       simultÃ¡neamente, se harÃ¡ por medio de diseÃ±os Aâˆ’Ã³ptimos locales (vÃ©ase
       (2), con p = âˆ’1), y diseÃ±os Aâˆ’Ã³ptimos promediados por una distribuciÃ³n a
       priori uniforme. La jâˆ’Ã©sima columna Kj , de K es el gradiente de funciÃ³n
       no lineal gj (Î¸), evaluada en Î¸0 . AsÃ­ se asegura que el diseÃ±o Ã³ptimo serÃ¡
       aquel que minimice el promedio de las varianzas del respectivo estimador
       linealizado, es decir, aquel que minimiza:

                                            Revista Colombiana de EstadÃ­stica 30 (2007) 37â€“51

Una introducciÃ³n a los diseÃ±os Ã³ptimos                                                  49


                      3
                   1X                     1X
                         var(gj (Î¸b0 )) â‰ˆ            b âˆ K T M âˆ’1 (Î¾)K
                                             var(KjT Î²)
                   3 j=1                  3

     En el caso de estudio, las primeras dos columnas de K estÃ¡n dadas por:
                                                h                         i
           K1T (Î¸0 ) = 0 âˆ’1/Î¸20 2 ,                    10 âˆ’xmax
                                        K2 (Î¸0 ) = 1/Î¸Î¸10 âˆ’Î¸20
                                                                xmax âˆ’1/Î¸20
                                                                 Î¸10 âˆ’Î¸20

     en forma anÃ¡loga
                        se halla la tercera columna de K. Como ilustraciÃ³n, se
     tomÃ³ Î¸0T = 0.7 0.2 , y en la tabla 4 se presentan los diseÃ±os Aâˆ’Ã³ptimos
     locales obtenidos para la estimaciÃ³n de las tres caracterÃ­sticas de interÃ©s
     simultÃ¡neamente. TambiÃ©n se considerÃ³ una a priori uniforme discreta para
     los siguientes cinco valores del vector de parÃ¡metros Î¸:

           Î˜ = {(0.70, 0.20), (0.65, 0.15), (0.75, 0.25), (0.65, 0.25), (0.75, 0.15)}

     es decir, Ï€(Î¸) = 1/5, âˆ€Î¸ âˆˆ Î˜, y en la tabla 4 se reporta el diseÃ±o Aâˆ’Ã³ptimo
     Bayesiano obtenido. Ambos diseÃ±os Aâˆ’Ã³ptimo local y Aâˆ’Ã³ptimo promedia-
     do por la a priori Ï€, presentan pocas diferencias. AdemÃ¡s en ambos casos se
     verificÃ³ que el diseÃ±o hallado satisfacÃ­a las equivalencias dadas por (3) y (6),
     respectivamente.

Tabla 4: DiseÃ±os Aâˆ’Ã³ptimos locales y promediados por la a priori Ï€ para el
         modelo 10.
                      Criterio                        Tiempo x      Pesos del diseÃ±o
                                                   Ë†            Ëœ   Ë†              Ëœ
     Aâˆ’optimalidad local                            1.313  6.602Ëœ
                                                   Ë†                Ë†0.276 0.724Ëœ
     Aâˆ’optimalidad promediado por la a priori Ï€     1.456 7.145       0.269 0.731




5.    Anotaciones finales
    En este trabajo se presentÃ³ una motivaciÃ³n inicial para el estudio de los diseÃ±os
Ã³ptimos en ambos casos lineal y no lineal. Se dio el enfoque matemÃ¡tico de cada
uno de los criterios de optimalidad usados en la prÃ¡ctica y se terminÃ³ presentando
algunos ejemplos tÃ­picos. Hay gran diversidad de bibliografÃ­a en torno a este tema,
donde el estudio en esta Ã¡rea es factible e interesante.
    En la mayorÃ­a de los artÃ­culos citados, los autores asumen que el modelo bajo
consideraciÃ³n es conocido, y el valor de los parÃ¡metros es desconocido. Con este
supuesto, usan criterios de optimalidad que son eficientes para la estimaciÃ³n de los
parÃ¡metros del modelo fijo. Sin embargo, existen aplicaciones donde la forma de
la funciÃ³n de regresiÃ³n no es conocida en forma exacta, es decir, el experimentador
debe decidir, entre un conjunto de clases de funciones competitivas, cuÃ¡les de estas
describen los datos en una forma mÃ¡s adecuada. Como lo afirman Biedermann
et al. (2005), el problema de diseÃ±o para discriminar entre modelos no lineales
competitivos ha encontrado muy poco interÃ©s en la literatura que aquellos proble-
mas de estimaciÃ³n de parÃ¡metros. En el caso de discriminaciÃ³n de modelos, lineal y no lineal, se pueden revisar los trabajos de: Atkinson & Cox (1974), Atkinson &
Fedorov (1975), Pukelsheim & Rosenberger (1993), Biswas & Chaudhuri (2002) y
Biedermann et al. (2005). Por lo anterior, estÃ¡ como trabajo futuro ahondar en el
estudio de diseÃ±os Ã³ptimos que sean eficientes para discriminar entre modelos no
lineales anidados, ademÃ¡s de que permitan estimar en forma simultÃ¡nea funciones
de los parÃ¡metros.
Agradecimientos
Agradecemos los comentarios hechos por los dos Ã¡rbitros, lo que hizo que este trabajo se mejorara considerablemente. El presente trabajo se realizÃ³ cuando el primer autor estaba haciendo su doctorado en Ciencias con OrientaciÃ³n en Probabilidad y EstadÃ­stica en el Centro de InvestigaciÃ³n en MatemÃ¡ticas (CIMAT), MÃ©xico. Parte de este trabajo fue apoyado por CIMAT, SecretarÃ­a de Relaciones Exteriores de MÃ©xico (SRE) y la Universidad Nacional de Colombia, sede MedellÃ­n.
Referencias
Atkinson, A. C. (1996), â€˜The Uselfulness of Optimum Experimental Designsâ€™, Journal of the Royal Statistical Society. Series B (Methodological) 58(1), 59â€“76.
Atkinson, A. C. & Cox, D. R. (1974), â€˜Planning Experiments for Discriminating Between Modelsâ€™, Journal of the Royal Statistical Society. Series B (Methodological) 36(3), 321â€“348.
Atkinson, A. C. & Donev, A. N. (1992), Optimum Experimental Designs, Oxford Science Publications, New York.
Atkinson, A. C. & Fedorov, V. V. (1975), â€˜Optimal Design: Experiments for Discriminating Between Several Modelsâ€™, Biometrika 62(2), 289â€“303.
Biedermann, S., Dette, H. & Pepelyshev, A. (2005), â€˜Optimal Discrimination Designs for Exponential Regression Modelsâ€™, Preprint.
Biswas, A. & Chaudhuri, P. (2002), â€˜An Efficient Design for Model Discrimination on Parameter Estimation in Linear Modelsâ€™, Biometrika 89(3), 709â€“718.
Brown, L. D., I., O., Sacks, J. & Wynn, H. P. (1985), Jack Karl Kiefer Collected Papers III, Design of Experiments, Springer Verlag, New York.
Chernoff, H. (1953), â€˜Locally Optimal Designs for Estimating Parametersâ€™, The Annals of Mathematical Statistics 24(24), 586â€“602.
Dette, H., Haines, L. M. & Imhof, L. A. (2003), â€˜Maximin and Bayesian Optimal Designs for Regression Modelsâ€™, Preprint. pp. 1-15.
Dette, H., Melas, V. B. & Pepelyshev, A. (2004), â€˜Optimal Designs for a Class of Nonlinear Regression Modelsâ€™, The Annals of Statistics 32(5), 2142â€“2167.
Dette, H., Melas, V. B. & Wong, W. K. (2005), â€˜Optimal Design for Goodness-of Fit of the Michaelis-Menten Enzyme Kinetic Functionâ€™, Journal of the American Statistical Association 100(472), 1370â€“1381.
Ford, I., Tornsney, B. & Wu, C. F. J. (1992), â€˜The Use of a Canonical Form in the Construction of Locally Optimal Designs for Nonlinear Problemsâ€™, Journal of the Royal Statistical Society, Series B (Methodological) 54.
Kiefer, J. (1959), â€˜Optimum Experimental Designs (With Discussion)â€™, Journal Royal Statistical Society, B 21, 272â€“319.
Kiefer, J. & Wolfowitz, J. (1960), â€˜The Equivalence of Two Extremum Problemsâ€™, Canadian Journal of Mathematics 12, 363â€“366.
Pukelsheim, F. (1993), Optimal Design of Experiments, John Wiley & Sons, New York.
Pukelsheim, F. & Rosenberger, J. L. (1993), â€˜Experimental Designs for Model Discriminationâ€™, Journal of the American Statistical Association 88(422), 642â€“649.
Pukelsheim, F. & Torsney, B. (1991), â€˜Optimal Weights for Experimental Designs on Linearly Independent Support Pointsâ€™, The Annals of Statistics 19(3), 1614â€“1625.
R Development Core Team (2006), R: A Language and Environment for Statistical Computing, R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0. http://www.R-project.org
Smith, K. (1918), â€˜On the Standard Deviations of Adjusted and Interpolates Values of an Observed Polynomial Functions and its Constants and the Guidance They Give Towards a Proper Choice of the Distribution of Observationsâ€™, Biometrika 12, 1â€“85.