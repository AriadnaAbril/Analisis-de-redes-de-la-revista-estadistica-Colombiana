C√°lculo de los estimadores de regresi√≥n cuant√≠lica lineal por medio del m√©todo ACCPM
Universidad de la Sabana;Universidad Nacional de Colombia
Resumen
Se muestra c√≥mo calcular los estimadores en regresi√≥n cuant√≠lica por medio del m√©todo de optimizaci√≥n no diferenciable ACCPM (Analytic Center Cutting Plane Method). El c√°lculo de dichos estimadores usualmente se encuentra por medio de programaci√≥n lineal y sus respectivas t√©cnicas de soluci√≥n (m√©todo simplex, m√©todos de punto interior, etc.). La primera parte presenta las generalidades de la regresi√≥n cuant√≠lica y su formulaci√≥n como un problema de programaci√≥n lineal. Adem√°s, se realiza una breve descripci√≥n del m√©todo ACCPM. Por √∫ltimo, se muestra la aplicaci√≥n del m√©todo ACCPM para el c√°lculo de estimadores por cuantiles y los resultados num√©ricos y comparaciones del m√©todo ACCPM con el paquete estad√≠stico R y el paquete de optimizaci√≥n GAMS.
Palabras clave: optimizaci√≥n, estimador de regresi√≥n, programaci√≥n lineal, estimaci√≥n cuant√≠lica.
Introducci√≥n a la regresi√≥n cuant√≠lica
En los modelos de regresi√≥n, los errores se asumen como una sucesi√≥n un de variables aleatorias independientes e id√©nticamente distribuidas con media cero Generalmente la distribuci√≥n que se asume es la normal. Sin embargo, no siempre se cumple el supuesto de normalidad ya que la distribuci√≥n puede ser asim√©trica. Koenker & Bassett (1978) introducen el concepto de regresi√≥n cuant√≠lica (RC ) como una soluci√≥n a dichos problemas y demuestran que los estimadores por cuantiles son m√°s eficientes que el estimador m√°ximo veros√≠mil de muchos modelos param√©tricos convencionales.
    En los m√©todos de regresi√≥n cl√°sicos el objetivo es minimizar la suma de los
residuales al cuadrado y utilizar la media como estimador. La regresi√≥n cuant√≠lica
busca minimizar una suma de errores absolutos ponderados con pesos asim√©tricos
y utiliza los cuantiles como estimadores.


1.1.     Definici√≥n de cuantil
     La RC utiliza la noci√≥n cl√°sica de cuantil para el c√°lculo de las estimaciones.
   Dado un œÑ ‚àà (0, 1) y una variable aleatoria Y (continua o discreta), el œÑ ‚àí√©simo
cuantil es definido como:

                               Q(œÑ ) = inf {y : F (y) ‚â• œÑ }

donde F es la funci√≥n de distribuci√≥n de Y .
    Por otro lado, si se tiene {Y1 , Y2 , . . . , Yn }, una muestra con observaciones inde-
pendientes, es posible encontrar una estimaci√≥n de la funci√≥n de distribuci√≥n por
medio de la distribuci√≥n emp√≠rica de la muestra definida como el cociente entre
el n√∫mero de las observaciones inferiores o iguales al valor de inter√©s y el n√∫mero
total de las observaciones:
                                                 #(Yi ‚â§ y)
                                  Fb (y) =                                              (1)
                                                      n
An√°logamente, es posible definir una estimaci√≥n de los cuantiles por medio de la
distribuci√≥n emp√≠rica as√≠:
                                               
                               b ) = inf y : Fb (y) ‚â• œÑ
                              Q(œÑ                                                       (2)

     El problema (2) es equivalente a:
                            (                                           )
                               X                  X
             b ) = argmin
             Q(œÑ                   œÑ |yi ‚àí ŒµœÑ | +   (1 ‚àí œÑ ) |yi ‚àí ŒµœÑ |
                       ŒµœÑ ‚ààR    yi ‚â•ŒµœÑ              yi <ŒµœÑ


                             b ) es a trav√©s de una funci√≥n de chequeo definida
    Otra manera de encontrar Q(œÑ
de la siguiente manera:

                       œÅœÑ (r) = r(œÑ ‚àí I(r < 0)),        0<œÑ <1

                                         Revista Colombiana de Estad√≠stica 30 (2007) 53‚Äì68

Estimadores de regresi√≥n cuant√≠lica lineal por medio del m√©todo ACCPM                     55
                       
                           1,       si r < 0;
donde:   I(r < 0) =
                           0,       si r ‚â• 0.
   De este modo el problema (3) correspondiente al c√°lculo del œÑ ‚àí√©simo cuantil
queda reformulado as√≠:
                                      X
                        b ) = argmin
                        Q(œÑ              œÅœÑ (yi ‚àí ŒµœÑ )                     (3)
                                          ŒµœÑ ‚ààR       i


1.2.     Regresi√≥n cuant√≠lica
    Dados m vectores x1 , . . . , xm ‚àà Rn , que representan las variables explica-
tivas y m valores reales y1 , y2 , . . . , ym , que representan la variable explicada1 ,
en los problemas de regresi√≥n por m√≠nimos cuadrados se busca un vector Œ≤ =
(Œ≤1 , . . . , Œ≤n‚àí1 , Œ≤n )T ‚àà Rn , soluci√≥n del siguiente problema de optimizaci√≥n:
                                                m
                                                X                       2
                                 min f (Œ≤) =              yi ‚àí Œ≤ T xi                    (4)
                                                i=1

    Si asumimos que yi ‚àí Œ≤ T xi = ui , i = 1, 2, . . . , n y que el valor
                                                                        esperado condi-
cional de ui con respecto a las observaciones es cero E(ui | xi = 0), entonces la
media condicional de yi con respecto a xi es
                                      E(yi | xi ) = Œ≤ T xi
La soluci√≥n del problema de optimizaci√≥n (4) est√° dada por
                                        ‚àí1 T
                              Œ≤ = XT X        X y
                              T
donde X = x1 x2 ¬∑ ¬∑ ¬∑ xm          y y = [y1 , y2 , . . . , ym ].
    Ahora, si se supone que yi = Œ≤œÑT xi + ui ,œÑ y adem√°s que el valor esperado con-
dicional no necesariamente es cero, pero el œÑ ‚àí√©simo cuantil del error con respecto
a las variables regresoras es cero QœÑ (ui ,œÑ | xi ) = 0 , entonces el œÑ ‚àí√©simo cuantil
de yi con respecto a las variables regresoras se puede escribir
                                      QœÑ (yi | xi ) = Œ≤œÑT xi
La estimaci√≥n de Œ≤œÑ se encuentra por medio de
                      Ô£±                                                  Ô£º
                      Ô£≤ X                      X                         Ô£Ω
       Œ≤bœÑ = arg minn          œÑ yi ‚àí Œ≤œÑT xi +      (1 ‚àí œÑ ) yi ‚àí Œ≤œÑT xi                 (5)
                Œ≤œÑ ‚ààR Ô£≥                                                  Ô£æ
                           T i
                           yi ‚â•Œ≤œÑ x             T i           yi <Œ≤œÑ x

que es equivalente al siguiente problema de optimizaci√≥n:
                                                m
                                                X                            
                                Œ≤bœÑ = argmin          œÅœÑ yi ‚àí Œ≤ T xi                     (6)
                                      Œ≤œÑ ‚ààRn i=1

donde œÅœÑ es la funci√≥n de chequeo y œÑ es un valor en (0, 1).
   El problema (6) resulta ser un problema de optimizaci√≥n convexa.
  1 Es decir, se tienen n variables explicativas y el tama√±o de la muestra es m.




                                            Revista Colombiana de Estad√≠stica 30 (2007) 53‚Äì68

56                                              H√©ctor Andr√©s L√≥pez & H√©ctor Manuel Mora


1.3.     C√°lculo de Œ≤bœÑ por medio de programaci√≥n lineal
   La t√©cnica m√°s usada para solucionar el problema de regresi√≥n cuant√≠lica (6) es
por medio de su representaci√≥n como un problema de programaci√≥n lineal (Koenker
2005). La funci√≥n de chequeo œÅœÑ se puede escribir como la suma de dos funciones
positivas:
                         œÅœÑ (r) = œÑ p+ (r) + (1 ‚àí œÑ ) p‚àí (r)
donde p+ (r) = max {0, r} y p‚àí (r) = max {0, ‚àír}.
   Sean ui = p+ (yi ‚àíŒ≤ T xi ), vi = p‚àí (yi ‚àíŒ≤ T xi ), u = (u1 , . . . , um ), v = (v1 , . . . , vm )
y 1 = [1, 1, 1, . . . , 1] un vector de unos de dimensi√≥n adecuada.
   La formulaci√≥n del problema de regresi√≥n cuant√≠lica como un problema de
programaci√≥n lineal est√° dada por:

               min{œÑ 1T u + (1 ‚àí œÑ )1T v : y = XŒ≤ + u ‚àí v, (u, v) ‚àà R2m
                                                                     + }                        (7)

   El problema de programaci√≥n lineal (7) tiene n + 2m variables, m restriccio-
nes y 2m variables no negativas. La formulaci√≥n dual del problema de regresi√≥n
cuant√≠lica es            
                     max y T d : X T d = 0, d ‚àà [œÑ ‚àí 1, œÑ ]m                (8)

donde d = [d1 , d2 , . . . , dm ]T es el vector de variables duales. Dicho problema tiene
n+2m restricciones y m variables. Es decir, son menos variables que en el problema
primal. Por lo tanto, en la pr√°ctica es m√°s f√°cil resolver el problema dual para
regresi√≥n cuant√≠lica que el problema primal.
   La formulaci√≥n del problema dual para regresi√≥n cuant√≠lica es equivalente a la
usada en la formulaci√≥n est√°ndar de los m√©todos de punto interior para progra-
maci√≥n lineal con variables acotadas. Dicho algoritmo se encuentra descrito en
Koenker (2005) e implementado en el paquete quantreg del software estad√≠stico
R. Este paquete es el m√°s usado por las personas que trabajan regresi√≥n cuant√≠lica.


2.     M√©todo ACCPM
   El m√©todo ACCPM (Analytic Center Cutting Plane Method) fue creado por
Goffin, Haurie & Vial (1992). El m√©todo ACCPM hace parte de los m√©todos
de planos de corte. Se presentan los conceptos b√°sicos del m√©todo y algunas
observaciones sobre su implementaci√≥n desarrollada en Pet√≥n & Vial (2001).


2.1.     M√©todos de planos de corte
    La mayor√≠a de algoritmos de planos de corte resuelven problemas como el
siguiente:

                                             min cT x
                                           s.a. x ‚àà X

                                             Revista Colombiana de Estad√≠stica 30 (2007) 53‚Äì68

Estimadores de regresi√≥n cuant√≠lica lineal por medio del m√©todo ACCPM               57

donde X ‚äÇ Rn es un conjunto convexo y acotado. Los problemas de la forma

                                     min f (y)
                                    s.a. y ‚àà Y

donde Y ‚äÇ Rn‚àí1 es un conjunto convexo y f es convexa, se pueden convertir a
problemas con la formulaci√≥n del problema inicial de la siguiente manera:

                                       min z
                                 s.a. f (y) ‚àí z ‚â§ 0
                                      y‚ààY

tomando x = (z, y) y X = {(z, y) : f (y) ‚àí z ‚â§ 0, y ‚àà Y } .
    Estos m√©todos construyen una aproximaci√≥n lineal de la regi√≥n factible X ‚Äúme-
jor√°ndola‚Äù en cada iteraci√≥n.
    Sea P0 una aproximaci√≥n poli√©drica de X (X ‚äÇ P0 ) y x0 el punto √≥ptimo de
cT x en P0 . La formulaci√≥n general de un algoritmo de planos de corte para resolver
el problema anterior es:

                           M√©todo de planos de corte
         Inicializaci√≥n
             k := 0
             Definir P0 ‚äÉ X
                                    Àò              ¬Ø
             Encontrar x0 = arg min cT x : x ‚àà P0
                     k
         Mientras x ‚àà  / X hacer
                                       Àò              ¬Ø
             Definir un hiperplano Hk : x : aTk x = bk que separe xk de X
                          Àò     T
                                       ¬Ø
             Pk+1 = Pk ‚à© x : ak x ‚â§ bk
                            Àò               ¬Ø
             xk+1 = arg min cT x : x ‚àà Pk+1
             k =k+1
         Fin mientras

    Los diversos algoritmos de planos de corte difieren en la manera de seleccionar
el nuevo punto xk+1 . Este es el aspecto de mayor importancia ya que cuanto mejor
sea el corte definido por xk+1 , m√°s r√°pido converger√° el algoritmo.
    Entre los m√©todos de planos de corte, se encuentran los m√©todos basados en
centros. Estos m√©todos definen xk+1 por medio del c√°lculo del centro de un con-
junto convexo y compacto llamado conjunto de localizaci√≥n L.
   El conjunto de localizaci√≥n L est√° formado por la intersecci√≥n de los semies-
pacios generados por la aproximaci√≥n lineal de la regi√≥n factible y por una cota
superior de la funci√≥n objetivo
                                
                           L = x : Ax ‚â§ b, cT x ‚â§ z

    Los m√©todos basados en centros difieren en la manera de definir dicho punto
del conjunto de localizaci√≥n. Entre los m√©todos m√°s conocidos se encuentran: el
m√©todo del centro de gravedad, el m√©todo volum√©trico y el ACCPM.

                                      Revista Colombiana de Estad√≠stica 30 (2007) 53‚Äì68

58                                       H√©ctor Andr√©s L√≥pez & H√©ctor Manuel Mora

2.2.     Fundamentos matem√°ticos del m√©todo ACCPM
   El m√©todo ACCPM se aplica a los problemas de optimizaci√≥n que pueden ser
representados de la siguiente manera:

                              min{f (x) : x ‚àà X ‚äÜ X0 }                              (9)

donde el conjunto X ‚äÇ Rn es convexo, la funci√≥n f : Rn ‚Üí R es convexa y X0 es
un poliedro acotado.
    Los m√©todos de planos de corte se basan en la interacci√≥n de dos procedimien-
tos: el or√°culo y el programa principal.
   El programa principal trabaja sobre la relajaci√≥n lineal de la regi√≥n factible del
problema de optimizaci√≥n (9), calculando en cada iteraci√≥n del m√©todo un nuevo
punto central. Adem√°s, controla la convergencia del proceso.
   El or√°culo toma el punto central como entrada y retorna uno o varios planos de
corte al programa principal. Estos planos son de dos tipos: cortes de optimalidad
o cortes de factibilidad, dependiendo de la naturaleza del punto.


2.2.1.    El or√°culo

     Dado el punto x ‚àà X0 , la salida del or√°culo est√° dada as√≠:

       Cortes de factibilidad: si x ‚àà   / X (x no es factible), el or√°culo retorna el
       vector (Œ≥0 , Œ≥) ‚àà R √ó Rn y el corte de factibilidad:

                               hŒ≥, x ‚àí xi + Œ≥0 ‚â§ 0, ‚àÄx ‚àà Xt                        (10)

       Cortes de optimalidad: el punto es factible (x ‚àà X); el or√°culo retorna
       f (x) y un subgradiente Œ≥ ‚àà ‚àÇf (x), que definen la desigualdad conocida como
       corte de optimalidad:

                            f (x) ‚â• f (x) + hŒ≥, x ‚àí xi , ‚àÄx ‚àà X                    (11)


2.2.2.    Conjunto de localizaci√≥n

   Sea (x1 , . . . , xK ) una sucesi√≥n de puntos centrales. El conjunto de √≠ndices K
puede ser expresado como la uni√≥n de dos conjuntos disyuntos IK y JK donde

                 Ik = {k : xk es no factible (corte de factibilidad)}
                 Jk = {k : xk es factible (corte de optimalidad)}

Si JK 6= ‚àÖ se define la cota superior de la soluci√≥n del problema (9) como
z K = min{f (xk ) | k ‚àà JK }. Tomando la uni√≥n de todos los cortes y desigualdades
obtenidos anteriormente, se define un subconjunto del ep√≠grafo de f . Este conjunto
contiene la soluci√≥n √≥ptima y se conoce como el conjunto de localizaci√≥n y se

                                       Revista Colombiana de Estad√≠stica 30 (2007) 53‚Äì68

Estimadores de regresi√≥n cuant√≠lica lineal por medio del m√©todo ACCPM                  59

denota LK ‚äÜ Rn+1 . El conjunto de localizaci√≥n est√° constituido por las siguientes
desigualdades:

                         z ‚â• f (xk ) + Œ≥ k , x ‚àí xk ,     k ‚àà JK                     (12)
                                k        k
                        0 ‚â• Œ≥ ,x ‚àí x         + Œ≥0k ,   ‚àÄk ‚àà IK                       (13)
                       zK ‚â• z                                                        (14)
                         b ‚â• hB, xi                                                  (15)

El primer conjunto de restricciones corresponde a los cortes de optimalidad (12), el
segundo conjunto recibe el nombre de cortes de factibilidad, el tercer conjunto de
restricciones (14) define la cota superior del problema de optimizaci√≥n, el √∫ltimo
conjunto de restricciones son fijas. Usualmente se utilizan restricciones de caja
para las variables de decisi√≥n con el objetivo de definir esta √∫ltima desigualdad
(15).
   Por otro lado, es posible asociar con (12), (13), (14) y (15) las variables duales
Œ±jk ‚â• 0, ¬µk ‚â• 0, ŒΩ ‚â• 0 y œÅ ‚àà R que satisfacen la desigualdad
             X                           X                        
        z‚â•        Œ±k f (xk ) ‚àí Œ≥ k , xk +        ¬µk Œ≥0k ‚àí Œ≥ k , xk + hb, œÅi      (16)
             k‚ààJK                             k‚ààIK

para todo z tal que (z, x) ‚àà LK . La expresi√≥n del lado derecho en (16) es una
cota inferior del problema (9). Dicha cota se notar√° z K . Dadas las cotas superior
e inferior es posible definir una brecha o salto de dualidad: dg,k = z K ‚àí z K . En la
implementaci√≥n del m√©todo es usual trabajar con la brecha de dualidad relativa:
                                          zK ‚àí zK
                                dg,k =
                                         max {1, z K }
Dicho valor es muy importante debido a que con √©l se construye el criterio de
parada. El m√©todo se detiene cuando dg,k ‚â§ .

2.2.3.   M√©todo gen√©rico de planos de corte

   A continuaci√≥n se presentan los pasos b√°sicos de los m√©todos de planos de
corte:
                       M√©todo gen√©rico de planos de corte
               1. Prueba de terminaci√≥n del m√©todo
               2. Escoger un punto central (x, z) ‚àà LK
               3. Calcular una cota inferior para z ‚àà LK
               4. Llamar al or√°culo para x. El or√°culo retorna
                  (a) Cortes de factibilidad (si x es no factible)
                  (b) Cortes de optimalidad (si x es factible)
               5. Calcular la cota superior para z ‚àà LK
               6. Agregar el nuevo corte al conjunto de localizaci√≥n LK

Los diversos m√©todos de planos de corte difieren en la forma de escoger el punto
central en LK . El m√©todo ACCPM encuentra el centro anal√≠tico del conjunto

                                         Revista Colombiana de Estad√≠stica 30 (2007) 53‚Äì68

60                                                 H√©ctor Andr√©s L√≥pez & H√©ctor Manuel Mora

de localizaci√≥n LK . En la biblioteca desarrollada por Pet√≥n & Vial (2001) se
encuentran implementados los pasos 2, 3 y 5 en Visual C++. Los pasos 1 y 4
debe implementarlos el usuario. Dicha implementaci√≥n depende del problema a
resolver.


2.2.4.    C√°lculo del centro anal√≠tico

    Como se mencion√≥ anteriormente, ACCPM calcula el centro anal√≠tico del con-
junto de localizaci√≥n. De forma compacta el conjunto de localizaci√≥n se escribe de
la siguiente manera:
                              LK = {ex : AT x
                                            e ‚â§ c}
El centro anal√≠tico del poliedro acotado LK es la √∫nica soluci√≥n (en caso de existir)
del siguiente problema de optimizaci√≥n2 :
                                          K
                                           X                           
                                                                      T
                          argmin       ‚àí         log(si ) : s = c ‚àí A x
                                                                      e
                                           i=1

En la ecuaci√≥n anterior, se penalizan los puntos cercanos a la frontera, es decir,
las variables de holgura (si ) que tiendan a cero.
    Para el c√°lculo del centro anal√≠tico, los m√©todos se basan en algoritmos de punto
interior para programaci√≥n no lineal, tales como m√©todo primal, m√©todo dual,
m√©todo primal-dual y m√©todo primal proyectivo (Vial 1998). La implementaci√≥n
usada del m√©todo ACCPM obtiene el centro anal√≠tico por medio del m√©todo primal
proyectivo desarrollado en Du Merle (1995).


2.2.5.    Restricciones de caja

    El conjunto de localizaci√≥n es acotado si X0 es acotado. En la mayor√≠a de
aplicaciones de optimizaci√≥n es posible asumir que cada una de las variables de
decisi√≥n se encuentra restringida por unos valores m√°ximos y m√≠nimos (restriccio-
nes de caja), es decir,

                             xm√≠n ‚â§ xi ‚â§ xm√°x ,          i = 1, . . . , n

La implementaci√≥n de ACCPM supone la existencia de restricciones de caja.


3.       C√°lculo de los estimadores de regresi√≥n
         cuant√≠lica por medio del m√©todo ACCPM
   En esta secci√≥n se presenta la forma de aplicar el m√©todo ACCPM en el c√°lculo
de los estimadores de regresi√≥n cuant√≠lica. Adem√°s, se presentan resultados nu-
                                                                        x : AT x
   2 Para que sea v√°lido el c√°lculo del centro anal√≠tico se supone que {e      e ‚â§ c} es acotado y
tiene interior no vac√≠o.

m√©ricos y comparaciones con el paquete R (R Development Core Team 2006) y
GAMS3 (L√≥pez 2006b).


3.1.     Subgradiente y or√°culo para el problema de regresi√≥n
         cuant√≠lica
   El modelo de optimizaci√≥n para regresi√≥n cuant√≠lica se escribe de la forma
                                                    m
                                                    X                          
                     Œ≤bœÑ = argminŒ≤‚ààRn f (Œ≤) =             œÅœÑ yi ‚àí Œ≤ T xi                (17)
                                                    i=1

donde yi ‚àà R, xi ‚àà Rn , i = 1, . . . , m. Es decir, m es el n√∫mero de datos y n el
n√∫mero de variables explicativas, œÅœÑ (u) es la funci√≥n de chequeo con œÑ ‚àà (0, 1).
   Para el anterior problema un subgradiente (Mora 2005) est√° dado por
                                            m
                                            X                     m
                                                                  X
                       Œ≥ = Œ≥(f, Œ≤) = ‚àíœÑ            xi ‚àí (œÑ ‚àí 1)           xi
                                            i=1                    i=1
                                           ri >0                  ri <0


donde ri = yi ‚àí Œ≤ T xi .
   El subgradiente descrito anteriormente se puede escribir en forma desagregada
Para efectos de programaci√≥n se da el valor vectorial 0 al subgradiente Œ≥i cuando
ri ‚àà (‚àí, ), con  positivo y peque√±o, es decir, cuando el residuo es casi 0. En
este caso Œ≥i queda reformulado de la siguiente manera:

Por otro lado, como el problema de regresi√≥n cuant√≠lica es un problema de op-
timizaci√≥n sin restricciones, solo se generan cortes de optimalidad en el m√©todo
ACCPM. Dichos cortes se expresan de la siguiente forma:

                           f (Œ≤) ‚â• f (Œ≤ k+1 ) + Œ≥ k+1 , Œ≤ ‚àí Œ≤ k+1
  3 GAMS (General Algebraic Modeling System) es un lenguaje de programaci√≥n que tiene por

objetivo encontrar soluci√≥n a diversos problemas de optimizaci√≥n a peque√±a y gran escala. Es
posible obtener m√°s informaci√≥n, manuales, ayuda y una versi√≥n demo en la p√°gina www.gams.com

donde Œ≤ k+1 y Œ≥ k+1 son, respectivamente, el centro anal√≠tico y el subgradiente
generados en la k‚àí√©sima iteraci√≥n.
   A continuaci√≥n se presenta la descripci√≥n del algoritmo del or√°culo para el
problema de regresi√≥n cuant√≠lica.

                 Or√°culo para el problema de regresi√≥n cuant√≠lica

   En el caso del m√©todo ACCPM, Œ≤b se obtiene por medio del c√°lculo del centro
anal√≠tico del conjunto de localizaci√≥n. Otro factor de gran importancia es el valor
de Œµs . Dicho valor se llamar√° √©psilon del subgradiente.
    Otros aspectos importantes del m√©todo ACCPM son la brecha de dualidad y
las restricciones de caja. La brecha de dualidad utilizada es
                                                  |Œ≤ k ‚àí Œ≤ k |
                                  dualitygap =
                                                 max{1, |Œ≤ k |}
donde Œ≤ k y Œ≤ k son las cotas superior e inferior del valor √≥ptimo obtenidas en la
k‚àí√©sima iteraci√≥n. Las restricciones de caja utilizadas son de la forma
                                        ‚àíc ‚â§ Œ≤i ‚â§ c
donde i = 1, . . . , n y c > 0.


3.2.     Resultados num√©ricos y comparaciones
    Los resultados num√©ricos presentados a continuaci√≥n corresponden al tiempo
de ejecuci√≥n de los paquetes utilizados para hallar los estimadores del problema
de regresi√≥n cuant√≠lica (ACCPM, R y GAMS4 ). El equipo utilizado para el desa-
rrollo de las pruebas fue un computador con sistema operacional Windows XP,
procesador Pentium 4 con 2.4 GHz y 512 Mb de RAM.
   4 El problema resuelto por GAMS fue el problema dual para regresi√≥n cuant√≠lica debido a que

es de menor tama√±o que el problema primal.
   El solver de optimizaci√≥n utilizado por GAMS es el BDMLP 1.3.
    Los tiempos dados son aproximados e incluyen el tiempo de algunas tareas
propias del sistema operativo. No se hacen comparaciones de requerimientos de
memoria debido a que no se tiene esta informaci√≥n para GAMS ni para R. El
criterio de parada utilizado en el m√©todo ACCPM es la obtenci√≥n de una brecha
de dualidad menor que un valor dado (Œ∏):

                                      dg,k ‚â§ Œ∏

    El valor de œÑ es 0.8. No es necesario presentar resultados con otros valores de œÑ
debido a que cambios en dicho valor no generan cambios en el tiempo de ejecuci√≥n
de los algoritmos.
    Todos los archivos de prueba utilizados fueron generados por medio de n√∫meros
aleatorios. Los valores de n se encuentran entre 5 y 20. Se supone que la matriz
de datos no tiene variables redundantes y es de naturaleza densa. En todas las
pruebas se tom√≥ el √©psilon del subgradiente como Œµs = 10‚àí5 .
   La tabla 1 muestra la soluci√≥n obtenida con R, GAMS y ACCPM para una
base de datos con m = 10000 y n = 13. Para el m√©todo ACCPM se hicieron
dos pruebas, con Œ∏ = 10‚àí3 y Œ∏ = 10‚àí6 . Adem√°s, para las restricciones de caja
c = 1000.
           Tabla 1: Comparaci√≥n de resultados para n = 13 y m = 10000.
    Es importante notar que con tres cifras decimales, la soluci√≥n obtenida con los
tres paquetes es la misma.
    La tabla 2 muestra las diferencias de c√°lculo del m√©todo ACCPM cambiando
el valor de Œ∏. Los diferentes valores son 10‚àí3 , 10‚àí4 , 10‚àí5 y 10‚àí6 con m = 25000
y n = 10 y las restricciones de caja : ‚àí1000 ‚â§ Œ≤i ‚â§ 1000, con i = 1, . . . , 15.
   Para una aproximaci√≥n de 10‚àí6 es necesario generar 57 cortes m√°s que en el
caso de 10‚àí3 . Es decir un 62% m√°s de cortes. Adem√°s, el tiempo de ejecuci√≥n con
Œ∏ = 10‚àí6 fue 2.3 segundos mayor que con Œ∏ = 10‚àí3 . Por lo tanto, utiliz√≥ el 61%
m√°s de tiempo.
               Tabla 2: Tiempos y cortes dependiendo del valor de Œ∏.

   La tabla 3 muestra la soluci√≥n de un problema con m = 15000 y n = 10,
variando los valores de las restricciones de caja: ‚àíc ‚â§ Œ≤i ‚â§ c, con i = 1, . . . , 10.
Se tom√≥ Œ∏ = 10‚àí4 .

      Tabla 3: Tiempos y cortes para varios valores de c (restricciones de caja).

   El tama√±o de la caja no tiene mayor influencia en el tiempo de ejecuci√≥n y
n√∫mero de iteraciones (cortes) del m√©todo ACCPM.
    La tabla 4 muestra la soluci√≥n del m√©todo ACCPM con n = 12, m = 18000 y
Œ∏ = 10‚àí3 para cuatro archivos distintos. El primer archivo es generado por n√∫-
meros aleatorios en el intervalo (0, 1), el segundo archivo es generado por n√∫meros
aleatorios en (0, 100), el tercer archivo con n√∫meros aleatorios en (100, 1000) y
el cuarto archivo con n√∫meros aleatorios en (1000, 100000). El objetivo de reali-
zar dichas comparaciones es revisar las diferencias de ejecuci√≥n del m√©todo para
problemas de igual tama√±o y datos diferentes.

     Tabla 4: Soluci√≥n de problemas con n = 12, m = 18000 para datos distintos.

    Seg√∫n la tabla 4, no existe mucha diferencia entre el n√∫mero de iteraciones y
el tiempo de ejecuci√≥n para problemas con datos distintos y el mismo tama√±o.
    Las siguientes tablas (5, 6 y 7) presentan el tiempo de ejecuci√≥n de cada uno
de los paquetes, variando el valor de n y el valor de m. El s√≠mbolo X indica que
no se dispone de ese valor de tiempo porque el problema result√≥ demasiado grande
y no pudo ser resuelto por el paquete indicado.
   La tabla 5 muestra los resultados para varios valores de m = 100, 300, 1000,
5000, 10000, 30000, 50000, 80000, 100000, 200000 300000 y 400000 con n = 5. Se
toma Œ∏ con un valor de 10‚àí3 y c = 1000.

        Tabla 5: Comparaci√≥n de tiempos ACCPM, GAMS y R para n = 5.

   An√°logamente, la tabla 6 muestra los resultados para los mismos valores de m

        Tabla 6: Comparaci√≥n de tiempos ACCPM, GAMS y R para n = 10.

   La tabla 7 muestra los resultados para los mismos valores de m de las tablas
anteriores y n = 20, Œ∏ = 10‚àí3 , c = 1000.
    De las tablas 5, 6 y 7 es posible notar lo siguiente:
    Para m ‚â§ 10000, el paquete R es muy eficiente debido a que en todas las pruebas
realizadas encuentra los estimadores en menos de 2 segundos y para m ‚â• 30000
el tiempo del m√©todo ACCPM es menor que el tiempo de R y el de la soluci√≥n
obtenida por GAMS para el problema dual de regresi√≥n cuant√≠lica. Adem√°s, el
tiempo de ACCPM aumenta de forma m√°s o menos lineal. Con los otros dos
paquetes el tiempo de ejecuci√≥n crece de forma m√°s acelerada y en algunos casos
no es posible encontrar la soluci√≥n. Por ejemplo, para n = 5 los tiempos con

       Tabla 7: Comparaci√≥n de tiempos ACCPM, GAMS y R para n = 20.

ACCPM se encuentran entre 0.8 y 3.1 segundos (0.8 segundos para m = 100 y
3.1 segundos para m = 400000). Los tiempos de R se encuentran entre 0 y 492
segundos. Gams entre 0 y 542 segundos. Este √∫ltimo no logra encontrar la soluci√≥n
cuando m ‚â• 200000.
   De forma an√°loga, para n = 20, los tiempos de ACCPM se encuentran entre 2.7
y 27.9 segundos. Adem√°s, con el m√©todo ACCPM en todos los casos fue posible
encontrar la soluci√≥n. Con R los tiempos de ejecuci√≥n se encuentran entre 0 y
130 segundos. En este caso no fue posible hallar la soluci√≥n cuando m ‚â• 300000.
Para GAMS los tiempos oscilan entre 0 y 778 segundos. Adem√°s, no fue posible
encontrar la soluci√≥n cuando m ‚â• 200000.
   Para un valor fijo de n y variando el valor de m, el n√∫mero de cortes de
optimalidad (iteraciones) no var√≠a de forma significativa. Por ejemplo, cuando
n = 10, el m√≠nimo n√∫mero de cortes generados es 52 con m = 100 y la mayor
cantidad es 63 con m = 400000.
    Con n = 20, el n√∫mero de cortes (iteraciones) var√≠a entre 106 y 127; se nota
que para este caso hay menor cantidad de cortes con m = 400000 que cuando
m = 30000, m = 50000, m = 1000, m = 5000. Es decir, el n√∫mero de iteraciones
del m√©todo depende exclusivamente del n√∫mero de variables explicativas y no de
la cantidad de datos. La diferencia del tiempo de ejecuci√≥n depende del c√°lculo
del subgradiente debido a que en problemas de mayor tama√±o es necesario hacer
m√°s operaciones para su obtenci√≥n.
  Para problemas con m ‚â§ 1000, la soluci√≥n del problema dual por medio de
GAMS se encuentra m√°s r√°pido que con ACCPM. Para m ‚â• 1000, el m√©todo
ACCPM es m√°s r√°pido.


3.3.   Dificultades del m√©todo ACCPM
   El m√©todo ACCPM tiene restricciones para su ejecuci√≥n y depende del valor de
Œ∏. En la tabla 8 se presentan diferentes tama√±os m√°ximos para la matriz de datos.
Varias de las dificultades se generan por la capacidad de c√°lculo del computador

y del m√©todo. En algunos casos fue posible resolver problemas de mayor tama√±o
que el indicado en la tabla 8 con valores de restricciones de caja de la forma

con los valores de di y gi cercanos a los estimadores de los par√°metros de regresi√≥n
cuant√≠lica y adem√°s no sim√©tricos, es decir gi 6= ‚àídi .

                    Tabla 8: Restricciones del m√©todo ACCPM.

4.    Conclusiones

    La implementaci√≥n del m√©todo de punto interior primal-dual de programaci√≥n
lineal para la soluci√≥n del problema de regresi√≥n cuant√≠lica desarrollada en el soft-
ware R por medio del paquete quantreg es la m√°s eficiente cuando el n√∫mero de
datos es menor que 10000. Por otro lado, el m√©todo ACCPM result√≥ ser el m√°s
eficiente cuando el n√∫mero de datos es mayor que 30000. Adem√°s, es bastante es-
table tanto en el tiempo de ejecuci√≥n como en el n√∫mero de iteraciones generadas.
Es decir, es recomendable usar el m√©todo ACCPM cuando se tiene un n√∫mero
grande de datos.
    Seg√∫n los resultados, para m ‚â• 1000 el paquete de optimizaci√≥n GAMS resulta
ser el menos eficiente. Esto se debe en parte a las restricciones de ejecuci√≥n con
respecto al tama√±o del problema de programaci√≥n lineal y al m√©todo utilizado
para encontrar la soluci√≥n (simplex). En el caso de m < 1000, GAMS resulta ser
el m√°s eficiente.
    El n√∫mero de cortes de optimalidad (iteraciones) generados en el m√©todo
ACCPM depende del n√∫mero de variables explicativas. En este caso, no es un
factor influyente el n√∫mero de datos, ni la naturaleza de los mismos.
    El tama√±o de las restricciones de caja no influye en el c√°lculo de los estimado-
res. La soluci√≥n generada para el problema de regresi√≥n cuant√≠lica por medio de
ACCPM no presenta cambios significativos con respecto a cambios sobre el valor
de la cota para la brecha de dualidad (Œ∏) y del √©psilon del subgradiente (s ).

Agradecimientos
El presente trabajo se deriva de la tesis de maestr√≠a en Matem√°tica Aplicada del primer autor (L√≥pez 2006a).
Agradecemos al profesor Edilberto Ruiz, del Departamento de Estad√≠stica de la Universidad Nacional de Colombia, por su asesor√≠a y lectura del documento en el √°rea de regresi√≥n cuant√≠lica y al estad√≠stico Rafael L√≥pez por su apoyo e indicaciones en el manejo del paquete estad√≠stico R. Tambi√©n, las sugerencias y comentarios hechos por los evaluadores de este art√≠culo.

Referencias
Du Merle O.Points int√©rieurs et plans coupants: mise en uvre et d√©veloppement d‚Äôune m√©thode pour l‚Äôoptimisation convexe et la programmation lin√©aire structur√©e de grand taille.(1995).Universidad de Ginebra.
Goffin J,Haurie A,Vial J.Decomposition and Nondiferentiable Optimization with the Projective Algorithm.(1992).Management Science.
Koenker R.Quantile Regression.(2005).Cambridge University Press.
Koenker R,Bassett G W.Regression Quantiles.(1978).Econometrics.
L√≥pez H.C√°lculo de la regresi√≥n cuant√≠lica por medio del m√©todo ACCPM.(2006).Universidad Nacional de Colombia.Bogot√°.
L√≥pez H.Introducci√≥n a GAMS y su aplicaci√≥n en la soluci√≥n de modelos matem√°ticos de optimizaci√≥n.(2006).Universidad Nacional de Colombia.Bogot√°.
Mora H.M√©todos num√©ricos para la estimaci√≥n de par√°metros en regresi√≥n cuant√≠lica.(2005).Revista Colombiana de Estad√≠stica.
Pet√≥n O,Vial J.A tutorial of ACCPM Version 2 01.(2001).Universidad de Ginebra.
R Development Core Team.R: A Language and Environment for Statistical Computing.(2006).R Foundation for Statistical Computing.Austria.
Vial J P.Analytic center of polytope.(1998).Universidad de Ginebra.