Un criterio para identificar datos atı́picos
Universidad Nacional de Colombia
Resumen
En este artı́culo se presenta un método para determinar las observaciones que son atı́picas en un modelo de regresión lineal múltiple; estos datos se estableceran de acuerdo al cambio que ejercen sobre la suma de los cuadrados de residuales del modelo.
Palabras Claves: Modelos lineales, mı́nimos cuadrados, formas cuadráticas, observaciones atı́picas, estadı́stica Qk.
Introducción
Draper & John (1981) proponen una metodologı́a para detectar un grupo de k observaciones atı́picas, análoga a la propuesta de Bartlett (1937), citada
en Little & Rubin (1987), para estimar los parámetros del modelo de regresión lineal cuando existen observaciones faltantes en la variable respuesta. En el planteamiento de Draper & John (1981) se considera el modelo de regresión lineal múltiple:
particionado de la siguiente manera:
donde Y1 es el bloque conformado por las observaciones consideradas atı́picas.
Para el modelo (2) establecen las estimaciones de β y γ mediante:
donde Hij = Xi (X 0 X)−1 Xj0 es una submatriz de la matriz
    La notación de H y el nombre de matriz hat fue introducido por Tukey
(1977); por otra parte, el cambio en la suma de cuadrados de residuales lo
calculan usando la estadı́stica:
     En resumen, el método descrito permite detectar el grupo de observaciones
atı́picas en base al cambio en la suma de cuadrados de residuales, lo cual se
cuantifica con la estadı́stica Qk , es decir, mediante este procedimiento se se-
lecciona el bloque Y1 que posee el Qk más alto, como el bloque más atı́pico, y
en muchos casos quedan datos atı́picos dentro de un bloque y el método no los
identifica. En este artı́culo se muestra un criterio para identificar el bloque Y1
que contiene el grupo más grande de observaciones atı́picas.

2.      Resultados básicos del ajuste del modelo de
        regresión lineal múltiple
    Mediante el método de estimación mı́nimos cuadrados ordinarios (MCO)
se obtiene para el modelo dado en (1) los siguientes estimadores:
    Obsérvese que la matriz H determina muchos de los resultados de las estimaciones por MCO; por ejemplo, cuando premultiplica al vector de respuestas
Y se obtienen los valores predichos de la variable dependiente, por eso en la literatura estadı́stica en algunos casos la denominan matriz de predicción, y a
la matriz I − H la llaman matriz residual, puesto que al anteponérsele a la
variable dependiente Y se obtienen los respectivos residuales.
Propiedades de las componentes de la matriz H
En Hoaglin & Welsch (1978) se establece para la matriz H = [hij ] de tamaño
n × n, las siguientes propiedades:
Si la matriz X de tamaño n × r es de rango r, entonces
donde tr(H) denota la traza de la matriz H.
     Dado que hij = xi (X 0 X)−1 x0j , entonces hii está determinado por la localización de xi en el espacio X, es decir, un valor pequeño (grande) de hii indica
que xi se encuentra cerca (lejos) de la masa de los otros puntos. Además,
sugieren que xi es un punto influyente si hii > 2r/n.

3.      Cálculo de la estadı́stica Qk
   En Jiménez (2001b) se establece para la estadı́stica dada en (3), la siguiente
expresión:
                Qk = SCE − SCE ∗ = −2γ 0 ˆ − γ 0 (I − H)γ ,                   (5)
donde SCE es obtenida en términos algebraicos como en (4) y SCE ∗ , repre-
senta la estimación vı́a mı́nimos cuadrados (EM C) de SCE sin el bloque Y1 de
observaciones. Además, muestra que si el interés es minimizar la SCE ∗ , esto
se logra haciendo:
                                     ∂Qk
                                          = 0,
                                      ∂γ
lo cual equivalente a hacer:
                                 + (I − H)b
                                b          γ = 0,                             (6)
       es la estimación vı́a mı́nimos cuadrados (EM C) de  del modelo (1).
donde b
     Al remplazar (6) en (5) se tiene:
                             b0 (I − H)b
                        Qk = γ           b0 γ
                                       γ=γ    b0 Hb
                                            b−γ   γ.                          (7)

    Esta nueva expresión de Qk tiene la ventaja de que está en términos de la
estimación del γ arbitrario, la cual para los objetivos de este trabajo es más
atractiva, ya que se podrá establecer su distribución de probabilidad correspondiente.

4.      Distribución de probabilidad de Qk
     En Jiménez (2001a) al asumir la restricción γ̂ = 1 , se llega a:

donde Ik es la matriz identidad de tamaño k × k, con k igual a la dimensión
del bloque Y1 y Mij = Xi (X20 X2 )−1 Xj0 .

   Si se reemplaza (8) en el primer término de la expresión (7) se obtiene

    Por otra parte, si se sustituye (8) en el segundo término de la expresión (7)
y se emplean los resultados dados en Jiménez (2001a), se tiene que:

   Finalmente, al sustituir (9) y (10) en la ecuación (7), se obtiene que:
Nótese que la matriz (M − H) es simétrica; además, es idempotente. Esto se
puede verificar de la siguiente manera:

                  (M − H) (M − H) =M 2 − M H − HM + H 2 ,

pero M 2 = M , ya que:
                                                             
           Ik     0    Ik        0     I         0        I      0
                                     = k                = k           .
            0 M22 0             M22     0      M22 M22     0    M22

   Esto se tiene, ya que para i, j = 1, 2:
                         0          0      0      0         0         0
   Mi2 M2j = [Xi (X2 X2 )−1 X2 ][X2 (X2 X2 )−1 Xj ] = Xi (X2 X2 )−1 Xj = Mij ;

por otra parte, HM = H lo cual se puede verificar como sigue:
                                                       
          H11 H12 Ik      0        H11 H12 M22        H11 H12
                               =                    =           .
          H21 H22 0 M22            H21 H22 M22        H21 H22

114                                                             José A. Jiménez M.

                                        
                                        X1
   Aquı́ cabe notar que cuando X =         es de rango completo, entonces:
                                        X2
                       0       0       0         0          0         0
      Hi2 M2j = [Xi (X X)−1 X2 ][X2 (X2 X2 )−1 Xj ] = Xi (X X)−1 Xj = Hij ,
para i, j = 1, 2; además, como las matrices H y M son simétricas se tiene que
H = (M H)t = HM . En consecuencia,
                           (M − H) (M − H) = M − H.
Para establecer la distribución de Qk , se presentan, sin demostración, los teo-
remas 1 y 2, mencionados en Searle (1971).
Teorema 1. Si Y es un vector aleatorio de tamaño n × 1, distribuido N (µ, V ),
donde µ es en si mismo un vector entonces:
   E [Y 0 AY ] = tr(AV ) + µ0 Aµ   y   Var [Y 0 AY ] =2 tr(AV )2 + 4µ0 AV Aµ.
                                                       0
Teorema 2. Si Y ∼ N (µ, V ), entonces Y 0 AY ∼ χ2(ν,λ) , con grados de liber-
tad ν = ρ(A) y parámetro de no centralidad λ = 21 µ0 Aµ, si y sólo si AV es
idempotente.

   Puesto que, bajo el supuesto de normalidad en los residuales se tiene que
                               Y ∼ N (Xβ, σ 2 In ).                            (12)

   Como la expresión dada en (11) es una forma cuadrática se establecerá a
continuación la respectiva distribución asociada. Por el teorema 1, se tiene que
                                       h en el modelo (1).
donde r es el rango de la matriz X definida              i Cuando esta
                                                  0         0
matriz es de rango completo se tiene que tr (X2 X2 )−1 (X2 X2 ) = r.

    Utilizando el teorema 2, también se concluye que Qk /σ 2 tiene distribución
ji-cuadrado central:
                                 Qk
                                        ∼ χ2(ν) ,                           (13)
                                  σ2
                       h 0            0
                                            i
donde ν = k − r + tr (X2 X2 )−1 (X2 X2 ) . Aquı́ el teorema 2 es aplicable ya
      1
que 2 (M − H)σ 2 In es una matriz idempotente.
     σ

Un criterio para identificar datos atı́picos                                   115


5.      Metodologı́a para establecer datos atı́picos
     Dado que la estadı́stica Qk se puede obtener de la forma cuadrática:
                                           0
                                       b (I − H)b
                                  Qk = γ        γ,                            (14)

al expresarla en términos del vector de respuestas Y , queda como:
                                                           
                       0  Ik       0         0  H11       H12
               Qk =Y                    Y −Y                    Y.            (15)
                           0     M22            H21       H22

                                              
                                               Y1
     Si se considera que en la partición Y =     , el bloque Y1 está conformado
                                               Y2
por las observaciones atı́picas, dicho bloque afectará todas las EMC del modelo
dado en (1). Por otra parte, si se reescribe la expresión (5), se tiene que:

                                SCE = SCE ∗ + Qk ,

y dado que SCE ∗ puede expresarse en forma matricial como sigue
                                        
                ∗    0  0          0             0
            SCE = Y                        Y = Y [In − M ] Y ;                (16)
                        0     In−k − M22

usando (12), se puede establecer que las expresiones,

                  SCE                                    SCE ∗
                                          y                    ,              (17)
                   σ2                                     σ2
tienen distribución ji-cuadrado central. Luego, si se divide la ecuación (13) por
cualquiera de las expresiones dadas en (17), se elimina el término σ 2 y queda
el cociente entre dos formas cuadráticas que se distribuyen ji-cuadrado.
    Por la teorı́a estadı́stica se sabe que cuando se realiza el cociente entre dos
variables aleatorias independientes con distribución ji-cuadrado y cada una se
divide por sus respectivos grados de libertad, se obtiene una nueva variable con
distribución F .
    Para llevar a cabo el cociente mencionado anteriormente se debe verificar
con cuál de las distribuciones asociadas a las expresiones dadas en (17) la
distribución de probabilidad expresada en (13) es independiente; para ello, se
enuncia sin demostración el teorema 3, citado en Searle (1971).

Teorema 3. Cuando Y ∼ N (µ, V ), las formas cuadráticas Y 0 AY y Y 0 BY ,
están distribuidas independientemente si y sólo si AV B = 0.

116                                                            José A. Jiménez M.


    Veamos si las distribuciones asociadas a Qk y SCE son independientes. Si
se retoman las ecuaciones dadas en (11) y (4), se tiene por el teorema 3 que
Qk y SCE no son independientes, pues,

   (M − H)(σ 2 In )(In − H) = σ 2 (M − H)(In − H)
                              = σ 2 [M − M H − H + H 2 ] = σ 2 (M − H) 6= 0;

en la última ecuación se tuvo en cuenta que H es idempotente y que M H = H.
    De manera análoga, se verifica si son independientes las distribuciones de
probabilidad de Qk y SCE ∗ ; de las ecuaciones (11) y (16) utilizando el teorema
3, se concluye que son independientes, ya que:

          (M − H)(σ 2 In ) (In − M ) = σ 2 (M − H) (In − M )
                                     = σ 2 M − M 2 − H + HM = 0.
                                                            


En esta última expresión se utilizaron los resultados: M H = H y M 2 = M .
   La media y varianza de la SCE ∗ se obtienen por el teorema 1, como sigue:
Como la media y la varianza de la distribución χ2η son η y 2η respectivamente, se
            0              
deduce que Y (In − M ) Y /σ 2 tiene distribución ji-cuadrado central. Se llega
                                1
a la misma conclusión, ya que 2 (In − M ) σ 2 In es idempotente, utilizando el
                               σ
teorema 2. Ası́ pues,
                                0
                              Y (In − M ) Y
                                            ∼ χ2η ,                           (18)
                                   σ2
                  0            0
con η = n−k−tr (X2 X2 )−1 (X2 X2 ) . Cuando la matriz X es de rango completo
                                     
                0           0
se tiene que tr (X2 X2 )−1 (X2 X2 ) = r.
                                   

   Como las distribuciones de probabilidad asociadas a las expresiones (15) y
(16) son independientes, al hacer el cociente entre las relaciones (13) y (18),

Un criterio para identificar datos atı́picos                                     117


dividiendo cada una por sus correspondientes grados de libertad, se llega a:

     Estos resultados se pueden resumir en los siguientes teoremas.
Teorema 4. Si en un modelo de regresión lineal múltiple particionado como:
                                        
                        Y1       X1         
                             =        β+ 1 ,
                        Y2       X2         2
se elimina el bloque Y1 de dimensión k, entonces el cambio que se presenta en
la SCE se calcula mediante la expresión:
                                            b0 [In − H] γ
                                          1 γ           b
                              ∆(Y1 ) =            2       ,                     (19)
                                          k      S(Y1 )

       2               SCE ∗
donde S(Y1)
             b2 =
            =σ                  es la estimación usual de σ 2 , después de eliminar
                     n−k−r                
                                          γ                            0         0
                                    b = 1 , con γ
las observaciones del bloque Y1 , y γ              b1 = −Y1 +X1 (X2 X2 )−1 X2 Y2 .
                                          b
                                           0
Teorema 5. En un modelo de regresión lineal múltiple Y = Xβ + , bajo el
supuesto de que  ∼ N (0, σ 2 In ), se tiene que:
                                               k = dimensión del bloque Y1 ,
        ∆(Y1 ) ∼ F(k,n−r−k) ,       con
                                               r = rango de la matriz X.

En este caso, se clasifica como atı́pico al bloque Y1 de observaciones, si con un
nivel de significancia α se satisface que:
                                ∆(Y1 ) > F(k,n−r−k,α/2) .                       (20)


6.      Ejemplo
   En la Tabla 1, se considera el conjunto de 21 observaciones (x, y), dado por
Mickey, Dunn & Clark (1967).
     Para este conjunto de datos, se presentan los siguientes resultados:

  1. La estimación del modelo de regresión lineal, con las 21 observaciones.

  2. Los elementos de la diagonal de la matriz H, las estimaciones de los γi y
     al eliminar el i-ésimo dato se establecen la estadı́stica Q1 , la distancia de
     Cook y la estadı́stica ∆(i) con su p-valor correspondiente.

  3. La estimación del modelo de regresión lineal, después de eliminar la ob-
     servación influyente determinada mediante distancia de Cook.

  4. La estimación del modelo de regresión lineal, sin la observación que se
     considera influyente por la estadı́stica ∆(i) .

  1. Análisis de varianza para el conjunto completo de datos:

   2. Compendio de estadı́sticas:

      De los resultados anteriores se tiene que:

        a) La observación que se clasifica como influyente, usando la estadı́stica
           propuesta por Cook, coincide con la que se detecta con el criterio
           para el elemento hii .
        b) Los otros métodos detectan la misma observación como atı́pica cuan-
           do se elimina una sola observación, pero cuando se eliminan dos
           o más observaciones el procedimiento más formal es el del p-valor
           asociado a la estadı́stica ∆(Y1 ) .

   3. Cuando se elimina la observación 18, se obtiene:
      La distancia de Cook nos indicó que la pareja (42, 57) era la que más
      afectaba la EM C de los parámetros, pero al eliminarla el modelo obtenido
      fue más deficiente que el modelo completo. Por lo tanto, la observación
      es solamente influencial pero no es atı́pica.
   4. Eliminando la observación 19 que detectó ∆(i) como atı́pica, se tiene:

El modelo que se obtiene al eliminar la pareja (17, 121) es mejor que el modelo
completo, pues el nuevo coeficiente de determinación es superior al del modelo
inicial. El valor crı́tico de la F es también inferior al valor crı́tico que se deter-
minó en el análisis de varianza del modelo inicial y, además, el cuadrado medio
del error (CM E) fue menor que el CM E del modelo completo. Aunque dicha
observación es atı́pica, no es influyente en la estimación de los parámetros del
modelo.

Conclusiones
La metodologı́a aquı́ presentada permite detectar en un grupo de observaciones la observación más atı́pica, es decir, el dato más influyente sobre el cambio en la suma de cuadrados de los residuales. Además, este procedimiento proporciona una manera de cuantificar el impacto de cada observación sobre la suma de cuadrados de los residuales, pues empleando la distribución F -central este método permite asignarle un p-valor a cada influencia; de esta manera se obtiene un criterio más exacto que el usado tradicionalmente.
Bibliografía
Bartlett M S.Some examples of statistical methods of research in agriculture and applied botany.(1937).Journal of the Royal Statistical Society.
Draper N R,John J A.Influential observations and outliers in regression.(1981).Technometrics.
Hoaglin D C,Welsch R E.The hat matrix in regression and anova.(1978).The American Statistician.
Jiménez J A.Una generalización de la estadística de Cook.(2001).Revista Colombiana de Estadı́stica.
Jiménez J A.Una maximización de la estadı́stica Qk.(2001).Revista Colombiana de Estadística.
Little R J,Rubin D B.Statistical Analysis With Missing Data.(1987).John Wiley & Sons.
Mickey M R,Dunn O J,Clark V.Note on the use of stepwise regression in detecting outliers.(1967).Computers and Biomedical Research.
Searle S.Linear Models.(1971).John Wiley & Sons.
Tukey J W.Exploratory Data Analysis.(1977).Addison Wesley.