Detección gráfica de la multicolinealidad mediante el h-plot de la inversa de la matriz de correlaciones
Universidad Central de Venezuela;Instituto de Investigaciones Económicas y Sociales UCV;Universidad de Salamanca
Resumen
La multicolinealidad origina imprecisión en los estimadores de los coeficientes de un modelo lineal. En este trabajo proponemos un gráfico basado en la representación h-plot de la inversa de la matriz de correlaciones, que permite visualizar con cierto grado de aproximación las relaciones lineales entre las variables predictoras. En este dispositivo se obtienen representaciones aproximadas de los coeficientes de inflación de varianza de cada variable y de las correlaciones parciales entre ellas. Con el objeto de ilustrar el método, éste se aplicó en una investigación sobre la caracterización morfológica de jóvenes nadadores venezolanos.
Palabras Claves: Multicolinealidad, h plot, correlación parcial, coeficiente de inflación de varianza.
Introducción
El Problema
Multicolinealidad es el término usualmente utilizado para referirse a la existencia de relaciones lineales o cuasilineales entre las variables predictoras en un modelo lineal, lo que indica que parte sustancial de la información en una o más de estas variables es redundante. Mandell (1982) señala que una de las principales dificultades en el uso de estimaciones mı́nimo cuadráticas es la presencia de este fenómeno que, aún cuando no afecta la capacidad predictiva del modelo, representa un problema grave si su propósito fundamental es evaluar la contribución individual de las variables explicativas. Esto es debido a que en presencia de multicolinealidad los coeficientes bj tienden a ser inestables, es decir sus errores estándar presentan magnitudes indebidamente grandes. Esta falta de precisión afecta los contrastes parciales diseñados para evaluar la contribución individual de cada variable explicativa, corriéndose un alto riesgo de no encontrar significación en variables que realmente la tengan. Jackson (1991) subraya además que bajo condiciones de colinealidad resulta imposible distinguir los efectos individuales de cada variable predictora, debido a que la fuerza de la correlación entre ellas produce relaciones lineales de similar magnitud entre los coeficientes.
Antecedentes
Numerosos métodos han sido desarrollados con el objeto de detectar la po-
sible existencia de multicolinealidad y sus efectos anómalos sobre un modelo de
regresión. Una primera aproximación al problema plantea analizar la matriz de
correlaciones R, procedimiento útil pero que no capta el fenómeno en toda su in-
tensidad puesto que estudia las relaciones entre las variables dos a dos, obviando
las relaciones de éstas con las otras variables predictoras. Otras propuestas alter-
nativas se basan en el coeficiente de determinación múltiple de cada variable Xj
con las restantes, y en los coeficientes de correlación parcial de las variables Xj
y Xk , controlando por los efectos lineales de las restantes. En particular, Kendall
(1957) enfoca el abordaje práctico de la colinealidad a través de un procedimiento
diseñado en función de los autovalores y autovectores de la matriz de correlacio-
nes. Silvey (1969) plantea como una forma de superar el problema, agregar nuevos
valores de las variables explicativas que eliminen la colinealidad, obtenidos como
función de los autovectores asociados a autovalores nulos. Mandell (1982) demues-

Detección gráfica de la multicolinealidad                                         209

tra que el error estándar del j-ésimo coeficiente de regresión puede expresarse como
el producto del error estándar residual de la regresión por el factor de inflación de
varianza (VIF), ampliamente utilizado para detectar multicolinealidad; en parti-
cular demuestra que el VIF se ve severamente afectado por los autovalores más
pequeños de la matriz R. Este coeficiente mide el incremento que se produce en
la varianza de bj respecto del valor mı́nimo que se alcanzarı́a en ausencia total de
colinealidad de la correspondiente variable Xj respecto de las restantes variables
predictoras (Glantz & Slinker 2001). Adicionalmente, la expresión VIF(j) - 1 coin-
cide, excepto por los grados de libertad, con el estadı́stico que contrasta la bondad
del ajuste de la regresión de Xj como función de las restantes variables explicati-
vas. Mason, Gunst & Webster (1975) proponen el ı́ndice conocido como condition
number, definido como el cociente entre el mayor y el menor autovalor de la ma-
triz R. Por su parte, Gleason & Staelin (1975) proponen un ı́ndice basado en los
autovalores de la matriz de correlaciones que toma el valor 0 cuando las variables
son independientes (R = I) y el valor 1 cuando las variables están perfectamente
correlacionadas (R = J). Raveh (1985) discute la importancia de ciertos elemen-
tos fuera de la diagonal principal de la inversa de la matriz de correlaciones para
detectar predictores importantes en un análisis de regresión y como criterio para
evaluar los supuestos requeridos para aplicar un análisis de factores. Whitakker
(1990) resalta la utilidad de la inversa de la matriz de correlaciones para estable-
cer relaciones de dependencia entre variables y propone su representación gráfica
mediante los denominados grafos de independencia condicional. Belsley (1991) re-
fiere las ventajas y debilidades de los VIF como medida para el diagnóstico de la
colinealidad. Yu (1998) desarrolla un programa multimedia para ilustrar visual-
mente la forma como un modelo de regresión puede colapsar cuando las variables
predictoras están intercorrelacionadas.


2.     Fundamentación teórica
    La propuesta que presentaremos en este trabajo está basada en la técnica h-plot
aplicada a la inversa de la matriz de correlaciones (Si R fuese singular, la técnica
se aplicarı́a sobre la inversa generalizada Rg ). En este apartado describiremos
los fundamentos de este método y centraremos nuestra atención en los elementos
genéricos de la matriz R−1 .


2.1.     Método h-plot
    El h-plot es un procedimiento introducido por Cornsten & Gabriel (1976) para
obtener representaciones gráficas de la información contenida en una matriz de va-
rianzas y covarianzas Spxp de rango r, sobre espacios reducidos de baja dimensión.
La selección adecuada de los vectores, denominados marcadores por sus autores,
garantiza que en su representación sobre el primer plano h-plot se cumple que:

(a) El producto escalar entre dos marcadores aproxima la covarianza entre las
    variables correspondientes,

210                     G. Ramı́rez, M. Vasquez, A. Camardiel, B. Perez & P. Galindo


(b) La longitud de los marcadores aproxima la desviación estándar de las variables,
(c) El coseno del ángulo entre dos marcadores aproxima la correlación entre las
    variables correspondientes, y,
(d) El plano proporciona la mejor representación bidimensional aproximada, desde
    el punto de vista de los mı́nimos cuadrados, de las relaciones entre las variables
    en términos de varianzas y correlaciones.

    La representación en referencia es aplicable a cualquier matriz simétrica Apxp
de rango r, y se efectúa eligiendo vectores marcadores (h1 , h2 , . . . , hp ) para sus
columnas, tales que los elementos de la matriz se obtienen a partir de operaciones
de producto interno entre los marcadores:

                          ajj = htj hj     y     ajk = htj hk

El procedimiento para la selección de los marcadores se basa en la descomposición
espectral de la matriz A:
                                A = V(r) D(r) Vt(r)
de manera que la matriz cuyas columnas contienen los marcadores se define en la
forma:
                               1/2
                     Ht(r) = D(r) Vt(r) = (h1 , h2 , . . . , hp )
siendo V(r) una matriz cuyas columnas son los autovectores de A asociados con sus
r autovalores no nulos, y D(r) una matriz diagonal que contiene tales autovalores.
    Además, Gabriel (1971) ha propuesto como medida de la bondad de la apro-
ximación sobre el primer plano h-plot, al cociente:
                                    2
                                     λ1 + λ22
                                              
                                      P 2
                                          λα
siendo λ1 y λ2 los dos mayores autovalores de la matriz A.


2.2.    Elemento genérico de la matriz R−1
    Con el objeto de detectar multicolinealidad en un conjunto de variables, cons-
truiremos un plano h-plot sobre el cual se representará la información contenida en
la inversa de la matriz de correlaciones R−1 = (rjk ). Con este fin consideraremos
la siguiente permutación de las columnas de la matriz de variables Xnxp :

                                X∗ = (Xj Xk X(−j,−k) )

   donde X(−j,−k) es una matriz cuyas columnas contienen la información de
todas las variables excepto la j y la k.
   La correspondiente matriz de correlaciones particionada:
                                                                       
                      R11    R12        R(j,k)           R(j,k),(−j,−k)
                R=                =
                      R21    R22     R(−j,−k),(j,k)       R(−j,−k)

Detección gráfica de la multicolinealidad                                         211

siendo R(j,k),(−j,−k) la matriz de correlaciones, de orden 2 × (p − 2), de Xj y Xk
respectivamente con las restantes variables:
                                                                   
                                                 Rj,(−j,−k)
                                R(j,k),(−j,−k) =
                                                 Rk,(−j,−k)
La inversa de R se denotará mediante:
                                                           
                                −1     Ř11          Ř12
                              R =
                                       Ř21          Ř22
                                                                        −1
donde Ř11 = (R(j,k) −R(j,k),(−j,−k) R−1 (−j,−k) R(−j,−k),(j,k) ) , siendo Ř−1
                                                                              11 de la
forma:
        "                                                                              #
   −1     1 − Rj,(−j,−k) R−1
                           (−j,−k) R(−j,−k),j    rjk − Rj,(−j,−k) R−1
                                                                    (−j,−k) R(−j,−k),k
Ř11 =
         rjk − Rk,(−j,−k) R−1
                            (−j,−k) R(−j,−k),j    1 − Rk,(−j,−k) R−1
                                                                   (−j,−k) R(−j,−k),k
        "                                                                             #
                       2                                           −1
                1 − RXj.{X(−j,−k)}              rjk − Rj,(−j,−k) R(−j,−k)  R(−j,−k),k
      =                     −1                                 2
         rjk − Rk,(−j,−k) R(−j,−k) R(−j,−k),j           1 − RXk.{X(−j,−k)}

matriz de orden 2 × 2 que puede escribirse como:
                                                          
                      T olXj.{X(−j,−k)}          T
                              T          T olXk.{X(−j,−k)}

donde T = (T olXj.{X(−j,−k)} )1/2 (T olXk.{X(−j,−k)} )1/2 rjk.X(−j,−k) y T ol denota
el ı́ndice conocido como Tolerancia, definido como uno menos el coeficiente de
determinación múltiple correspondiente.
     El término (1, 2) de la matriz anterior (Ř−111 ), normalizado por la raı́z del
producto de los elementos correspondientes en la diagonal, es el coeficiente de
correlación parcial entre Xj y Xk . Este término coincide, excepto por el signo, con
el correspondiente término normalizado en la matriz inversa (Ř11 ):

                                    rjk
                                 √ √      = −rjk.{−j,−k}
                                  rjj rkk
y el término (1, 1) de la matriz Ř11 (inversa de Ř−1
                                                     11 ) es de la forma:

                                        1                       1
                        rjj =
                                (1 − r2jk.(−j,−k) ) T olXj.{X(−j,−k)}

Se demuestra además que este último término es igual a:
                                              1
                                       T olXj.{X(−j)}

expresión que coincide con VIF(j). Farrar & Glauber (1967) señala que una ins-
pección de los rjj puede dar importantes pistas acerca de la severidad de las
redundancias en el modelo.

212                     G. Ramı́rez, M. Vasquez, A. Camardiel, B. Perez & P. Galindo


3.     Propuesta
    En este trabajo proponemos un dispositivo gráfico cuyo propósito es obtener
una representación aproximada de las relaciones de dependencia lineal que se pro-
ducen entre un conjunto de variables. Especı́ficamente, en dicha representación se
visualizan los VIF para cada variable y los coeficientes de correlación parcial.
    En este caso la descomposición espectral de rango r de la matriz R−1 toma la
forma:
                          R−1 = (rij ) = V(r) D−1     t
                                                 (r) V(r)

siendo V(r) la matriz cuyas columnas son los autovectores de R−1 asociados con
sus r autovalores no nulos, organizados sobre la matriz diagonal D−1
                                                                  (r) .
                                                            −1/2
    Al definir la matriz de marcadores como Ht(r) = D(r) Vt(r) = (h1 , h2 , . . . , hp ),
su representación gráfica sobre el primer plano h-plot garantiza que:

1.- El coeficiente de inflación de varianza de la variable Xj es aproximado por el
    cuadrado de la longitud del marcador correspondiente:

                         htj(2) hj(2) ≃ VIF(j)     ∀j = 1, 2, . . . , p

2.- La correlación parcial entre las variables Xj y Xk es aproximada, excepto por
    el signo, a través del coseno del ángulo entre sus marcadores:
                                     htj hk
                             q           q     = −rjk.X(−j,−k)
                                 htj hj htk hk


4.     Información captada por la representación
       R−1 h−plot
    Tomando en cuenta que en un modelo de regresión el VIF(j) se interpreta como
el incremento en la varianza del coeficiente bj , debido a la multicolinealidad de Xj
con las restantes variables explicativas, es posible definir los siguientes indicadores:

Imprecisión global. Este indicador se define como traza(R-1) y se interpreta
como una medida de la imprecisión global de los coeficientes de regresión debido
a la multicolinealidad, ya que:
                                       X           X
                         traza(R−1 ) =   VIF(j) =      λα

siendo λα el α-ésimo autovalor de R−1 .

Imprecisión captada por un eje. Este indicador se define como el cociente
λα /traza(R−1 ) y se interpreta como la proporción de la imprecisión global que es
captada por el α-ésimo eje h-plot.

Detección gráfica de la multicolinealidad                                        213

Contribución de cada variable a la imprecisión captada por un eje.               Da-
do que:
                      X          X p        2
                          h2jα =      λα vjα = λα

se define como contribución de la variable Xj a la imprecisión captada por el eje
α al cociente:
                                             h2jα
                                  CVj Fα =
                                             λα


Contribución de cada eje a la imprecisión del coeficiente asociado a una
variable. Dado que:
                         X
                             h2jα = htj hj = VIF(j)

se define como contribución del eje α a la imprecisión del coeficiente de regresión
bj al cociente:
                                                h2jα
                                    CFα Vj =
                                               VIF(j)


5.       Ilustración
    Con el objeto de validar el dispositivo propuesto, éste ha sido aplicado en una
investigación sobre la caracterización morfológica de jóvenes nadadores venezo-
lanos (Pérez, Vásquez, Tomei, Landaeta & Ramı́rez 2004). El objetivo principal
de este estudio consistió en identificar las variables antropométricas con mayor
capacidad predictiva para clasificar correctamente un atleta según su estatus de
maduración sexual (prepúber, púber inicial o púber avanzado). El procedimiento
estadı́stico utilizado para construir la regla de clasificación es el Análisis Lineal
Discriminante, del cual se conoce que resulta sensible a la presencia de multicoli-
nealidad en las variables predictoras. Especı́ficamente se ilustra la aplicación del
método en la evaluación de la posible existencia de relaciones lineales en el conjun-
to de variables que describen el patrón de distribución de la grasa. Las variables
consideradas aquı́ son los pliegues (panı́culos adiposos) medidos en milı́metros,
en diferentes partes del cuerpo: triceps (trice), subescapular (subes), biceps (bi-
cep), cresta ilı́aca (iliac), supraespinal (supra), abdomen (abdom), muslo (muslo)
y pantorrilla media (panto), en el grupo de nadadores prepúberes.


5.1.     Resultados
5.1.1.    Matriz de correlaciones R

   En esta matriz se evidencian las fuertes relaciones lineales positivas entre los
niveles de grasa medidos en los ocho puntos considerados.

214                     G. Ramı́rez, M. Vasquez, A. Camardiel, B. Perez & P. Galindo


                         Tabla 1: Matriz de Correlaciones R
              trice    subes   bicep    iliac     supra     abdom      muslo      panto
      trice   1.00      0.90   0.95     0.92       0.91      0.94      0.92        0.91
      subes   0.90      1.00   0.91     0.87       0.95      0.95      0.89        0.87
      bicep   0.95      0.91   1.00     0.91       0.93      0.92      0.89        0.91
      iliac   0.92      0.87   0.91     1.00       0.85      0.91      0.95        0.88
      supra   0.91      0.95   0.93     0.85       1.00      0.95      0.88        0.89
      abdom   0.94      0.95   0.92     0.91       0.95      1.00      0.93        0.86
      muslo   0.92      0.89   0.89     0.95       0.88      0.93      1.00        0.93
      panto   0.91      0.87   0.91     0.88       0.89      0.86      0.93        1.00


                Tabla 2: Inversa de la Matriz de Correlaciones R−1
            trice     subes    bicep     iliac     supra     abdom       muslo      panto
 trice     18.48       -0.08     0.34      0.11     -0.34       0.50      -0.21       0.38
 subes       1.25     14.18      0.04      0.09      0.29       0.19       0.00       0.03
 bicep      -6.84      -0.77   21.40       0.56      0.17       0.08      -0.42       0.33
 iliac      -2.12      -1.46   -11.19   18.68       -0.09      -0.14       0.61      -0.34
 supra       8.83      -6.76    -4.94      2.28    37.40        0.75      -0.49       0.63
 abdom     -16.81      -5.50    -3.09      4.94    -36.04     62.21        0.71      -0.78
 muslo       6.42      -0.10    13.78   -18.98      21.29     -40.14     51.49        0.84
 panto      -9.19      -0.67     8.59      8.43    -21.92      34.73     -34.24     32.02


5.1.2.   Inversa de la matriz de correlaciones R−1

Nota: En el triángulo superior de esta matriz se han colocado las correlaciones
parciales
    En la diagonal de R−1 aparecen los coeficientes de inflación de varianza. Se
destacan fundamentalmente los correspondientes a los pliegues abdominal (62.2),
muslo (51.5), supraespinal (37.4) y pantorrilla (32.0), que acumulan un 71.6 % del
total de la multicolinealidad existente en el sistema de variables. Se evidencian
además importantes correlaciones parciales entre pantorrilla y muslo (0.84), pan-
torrilla y abdomen (-0.78), abdomen y muslo (0.71), y, supraespinal y abdomen
(cresta ilı́aca y muslo).


5.1.3.   Autovalores de R−1

                 Imprecisión Global 255.87 ←− traza(R−1 )
    La estructura de las relaciones de multicolinealidad entre las variables es ex-
plicada en un 78 % por el primer plano R−1 h-plot.

Detección gráfica de la multicolinealidad                                         215


                              Tabla 3: Autovalores de R−1
                                            Porcentaje       Porcentaje
                  Factor    Autovalor     de Imprecisión    acumulado
                    1          150.15               58.7           58.7
                    2           48.15               18.8           77.5
                    3           22.57                8.8           86.3
                    4           15.15                5.9           92.2
                    5            8.64                3.4           95.6
                    6            6.74                2.6           98.3
                    7            4.33                1.7           99.9
                    8            0.14                0.1          100.0


5.1.4.    Coordenadas y VIF



                               Tabla 4: Coordenadas y VIF

                                                                VIF          VIF
         Variable           Factor 1     Factor 2             suma de      suma de
                                                             cuadrados    cuadrados
                                                                (1-8)        (1-2)
   trice                        -1.89          1.72               18.48          6.53
   subes                        -0.12         -0.32               14.18          0.12
   bicep                        -1.06         -3.78               21.40        15.41
   iliac                         1.57          3.23               18.68        12.93
   supra                        -4.62          3.05              37.40         30.69
   abdom                         7.35         -1.74              62.21         57.04
   muslo                        -6.37         -2.75              51.49         48.08
   panto                         5.20          0.68              32.02         27.51
   suma de cuadrados           150.15         48.15   suma       255.87       198.30




5.1.5.    Contribuciones

     Las variables con mayor contribución a la multicolinealidad (CVF) captada
por el primer factor son abdomen (36 %), muslo (27 %), pantorrilla (18 %) y su-
praespinal (14 %). El segundo eje queda fundamentalmente definido por biceps
(30 %), cresta ilı́aca (22 %), supraespinal (19 %) y muslo (16 %). Con la excep-
ción de triceps y subescapular, la multicolinealidad de todas las variables (VIF) es
aproximada con alta calidad (> 69 %)

216                       G. Ramı́rez, M. Vasquez, A. Camardiel, B. Perez & P. Galindo


                                Tabla 5: Contribuciones
                                CVF       CVF    CFV       CFV
                 Variable         1         2      1         2    suma
                 trice          0.02      0.06   0.19      0.16    0.35
                 subes          0.00      0.00   0.00      0.01    0.01
                 bicep          0.01      0.30   0.05      0.67   0.72
                 iliac          0.02      0.22   0.13      0.56   0.69
                 supra          0.14      0.19   0.57      0.25   0.82
                 abdom          0.36      0.06   0.87      0.05   0.92
                 muslo          0.27      0.16   0.79      0.15   0.94
                 panto          0.18      0.01   0.84      0.01   0.85


5.1.6.   Primer plano factorial




                                                   iliac
                  supra
                                 trice

                                                                    panto


                                         subes
                                                                            abdom

                muslo
                                         bicep




                              Figura 1: Plano principal 1-2

    La multicolinealidad presente en las variables queda claramente reflejada por
la longitud de los rayos. Se aprecia visualmente por ejemplo, la fuerte correlación
parcial positiva entre muslo y pantorrilla a partir del ángulo entre ambos vecto-
res. De igual manera, las posiciones de los vectores correspondientes a abdomen
y pantorrilla indican una correlación inversa importante entre ellas, después de
controlar por las restantes variables.

Detección gráfica de la multicolinealidad                                                   217

5.1.7.    Ángulos entre algunos vectores

   En la figura 2 se representan solamente cuatro de las ocho variables, indicándose
además los ángulos entre los vectores correspondientes.




                                                      159º

                   supra

                                                                      panto
                                                        21º
                                               143º
                                                       164º                      abdom
                 muslo




                           Figura 2: Ángulos entre vectores

5.1.8.    Algunas correlaciones parciales

    A continuación comparamos algunas de las correlaciones parciales y su aproxi-
mación a través de los cosenos de los ángulos correspondientes (multiplicados por
-1) en la representación bidimensional anterior.

    Corr(panto, abdom, restantes)          =      -0.78       ≈   − cos(164o )   =     0.96
    Corr(muslo, panto, restantes)          =       0.84       ≈   − cos(21o )    =    -0.94
    Corr(muslo, abdom, restantes)          =       0.71       ≈   − cos(143o )   =     0.80
    Corr(supra, abdom, restantes)          =       0.75       ≈   − cos(159o )   =     0.93

5.1.9.    Algunos coeficientes de inflación de varianza

   En forma análoga presentamos algunos coeficientes de inflación de varianza y
su aproximación a través de la norma cuadrado del vector correspondiente.

            VIF(panto)        =    32.02      ≈   norma2 (vector)       =     27.51
            VIF(abdom)        =    62.21      ≈   norma2 (vector)       =     57.04
            VIF(muslo)        =    51.49      ≈   norma2 (vector)       =     48.08
            VIF(supra)        =    37.40      ≈   norma2 (vector)       =     30.69

218                    G. Ramı́rez, M. Vasquez, A. Camardiel, B. Perez & P. Galindo


5.1.10.   Comentarios finales

    El procedimiento anterior se repitió para cada uno de los grupos de variables
que definen las distintas dimensiones consideradas en el estudio (tamaño, longitu-
des, anchuras y pliegues), lográndose reducir el número de variables predictoras
de un total de 35 a 8. Los resultados obtenidos indicaron un notable aumento en
el porcentaje de la correcta clasificación al eliminar del conjunto original las va-
riables colineales. Especı́ficamente en la dimensión grasa, las variables predictoras
que se incorporaron al modelo de predicción fueron los pliegues del triceps y del
subescapular.
Referencias
Belsley D.Conditioning Diagnostics, Collinearity and Weak Data in Regression.(1991).Wiley.New York.
Cornsten L,Gabriel K.Graphical exploration in comparing variance matrices.(1976).Biometrics.
Farrar D,Glauber R.Multicollinearity in regression analysis: The problem revisited.(1967).Review of Economic Statistics.
Gabriel K.The biplot graphic display of matrices with application to principal component analysis.(1971).Biometrika.
Glantz S,Slinker B.Primer of Applied Regression and Analysis of Variance.(2001).McGraw-Hill.New York.
Gleason T,Staelin R.A proposal for handling missing data.(1975).Psychometrika.
Jackson J.A User’s Guide to Principal Components.(1991).Wiley.New York.
Kendall M.A Course in Multivariate Analysis.(1957).Griffin.London.
Mandell J.Use of the singular value decomposition in regression analysis.(1982).The American Statistician.
Mason R,Gunst R,Webster J.Regression analysis and the problem of multicollinearity.(1975).Communications in Statistics.
Pérez B,Vásquez M,Tomei C,Landaeta M,Ramírez G.Anthropometrics characteristics of young venezuelan male swimmers according with biological maturity status.(2004).International Congress of Auxology.Italia.
Raveh A.On the use of the inverse of the correlation matrix in multivariate data analysis.(1985).The American Statistician.
Silvey S.Multicollinearity and imprecise estimation.(1969).Journal of the Royal Statistical Society.
Whitakker J.Graphical Models in Applied Multivariate Analysis.(1990).Wiley.New York.
Yu C.Multi-collinearity variance inflation and orthogonalization in regression.(1998).http://seamonkey ed asu edu/alex/computer/sas/collinear html.