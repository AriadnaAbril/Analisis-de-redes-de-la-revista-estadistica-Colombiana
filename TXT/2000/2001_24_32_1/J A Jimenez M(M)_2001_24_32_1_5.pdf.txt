UNA PROPUESTA PARA LA MAXIMIZACIÓN DE LA ESTADÍSTICA QK
Universidad Nacional de Colombia
Resumen
En este artı́culo mediante el método de los multiplicadores de Lagrange se presenta una forma de maximizar la estadística Qk , y de esta manera cuantificar el impacto que ejerce en la suma de cuadrados de los residuales un grupo de observaciones previamente seleccionadas si son corregidas o modificadas.
Palabras claves Modelos Lineales, Mı́nimos cuadrados, Observaciones Influyentes, estadística Qk , Multiplicadores de Lagrange
Introducción
Para el modelo de regresión lineal múltiple
Draper and John (1981) desarrollaron una metodologı́a para detectar un grupo de k observaciones influyentes o atı́picas, equivalente a la propuesta por Bartlett (1937), citada en Little and Rubin (1987), para estimar los parámetros del modelo de regresión lineal cuando existen observaciones faltantes en la variable respuesta. En la propuesta de Draper and John (1981) se analiza el modelo particionado donde Y1 es el bloque conformado por las observaciones consideradas atı́picas.
Las estimaciones β y γ del modelo están definidas por
donde Hij es submatriz de la matriz H la cual se conoce como Matriz Hat, Tukey (1977) y el cambio en la suma de cuadrados de residuales está dado por la estadística Qk 
En resumen el método descrito permite detectar el grupo de observaciones atı́picas en base al cambio en la suma de cuadrados de residuales, lo cual se cuantifica con la estadística Qk . Sin embargo, el método no mide la influencia de estas observaciones en la estimación de los parámetros. En este artı́culo se muestra la maximización de dicha estadística sujeta a algunas condiciones
Nueva expresión de la estadística Qk
Para el modelo de regresión lineal múltiple definido en el método de estimación mı́nimos cuadrados proporciona el estimador β de los parámetros los valores estimados Ŷ los errores estimados la suma de cuadrados de los
residuales SCE según las siguientes expresiones
En Jiménez (1999) se plantea el modelo siendo con un vector arbitrario no nulo; bajo estos criterios se establece por el método de mínimos cuadrados las expresiones de los estimadores del modelo en función de y de los estimadores descritos anteriormente. Así los nuevos estimadores están dados por
esta nueva expresión de la estadística Qk tiene la ventaja de estar en términos de los residuales estimados del modelo y del arbitrario Para los objetivos de este trabajo es más atractiva, ya que permite maximizarla sujeta a las restricciones que se presentaran a continuación
Maximización de la estadística Qk 
Para la maximización de esta estadística se particiona el modelo como y de manera análoga a la propuesta de Draper and Jhon (1981) se asume que el bloque de dimensión k × 1, k < n, esta conformado por las observaciones
atı́picas el interés en este artı́culo es presentar valores óptimos para que permitan corregir dichas observaciones de tal manera que la nueva suma de cuadrados del error sea mı́nima. Para ello se consideran las siguientes restricciones
Restricción sobre el vector
Si se toma como condición que entonces para que Qk dada en tenga un extremo en es necesario que al efectuar dicha derivada se tiene que
como el rango de la matriz (I − H) es n − r donde r es el número de parámetros del modelo entonces esta matriz no tiene inversa; por lo tanto para encontrar las componentes de que maximiza a Qk se resuelve el sistema que aparece a continuación, el cual se obtiene de efectuar los respectivos productos
si se premultiplica por H12 y se suma con se obtiene
pero como la matriz (I − H) es idempotente, se puede probar fácilmente que reemplazando las expresiones en la ecuación se tiene que utilizando el hecho de que se obtiene que en la última ecuación se utilizó el hecho de que Hij = Hji , luego, reemplazando la expresión se tiene
Nótese que esta última expresión no depende de la estimación del vector de parámetros β ni de la estimación del vector de errores
Restricción sobre el cambio en el vector de parámetros β
Si se asume como requisito que los parámetros del modelo (1) se modifican en un vector fijo de constantes al sumársele al bloque Y el vector entonces se debe maximizar bajo la siguiente restricción
Sea b un vector de constantes que indica la diferencia entre entonces la
condición dada anteriormente se puede expresar como si se denota por λ el vector de Lagrange, la función a maximizar es:
luego, para que Qk tenga un extremo en es necesario que
por consiguiente si se hacen las respectivas derivadas y se igualan a cero se tiene:
dividiendo esta expresión por 2 y efectuando los productos correspondientes se tiene
si se premultiplica por H12 y se suma se obtiene
reemplazando las expresiones dadas en esta última ecuación queda
si se sustituye en esta ecuación y se simplifican términos, se llega a
si se efectúan los productos indicados, da como resultado
obsérvese que la última expresión no queda ligada a la estimación del vector de parámetros β pero si depende del cambio en los parámetros del modelo completo y de la estimación del vector de errores
Restricción sobre el cambio en el vector de errores
Si se toma como condición que los residuales del modelo cambian en un vector fijo de constantes, cuando se le suma al bloque Y el vector entonces se debe maximizar bajo la siguiente restricción
sea el vector de constantes que indica la diferencia entre entonces la condición dada anteriormente se puede expresar como
si se considera λ como el vector de Lagrange, la función a maximizar es:
luego, para que Qk tenga un extremo en es necesario que
por consiguiente, realizando las respectivas derivadas e igualando a cero, se tiene
si se multiplica por -2 y se suma se obtiene que
si se reemplaza este resultado en se llega a
como se necesita que únicamente cambien los primeros k residuales es decir en los n − k residuales siguientes no se desea ningún cambio, luego se debe hallar el que permita esto; si se reescribe queda
como la matriz (I − H) no es de rango completo, entonces no tiene inversa, por lo tanto para obtener los valores de que maximizan a Qk se resuelve el siguiente sistema que se obtiene de efectuar los correspondientes productos
si se asume que el rango de la matriz (I − H22 ) es menor o igual que n − r; se puede despejar de y al reemplazarlo se tiene que en este caso, la inversa de (I − H22 ) está dada por
si se sustituye en esta última fórmula las expresiones dadas se establece que utilizando el hecho de que
como era de esperarse en la última expresión no interviene es decir que en el cambio de la suma de cuadrados únicamente interesa la modificación que se le haga a los primeros k residuales.
Por otra parte, cuando la matriz (I − H22 ) no sea de rango completo, la
ecuación queda expresada de la siguiente manera
Restricciones sobre el cambio en el vector de parámetros β y el vector de errores
Si se consideran las dos últimas condiciones simultáneamente, se debe maximizar la estadística Qk dada sujeta a
denotando como b el vector de constantes que indica la diferencia entre el vector de constantes que representa la diferencia entre entonces las condiciones anteriores se pueden expresar como
los vectores de Lagrange, luego la función a maximizar es para que Qk tenga un extremo en es necesario que efectuando las respectivas derivadas e igualando a cero se obtiene
como era de esperarse, en esta última expresión no intervienen las estimaciones de los parámetros ni la de los residuales
Ejemplo
Para el conjunto de 21 observaciones (x, y) dado por Mickey, Dunn, and Clark (1967), tabla se presentan los siguientes resultados
La estimación del modelo de regresión lineal, con las 21 observaciones
Los términos hii , las estimaciones de los y el cálculo de la estadística Q1 al eliminar el i-ésimo dato
La estimación del modelo de regresión lineal, después de modificar con el bi la observación que tiene el Q1 más grande
La estimación del modelo de regresión lineal, después de eliminar la observación con el Q1 más grande
En los resultados anteriores se verifica que los valores de la estadística Q1 corresponden a la expresión
Cuando se modifica la observación 19 con el correspondiente se obtiene
El modelo que se obtiene al modificar la pareja (17, 121) por (17; 89,0184 es mejor que el modelo original; pues el nuevo coeficiente de determinación es superior al del modelo inicial, el valor crı́tico de la F es también inferior al valor crı́tico que se determinó en el análisis de varianza del modelo inicial y además se logra minimizar la SCE lo cual implica que el nuevo cuadrado medio del error (CME) sea menor que el CME del modelo original
Eliminando la observación 19 que tiene el Q1 más alto, se tiene
Coeficiente de determinación R2 = 0, 57163103
Error tı́pico σ = 8,628196
Cambio en la suma de los residuales Qk = 968, 5619674
El modelo que se obtiene al eliminar la pareja (17, 121) es mejor que el modelo completo; pues el nuevo coeficiente de determinación es superior al del modelo inicial, el valor crı́tico de la F es también inferior al valor crı́tico que se determinó en el análisis de varianza del modelo inicial y además el cuadrado medio del error (CME) fue menor que el CME del modelo completo
Conclusiones
Si el propósito es minimizar la suma de cuadrados de los residuales de un modelo de regresión cuando se tiene un bloque de observaciones influyentes, la recomendación que se hace en este artı́culo es la de corregirlo con alguno de los vectores propuestos, de tal manera que no sea necesario eliminar dicho bloque y por consiguiente no se pierdan grados de libertad
Referencias
Bartlett M S.Some examples of statistical methods of research in agriculture and applied botany. (1937).Journal of the Royal Statistical Society.
Belsley D.Regression diagnostics.(1980).John Wiley.New York.
Draper N R,John J A.Influential observations and outliers in Regression.(1981).Technometrics.
Jiménez J A.Propuesta Metodológica para Imputar Valores no Influyentes en Modelos de Regresión Lineal Múltiple con Información Incompleta.(1999).Universidad Nacional de Colombia.
Little R J,Rubin D B.Statistical Analysis with Missing Data.(1987).John Wiley.
Mickey M R,Dunn O J,Clark V.Note on the use of stepwise regression in detecting outliers.(1967).Computers and Biomedical Research.
Searle S R.Linear Models.(1971).John Wiley & Sons.New York.
Tukey J.Exploratory Data Analysis.(1977).Addison Wesley Publishing Company.United States of America.