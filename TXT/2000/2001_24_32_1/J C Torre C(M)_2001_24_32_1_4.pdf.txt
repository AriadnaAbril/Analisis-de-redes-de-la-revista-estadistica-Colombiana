COMPARACIÓN DE TRES MÉTODOS DE REGRESIÓN LINEAL USANDO PROCEDIMIENTOS DE SIMULACIÓN
Universidad Nacional de Colombia; Departamento Administrativo Nacional de Estadı́stica DANE
Resumen
Cuando desea ajustarse un modelo lineal a un conjunto de datos, el método de regresión usualmente más empleado es el de mı́nimos cuadrados. Este método es óptimo si la distribución de los residuos es gaussiana Existen casos en donde el supuesto de normalidad en los residuales no se cumple y se hace necesario el uso de métodos alternativos de regresión, como la regresión vı́a mı́nimas desviaciones absolutas (LAD) o la regresión no paramétrica basada en rangos, los cuales no requieren de supuestos distribucionales sobre los residuos y permiten obtener una mejor estimación de los parámetros del modelo de regresión
Palabras clave: Modelo lineal, análisis de regresión, simulación
Abstract
When it’s necessary to fit a lineal model to a data set, least squares regression method is usually used. This method is optimum if the residuals distribution is normal. When the assumption of residuals normality doesn’t comply it’s necessary to use alternative regression methods, as Least absolute deviations (LAD) or Non parametric regression based on ranks, which don’t need the assumption about the residuals distribution and allow a better estimation of regression model parameters
Key words: Linear model, regression analysis, simulation
Introducción
El objetivo de este artı́culo es determinar cuál de los tres métodos de regresión: Mı́nimo cuadrática, No paramétrica basada en rangos o Mı́nima desviación absoluta, presenta un mejor ajuste cuando la distribución de los errores aleatorios tiene forma diferente de la gaussiana en elongación y cuando hay presencia de observaciones atı́picas u outliers Para cumplir con el objetivo se calcularon estimaciones de los coeficientes del modelo de regresión y sus medidas de ajuste para los tres métodos de regresión mediante procedimientos de simulación realizados con el paquete estadı́stico SAS
Regresión Mı́nimo Cuadrática
Para el modelo dado por la expresión Y = Xβ + e, el método construye el estimador que minimiza la suma de cuadrados de los errores Para un conjunto de datos observados, cuando la expresión se hace mı́nima, el vector de valores b se conoce como la estimación mı́nimo cuadrática de β. En la función anterior xi representa la i-ésima fila de la matriz X. Se estima la varianza de la población de los errores con s2 en donde los ei son los errores obtenidos con los datos observados, n es el número de observaciones realizadas y p es el número de variables explicativas en el modelo
Regresión no Paramétrica Basada en Rangos
El método se basa en el modelo lineal Y = Xβ + e. No existen fórmulas explı́citas para el estimador β. Sin embargo, mediante un algoritmo iterativo es posible obtener el vector de estimación de β. Para un conjunto de datos observados, el vector β se conoce como la estimación no paramétrica de β, en donde el vector b de tamaño 1×p, minimiza la función en donde b y xi y la estimación no paramétrica de se obtiene como la mediana de las diferencias de yi − xi b. Ya que no existen fórmulas explı́citas para calcular los coeficientes estimados, Birkes y Dodge (1993) describen un algoritmo para estimarlos. De igual manera, los mismos autores describen un algoritmo para estimar la desviación estándar de los errores, cuya notación usual es τ . Además afirman que un estimador para la desviación estándar de los errores es s̃ = τ / 1.023 si la distribución de los errores es normal
Regresión Vı́a Mı́nima Desviación Absoluta
El método se basa en el modelo lineal Y = Xβ + e. No existen fórmulas explı́citas para el estimador β.b Sin embargo, mediante un algoritmo iterativo es posible obtener el vector de estimación de β el cual es escogido de tal forma que la suma de los valores absolutos de los errores sea la más pequeña posible Para un conjunto de datos observados, cuando   la expresión se hace mı́nima, el vector de estimación b obtenido, es conocido como la estimación mı́nima desviación absoluta de β. En la función anterior xi representa la i-ésima fila de la matriz X. Como no existen fórmulas explı́citas para calcular los coeficientes estimados, Birkes y Dodge (1.993) describen un algoritmo para estimar los coeficientes de regresión. La desviación estándar de los errores se estima con s̃ = 1,483 ∗ MAD, donde MAD es la mediana de las desviaciones absolutas con respecto a la mediana de los residuales, calculado con los residuos diferentes de cero
Distribuciones g y h de Tukey
La familia de distribuciones g y h comprende una considerable variedad de distribuciones con caracterı́sticas especiales en cuanto a asimetría y elongación por lo cual resulta de gran utilidad cuando se desea simular datos que provengan de distribuciones con formas diferentes a la distribución normal
Asimetría
Si Z es una variable normal estándar y g es una constante real, la variable aleatoria Yg(Z), definida como se dice que tiene la distribución g de Tukey para un valor dado de g. El parámetro g controla la magnitud y la dirección de la asimetría
Elongación
Si Z es una variable aleatoria normal estándar y h es una constante real, la variable aleatoria Yh(Z) dada por se dice que tiene distribución h de Tukey para un valor dado de h. El parámetro h controla la cantidad de la elongación de la distribución. Las distribuciones de la familia h de Tukey son simétricas y su valor esperado es cero
Proceso de Simulación
Para el cálculo de las estimaciones del ajuste con errores aleatorios provenientes de la familia de distribuciones h de Tukey, se efectuaron 1000 simulaciones de modelos lineales con una y dos variables independientes, con tamaños de muestra 20 y 50, y usando cada uno de los tres métodos de regresión. Se utilizó el mismo procedimiento para errores provenientes de una distribución normal contaminada con un porcentaje de “outliers” dado. Todas las simulaciones fueron llevadas a cabo con el paquete estadı́stico SAS.
En el caso de la regresión simple, el modelo simulado fue Y = β0 + β1 X1 + e donde β0 = 4 y β1 = 5 y la variable explicativa X1 se dejó fija en cada simulación y fue generada de una distribución uniforme U (2, 5) para los tamaños de muestra 20 y 50. Para la regresión múltiple, el modelo simulado fue Y = β0 + β1 X1 + β2 X2 + e donde β0 = 3, β1 = 4, y β2 = 5. Las variables explicativas X1 y X2 se fijaron para cada simulación y fueron generadas de distribuciones U(1,4) y U(2,5) respectivamente, para los tamaños de muestra 20 y 50. Para la simulación de errores aleatorios con formas diferentes a la distribución normal se utilizaron las siguientes distribuciones
Tabla 1: Distribuciones h de Tukey Simuladas con su varianza teórica
En la simulación de errores aleatorios con distribución normal y presencia de outliers se utilizó el modelo contaminado donde φ(x) es la función de distribución normal estándar y k es igual a 3. Los valores de contaminación que se utilizaron fueron 0.05, 0.10, 0.15 y 0.20, lo que equivale a contaminar la distribución N(0,1) con los porcentajes de outliers 5%, 10%, 15% y 20%, generados de una distribución N(0,3), para cada uno de los modelos descritos anteriormente
Criterios de Comparación.
Los criterios de comparación utilizados fueron los siguientes
Error cuadrático medio de los estimadores de los coeficientes del modelo (ECM)
Error absoluto medio de los estimadores de los coeficientes del modelo (MAE)
Promedios de los coeficientes de determinación estimados
Cuando se simulan los errores de una distribución normal con un porcentaje de outliers se compararon los promedios de las cantidades de outliers detectados que fueron generados para cada método de regresión, usando el método de los residuales estandarizados. Según Birkes y Dodge (1993), un residual estandarizado mayor a 2.5 en valor absoluto, puede catalogarse como outlier
Resultados de las Simulaciones
Para el análisis de los resultados, se compararon el error cuadrático medio (ECM) y el valor absoluto medio (MAE) de los estimadores de los parámetros obtenidos con cada uno de los tres métodos de regresión: mı́nimos cuadrados, no paramétrica basada en rangos y LAD. Se exponen solamente los resultados para regresión simple ya que los correspondientes a regresión múltiple con dos variables son similares. Ası́ mismo, se presentan los resultados para el error cuadrático medio (ECM) de los estimadores, ya que los resultados para el error absoluto medio (MAE) son similares (Ver Pulido y Torres, 1997). En cada una de las tablas presentadas los datos en letra negrilla son los menores valores de las medidas de comparación, los que poseen letra cursiva son los que siguen en el orden de menor a mayor y los datos en letra normal corresponden a los mayores valores.
Tabla 2: Resultados de Regresión simple n = 20
Tabla 3: Resultados de regresión simple n = 50
Resultados de la Estimación de Parámetros
Los resultados se presentan en las Tablas 2 y 3. En ellas puede observarse que el ECM de los estimadores de los parámetros tiene un comportamiento similar para los tamaños de muestra 20 y 50.
Vector de errores con distribución normal
Observando las tablas para los tamaños de muestra 20 y 50, el menor ECM de los estimadores de los parámetros se obtuvo con la regresión mı́nimo cuadrática y sigue en orden ascendente la regresión no paramétrica basada en rangos
Vector de error con distribución h de Tukey
Se observa en la Figura 1, realizada para el ECM de β1 , regresión simple y tamaño de muestra 50, que cuando el parámetro h de la distribución de los errores es igual 0.2, 0.4, 0.6, 0.8 y 1, el ECM de los estimadores calculados con la regresión de mı́nimos cuadrados aumenta aceleradamente, mientras que con la regresión LAD y la regresión no paramétrica basada en rangos es más estable
Figura 1. Error cuadrático medio. Estimador de β1 Errores familia h de Tukey
En la figura también se observa que para distribuciones con parámetros h igual a 0.2 y 0.4 el menor ECM de los parámetros estimados fue obtenido con la regresión no paramétrica basada en rangos y le sigue la regresión LAD. Para distribuciones h igual a 0.6, 0.8 y 1 el menor ECM de los parámetros estimados fue obtenido con la regresión LAD y le sigue la regresión no paramétrica basada en rangos
Distribución de errores normal con un porcentaje de outliers
Como se observa tanto en la Figura 2, realizada para β1 , con el modelo simple y tamaño de muestra 50, ası́ como en la Tabla 2, el menor ECM de los estimadores de los parámetros para los porcentajes de outliers simulados fue obtenido con la regresión no paramétrica basada en rangos
En orden ascendente siguen las estimaciones obtenidas con la regresión mı́nimo cuadrática, con un 5% y 10% de “outliers” en la distribución de los errores y cuando se tiene un 15% y 20% de outliers en la distribución de los errores le sigue el ECM de los estimadores calculados vı́a regresión LAD
Figura 2. Error cuadrático Medio para β1 Errores con porcentaje de outliers
Resultados de Generación y Detección de Outliers
Como se puede observar en la tabla 4, el método que más outliers detectó en promedio, usando el método de los residuales estandarizados fue la regresión LAD y le sigue la regresión no paramétrica basada en rangos, para los diferentes porcentajes de outliers generados
Tabla 4: Resultados, Generación y Detección de Outliers Regresión Simple (Promedio)
Resultados de la Medida de Ajuste R2
En la Tabla se puede observar que los valores de R2 son mayores con la regresión mı́nimo cuadrática y sigue la regresión no paramétrica basada en rangos, para las diferentes distribuciones y los tamaños de muestra usados en la simulación
Conclusiones
Cuando la distribución de los errores es simétrica, de colas pesadas, generada de las distribuciones h de Tukey, se obtienen mejores estimaciones de los parámetros del modelo con los métodos de regresión no paramétrica basada en rangos y LAD que con la regresión mı́nimo cuadrática. Si la distribución de los errores es simétrica de colas muy pesadas, las mejores estimaciones se obtienen con la regresión LAD
Tabla 5: Valores de R2 para Regresión Simple (Promedio)
Se puede observar la robustez de los métodos de regresión LAD y no paramétrica basada en rangos, frente a la regresión mı́nimo cuadrática cuando la distribución de los errores presenta colas pesadas
Cuando la distribución de los errores es normal, con un porcentaje de outliers mayor o igual al 5%, se obtienen mejores estimaciones de los parámetros con la regresión no paramétrica y le sigue la regresión LAD
El empleo del coeficiente de determinación usual no es adecuado para comparar el ajuste de modelos con criterios de estimación diferentes, ya que para todas las distribuciones simuladas, éste fue mayor para la regresión mı́nimo cuadrática a pesar de que no siempre tuvo la mejor estimación de los parámetros del modelo de regresión
En cuanto a la detección de outliers cuando la distribución de los errores es normal, el método que detectó más en promedio usando el criterio de los residuales estandarizados, fue la regresión LAD, seguido de la regresión no paramétrica basada en rangos
En general, se puede observar que ninguno de los tres métodos de regresión en estudio fue uniformemente mejor para la estimación de los parámetros. Si la distribución de los errores aleatorios simétrica de colas muy pesadas, es mejor estimar los parámetros con la regresión LAD mientras que en el caso de que los errores tengan distribución normal con altos porcentajes de outliers es preferible estimar los parámetros usando la regresión no paramétrica basada en rangos
Agradecimientos El autor expresa sus agradecimientos al Doctor Jorge Martı́nez Collantes, por sus valiosos aportes en la elaboración de este documento. Este artı́culo se basó en el trabajo de grado del mismo nombre realizado por Elsa Leonor Pulido y Juan Carlos Torres en 1.997, para optar al tı́tulo de Estadı́stico en la Universidad Nacional de Colombia
Referencias
BIRKES D,DODGE Y.Alternative methods of regression.(1993).John Wiley & Sons.
PULIDO E,TORRES J.Análisis comparativo de tres métodos de regresión: mínimos cuadrados, no paramétrica basada en rangos y mı́nima desviación absoluta, usando métodos de simulación.(1997).Universidad Nacional de Colombia.