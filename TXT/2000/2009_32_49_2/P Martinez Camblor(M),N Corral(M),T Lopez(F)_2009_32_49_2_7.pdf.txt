Cramér-Chernoff Theorem for L1 -norm in Kernel Density Estimator for Two Independent Samples
CIBER de Epidemiología y Salud Pública (CIBERESP);Universidad de Oviedo
Resumen
In this paper a Chernoff type theorem for the L1 distance between kernel estimators from two independent and identically distributed random samples is developed. The harmonic mean is used to correct the distance for inequal sample sizes case. Moreover, the proved result is used to compute the Bahadur slope of a test based on L1 distance and to compare it with the classical nonparametric Mann-Whitney test by using the Bahadur relative efficiency.
Palabras clave: Kernel estimator, Large deviation, Bahadur slope. 
Introduction
    Let X = {x1 , . . . , xn } be an independent random sample from a random va-
riable X which is absolutely continuous with probability density function f . The
kernel density estimator (KDE) of f introduced by Rosenblatt (1956) and Parzen
(1962) is defined for each t ∈ R as
                                               n            
                                           1 X        xi − t
                            fn (X, t) =           K
                                          nhn i=1      hn

where K is a kernel function which is often chosen to be a continuous symmetric
density with finite variance, and {hn }n∈N is a sequence of positive real numbers.
    The kernel estimator and its properties have been widely studied by several
authors. Silverman (1978) proved its uniform consistence and Konakov (1978)
derived the asymptotic distribution for the L∞ -norm. Devroye & Wagner (1979)
proved the L1 convergence between the kernel density estimator and its target,
and Devroye & Gyorfi (1985) carry out a widely study about the kernel density
estimators from the L1 approach. Berlinet et al. (1995) proved the asymptotic
normality for the L1 -norm for the histogram density estimator, Hórvath (1991)
demonstrated the asymptotic normality of the Lp norm between the kernel density
estimator and the underlying density function, Martínez-Camblor & Corral (2008)
proved the same result under weaker assumptions. Large deviation approaches
were also considered. Louani (1998, 2000, 2005) studied Chernoff type theorems
for the L1 distance using for goodness of fit test. Cao & Lugosi (2005) studied the
propierties of several goodness of fit tests based on the kernel density estimate.
Osmoukhina (2001) applied these techniques in a symmetry test and Beirlant et al.
(2001) studied the large deviations of divergence measures.
    Let X = {x1 , . . . , xn1 } and Y = {y1 , . . . , yn2 } be two independent random
samples from a random variable with density function f . The L1 distance between
two estimators fn1 and fn2 of f is defined by
                                              Z
            D(fn1 , fn2 ) = kfn1 − fn2 kL1 = |fn1 (X, t) − fn2 (Y, t)| dt

   The main objective of this paper is to obtain a Chernoff type theorem for the
L1 distance, D(fn1 , fn2 ), between two kernel density estimators when both are
computed from samples which are taken from the same population. That is, our
purpose is to investigate the expression
                          Z                                   
                       P       |fn1 (X, t) − fn2 (Y, t)| dt > λ              (1)

where λ is close to zero.
   We are interested in this result because it can be applied in two sample pro-
blems and let us to compare different tests by using the Bahadur relative slope
(BRS). In Section 2 and, in a similar way than Louani (2000), we prove the large
deviation theorem. In Section 3 we consider several alternative hypothesis and,

                                          Revista Colombiana de Estadística 32 (2009) 289–299

Cramér-Chernoff Theorem for Two Independent Samples                                 291

from the previous results, the kernel density estimator based test with the clas-
sical nonparametric one of Mann-Whitney are compared in the Bahadur relative
efficiency (BRE) sense.


2. Results and Proofs
   In order to prove the main result, we will consider the following assumptions:

(C1) X = {x1 , . . . , xn1 } and Y = {y1 , . . . , yn2 } are independent random samples
     from a continuous random variable.

(C2) lı́mn n1 /n2 = 1, where n → ∞ means that n1 → ∞ and n2 → ∞.

(C3) The used kernel function, K, is a continuous, symmetric density function.

(C4) lı́mn1 hn1 = 0, lı́mn2 hn2 = 0, lı́mn1 n1 hn1 = ∞ and lı́mn2 n2 hn2 = ∞

Nota 1. In practice, the used bandwith, hn , is chosen to minimize certain error
criterion (for example, the mean integrated squared error; MISE). Since the kernel
function is a symmetric and differentiable density function having finite variance,
these conditions are satisfied for most commonly used kernel functions like Gaus-
sian, Epanechnikov, Triangular, among others. As consequence, the assumptions
(C3) and (C4) are mild conditions.

Lema 1. Let X = {x1 , . . . , xn } and Y = {y1 , . . . , yn } be two independent random
samples from a random variable with density function f . For each interval B ∈ B
(Borel σ-field) we define
                Z                                        
                       1       xi − t      1         yi − t
      ZB,n,i =            K             −     K                  dt with 1 ≤ i ≤ n
                 B   hn1        hn1       hn2          hn2

   Then, under conditions (C1) (C2) and (C4), we have that

 (1)
                                                            c
                             
                             
                               1       if (xi , yi ) ∈ (B̊ × B )
                                                               c S
                              1/2      if (xi , yi ) ∈ (∂B × B ) (B̊ × ∂B)
                             
                             
                             
                                                           c      S
                lı́m ZB,n,i = −1/2      if (xi , yi ) ∈ (B × ∂B) (∂B × B̊)
                  n                                       c
                               −1       if (xi , yi ) ∈ (B × B̊)
                             
                             
                             
                             
                             
                              0        otherwise

       where B̊ and ∂B denote the interior and the boundary of B, respectively,
       and B = B ∪ ∂B.
                                   Pn
 (2) For all λ >R 0, limn n1 log P n1 i=1 ZB,n,i > λ = inf{t>0} {−λt + qa (t)}
     where a = B f (t)dt and qa (t) = log(a2 + (1 − a)2 + 2a(1 − a) cosh(t))


                                     Revista Colombiana de Estadística 32 (2009) 289–299

292                                   Pablo Martínez-Camblor, Norberto Corral & Teresa López

Demostración. For 1 ≤ i ≤ n, we have that
          Z                                           
                 1          xi − t          1      yi − t
  lı́m              K                   −      K              dt =
      n   B     hn1          hn1           hn2      hn2
                                                                                      
                                               1      xi − t               1      yi − t
                                          Z                              Z
                                     lı́m        K             dt − lı́m      K            dt
                                      n1 B hn1         hn1           n2 B hn2      hn2

   Taking xi = t + hn1 u and yi = t + hn2 v we have that
          Z                                     
                 1      xi − t      1        yi − t
  lı́m              K            −      K              dt =
      n   B     hn1      hn        hn2        hn2
                        Z 1                         Z
                    lı́m IBn1 (u) K(u)du − lı́m IBn2 (v) K(v)dv =
                     n1 R                        n2 R
                                   Z                          Z
                                      [lı́m IBn1 (u)] K(u)du − [lı́m IBn2 (v)] K(v)dv
                                           R n1                              R n2

where Bn1 = {u/xi − uhn1 ∈ B} and Bn2 = {v/yi − vhn1 ∈ B}.
   By taking into account that
                                              
                                              
                                               R          if xi ∈ B̊
                                               R+
                                              
                                                           if xi = sup(B)
                                   lı́m Bn1 =
                                    n1        
                                               R−         if xi = ı́nf(B)
                                                                     c
                                                φ          if xi ∈ B
                                              

the result (1) is easily concluded.
   Therefore, from the first part of this Lemma, for 1 ≤ i ≤ n there exists

                                           ZB,i = lı́m ZB,n,i
                                                       n

random sample from the same random variable, ZB , whose moment generating
function is given by
            ZZ
  gZB (t) =     etZB f (x)f (y)dxdy
            Z Z                      Z Z
          =        et f (x)f (y)dxdy       e−t f (x)f (y)dxdy
             B Bc                     Bc B
                                     Z Z                    Z Z
                                  +       f (x)f (y)dxdy +      f (x)f (y)dxdy
                                               B   B                         Bc     Bc
                2              2
        =a + (1 − a) + 2a(1 − a) cosh(t)
        R
with a = B f (t)dt.
   Applying the Cramér-Chernoff Theorem (Van der Vaart 1998) the proof is
completed.
Observación 1. This result can be immediately extended to the case in which B
is a countable union of intervals.

                                              Revista Colombiana de Estadística 32 (2009) 289–299

Cramér-Chernoff Theorem for Two Independent Samples                                     293

    From the above result we can derive a Chernoff type theorem. This result lets
us to compare the test based on the L1 -norm of the kernel density estimator with
different tests by using their respective Bahadur relative slope (BRS).
Teorema 1. If conditions (C1), (C2), (C3) and (C4) are fulfilled and λ is a
nonnegative constant close to zero, then

                                                             λ2
                      Z                                 
        n1 + n2
   lı́m         log P    |fn1 (X, t) − fn2 (Y, t)| dt > λ = − (1 + o(1)) a.s.
     n   2n1 n2                                              4

Demostración. To prove this theorem we will assume that n1 = n2 = n. Now,
we define the function,
                                               p                             !
              λ           (1 − 2a + 2a2 )λ2 + 2 λ2 (1 − 2a)2 + 16a2 (a − 1)2
  Qa (λ) = −     arccosh
              2                            2a(a − 1)(λ2 − 4)
                                              p                             !
                 2    2  (1 − 2a + 2a2 )λ2 + 2 λ2 (1 − 2a)2 + 16a2 (a − 1)2
  + log (1 − a) + a +
                                              (4 − λ2 )

   It is easy to check that Qa (λ) = ı́nf t>0 − λ2 t + qa (t) , and by using its Taylor
                                             

expansion with λ in a neighborhood of zero, we have that

                                                     λ2
                      sup Qa (λ) = Q1/2 (λ) = −         (1 + o(1))
                     a∈(0,1)                         4

   On the other hand, by using the Scheffé Theorem
       Z                                       Z
          |fn1 (X, t) − fn2 (Y, t)| dt = 2 sup   (fn1 (X, t) − fn2 (Y, t)) dt
                                          B∈B    B


   Moreover, for every B ∈ B
    Z                                  
  P 2   (fn1 (X, t) − fn2 (Y, t)) dt > λ =
      B
                   ( n Z                                                )
                     1X             1     xi − t      1      yi − t         λ
                P                     K            −     K             dt >
                     n i=1 B hn1           hn1       hn2      hn2           2
                                                                             R
from the properties of Qa (λ), the previous Lemma 1 and by taking a =            B
                                                                                     f (t)dt,
we conclude that
                         Z                                    
                1
            lı́m log P 2       (fn1 (X, t) − fn2 (Y, t)) dt > λ = Qa (λ)
              n n           B


   Now, taken an arbitrary B0 ∈ B such that a = 1/2,

                                  4 + λ2                4 + λ2
                                                           
                       λ                          1
          Q1/2 (λ) = − arccosh             + log     1+
                       2          4 − λ2          2     4 − λ2

                                     Revista Colombiana de Estadística 32 (2009) 289–299

294                          Pablo Martínez-Camblor, Norberto Corral & Teresa López

and applying the Taylor expansion of Q1/2 (λ) we get,
                     Z                                    
               1
      lı́m ı́nf log P         |fn (X, t) − fn (Y, t)| dt > λ
        n      n
                                                                               
                            1                                                  λ
                                              Z
                 = lı́m ı́nf log P sup           (fn (X, t) − fn (Y, t)) dt >
                     n      n            B∈B                                   2
                                    Z                                                (2)
                            1                                               λ
                 ≥ lı́m ı́nf log P            (fn (X, t) − fn (Y, t)) dt >
                     n      n             B0                                2
                      λ2
                 ≥−      (1 + o(1))
                      4

  To prove the upper bound, we know that for δ > 0 and any density function
K we can find a kernel L in the form
                                             Nδ
                                             X
                                      L=           αj IRj
                                             j=1

satisfying (C3) and such that
                                      Z
                                          |K − L| < δ

where Nδ only depends on δ, αj ’s are nonnegative finite constants and Rj ’s are
disjoint open finite intervals.
    Hence, if we define
                                              n                     
                                       1X 1                 zi − t
                           Ln (Z, t) =          L
                                       n i=1 hn               hn

we have the inequality
           Z                                   Z
              |fn (X, t) − fn (Y, t)| dt < 2δ + |Ln (X, t) − Ln (Y, t)| dt

   Following the proof of Theorem 3.1 (Devroye 1987) we obtain that for  > 0
and, if Λn (X, .) and Λn (Y, .) are the empirical probability measures associated to
the samples X and Y , respectively,
            Z                                  X
              |fn (X, t) − fn (Y, t)| dt <  +     |Λn (X, B) − Λn (Y, B)|
                                                   B∈∆r,l


where Πr,l is a partition of (−r, r), into intervals of length hn /l for some l > 0,
and ∆r,l = Πr,l ∪ {(−∞, −r) ∪ (r, ∞)} is a partition of R. As consequence
                                                                                  
  Z                                      X                                      
P     |fn (X, t) − fn (Y, t)| dt > λ ≤ P           |Λn (X, B) − Λn (Y, B)| > λ − 
                                                                                  
                                                   B∈∆r,l



                                          Revista Colombiana de Estadística 32 (2009) 289–299

Cramér-Chernoff Theorem for Two Independent Samples                                              295

   If =r,l is the set of all posible sets given by unions of elements of the partition
∆r,l , using a similar argument as in the Scheffé Theorem, we have
      Z                                    ff       (                                        )
                                                                                     1
  P        |fn (X, t) − fn (Y, t)| dt > λ        ≤P     sup |Λn (X, B) − Λn (Y, B)| > (λ − )
                                                      B∈=r,l                         2
                                                                                             ff
                                                    X                                1
                                                 ≤        P |Λn (X, B) − Λn (Y, B)| > (λ − )
                                                                                     2
                                                   B∈=r,l
                                                    X
                                                 ≤        exp{nQP{B} (λ − )}
                                                   B∈=r,l

                                                                    n(λ − )2
                                                                                        ff
                                                 ≤ Car(=r,l ) exp −           (1 + o(1))
                                                                        4


   As the cardinality of =r,l , Car(=r,l ), is at most 2(2+2rl/hn ) if we choose l such
that log(Car(=r,l )) = o(n) and taking into account that the previous inequality is
true for every , it is straightforward to deduce that
                                                                    λ2
                       Z                                   
               1
       lı́m sup log P          |fn (X, t) − fn (Y, t)| dt > λ   ≤ − (1 + o(1))      (3)
         n     n                                                    4

   Hence, from (2) and (3) the proof is completed when n1 = n2 . Under condition
(C2) this result can be generalized for any n1 and n2 as follows:
   Let be
                         m                       m                
                      1 X 1          xi − t     1 X 1         yi − t
                   Z
         Dm,m =                 K             −          K             dt
                      m i=1 hm        hm        m i=1 hM       hM

where m = mı́n{n1 , n2 } and M = máx{n1 , n2 }. By comparing Dm,m and Dn1 ,n2
we obtain the following inequalities:
                                  M −m                      M −m
                     Dm,m − 2          ≤ Dn1 ,n2 ≤ Dm,m + 2
                                    m                         m

   Under condition (C2) we have that, for all  > 0, there exists n1 such that for
each m > n1 where 2(M − m)/m < . Therefore, for all n1 , n2 > n1 , we obtain
that
   n1 + n2                            n1 + n2
           log P {Dn1 ,n2 > λ} = lı́m         log P {Dn1 ,n2 > λ + }
    2n1 n2                       → 0 2n1 n2
                                                 
               1                  M −m                 1
           ≤     log P Dm,m + 2           >λ+ ≤          log P {Dm,m > λ}                       (4)
              m                       m               m

      On the other hand,
                                                              
      n1 + n2                       1                M −m
              log P {Dn1 ,n2 > λ} ≥   log P Dm,m − 2        >λ
       2n1 n2                       M                  m
                                             
                  1                      M −m      1
              =      log P Dm,m > λ + 2         ≥    log P {Dm,m > λ + }
                 M                         m      M


                                                  Revista Colombiana de Estadística 32 (2009) 289–299

296                          Pablo Martínez-Camblor, Norberto Corral & Teresa López

then
                n1 + n2                       1
                        log P {Dn1 ,n2 > λ} ≥   log P {Dm,m > λ}                     (5)
                 2n1 n2                       M

   Taking into account inequalities (4) and (5), the proof is straightforward.

Observación 2. A weaker upper bound can be derived, easily, from the triangular
inequality and the result of Louani (2000). Directly,
   Z                                      Z                            
                                                                        λ       nλ2
 P      |fn (X, t) − fn (Y, t)| dt > λ ≤ 2P    |fn (X, t) − f (t)| dt >     ≤ e− 8
                                                                        2


3. Bahadur Relative Efficiency
    In order to show the application of the previous result, we use it to calculate the
Bahadur slope (BS) of a test, based on the Dn1 ,n2 distance, to determine whether
or not two continuous random variables have the same distribution. On the other
hand, we study its Bahadur relative efficiency (BRE) with respect to the classical
nonparametric Mann-Whitney test.
   Let X = {x1 , . . . , xn1 } and Y = {y1 , . . . , yn2 } be two independent random
samples from two continuous distributions F1 and F2 with densities f1 and f2 ,
respectively. From the above result one can compute the Bahadur slope (Bahadur
& Zabell 1979) for a test based on L1 (fn1 , fn2 ) statistic and an arbitrary alternative
hypothesis.
   From Devroye (1983) we obtain,
                       Z                  Z
                                        P
                          |fn1 − fn2 | −→ |f1 − f2 |

so the Bahadur slope of L1 (fn1 , fn2 ) is
                                    Z             2
                               1
                        BSL1 =           |f1 − f2 | (1 + o(1))
                               2

    Under the same conditions, it follows from the asymptotic distribution of the
statistic that the Bahadur slope of the Mann-Whitney (MW ) test is
                                                              
                          3
                                    Z                 Z
                                            2                2
                 BSMW =       (1 − 2 F1 f2 ) + (1 − 2 F2 f1 ) )
                          4

  Hence, the Bahadur relative efficiency between the test based on L1 and the
Mann-Whitney one is

Figura 1: Bahadur relative efficiency (BRE) between test based on L1 (fn1 , fn2 ) and
          the Mann-Whitney one for the four different models.



    In order to get some particular illustration of the relative efficiency for both
considered tests, we have computed (6) in four different situations (see Figure 1).
A sample is drawn from the standard normal density ϕ0,1 (t) and the other one,
f2 (t), follows one of the following densities,

       MD 1. f2 (t) = ϕa,1 (t)
       MD 2. f2 (t) = ϕa,3 (t)
       MD 3. f2 (t) = χ23 (at)
       MD 4. f2 (t) = χ24 (at)

here, ϕµ,σ (t) is the normal density function with mean µ and standard deviation
σ, χ2k (t) is the density function of a χ2 distribution with k degrees of freedom and
a takes values within (−3, 3).
    Figure 1 reveals that the MW test is more efficient (in the Bahadur sense)
than the L1 test whenever the difference among the densities is mainly in location
and large while the L1 test is more efficient when the main difference is in the
shape and neither function uniform dominates to the other. These conclusions are
strongly consistent with the obtained ones in other studies, which consider two
sample tests based on kernel density estimator as Cao & Van Keilegom (2006) or
Martínez-Camblor (2008).
Referencias
Bahadur R R,Zabell S L.Large Deviations of the Sample Mean in General Vector Space.(1979).Annals of Probability.
Beirlant L,Devroye L,Györfi L,Vajda A.Large Deviations of Divergence Measures on Partitions.(2001).Journal of Statistical Planning and Inference.
Berlinet A,Devroye L,Györfi L.Asymptotic Normality of the L1 error in the Histogram Density Estimation.(1995).Statistics.
Cao R,Lugosi G.Goodness-of-fit Tests Based on teh Kernel Density Estimate.(2005).Scandinavian Journal of Statistics.
Cao R,Van Keilegom I.Empirical Likelihood Tests for Two-Sample Problems via Nonparametric Density Estimation.(2006).Canadian Journal of Statistics.
Devroye L.The Equivalence of Weak, Strong and Complete Convergence in L1 for Kernel Density Estimates.(1983).Annals of Statistics.
Devroye L.A course in Density Estimation.(1987).Birkhauser.Boston.
Devroye L,Gyorfi L.Nonparametric Density Estimation, The L1 -View.(1985).Wiley.New York.
Devroye L,Wagner T J.The L1 Convergence of Kernel Density Estimates.(1979).Annals of Statistics.
Hórvath L.On Lp -Norms of Multivariate Density Estimations.(1991).Annals of Statistics.
Konakov V.Complete Asymptotic Expansions for the Maximun Deviation of the Empirical Density Function.(1978).Theory Probability Applied.
Louani D.Large Deviations Limit Theorems for the Kernel Density Estimator.(1998).Scandinavian Journal of Statistics.
Louani D.Large Deviation for L1 -Distance in Kernel Density Estimation.(2000).Journal of Statistical Planning and Inference.
Louani D.Uniform L1 -Distance Large Deviations for Nonparametric Density Estimation.(2005).Test.
Martínez Camblor P.Tests de hipótesis para contrastar la igualdad entre k poblaciones.(2008).Revista Colombiana de Estadística.
Martínez Camblor P,Corral N.Weaker Conditions for Asymptotic Approximation to LP -norms of the Kernel Estimators.(2008).InterSTAT Journal.
Osmoukhina A V.Large Deviations Probabilities for a Test of Symmetry Based on Kernel Density Estimator.(2001).Statistics and Probability Letters.
Parzen E.On Estimation of a Probability Density Function and Mode.(1962).Annals of Mathematical Statistics.
Rosenblatt M.Remarks on Some Nonparametric Estimates of a Density Functions.(1956).Annals of Mathematical Statistics.
Silverman B W.Weak and strong uniform consistency of the kernel estimate of a density and its derivates.(1978).Annals of Statistics.
Van der Vaart A W.Asymptotic Statistics.(1998).Cambridge University Press.London.