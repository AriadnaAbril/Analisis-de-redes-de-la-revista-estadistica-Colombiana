Comparaci√≥n entre dos m√©todos de reducci√≥n de dimensionalidad en series de tiempo
Universidad Santo Tom√°s
Resumen
En este trabajo se analizan dos m√©todos de reducci√≥n de dimensionalidad en series de tiempo multivariadas estacionarias: el m√©todo de Pe√±a y Box,basado en el dominio del tiempo, y el m√©todo de Brillinger, basado en el dominio de las frecuencias. Se encontraron dos fallas en el m√©todo de Pe√±a y Box, y se propusieron correcciones a estas. Tambi√©n se compararon los dos m√©todos con respecto a la capacidad para identificar el n√∫mero de factores latentes mediante simulaciones y se realiz√≥ una aplicaci√≥n emp√≠rica.
Palabras clave: series de tiempo multivariadas, reducci√≥n de dimensionalidad, dominio del tiempo, dominio de las frecuencias.
1. Introducci√≥n
   Dos de los aspectos m√°s importantes en la estad√≠stica son las variables y los
individuos, y existen m√©todos estad√≠sticos de reducci√≥n tanto para el n√∫mero de
  a Docente investigadora. E-mail: hanwenzhang@usantotomas.edu.co

individuos como para el n√∫mero de variables. Entre de los m√©todos de reducci√≥n
de variables se encuentran el m√©todo de componentes principales (ACP) y el mo-
delo factorial. Estos m√©todos tienen como objetivo no solo reducir el n√∫mero de
variables sino hacerlo de forma √≥ptima y as√≠ poder representar los datos en una
dimensi√≥n inferior logrando una interpretaci√≥n m√°s simple y compacta; siendo de
gran importancia y uso frecuente en diferentes √°reas de la estad√≠stica, incluyendo
su extensi√≥n al an√°lisis multivariado de series de tiempo.
    El aspecto m√°s importante en el an√°lisis de series de tiempo es que los datos
son correlacionados, entonces cuando se trabaja con una serie de tiempo multiva-
riada, el n√∫mero de par√°metros en los modelos es muy grande debido a la presencia
de correlaci√≥n tanto dentro de cada serie como entre las series univariadas que la
componen. Por consiguiente, si se logra reducir el n√∫mero de series, se puede sim-
plificar considerablemente la estructura del modelo; adem√°s, descubrir los factores
latentes que generan las series observadas es muy importante en series econ√≥micas,
demogr√°ficas, meteorol√≥gicas, etc.
    En el an√°lisis de series de tiempo existen dos enfoques: an√°lisis en el dominio del
tiempo y en el dominio de las frecuencias. Las herramientas principales del enfoque
en el dominio del tiempo son las funciones de autocorrelaci√≥n y autocorrelaci√≥n
parcial; mientras que el enfoque en el dominio de las frecuencias asume que la
serie es la suma de ondas de diferentes frecuencias y se estudian las frecuencias
m√°s importantes. Estos dos enfoques son matem√°ticamente equivalentes pues la
funci√≥n espectral en el dominio de las frecuencias es simplemente la transformada
de Fourier de la funci√≥n de autocovarianzas en el dominio del tiempo.
    En el campo de reducci√≥n de dimensionalidad en series de tiempo, tambi√©n se
han usado estos dos enfoques. Dentro del enfoque del dominio de tiempo se encuen-
tran el modelo de index propuesto por Reinsel (1983), el modelo de rango reducido
de Ahn & Reinsel (1988) y el modelo de componente escalar de Tiao & Tsay (1989).
Uno de los dos m√©todos considerados en esta investigaci√≥n, el m√©todo de Pe√±a &
Box (1987), est√° restringido a series estacionarias, y tiene como herramienta prin-
cipal las matrices de autocovarianzas muestrales. Debido a la popularidad de este
m√©todo, ha sido extendido para series no estacionarias por Pe√±a & Poncela (2006)
y para series no lineales por Correal & Pe√±a (2008). Por otro lado, en el enfoque del
dominio de las frecuencias, se encuentra el m√©todo de Stoffer (1999) para detectar
se√±ales comunes en una determinada frecuencia y el m√©todo de Brillinger (1981).
Este √∫ltimo es similar al m√©todo cl√°sico de componentes principales, pues utiliza la
varianza recogida por cada componente principal para determinar la reducci√≥n del
n√∫mero de variables. El m√©todo de Pe√±a y Box y el m√©todo de Brillinger tienen
b√°sicamente el mismo objetivo, y es natural, al momento de tener la necesidad
de reducir la dimensi√≥n de una serie multivariada, preguntar cu√°l de estos dos
m√©todos es mejor o, equivalentemente, cu√°l m√©todo utilizar. Sin embargo, en la
literatura estad√≠stica, no se ha presentado ning√∫n estudio comparativo de estos
m√©todos1 . Por consiguiente, el objetivo de esta investigaci√≥n es llenar este vac√≠o

   1 Tampoco existe, en la literatura, una relaci√≥n matem√°tica directa entre estos dos m√©todos y

el m√©todo cl√°sico de componentes principales.


                                         Revista Colombiana de Estad√≠stica 32 (2009) 189‚Äì212

Comparaci√≥n entre dos m√©todos de reducci√≥n de dimensionalidad...                      191

comparando los dos m√©todos en t√©rminos de su capacidad de identificaci√≥n del
n√∫mero de factores latentes usando tanto simulaciones como datos reales.


2. M√©todo de Pe√±a y Box
2.1. El modelo b√°sico
   El m√©todo de Pe√±a & Box (1987) supone que el proceso observable {zt } cen-
trado y de dimensi√≥n k es generado por factores {yt } de dimensi√≥n r con r ‚â§ k
m√°s un t√©rmino de error t , espec√≠ficamente:

                                     z t = P yt + t                                  (1)

donde P es una matriz de tama√±o k √ó r cuyo elemento pij representa el peso del
j-√©simo factor sobre la i-√©sima serie observada, y t es un proceso ruido blanco con
matriz de covarianzas Œ£ de rango completo k. Igual que en el modelo factorial
cl√°sico, la ecuaci√≥n (1) no est√° determinada de manera √∫nica, pues para cualquier
matriz invertible de tama√±o r √ó r, H, si se definen P ‚àó = P H y yt‚àó = H ‚àí1 yt se
tiene que zt = P ‚àó yt‚àó + t , y se logra la misma representaci√≥n con nuevas matrices
P ‚àó y yt‚àó . Para evitar este problema de identificaci√≥n se toma P 0 P = I.
    Otro supuesto es que el proceso {yt } obedece a un proceso V ARM A(py , qy ),
donde los polinomios autorregresivos y promedio m√≥vil se denotan por Œ¶y (B) y
Œòy (B) con Œ¶y (B) = I ‚àíŒ¶1y B ‚àí¬∑ ¬∑ ¬∑‚àíŒ¶py y B py y Œòy (B) = I ‚àíŒò1y B ‚àí¬∑ ¬∑ ¬∑‚àíŒòqy y B qy ,
entonces el modelo de {yt } es Œ¶y (B)yt = Œòy (B)at . Tambi√©n se supone que las
ra√≠ces de los determinantes |Œ¶y (B)| y |Œòy (B)| est√°n fuera del c√≠rculo unitario;
el ruido del proceso V ARM A {at } es un ruido blanco gaussiano con matriz de
covarianzas Œ£a , adem√°s E(at 0s ) = 0 para todo t y s.

2.1.1. Factores independientes

   En primera instancia se supone que las componentes de yt son mutuamente
independientes y las matrices Œ¶iy y Œòjy con i = 1, . . . , py y j = 1, . . . , qy son dia-
gonales. Sea Œìz (k) la funci√≥n matricial de covarianzas del proceso observado {zt }
y Œìy (k) la funci√≥n matricial de covarianzas del proceso estoc√°stico {yt }. Algunas
propiedades de estas matrices son:

   ‚Ä¢ Œìz (0) = P Œìy (0)P 0 + Œ£
   ‚Ä¢ Œìz (k) = P Œìy (k)P 0 , para k ‚â• 1
   ‚Ä¢ r(Œìz (k)) = r, para k ‚â• 1,

donde r(A) es el rango de la matriz A. Bajo el supuesto de que los factores son
independientes y la matriz Œ£a es diagonal se tiene que Œìy (k) es diagonal, entonces
Œìz (k) es sim√©trica para k ‚â• 1 y las columnas de la matriz P ser√°n los vectores
columnas de Œìz (k) asociado a los r valores propios distintos de cero, para todo
k ‚â• 1. N√≥tese tambi√©n que si Œìy (k) = 0 para todo k 6= 0, entonces Œìz (k) = 0 para

                                       Revista Colombiana de Estad√≠stica 32 (2009) 189‚Äì212

192                                                                                          Hanwen Zhang

todo k 6= 0, es decir, el proceso {zt } es un proceso ruido blanco, en cuyo caso se
tiene el modelo factorial cl√°sico representado por la ecuaci√≥n (1).
     Por otro lado, la identificaci√≥n del n√∫mero de factores r se puede realizar obser-
vando la matriz de autocorrelaciones parciales ‚Ñò(l) (Tiao & Box 1981). Se puede
demostrar que el rango de ‚Ñò(l) es a lo m√°s r. Existe una tercera forma de iden-
tificar el n√∫mero de factores, que es consecuencia del siguiente teorema (Pe√±a &
Box 1987).

Teorema 1. Si zt = P yt + t , donde yt ‚àº V ARM A(py , qy ), P es de rango
r con r ‚â§ k, {t } es un proceso ruido blanco, entonces {zt } sigue un proceso
V ARM A(pz , qz ) con pz = py y qz = max(py , qy ).

                                       P de identificar el n√∫mero de factores: las
   De este teorema se obtiene otra forma
matrices Œ®i en la representaci√≥n zt = ‚àû i=1 Œ®i t‚àíi tambi√©n tienen rango igual a r.
En conclusi√≥n, existen diferentes formas para encontrar el n√∫mero de factores r,
pero el c√°lculo de las matrices ‚Ñò(k) y Œ® es m√°s complicado que el de las matrices
Œì(k), y por eso la metodolog√≠a de esta investigaci√≥n se basa en el rango de las
matrices Œì(k).


2.1.2. Transformaci√≥n can√≥nica

   Se puede encontrar una transformaci√≥n del proceso {zt } de la cual se obtiene
una reducci√≥n de dimensi√≥n del proceso. Esta transformaci√≥n est√° dada por la
matriz M definida por:
                                     ‚àí 
                                       P
                                M=                                           (2)
                                       B
donde P ‚àí es la inversa de Moore-Penrose de la matriz P y B es una matriz con
BP = 0. Es posible definir B como la matriz formada por los k ‚àí r vectores
propios ligados a los valores propios nulos de la matriz P P 0 . As√≠, podemos obtener
las siguientes igualdades:
                                                                                                
                        P ‚àí zt               yt + P ‚àí t               yt + P ‚àí t               x1t
      xt = M zt =                    =                         =                         =                 (3)
                        Bzt                  BP yt + Bt               Bt                       x2t

     Es decir, los primeros r componentes de xt son iguales a los factores yt m√°s
un ruido y los restantes k ‚àí r componentes son iguales a un ruido blanco. De esta
forma, la estructura del proceso zt queda resumida dentro del proceso x1t que tiene
dimensi√≥n inferior, y as√≠ se logra una reducci√≥n de la dimensionalidad del proceso
zt .


2.1.3. Factores dependientes

    Cuando los factores generadores {yt } son dependientes, se trabaja con las ma-
trices de coeficientes del modelo V ARM A en vez de las matrices de covarianzas,
y se tienen las siguientes propiedades:

                                                Revista Colombiana de Estad√≠stica 32 (2009) 189‚Äì212

Comparaci√≥n entre dos m√©todos de reducci√≥n de dimensionalidad...                   193

(1) Œ¶y (s) = Ws Fs Ws‚àí1 donde Fs de tama√±o r√ór es diagonal y contiene los valores
    propios de Œ¶y (s) y Ws contiene los vectores propios de Œ¶y (s),
(2) Œ¶z (s)P Ws = P Ws Fs . Por ser Fs diagonal, esta contiene los valores propios
    de Œ¶z (s) y la matriz P Ws de tama√±o k √ó r contiene los vectores propios de
    Œ¶z (s).

   En conclusi√≥n, se puede establecer que los valores propios de Œ¶y (s) son los
mismos de Œ¶z (s). Por el otro lado, se considera la primera matriz de coeficientes
autorregresivos Œ¶z (1) y suponga que H es la matriz de tama√±o k √ó k que contiene
a todos sus vectores propios. Entonces las r columnas de H conforman la matriz
P W1 y estas corresponden a los vectores propios asociados a los valores propios
no nulos de Œ¶z (1).
                                                
                                        W1‚àí1 P 0
                                M=                                             (4)
                                        V0
donde V es la matriz de vectores propios de la matriz P P 0 = (P W1 )(W1‚àí1 P 0 )
asociados a los valores propios nulos. Definida M de esta forma podemos obtener
una transformaci√≥n similar al caso de factores independientes:
                                                           
                                  W1‚àí1 yt + W1‚àí1 t       x1t
                   xt = M zt =                       =                      (5)
                                  V 0 t                  x2t

   An√°logo al caso de los factores independientes, se obtiene un proceso x1t con
dimensi√≥n inferior que la del proceso original zt .


2.2. Implementaci√≥n pr√°ctica
    Esta secci√≥n introduce dos inconvenientes generados por el m√©todo de Pe√±a y
Box al momento de su implementaci√≥n en la pr√°ctica. As√≠ mismo, se proponen dos
alternativas te√≥ricas para lidiar con estos inconvenientes sin afectar los resultados
finales del m√©todo.

2.2.1. Identificaci√≥n del n√∫mero de factores

    El m√©todo de Pe√±a y Box sugiere determinar el n√∫mero de factores, obser-
vando los valores propios de las matrices de covarianzas  Pmuestrales de las series
                                                 b z (h) = n‚àíh (zt ‚àí z)(zt+h ‚àí z)0 /n
            b z (h). Sin embargo, estas matrices Œì
observadas, Œì                                               t=1
claramente no son sim√©tricas, y por consiguiente pueden tener valores propios ne-
gativos y, m√°s a√∫n, complejos, lo cual dificulta la determinaci√≥n del n√∫mero de
factores puesto que no es correcto usar las magnitudes de los valores propios para
determinar el n√∫mero de factores como lo ilustra el siguiente ejemplo de simulaci√≥n.
Se simul√≥ una serie zt de dimensi√≥n 4 de acuerdo con la ecuaci√≥n (1) con
                                    Ô£Æ                 Ô£π
                                       0.51    0.30
                                    Ô£Ø 0.54     0.60 Ô£∫
                               P =Ô£Ø Ô£∞ 0.60 ‚àí0.54 Ô£ª
                                                      Ô£∫

                                       0.30 ‚àí0.51

                                    Revista Colombiana de Estad√≠stica 32 (2009) 189‚Äì212

194                                                                      Hanwen Zhang

Œ£ = I, yt ‚àº V ARM A(1, 0) con Œ¶ = diag(0.3, 0.5) y
                                              
                                     1.0 0.6
                              Œ£a =
                                     0.6 1.0

      La matriz de autocovarianzas muestrales de orden 1 est√° dada por:
                              Ô£Æ                               Ô£π
                                ‚àí0.11 ‚àí0.13 ‚àí0.23 ‚àí0.01
                              Ô£Ø                  0.26 ‚àí0.01 Ô£∫
                    b z (1) = Ô£Ø 0.02 ‚àí0.1
                    Œì                                         Ô£∫                      (6)
                              Ô£∞ 0.16    0.11 ‚àí0.09 ‚àí0.27 Ô£ª
                                  0.02       0.16 ‚àí0.26 ‚àí0.18

    Al calcular los valores propios de la matriz Œìb z (1), se encontr√≥ que estos son
‚àí0.46, 0.043 ¬± 0.15i y ‚àí0.11. De tal forma que las magnitudes son: 0.46, 0.16,
0.16 y 0.11, respectivamente. Los anteriores valores llevan a la conclusi√≥n que el
n√∫mero de factores es 1 (considerando a 0.16 como peque√±o) o 3 (considerando
a 0.16 significativo), lo cual es err√≥neo de cualquier forma puesto que el n√∫mero
correcto de factores es 2. Afortunadamente la anterior inconveniencia se puede
resolver modificando la metodolog√≠a de Pe√±a y Box, de la forma sugerida en esta
investigaci√≥n, teniendo en cuenta que el n√∫mero de factores es igual al rango de
las matrices Œìb z (h), y este es igual al n√∫mero de valores singulares no nulos. En
primer lugar, es necesario recordar la definici√≥n de los valores singulares de una
matriz cuadrada (Jim√©nez 2004).
Definici√≥n 1. Los valores singulares de una matriz real A de tama√±o n√ó n son las
ra√≠ces cuadradas de los valores propios asociados a la matriz sim√©trica A0 A (listados
con sus multiplicidades algebraicas). Estos valores se denotan por œÉ1 , œÉ2 , . . . , œÉn ,
y se colocan en orden decreciente:
                                 œÉ1 ‚â• œÉ2 ‚â• ¬∑ ¬∑ ¬∑ œÉn ‚â• 0

    N√≥tese que por definici√≥n, a diferencia de los valores propios, los valores singu-
lares de una matriz son siempre reales positivos. El siguiente teorema establece la
relaci√≥n entre el rango de una matriz cuadrada y sus valores singulares.
Teorema 2. (Descomposici√≥n en Valores Singulares) Sea A una matriz real de
tama√±o n √ó n con rango r, r < n. Entonces existen matrices ortogonales U y V
de tama√±o n √ó n, tales que
                                A = U SV t                                (7)
donde S es la matriz particionada de tama√±o m √ó n, dada por
                                            
                                        Dr 0
                                 S=
                                        0 0
con                                  Ô£Æ                   Ô£π
                                      œÉ1       ¬∑¬∑¬∑    0
                                     Ô£Ø ..      ..     .. Ô£∫
                                Dr = Ô£∞ .          .    .Ô£ª
                                         0     ¬∑¬∑¬∑    œÉr
donde los œÉi con i = 1, ¬∑ ¬∑ ¬∑ , r son los valores singulares no nulos de A.

                                      Revista Colombiana de Estad√≠stica 32 (2009) 189‚Äì212

Comparaci√≥n entre dos m√©todos de reducci√≥n de dimensionalidad...                     195

    En el teorema anterior, como V t es una matriz invertible, entonces A y U S
tienen el mismo rango; usando el mismo argumento y el hecho que U tambi√©n
es invertible, por consiguiente A y S tienen el mismo rango, y por construcci√≥n
el rango de S es r, el n√∫mero de valores singulares no nulos de A. Por lo tanto,
se concluye que el rango de una matriz cuadrada es igual al n√∫mero de valores
singulares no nulo, y por consiguiente es m√°s conveniente usar los valores singulares
de Œìb z (h), para determinar el rango, que usar los valores propios. Retomando el
ejemplo formulado al principio de esta secci√≥n, donde se vio que el uso de valores
propios no es siempre adecuado, se tiene que con la propuesta de usar valores
                         b
singulares de la matriz Œì(1)  dada por (6), se llega a que estos son 0.51, 0.34, 0.16,
0.05. De esta manera, si consideramos los dos primeros valores singulares los m√°s
importantes, se puede llegar a la conclusi√≥n correcta de 2 factores; mientras que
con el uso de los valores propios, el n√∫mero de factores identificado ser√° 1 o 3,
mas nunca el valor correcto, 2. Ahora, observar directamente los valores de los
valores singulares y decidir a simple vista que los dos primeros son importantes no
es un m√©todo riguroso para identificar r, pues se necesitar√≠a cierto tipo de prueba
estad√≠stica para decidir sobre si estos son significativos o no; sin embargo, en la
literatura estad√≠stica no existe tal prueba basada en valores singulares, y ser√° un
tema abierto para futuras investigaciones.


2.2.2. Estimaci√≥n del modelo

    Seg√∫n lo mencionado en la secci√≥n 2.1, solo se puede encontrar la matriz P
cuando se asume que las componentes de yt son independientes para cada t. En
este caso, existen dos formas de hacerlo:

                                                  b z (k).
   1. Usando los vectores propios de las matrices Œì

   2. Usando los vectores propios de las matrices de coeficientes Œ¶z (k) del modelo
      V ARM A ajustado a la serie zt .

    En ambos casos los vectores propios deben ser ortonormales para garantizar
que P 0 P = I, de tal manera que el modelo quede determinado de forma √∫nica.
Sin embargo, existen dificultades al momento de ejecutar cualquiera de estas dos
alternativas en la pr√°ctica.
    En el caso de usar Œì   b z (k), como se indica en la secci√≥n 2.1.1, bajo el supuesto
(1), las matrices de autocovarianzas Œìz (k) son sim√©tricas; pero en la pr√°ctica la
estimaci√≥n de esta, Œì  b z (k) = 1 Pn‚àík (zt+k ‚àí z)(zt ‚àí z)0 puede no ser sim√©trica,
                                    n  t=1
y no siempre es posible encontrar vectores propios ortonormales de una matriz
no sim√©trica y, por lo tanto, no siempre se puede hallar la matriz P . El mismo
problema surge cuando se usan los vectores propios de Œ¶z (k) para construir P . Sin
embargo, este problema se puede resolver usando la representaci√≥n del modelo de
estados y el filtro de Kalman como lo sugieren por Pe√±a & Poncela (2006), m√©todo
que se describe a continuaci√≥n.

                                      Revista Colombiana de Estad√≠stica 32 (2009) 189‚Äì212

196                                                                      Hanwen Zhang

    Siguiendo a Brockwell & Davis (1996), un modelo de estados est√° dado por las
siguientes ecuaciones:


                                     zt = Gt xt + wt                                (8a)
                                 xt+1 = Ft xt + Rt vt                               (8b)


donde {zt } es de dimensi√≥n w √ó 1, que se expresa como una funci√≥n lineal del
vector de estado xt de dimensi√≥n v √ó 1 mediante la matriz Gt de tama√±o w √ó v,
{wt } ‚àº RB(0, Rt ), {vt } ‚àº RB(0, Qt ), E(wt vs0 ) = 0 para todo t y s. Adem√°s, el
vector de estado inicial x1 es incorrelacionado con los errores vt y wt para todo
t ‚â• 1. La primera ecuaci√≥n se llama ecuaci√≥n de observaci√≥n y la segunda, ecuaci√≥n
de estado o de transici√≥n.
    La presentaci√≥n del modelo de Pe√±a y Box en forma de un modelo de estados
es como sigue:


                                       zt = Pe xt + t                              (9a)
                                  xt = F xt‚àí1 + Rvt                                 (9b)

                   0
donde xt = [yt0 , yt‚àí1            0
                       , . . . , yt‚àíl+1 ]0 es de tama√±o rl √ó 1 con l = max(py , qy + 1),
donde py y qy denotan los √≥rdenes del modelo V ARM A de {yt } y r es el n√∫mero
de factores. Pe es una matriz de tama√±o r √ó rl que contiene la matriz P y matrices
nulas,

        Ô£Æ                                 Ô£π
        Œ¶1y     Œ¶2y    ¬∑ ¬∑ ¬∑ Œ¶l‚àí1,y   Œ¶ly         Ô£Æ                            Ô£π
      Ô£ØI                                           Ir       Œò1y   ¬∑ ¬∑ ¬∑ Œòl‚àí1,y
      Ô£Ø r        0     ¬∑¬∑¬∑     0       0 Ô£∫Ô£∫       Ô£Ø0
      Ô£Ø                                   Ô£∫                  0    ¬∑¬∑¬∑     0 Ô£∫
   F =Ô£Ø  0       Ir    ¬∑¬∑¬∑     0       0 Ô£∫, y R = Ô£Ø
                                                  Ô£Ø.         ..           .. Ô£∫
                                                                               Ô£∫
                                                                                    (10)
      Ô£Ø .                                                         ..
      Ô£Ø .         ..   ..       ..     .. Ô£∫
                                          Ô£∫       Ô£∞ ..        .       .    . Ô£ª
      Ô£∞ .          .       .     .      . Ô£ª
                                                    0        0    ¬∑¬∑¬∑     0
         0       0     ¬∑¬∑¬∑     Ir      0


    Cabe resaltar que Pe√±a & Poncela (2006) proponen el modelo de estados para la
estimaci√≥n puesto que su trabajo est√° enmarcado dentro del contexto de factores no
estacionarios. Seg√∫n lo se√±alado en este trabajo, en el caso de factores estacionarios,
tambi√©n es adecuado utilizar la misma metodolog√≠a. Por otro lado, para estimar
el modelo (9), es necesario conocer los √≥rdenes py y qy . Para esto se hace uso
del teorema 1, donde se establece que, bajo los supuestos en (1), se tiene que
pz = py y qz = maÃÅx(py , qy ). Entonces el procedimiento ser√° (i) obtener los valores
adecuados para pz y qz usando las series observadas {zt }; (ii) usar las relaciones
antes mencionadas para encontrar valores candidatos de py y qy ; por ejemplo, si
pz = qz = 2, entonces se tiene que py = pz y los valores candidatos para qy ser√°n
2, 1 y 0; (iii) usar criterios adecuados (por ejemplo, los criterios de informaci√≥n)
para escoger los valores finales de py y qy .

                                      Revista Colombiana de Estad√≠stica 32 (2009) 189‚Äì212

Comparaci√≥n entre dos m√©todos de reducci√≥n de dimensionalidad...                     197

3. M√©todo de Brillinger
3.1. M√©todo te√≥rico
    El m√©todo de Brillinger (1981) hace supuestos inversos a los supuestos del m√©-
todo de Pe√±a y Box, en el sentido de que este √∫ltimo expresa el proceso observado
en t√©rminos de los factores generadores, pero el m√©todo de Brillinger expresa los
factores generadores como combinaci√≥n del proceso observado. Suponga que el pro-
ceso observado Xt es de tama√±o k √ó 1, y los factores generadores, que Brillinger
llama los componentes principales, Œ∂t son de tama√±o r √ó 1 con r ‚â§ k. Brillinger
plantea la siguiente relaci√≥n entre estos dos procesos:
                                         X
                                 Œ∂t =              bt‚àíu Xu ,                        (11)
                                           u

donde las matrices bt son de tama√±o r √ó k. Dada esta ecuaci√≥n, la serie observada
Xt se puede estimar mediante los Œ∂t de la siguientes forma:
                                           X
                                    bt =
                                    X               ct‚àíu Œ∂u                         (12)
                                               u

Entonces el problema de encontrar los componentes principales Œ∂t se convierte en
buscar las matrices bu y cu tales que Xbt sea cercano a Xt . El siguiente teorema
provee la soluci√≥n para este problema:

Teorema 3. (Brillinger, 1981) Sea {Xt } de dimensi√≥n k √ó 1, un proceso esta-
cionario en sentido d√©bil con media cero, funci√≥n matricial de autocovarianzas
absolutamente sumable y funci√≥n matricial
                                     n      de densidad espectral f (œâ). Entonces
                                            P                P          o
las matrices bt y ct que minimizan E [Xt ‚àí u ct‚àíu Œ∂u ]t Xt ‚àí u ct‚àíu Œ∂u est√°n
dadas por
                                     Z 2œÄ
                                   1
                             bt =         B(Œ±)eitŒ± dŒ±                        (13)
                                  2œÄ 0
y
                                         Z 2œÄ
                                     1
                             ct =                  C(Œ±)eitŒ± dŒ±                      (14)
                                    2œÄ     0

donde
                                        Ô£Æ        Ô£π
                                         V1 (Œª)t
                                 B(Œª) = Ô£∞ ... Ô£ª
                                        Ô£Ø        Ô£∫
                                                                                    (15)
                                         Vr (Œª)t
y
                                                                        
                      C(Œª) = B(Œª)t = V1 (Œª)               ¬∑ ¬∑ ¬∑ Vr (Œª)              (16)

    Aqu√≠, Vj (Œª) denota el j-√©simo vector propio de la matriz de densidad espectral
f (Œª) ligado al valor propio ¬µj (Œª), para j = 1, . . . , r y cada Œª ‚àà [0, 2œÄ].

                                      Revista Colombiana de Estad√≠stica 32 (2009) 189‚Äì212

198                                                                                     Hanwen Zhang

   Para encontrar el j-√©simo componente principal Œ∂j (t) se debe escribir Xt en
t√©rminos de la representaci√≥n de Cram√©r dado por
                                   Z 2œÄ
                              Xt =      eiŒªt dZX (Œª)                       (17)
                                            0

donde                                            Z 2Œª
                              2œÄZX (Œª) =                   dX (Œ±)dŒ±                             (18)
                                                     0
y
                                                X
                                                ‚àû
                               dX (Œª) =                  X(t)e‚àíiŒªt                              (19)
                                             t=‚àí‚àû

    Se obtiene que Œ∂(t) est√° dado por
                                      Z 2œÄ
                             Œ∂(t) =             B(Œª)t eiŒªt dZX (Œª)                              (20)
                                       0

    Los componentes principales se pueden definir de otra manera como indica el
siguiente teorema:
Teorema 4. Brillinger (1981) suponga que se satisfacen las condiciones del teo-
rema 3, entonces la componente j-√©sima de Œ∂t , Œ∂j (t), est√° dada por
                                Z 2œÄ
                                       Bj (Œª)eiŒªt dZX (Œª)                                       (21)
                                 0

j = 1, . . . , r y t ‚àà Z, donde Bj (Œª) es de tama√±o 1√ór que satisface Bj (Œª)Bj (Œª)t = 1,
Œ∂j (t) tiene varianza m√°xima y coherencia 0 con Œ∂k (t) con k < j. La varianza
m√°xima alcanzada por Œ∂j (t) es
                                       Z 2œÄ
                                                ¬µj (Œ±)dŒ±                                        (22)
                                        0


    En la pr√°ctica, se obtiene la estimaci√≥n de f (Œª), y sus valores y vectores propios
y finalmente los componentes principales. La forma de calcular los componentes
principales se presenta en la siguiente secci√≥n.


3.2. Implementaci√≥n pr√°ctica
    An√°logo al m√©todo cl√°sico de componentes principales, se puede escoger el n√∫-
mero de componentes usando la cantidad de varianza recogida en cada componente
definida en (22). En la pr√°ctica, esta integral se puede evaluar usando la definici√≥n
de las integrales de Riemann-Stieltjes, esto es,
                   Z 2œÄ                n‚àí1
                                       X                            
                                                         ti+1 + ti
                          ¬µj (Œ±)dŒ± ‚âà            ¬µj                       (ti+1 ‚àí ti )           (23)
                    0                  i=1
                                                             2


                                        Revista Colombiana de Estad√≠stica 32 (2009) 189‚Äì212

Comparaci√≥n entre dos m√©todos de reducci√≥n de dimensionalidad...                         199

donde (ti , ti+1 ], con i = 1, . . . , n ‚àí 1, es una partici√≥n regular del intervalo [0, 2œÄ].
De manera an√°loga se eval√∫a las expresiones en las ecuaciones (18), (19) y (20).
    N√≥tese que para calcular estas varianzas recogidas dadas por (23), es necesario
calcular los valores propios de las matrices de densidad espectral; la estimaci√≥n de
estas matrices son matrices hermitianas, por lo tanto tienen valores propios reales,
y finalmente las varianzas recogidas por cada componente tambi√©n ser√°n n√∫meros
reales. Esta es una ventaja frente al m√©todo de Pe√±a y Box, pero como se ver√°
en las simulaciones de la secci√≥n 4, si se modifica la metodolog√≠a de Pe√±a y Box
siguiendo la sugerencia de la secci√≥n anterior, los dos m√©todos tendr√°n la misma
capacidad para identificar el mismo n√∫mero de factores.


4. Comparaci√≥n de los m√©todos
    Para realizar la comparaci√≥n de los dos m√©todos, recurrimos a simulaciones de
procesos estacionarios debido a la complejidad te√≥rica que se presenta al intentar
relacionar valores y vectores propios de las matrices de autocovarianzas con los
de las matrices de densidad espectral. La idea de las simulaciones es crear unas
series observadas zt que seas generadas por un n√∫mero conocido, r, de series no
observables yt , y se aplican los dos m√©todos a fin de comparar sus capacidades
para identificar el n√∫mero r.
    Sin embargo, el m√©todo de Pe√±a y Box y el m√©todo de Brillinger tienen supues-
tos diferentes con respecto a la relaci√≥n existente entre las series observadas y las
no observables. Y debido a esa diferencia, la forma de crear las series observadas
no puede ser ninguno de estos dos m√©todos, pues de lo contrario se estar√≠a dando
ventaja a uno de los dos. Por lo tanto, se decide crear las series observadas a partir
de cierta estructura de matriz de autocovarianzas de rango inferior a la dimensi√≥n,
lo cual es una consecuencia del m√©todo de Pe√±a y Box, y a la vez no est√° restringido
a un modelo espec√≠fico, pues, dada una funci√≥n de autocovarianzas, pueden existir
varios modelos que tienen a esa funci√≥n como su funci√≥n de autocovarianzas.
    En esta investigaci√≥n se deriv√≥ el siguiente teorema que permite crear una serie
multivariada a partir de una estructura de matriz de autocovarianzas, y extiende
los resultados del teorema 1.5.1 de Brockwell & Davis (1991, P√°g.27).
Teorema 5. Una sucesi√≥n de matrices, Œì(k), k = 0, 1, 2, . . ., de tama√±o p √ó p, es
una funci√≥n matricial de autocovarianzas de un proceso estoc√°stico multivariado
{Xt }, con Xt de dimensi√≥n p, si para todo n entero positivo, la matriz
                      Ô£Æ                                        Ô£π
                          Œì(0)        Œì(1)     ¬∑ ¬∑ ¬∑ Œì(n ‚àí 1)
                      Ô£Ø Œì(1)0         Œì(0)     ¬∑ ¬∑ ¬∑ Œì(n ‚àí 2)Ô£∫
                      Ô£Ø                                        Ô£∫
                  Œ£=Ô£Ø       ..          ..     ..       ..     Ô£∫              (24)
                      Ô£∞      .           .         .     .     Ô£ª
                        Œì(n ‚àí 1)0 Œì(n ‚àí 2)0 ¬∑ ¬∑ ¬∑      Œì(0)
de tama√±o np √ó np es definida no negativa.

Demostraci√≥n. Dado n cualquier entero positivo, sea t = (t1 , t2 , . . . , tnp )0 ‚àà Znp
con t1 < t2 ¬∑ ¬∑ ¬∑ < tnp , y sea Ft la funci√≥n de distribuci√≥n con funci√≥n caracter√≠stica

                                        Revista Colombiana de Estad√≠stica 32 (2009) 189‚Äì212

200                                                                                         Hanwen Zhang

dada por
                                      œÜt (u) = exp{‚àíu0 Œ£u/2}                                         (25)
con u = (u1 , u2 , . . . , unp )0 ‚àà Rnp . Como la matriz Œ£ es definida no negativa,
œÜt (u) es la funci√≥n caracter√≠stica de una distribuci√≥n Nnp (0, Œ£). Entonces Ft es
consistente y, por el teorema de Kolmogorov, existe un proceso estoc√°stico {Xt }
con Ft como su funci√≥n de distribuci√≥n finito-dimensional y œÜt (u) como su funci√≥n
caracter√≠stica.
   Por otro lado, si se toma cualquier vector del proceso {Xt }, lo podemos escribir
como

           (X11 , X21 , . . . , Xp1 , X12 , X22 , . . . , Xp2 , . . . , X1n , X2n , . . . , Xpn )0

y se toma el operador vec inversa para convertirlo en una matriz de tama√±o p √ó n,
vamos a obtener la siguiente matriz
                           Ô£Æ                             Ô£π
                        X11          X12      ¬∑¬∑¬∑    X1n
                      Ô£ØX21           X22      ¬∑¬∑¬∑    X2n Ô£∫
                      Ô£Ø                                  Ô£∫
                    X=Ô£Ø .             ..      ..      .. Ô£∫ = [X1 , X2 , . . . , Xn ]                 (26)
                      Ô£∞ ..             .         .     . Ô£ª
                               Xp1   Xp2      ¬∑ ¬∑ ¬∑ Xpn

donde Xi con i = 1, . . . , n son vectores de dimensi√≥n p. Y, por construcci√≥n,
tenemos que Cov(Xt+h , Xt ) = Œì(h), t = 1, . . . , n ‚àí h y h = 0, 1, . . . , n ‚àí 1. A
{X1 , . . . , Xn } se puede considerar como un proceso estacionario de longitud n.

     A continuaci√≥n se consideran casos cuando k series observadas son generadas
por r series no observadas con r ‚â§ k. Para el m√©todo de Pe√±a y Box, se calcula
los porcentajes acumulados de los valores singulares de Œì      b z (h). Como los valores
singulares siempre son reales positivos, y el n√∫mero de valores singulares no nulos
es el rango de Œìb z (h), entonces se pueden usar estos porcentajes acumulados para
determinar el rango de Œì    b z (h). Por ejemplo, si estos porcentajes acumulados de
b z (h) son (0.65, 0.92, 0.97, 0.99, 1), se puede decir que el rango de Œì(h) es 2. Por
Œì
otro lado, para el m√©todo de Brillinger, se calcula el porcentaje acumulado de
varianza. En esta investigaci√≥n se consideraron los siguientes casos:

   ‚Ä¢ Tres series observadas generadas por uno, dos y tres factores.

   ‚Ä¢ Cinco series observadas generadas por uno, dos, tres, cuatro y cinco factores.

    Para cada uno de los anteriores casos, las simulaciones se realizaron con 1000
r√©plicas, y las series simuladas son de tama√±o 100. Para el m√©todo de Pe√±a y Box,
se calcul√≥ en cada iteraci√≥n los porcentajes acumulados de los valores singulares
    b
de Œì(h)  con h = 0, 1, 2, 3; para el m√©todo de Brillinger, se calcul√≥ en cada iteraci√≥n
el porcentaje acumulado de varianzas. Si la serie de dimensi√≥n k es generada por r
factores, un indicio de que el m√©todo de Pe√±a y Box es adecuado es el hecho de que
el r-√©simo porcentaje acumulado de los valores singulares toma un valor cercano a
100 %; el criterio para evaluar el m√©todo de Brillinger es an√°logo, pero aplicado al

                                               Revista Colombiana de Estad√≠stica 32 (2009) 189‚Äì212

Comparaci√≥n entre dos m√©todos de reducci√≥n de dimensionalidad...                     201

porcentaje acumulado de varianzas. Por esta raz√≥n, los resultados de las simula-
ciones corresponden al n√∫mero de veces de las 1000 r√©plicas donde los porcentajes
acumulados se ubican en rangos determinados. Por limitaci√≥n de espacio, aqu√≠ se
presentan √∫nicamente los resultados de los siguientes tres casos:

Modelo 1: 3 series generadas por 2 factores, con estructura de covarianzas de los
   factores dada por Œì(0) = diag(1.09, 1.01), Œì(1) = diag(0.3, ‚àí0.1) y Œì(h) = 0
   para h > 1; los resultados se encuentran en la tabla 1.

Modelo 2: 3 series generadas por 3 factores, con estructura de covarianzas de los
   factores dada por
   Œì(0) = diag(1.39, 2.79, 2.11),
   Œì(1) = diag(‚àí0.28, 0.155, 1.46), Œì(2) = diag(‚àí0.61, 2.22, 0.68) y Œì(h) =
   Œì(h ‚àí 1)diag(‚àí0.3, 0.1, 0.9) + Œì(h ‚àí 2)diag(‚àí0.5, ‚àí0.8, ‚àí0.3) para h > 2; los
   resultados se encuentran en la tabla 2.

Modelo 3: 5 series generadas por 3 factores, con estructura de covarianzas de los
   factores dada por
   Œì(0) = diag(1.39, 2.79, 2.11), Œì(1) = diag(‚àí0.28, 0.155, 1.46),
   Œì(2) = diag(‚àí0.61, 2.22, 0.68) y Œì(h) = Œì(h ‚àí 1)diag(‚àí0.3, 0.1, 0.9) + Œì(h ‚àí
   2)diag(‚àí0.5, ‚àí0.8, ‚àí0.3) para h > 2; los resultados se encuentran en la tabla
   3.

Tabla 1: El n√∫mero de iteraciones donde el porcentaje acumulado de valores singulares
         (para el m√©todo Pe√±a y Box) y de varianzas (para el m√©todo de Brillinger) se
         ubica en determinados rangos para el modelo 1.

De la tabla 1, se observa en primer lugar que para las matrices Œì(h)b     con h =
0, 1, 2, 3, el n√∫mero de veces que el segundo porcentaje acumulado de los valores
singulares es mayor a 90 % es igual al n√∫mero de r√©plicas 1000; por otro lado,
para el m√©todo de Brillinger, el porcentaje de la varianza acumulada del segundo
componente tambi√©n es mayor a 90 % para cada una de las 1000 r√©plicas. Lo
anterior indica que los dos m√©todos tienen la misma capacidad para identificar
el n√∫mero de factores. N√≥tese que, dentro del m√©todo de Pe√±a y Box, con el uso
    b
de Œì(0),    en ninguna de las 1000 iteraciones, el primer valor singular pesa m√°s

                                      Revista Colombiana de Estad√≠stica 32 (2009) 189‚Äì212

202                                                                      Hanwen Zhang

                           b
del 90 %, mientras que con Œì(h) con h > 0, hay m√°s posibilidad de subestimar el
n√∫mero de factores. Con respecto a la tabla 2, n√≥tese que en el modelo 2 el n√∫mero


Tabla 2: El n√∫mero de iteraciones donde el porcentaje acumulado de valores singulares
         (para el m√©todo Pe√±a y Box) y de varianzas (para el m√©todo de Brillinger) se
         ubica en determinados rangos para el modelo 2.

de factores es igual al n√∫mero de series observadas, esto es, 3; entonces un buen
m√©todo debe tener poca posibilidad de subestimar el n√∫mero de factores. Con el
       b
uso de Œì(0) del m√©todo de Pe√±a y Box, en ninguna de las 1000 r√©plicas el segundo
porcentaje acumula m√°s del 90 %; tambi√©n el n√∫mero de veces que acumula entre
                                                                           b
80 y 90 % es bajo (105 de las 1000 r√©plicas). Mientras que con el uso de Œì(h)  con
h > 0, hay m√°s iteraciones donde se identifica err√≥neamente 2 factores. Por otro
lado, con el m√©todo de Brillinger, el n√∫mero de veces que el segundo porcentaje
acumula m√°s del 90 % tambi√©n es muy bajo (8 de las 1000 r√©plicas). En conclusi√≥n,
                                     b
el desempe√±o de Pe√±a y Box usando Œì(0)    es levemente mejor que el desempe√±o del
                                                                   b
m√©todo de Brillinger, y el desempe√±o de este es mejor que el de Œì(h)     con h > 0.
Con respecto a la tabla 3, para los resultados de los m√©todos aplicados a datos
simulados a partir del modelo 3, se observa, en primer lugar, que el desempe√±o del
                                                  b
m√©todo de Brillinger es, de nuevo, similar al de Œì(0)  del m√©todo de Pe√±a y Box.
                                b                       b
Por otro lado, el desempe√±o de Œì(1) es mejor que el de Œì(0)   puesto que el n√∫mero
                                                           b
de veces que el primer porcentaje de valores singulares de Œì(1) sea mayor de 90 %
                                                b
(149 de las 1000 r√©plicas) es menor que el de Œì(0) (484 de las 1000 iteraciones).
                        b
Es decir, con el uso de Œì(1) hay menos posibilidad de identificar err√≥neamente un
                         b
factor que con el uso de Œì(0). De la misma manera, se puede ver que con el uso de
b
Œì(1) hay menos posibilidad de identificar err√≥neamente 2 factores que con el uso
    b
de Œì(0).

Tabla 3: El n√∫mero de iteraciones donde el porcentaje acumulado de valores singulares
         (para el m√©todo Pe√±a y Box) y de varianzas (para el m√©todo de Brillinger) se
         ubica en determinados rangos para el modelo 3.
  
   Para las simulaciones cuyos resultados por limitaci√≥n de espacio, no se presen-
tan aqu√≠, se muestran comportamientos an√°logos a los presentados anteriormente.
En s√≠ntesis, se observan los siguientes comportamientos:

   ‚Ä¢ Para el caso cuando el n√∫mero de factores, r, es estrictamente menor que el
     n√∫mero de series observadas, k:
                     b z (0) conduce al mismo resultado que el m√©todo de Brillinger.
        1. El uso de Œì
                                                b z (h) para h > 1.
                     b z (1) es mejor que el de Œì
        2. El uso de Œì
                               b z (1) conduce a los mismos resultados.
                     b z (0) y Œì
        3. El uso de Œì
   ‚Ä¢ Para el caso cuando el n√∫mero de factores, r, es igual al n√∫mero de series
                                                                   b z (h) para h > 1
                              b z (0) es levemente mejor que el de Œì
     observadas, k: el uso de Œì
                                                b
     y que el m√©todo de Brillinger, pues con Œìz (h) o el m√©todo de Brillinger hay
     m√°s posibilidad de subestimar el n√∫mero de factores.


5. Una aplicaci√≥n emp√≠rica
   En esta secci√≥n se aplican los dos m√©todos a 9 variables econ√≥micos de Colombia
analizadas en Melo et al. (2001): situaci√≥n econ√≥mica actual de la industria, volu-
men actual de pedidos por atender de la industria, √≠ndice de producci√≥n real de la

industria manufacturera sin trilla de caf√©, √≠ndice de empleo de obreros de la indus-
tria, producci√≥n de cemento, demanda de energ√≠a m√°s consumo de gas residencial e
industrial, importaciones reales exceptuando las de bienes de capital y duraderos,
cartera neta real en moneda legal y saldo de efectivo en t√©rminos reales. En Melo
et al. (2001) se analizaron estas series desde enero de 1980 hasta agosto de 2001;
en esta aplicaci√≥n se ampli√≥ el periodo de observaci√≥n hasta diciembre de 2005.
Los datos se desestacionalizaron usando m√©todos de suavizamiento y est√°n libres
de datos at√≠picos e intervenciones (Mart√≠nez 2007). La gr√°fica de estas nueve series
econ√≥micas se encuentra en la figura 1.

Figura 1: Series econ√≥micas de Colombia, concernientes a la aplicaci√≥n de la secci√≥n 5,
          sin diferenciar.


    Los correlogramas y los p-valores de la prueba de ra√≠z unitaria de Dickey y
Fuller sugieren que cada una de las nueve series tiene una ra√≠z unitaria, y con la
aplicaci√≥n de la diferenciaci√≥n de orden 1 se obtiene la estacionariedad, de donde
se encuentra que cada una de las nueve series son integradas de orden 1 (I(1)). Un

                                            Revista Colombiana de Estad√≠stica 32 (2009) 189‚Äì212

Comparaci√≥n entre dos m√©todos de reducci√≥n de dimensionalidad...                       205

an√°lisis de cointegraci√≥n muestra que las series son cointegradas de orden CI(1, 1).
Aplicando la prueba de Johansen (1991) con la estad√≠stica de prueba ŒªmaÃÅx para
hallar el rango de integraci√≥n s, se obtuvo que, con un nivel de significaci√≥n del
5 %,

   ‚Ä¢ para la hip√≥tesis nula de H0 : s ‚â§ 6, ŒªmaÃÅx = 11.27, el valor cr√≠tico es 21.07,
     de donde no se rechaza H0 ;

   ‚Ä¢ para la hip√≥tesis nula de H0 : s ‚â§ 5, ŒªmaÃÅx = 28.77, el valor cr√≠tico es 27.14,
     de donde se rechaza H0 .

    Por lo tanto, se concluye que el rango de cointegraci√≥n es 6, y por consiguiente
las series comparten en total 3 tendencias estoc√°sticas comunes. N√≥tese que un
interesante estudio es aplicar el m√©todo de Pe√±a & Poncela (2006) para series no
estacionarias y comparar con el resultado del anterior an√°lisis de cointegraci√≥n.
    A continuaci√≥n se aplican los m√©todos de Pe√±a y Box y el de Brillinger. Para
tal fin las series deben ser estacionarias; por lo tanto, en adelante se trabajar√° con
las series diferenciadas de orden 1 las cuales se presentan en la figura 2.


5.1. Identificaci√≥n del n√∫mero de factores
    Para identificar el n√∫mero de factores comunes, se calculan los valores propios
de las matrices de autocovarianzas muestrales siguiendo lo sugerido por Pe√±a y
Box. En la tabla 4 se muestran dichos valores; se observa que los dos primeros
valores propios son los m√°s importantes, lo cual indica que las nueve series se
pueden reducir a dos; sin embargo, los primeros dos valores propios de las matrices
de covarianzas muestrales de rezago 2 y 4 son complejos, por eso se examinan
los valores singulares de la tabla 5. Estos conducen a la misma conclusi√≥n de
dos factores. Se puede observar c√≥mo el uso de los valores singulares corrige las
inconveniencias que se presentan al usar los valores propios. Por el lado del m√©todo
de Brillinger, la varianza acumulada de la primera componente pesa un 81 %, y los
dos primeros componentes acumulan el 100 % de la varianza, de donde se concluye
que tambi√©n se identifican 2 factores.

Tabla 4: Valores propios de las matrices de autocovarianzas muestrales de las series
         econ√≥micas de la aplicaci√≥n emp√≠rica para los rezagos h = 0, 1, 2, 3, 4.

Figura 2: Series econ√≥micas de Colombia, concernientes a la aplicaci√≥n de la secci√≥n 5,
          despu√©s de diferenciar una vez.


Tabla 5: Valores singulares de las matrices de autocovarianzas muestrales de las series
         econ√≥micas de la aplicaci√≥n emp√≠rica para los rezagos h = 0, 1, 2, 
5.2. Extracci√≥n de los factores
    Para estimar el modelo de Pe√±a y Box con el modelo de estados, se necesitan los
valores de py y qy . Siguiendo lo indicado en la secci√≥n 2.2.2, primero se identifican
los valores de pz y qz . Para esto se examinaron las matrices de correlaci√≥n muestral
œÅb(k) cuyo i, j-√©simo elemento est√° dado por


y las estimaciones de matrices de autocorrelaci√≥n parcial

    Ninguna de las matrices œÅb(k) y ‚Ñò(k)
                                    b    muestra tendencia de extinguirse a medida
que crece k, lo cual indica que el modelo apropiado para zt no es V AR o V M A
puro, sino un modelo V ARM A mixto; pues para un proceso V AR(p) las matrices
‚Ñò(k) son iguales a 0 para k > p, mientras que para un proceso V M A(q) las
matrices de correlaci√≥n son nulas para rezagos mayores que q, (Wei 2006, pg. 402).
Debido a la dificultad que tiene identificar y estimar un modelo V ARM A para
un n√∫mero grande de series, se procede a estimar el modelo de estados (9) y (10)
directamente para diferentes valores de py y qy . En la tabla 6 se encuentran los
valores de algunos criterios de selecci√≥n para diferentes valores de py y qy , donde
se escoge el modelo V ARM A(2, 1) para los factores yt .
    Para facilitar la interpretaci√≥n de los factores, los modelos (9) y (10) fueron
estimados con la restricci√≥n que los componentes de factores yt son independientes.
Esta restricci√≥n es equivalente a que las matrices Œ¶ y Œò del modelo V ARM A para

Tabla 6: Valores de criterios de selecci√≥n de diferentes modelos V ARM A para el proceso
         de los factores latentes.
 

yt son diagonales y el t√©rmino de error, at , sea ruido blanco gaussiano con Œ£a = I.
Finalmente el modelo estimado es:

con Œ£a = I y Œ£ = diag(0, 0.05, 0.04, 0.22, 0.14, 0.29, 0.05, 0.28, 0.02). De las esti-
maciones obtenidas2 , vemos que el primer factor est√° asociado principalmente con
la variable P RCEM diferenciada, pues esta tiene el mayor peso, 0.94, sobre el
primer factor; mientras que el segundo factor est√° asociado de manera negativa
con las variables P RCEM , IP R, IEM OB y EF ECRC.


5.3. An√°lisis de residuales
   Para verificar que los residuales t son aproximadamente un proceso ruido
blanco, se utiliza la estad√≠stica Q, siguiendo a Li (2004), definida como sigue:
                                                  m
                                                  X                              
                                  Q(m) = n              tr Ck0 C0‚àí1 Ck C0‚àí1                                   (31)
                                                  k=1

donde
                                                 n
                                               1 X
                                    Ck =           (b       t ‚àí )0
                                                    t ‚àí )(b                                                 (32)
                                               n
                                                 t=k+1

    2 Con el anterior modelo estimado, se puede extraer los dos factores identificados usando la

presentaci√≥n en modelos de estados; por el otro lado, aunque con el m√©todo de Brillinger tambi√©n
se puede calcular num√©ricamente los componentes principales, estos son complejos, lo que dificulta
la interpretaci√≥n, por lo cual se recomienda usar el m√©todo de Pe√±a y Box en la pr√°ctica.


                                                  Revista Colombiana de Estad√≠stica 32 (2009) 189‚Äì212

Comparaci√≥n entre dos m√©todos de reducci√≥n de dimensionalidad...                    209

se tiene que la estad√≠stica se distribuye asint√≥ticamente chi-cuadrado con l2 m
donde l es la dimensi√≥n de la serie t . Se aplic√≥ la anterior prueba a los residuales
t del modelo (29); el valor de la estad√≠stica Q(m) fue de 150.5, un valor grande
b
a primera vista, pero al tener en cuenta que en esta aplicaci√≥n la dimensi√≥n de b   t
es l = 9, entonces para cualquier valor de m > 1 se tiene que el percentil 95 %
de la distribuci√≥n chi-cuadrado es mayor que 150.5, y as√≠ se acepta a b    t como la
realizaci√≥n de un proceso ruido blanco.


6. Conclusiones y sugerencias
    En este trabajo se compar√≥ el m√©todo de Pe√±a y Box y el m√©todo de Brillinger
para la reducci√≥n de dimensionalidad bajo los supuestos respectivos de los dos
m√©todos. Se encontr√≥ que tanto en las simulaciones como en la aplicaci√≥n emp√≠rica
los dos m√©todos tienen la misma capacidad para identificar el n√∫mero de factores
comunes. Cabe resaltar que en las simulaciones y en la aplicaci√≥n emp√≠rica se
utiliz√≥ el m√©todo de Pe√±a y Box modificado, seg√∫n los resultados de este trabajo.
Adicionalmente se encontr√≥ que:

   ‚Ä¢ Con respecto a la identificaci√≥n del n√∫mero de factores comunes, el m√©todo
     de Brillinger conduce a los mismos resultados que el m√©todo de Pe√±a y Box
     usando Œì b z (0).

   ‚Ä¢ Cuando el n√∫mero de factores es cercano al n√∫mero de series observadas, el
                                                                         b z (1), pues
                                              b z (0) es mejor que el de Œì
     uso del m√©todo de Brillinger o el uso de Œì
     b z (1) tiende a subestimar el n√∫mero de factores.
     Œì

   ‚Ä¢ Con respecto a la extracci√≥n de factores comunes, se encontr√≥ que el m√©todo
     de Brillinger conduce a factores complejos que no tienen interpretaci√≥n en la
     pr√°ctica, mientras que el m√©todo de Pe√±a y Box, mediante el uso de modelos
     de estados, permite una interpretaci√≥n m√°s di√°fana.

Como resumen, en la tabla 7 se encuentran los principales inconvenientes de cada
uno de los dos m√©todos. Para algunos se proveen algunas soluciones respectivos y
en la tabla 8 se encuentran los principales resultados de comparaci√≥n de los dos
m√©todos.
     Como motivo para futuras investigaciones, n√≥tese que en el m√©todo de Pe√±a y
Box modificado propuesto en este trabajo, la identificaci√≥n del n√∫mero de factores
se lleva a cabo observando la magnitud de los valores singulares de las matrices
b z (k) de manera emp√≠rica o heur√≠stica, pues en la literatura a√∫n no se conoce
Œì
la distribuci√≥n probabil√≠stica de estos valores singulares, y por lo tanto tampoco
existe una prueba estad√≠stica basada en valores singulares para determinar el n√∫-
mero de factores. Otro tema abierto a investigaci√≥n es probar matem√°ticamente
que los dos m√©todos tienen las mismas capacidades de identificaci√≥n del n√∫mero
de factores, y adicionalmente su capacidad predictiva. El tema de investigaci√≥n
de este art√≠culo contin√∫a abierto pues los resultados encontrados ac√° est√°n supe-
ditados a los supuestos originales de los autores de cada m√©todo. Sin embargo es

                                     Revista Colombiana de Estad√≠stica 32 (2009) 189‚Äì212

210                                                                              Hanwen Zhang

                  Tabla 7: Inconvenientes y mejoras de los dos m√©todos.
                                 M√©todo de Pe√±a-Box
                   Inconveniente                            Soluci√≥n
                         d pueden ser ne-
      Valores propios de Œì(k)               Usar valores singulares en vez de valo-
      gativos o complejos, causando problema      res propios, pues son siempre reales y
      para determinar el n√∫mero de factores r.    positivos.

      No se puede encontrar los vectores pro-   Usar el modelo de estado para estimar el
                            b
      pios ortonormales de Œì(k),  pues estas no modelo y extraer los factores
      son sim√©tricas.
                                      M√©todo de Brillinger
                    Inconveniente                               Soluci√≥n
      Conduce a factores complejos que dificul- No hay soluci√≥n
      tan la interpretaci√≥n


                      Tabla 8: Comparaciones entre los dos m√©todos.
                M√©todo de Pe√±a-Box                         M√©todo de Brillinger
           Tienen la misma capacidad para identificar el n√∫mero de factores latentes.
      La metodolog√≠a original usa valores pro-  Usa varianza recogida por cada factor
              b
      pios de Œì(k) que pueden ser complejos o   que siempre es real y positivo.
      negativos
      Procedimientos relativamente sencillos    Procedimientos num√©ricos largos y costos

      Conduce a factores reales                   Conduce a factores complejos

      Factores con f√°cil interpretaci√≥n           Factores carecen de interpretaci√≥n

      Se puede extender para series no estacio-   En la literatura no se conoce todav√≠a ex-
      narias                                      tensi√≥n para series no estacionarias



posible plantearse qu√© tan robustos son los m√©todos mediante el uso de residuales
caracterizados porque su funci√≥n de densidad tuviese colas pesadas, o mostrasen
evidencia de heterocedasticidad u otras caracter√≠sticas. Por otro lado, los resul-
tados encontrados en este art√≠culo se basan en el m√©todo de simulaci√≥n de series
multivariadas dado por el teorema 5. Cabe resaltar que no es la √∫nica alternativa
para la simulaci√≥n de las series. Por ejemplo, otra opci√≥n est√° dada por la simula-
ci√≥n de una serie multivariada que sea una combinaci√≥n convexa entre un proceso
generado de acuerdo con el m√©todo de Pe√±a y Box y otro respecto al m√©todo de
Brillinger.



Agradecimientos
    El autor da gracias a Dios por su bondad, al profesor Fabio Nieto por su
paciencia, a Andr√©s Guti√©rrez por su ayuda en la redacci√≥n y a los √°rbitros por los
valiosos comentarios.

Referencias
Ahn S K,Reinsel G C.Nested Reduced-Rank Autoregressive Models for Multiple Time Series.(1988).Journal of the American Statistical Association.
Brillinger D R.Time Series: Data Analysis and Theory.(1981).Holden-Day.San Francisco.
Brockwell P J,Davis R A.Time Series: Theory and Methods.(1991).Springer.New York.
Brockwell P J,Davis R A.Introduction to Time Series and Forecasting.(1996).Springer.New York.
Correal M E,Pe√±a D.Modelo factorial din√°mico threshold.(2008).Revista Colombiana de Estad√≠stica.
Jim√©nez J A.√Ålgebra Lineal II, con aplicaciones en estad√≠stica.(2004).Unibiblos.Bogot√°.
Johansen S.Estimation and Hypothesis Testing of Cointegration Vectors in Gaussian Vector Autoregressive Models.(1991).Econometrica.
Li W K.Diagnostic Checks in Time Series.(2004).Chapman & Hall/CRC.
Mart√≠nez W.Uso de tendencias comunes en la construcci√≥n de √≠ndices coincidentes.(2007).Universidad Nacional de Colombia.Bogot√°.
Melo L F,Nieto F,Posada C E,Betancourt Y R,Bar√≥n J D.Un √≠ndice coincidente para la actividad econ√≥mica de Colombia.(2001).Ensayos Sobre Pol√≠tica Econ√≥mica.
Pe√±a D,Box G B P.Identifying a Simplifying Structure in Time Series.(1987).Journal of the American Statistical Association.
Pe√±a D,Poncela P.Nonstationary Dynamic Factor Analysis.(2006).Journal of Statistical Planning and Inference.
Reinsel G C.Some Results on Multivariate Autoregressive Index Models.(1983).Biometrika.
Stoffer D S.Detecting Common Signals in Multiple Time Series using the Spectral Envelope.(1999).Journal of the American Statistical Association.
Tiao G C,Box G E P.Modelling Multiple Time Series with Applications.(1981).Journal of the American Statistical Association.
Tiao G C,Tsay R S.Model Specification in Multivariate Time Series.(1989).Journal of the Royal Statistical Society.
Wei W S.Time Series Analysis: Univariate and Multivariate Methods.(2006).Pearson.Boston.