Comparaci√≥n entre tres t√©cnicas de clasificaci√≥n
Universidad de S√£o Paulo;Universidad Nacional de Colombia
Resumen
En este art√≠culo se muestran los resultados de un estudio de comparaci√≥n mediante simulaci√≥n de tres t√©cnicas de clasificaci√≥n, regresi√≥n log√≠stica multinomial (MLR), an√°lisis discriminante no m√©trico (NDA) y an√°lisis dis- criminante lineal (LDA). El desempe√±o de las t√©cnicas se midi√≥ usando la tasa de clasificaci√≥n err√≥nea. Se encontr√≥ que las t√©cnicas MLR y LDA tuvieron un desempe√±o similar y muy superior a NDA cuando la distribuci√≥n multivariada de las poblaciones es normal o logit-normal; en el caso de distribuciones multivariadas log-normal y Sinh‚àí1 -normal la t√©cnica MLR tuvo mejor desempe√±o.
Palabras clave: regresi√≥n log√≠stica multinomial, an√°lisis discriminante no m√©trico, an√°lisis discriminante lineal, clasificaci√≥n.
Introducci√≥n
    El proceso de asignar una observaci√≥n p variada en uno de varios grupos prees-
tablecidos se denomina clasificaci√≥n. El objetivo b√°sico es construir una funci√≥n
discriminante que tome la informaci√≥n de las p variables para resumirla en un
indicador con el cual se pueda clasificar la observaci√≥n de manera correcta en uno
de los grupos. En la literatura estad√≠stica se pueden encontrar varios m√©todos
desarrollados para abordar el problema de clasificaci√≥n.
    Una de las t√©cnicas m√°s conocidas en clasificaci√≥n fue propuesta por Fisher
(1936); este enfoque se denomina An√°lisis Discriminante Lineal (LDA) y b√°sica-
mente divide el espacio muestral en subespacios mediante hiperplanos que permi-
ten separar lo mejor posible los grupos en estudio. Los supuestos para la utilizaci√≥n
de LDA son: normalidad multivariada e igualdad de matrices de covarianzas en-
tre los grupos. Welch (1939) mostr√≥ la optimalidad de LDA bajo condiciones de
normalidad multivariada. Rao (1948) propuso el an√°lisis discriminante can√≥nico el
cual es una generalizaci√≥n del an√°lisis discriminante lineal para el caso de varios
grupos. Clunies & Riffenburgh (1960) y Anderson (1972) encontraron una funci√≥n
discriminante para el caso donde no se cumple el supuesto de igualdad de matrices
de covarianzas.
    En estimaci√≥n recientemente Hawkins & McLachan (1997) y Croux & Dehon
(2001) propusieron un procedimiento llamado high breakdown para remover ob-
servaciones at√≠picas que pueden afectar las estimaciones de los vectores de medias
y de las matrices de covarianzas para cada uno de los grupos utilizados por LDA y
que, por tanto, pueden afectar el proceso de clasificaci√≥n. Cheng et al. (2002) pro-
pusieron dos formas alternativas de realizar las estimaciones del vector de medias
y de la matriz de covarianzas cuando hay datos faltantes; las propuestas consis-
ten b√°sicamente en combinar el algoritmo ER (Estimation-Robust) propuesto por
Little & Smith (1987) con los estimadores high breakdown.
    Otra de las t√©cnicas de clasificaci√≥n se debe a Raveh (1983 y 1989) quien pro-
puso el An√°lisis Discriminante no M√©trico (NDA), el cual utiliza como punto de
partida la funci√≥n discriminante generada por LDA y por medio de un proceso
iterativo propuesto por Choulakian & Almhana (2001) se mejora la funci√≥n discri-
minante inicial; la calidad de la nueva funci√≥n discriminante se mede por medio de
un √≠ndice de separaci√≥n entre los grupos; este √≠ndice fue propuesto por Guttman
(1998). La ventaja que tiene NDA sobre LDA es que NDA no requiere supuestos
distribucionales.
    Otra de las t√©cnicas de clasificaci√≥n es el modelo de regresi√≥n log√≠stica, sugerido
por Cornfield (1962), Cox (1966) y Day & Kerridge (1967) para una variable con
respuesta binaria; Anderson (1972) propuso el modelo de regresi√≥n log√≠stica mul-
tinomial o policotomo (MLR). Pregibon (1981) plante√≥ un m√©todo para detectar
posibles observaciones at√≠picas en la muestra de entrenamiento que se usa para
encontrar los estimadores de m√°xima verosimilitud para un modelo de regresi√≥n
log√≠stico. Trevor & Ferry (1991) presentaron un nuevo modelo de regresi√≥n log√≠sti-
ca robusto que mostr√≥ tener mejor desempe√±o en un estudio de simulaci√≥n y en un
estudio con datos reales. Carroll & Pederson (1993) mostraron que existen otras

                                     Revista Colombiana de Estad√≠stica 32 (2009) 247‚Äì265

Comparaci√≥n entre tres t√©cnicas de clasificaci√≥n                                          249

dos formas de estimaciones resistentes y robustas que tienen sesgos iguales o me-
nores en las estimaciones cuando se tienen observaciones at√≠picas en el conjunto de
observaciones. Una ventaja de la herramienta MLR es que no requiere supuestos
distribucionales.


1.1. T√©cnicas de clasificaci√≥n
    Sup√≥ngase que se tiene la tarea de clasificar observaciones (sujetos u objetos)
p variadas en uno de g grupos ya establecidos y que para esto se cuenta con un
conjunto de observaciones proveniente de cada uno de los grupos. Sean xij ‚àà <p las
observaciones donde j = 1, . . . , g , i = 1, . . . , nj tal que nj representa el n√∫mero
de observaciones que pertenecen al grupo j. Este conjunto de n observaciones1
se denomina conjunto de entrenamiento y permite la construcci√≥n de la funci√≥n
discriminante para cada una de las t√©cnicas. A continuaci√≥n se describe de cada
una de las tres t√©cnicas estudiadas.

1.1.1. An√°lisis discriminante lineal

    Sean xj y Sj los vectores de medias y las matrices de covarianzas de cada uno
de los g grupos y sea x el vector de medias global del conjunto de entrenamiento.
Usando estos elementos se pueden definir dos matrices B y W que representan la
variabilidad entre los grupos y la variabilidad dentro de los grupos respectivamente
y que est√°n dadas por:
                                     g
                                     X
                                B=         nj (xj ‚àí x)(xj ‚àí x)0                           (1)
                                     j=1


                                            g
                                            X
                                 W =              (nj ‚àí 1)Sj                              (2)
                                            j=1

   El objetivo de la t√©cnica LDA es encontrar un vector a ‚àà <p de tal manera
que se maximice el cociente Œõ definido por (3); as√≠ se encuentra un hiperplano que
genera la m√°xima diferencia entre la variabilidad intergrupal e intragrupal.

                                                  a0 Ba
                                           Œõ=                                             (3)
                                                  a0 W a
    Rencher (1998) muestra que los valores de a que maximizan Œõ se pueden estimar
por medio de los vectores propios e1 , e2 , . . . , es asociados a los valores propios
positivos2 Œª1 > Œª2 > ¬∑ ¬∑ ¬∑ > Œªs de W ‚àí1 B. De esta manera si b      a = e1 entonces aÃÇ
se denomina primer discriminante lineal estimado (LD1 ) y este corresponde a la
funci√≥n discriminante. La regla para clasificar una nueva observaci√≥n x en uno de
                   g
  1 En total n =
                   P
                   nj .
               j=1
  2 Donde s = min {p, g ‚àí 1}.




                                           Revista Colombiana de Estad√≠stica 32 (2009) 247‚Äì265

250                            Freddy Hern√°ndez Barajas & Juan Carlos Correa Morales

los grupos basada en el primer discriminante lineal consiste en asignar x al grupo
                  h 0         i2
                   a (x ‚àí xj ) es m√≠nimo.
j si se cumple que b


1.1.2. An√°lisis discriminante no m√©trico

   La t√©cnica NDA propuesta por Raveh (1989) se basa en los resultados de LDA
y consiste en la b√∫squeda de Œ∑ ‚àà <p que maximice a Disco (coeficiente de discri-
minaci√≥n) propuesto por Guttman (1998) dado en (4).
                                        g
                                      g P
                                      P
                                         nk nh Œ∑ 0 [xk ‚àí xh ]
                                k=1 h=1
                        Disco = g g nk nh                                              (4)
                                P P PP 0
                                               Œ∑ [xik ‚àí xlh ]
                               k=1 h=1 i=1 l=1


    El numerador de (4) representa la separaci√≥n entre los diferentes grupos mien-
tras que el denominador representa la variaci√≥n total entre todas las observaciones.
Una propiedad importante es que 0 ‚â§ Disco ‚â§ 1, donde el valor de cero se obtiene
solamente cuando todos los vectores de medias para cada grupo del conjunto de
entrenamiento son iguales y el valor de uno cuando los puntajes3 para cada grupo
al ser puestos sobre una recta quedan completamente separados unos de otros, es
decir, no se observan traslapes entre los puntajes de grupos diferentes.
    Con base en los g vectores de medias y los n vectores de observaciones se pueden
definir dos tipos de matrices de orden p √ó p as√≠:
                                                             0
                               Bkh = [xk ‚àí xh ] [xk ‚àí xh ]                             (5)


                                                                 0
                           Vkh (i, l) = [xik ‚àí xlh ] [xik ‚àí xlh ]                      (6)

   De esta manera Disco en (4) puede representar en forma matricial como una
funci√≥n del vector Œ∑:
                                       Œ∑ 0 B(Œ∑)Œ∑
                            Disco(Œ∑) = 0                                  (7)
                                       Œ∑ V (Œ∑)Œ∑
donde B(Œ∑) y V (Œ∑) son matrices sim√©tricas de orden p √ó p que dependen del
par√°metro Œ∑ de la siguiente manera:
                                         Xg X g
                                                 nk nh Bkh
                                B(Œ∑) =           ‚àö 0                                   (8)
                                         k=1 h=1
                                                   Œ∑ Bkh Œ∑

                                     g X
                                   g X
                                   X      nh
                                       nk X
                                                     Vkh (i, l)
                         V (Œ∑) =                   p                                   (9)
                                   k=1 h=1 i=1 l=1
                                                    Œ∑ 0 Vkh (i, l)Œ∑

   Para maximizar Disco en (4) con respecto a Œ∑, Choulakian & Almhana (2001)
propusieron el siguiente algoritmo:
  3 El puntaje z de una observaci√≥n x por medio de Œ∑ se define como z = Œ∑ 0 x.




                                        Revista Colombiana de Estad√≠stica 32 (2009) 247‚Äì265

Comparaci√≥n entre tres t√©cnicas de clasificaci√≥n                                      251

   1. Comenzar con Œ∑0 = Œ∏‚àó , siendo Œ∏‚àó el vector propio4 (LD1 ).
   2. Calcular Œ∑k+1 = Œ∑k [1 ‚àí 2Disco(Œ∑k )] + 2V (Œ∑k )‚àí1 B(Œ∑k )Œ∑k , para k = 0, 1, 2, . . .
   3. Detener el proceso cuando |Disco(Œ∑k+1 ) ‚àí Disco(Œ∑k )| ‚â§ , donde  es un
      valor real positivo definido con anterioridad; por ejemplo,  = 10‚àí5 .
   4. Obtener el valor √≥ptimo de la funci√≥n discriminante Œ∑ haciendo Œ∑ = Œ∑k .

    Luego de encontrar el valor √≥ptimo de Œ∑, este puede utilizarse como funci√≥n
discriminante para clasificar nuevas observaciones en uno de los g grupos. Para rea-
lizar la clasificaci√≥n se determinan g ‚àí1 puntos de corte (CP ) de la siguiente mane-
ra: se toman los n puntajes zij del conjunto de entrenamiento con i = 1, 2, . . . , nj
y con j = 1, 2, . . . , g; sin p√©rdida de generalidad, se puede suponer que los prime-
ros n1 puntajes son menores que los segundos n2 y as√≠ sucesivamente. El punto
de corte CP1 que separa los grupos 1 y 2 es igual al percentil 100(n1 /n) % de
los n puntajes ordenados, el segundo CP2 que separa los grupos 2 y 3 es igual al
percentil 100((n1 + n2 )/n) % de los n puntajes ordenados; de manera similar se
obtienen los G ‚àí 3 CP restantes. La regla para clasificar una nueva observaci√≥n x
en uno de los grupos es calcular el puntajes z de x por medio de Œ∑ y clasificar la
nueva observaci√≥n x al grupo 1 si z ‚â§ CP1 , al grupo k si CPk‚àí1 < z ‚â§ CPk o al
grupo g si z > CPg‚àí1 .

1.1.3. Regresi√≥n log√≠stica multinomial

    La t√©cnica MLR es un caso particular de modelos lineales generalizados donde
la variable respuesta y corresponde a una variable aleatoria independiente multino-
mial cuya media se modela por medio de un conjunto de covariables. Para el pro-
blema de clasificaci√≥n el valor de la variable respuesta corresponde a y = 1, . . . , g
y el conjunto de covariables se denota por x0 = (1, x1 , . . . , xp )0 que conforman el
conjunto de entrenamiento.
     Sea œÄj la probabilidad de que una observaci√≥n x pertenezca al grupo j =
1, . . . , g, uno de los grupos es considerado como grupo de referencia, sin p√©rdida
de generalidad, se puede tomar el grupo 1 como referencia; entonces el logit para
cada uno de los otros grupos se define como:
                                        
                                        œÄj
                      logit(œÄj ) = log      = x0 Œ≤j , j = 2, . . . , g          (10)
                                        œÄ1

   Una vez estimados los g ‚àí 1 vectores Œ≤j usando el conjunto de entrenamiento,
es posible estimar la probabilidad de que una nueva observaci√≥n x pertenezca a
uno de los grupos usando las expresiones 11 y 12 y asignar la observaci√≥n al grupo
que tenga mayor probabilidad.

                                                  1
                                b1 =
                                œÄ           g                                       (11)
                                            P
                                       1+         exp x0 Œ≤bj
                                            j=2

  4 Obtenido mediante An√°lisis Discriminante Lineal.




                                       Revista Colombiana de Estad√≠stica 32 (2009) 247‚Äì265

252                          Freddy Hern√°ndez Barajas & Juan Carlos Correa Morales

                                        
                             exp x0 Œ≤bj
                     bj =
                     œÄ        g                ,   j = 2, . . . , g               (12)
                              P
                          1+    exp x0 Œ≤bj
                               j=2



1.2. Estudios previos de simulaci√≥n
    En la literatura estad√≠stica se pueden encontrar estudios que comparan el
desempe√±o de las t√©cnicas an√°lisis discriminante lineal, regresi√≥n log√≠stica y an√°li-
sis discriminante no m√©trico. A continuaci√≥n se muestran los principales resultados
obtenidos en estos estudios.


1.2.1. Comparaciones entre LDA y regresi√≥n log√≠stica (LR)

    Efron (1975) compar√≥ las dos t√©cnicas para el caso de dos grupos con igual
matriz de covarianzas y encontr√≥ que la eficiencia relativa asint√≥tica (ARE) de LR
con respecto a LDA est√° entre un medio y dos tercios. Crawley (1979) compar√≥
LR con LDA para muestras peque√±as con dos grupos y encontr√≥ que para el caso
de matrices de covarianza iguales LDA tiene un mejor desempe√±o que LR en el
proceso de clasificaci√≥n, para el caso de matrices de covarianzas diferentes LR tuvo
ligeramente un mejor desempe√±o y para el caso de dos poblaciones distribuidas
no normal LR tuvo un desempe√±o muy superior a LDA. Harrell & Lee (1985)
realizaron una comparaci√≥n entre las t√©cnicas para el caso de dos grupos consi-
derando normalidad con matrices de covarianzas iguales, tama√±os de muestra de
50 y 130 con seis distancias de Mahalanobis entre los vectores de medias de las
dos poblaciones que variaron entre los valores de 0.94 y 4.68; en este estudio se
encontr√≥ que el desempe√±o de LDA fue mejor que LR pero que las diferencias no
eran significativas.
    Pohar et al. (2004) llevaron a cabo un estudio donde compararon LDA y LR por
medio de simulaci√≥n. Para comparar los desempe√±os de cada una de las t√©cnicas
utilizaron el √≠ndice t√≠pico de tasa de clasificaci√≥n err√≥nea y los √≠ndices A, B, C y Q
propuestos por Harrell & Lee (1985). La comparaci√≥n se inici√≥ en un escenario en el
cual se cumpl√≠an los supuestos de LDA y luego realizaron cambios en los tama√±os
de muestra, matriz de covarianzas y distancia de Mahalanobis entre las medias
de los grupos simulados. Se encontr√≥ que los desempe√±os de LDA y LR fueron
muy cercanos, siempre y cuando los supuestos de normalidad no sean afectados
fuertemente, y presentaron lineamientos para identificar este tipo de situaciones;
adicionalmente, discutieron las situaciones donde es inapropiado utilizar LDA para
clasificaci√≥n.


1.2.2. Comparaciones entre LDA y MLR

   Shelley & Donner (1987) llevaron a cabo un estudio de comparaci√≥n con el
objetivo de extender los resultados de Efron (1975) para medir la eficiencia relati-
va asint√≥tica (ARE) de regresi√≥n log√≠stica multinomial con respecto a LDA para

                                     Revista Colombiana de Estad√≠stica 32 (2009) 247‚Äì265

Comparaci√≥n entre tres t√©cnicas de clasificaci√≥n                                     253

el caso de poblaciones distribuidas normal multivariada con igual matriz de co-
varianzas. Los casos que estudiaron consideraron dos, tres y cuatro grupos. Los
autores encontraron que en el caso de vectores de pendientes log√≠stico colineales
ARE estaba en el intervalo de 50 % a 65 % para dos grupos y de 35 % a 95 % para
el caso de cuatro grupos cuando la distancia de Mahalanobis entre el grupo de
referencia y los dem√°s estuvo en 3.0 a 3.5. Para el caso de vectores ortogonales se
encontr√≥ que ARE decae r√°pidamente a medida que aparecen m√°s grupos en el
proceso de clasificaci√≥n.


1.2.3. Comparaciones entre LDA y NDA

    Raveh (1989) llev√≥ a cabo un estudio de simulaci√≥n donde compar√≥ LDA y NDA
para el caso de dos grupos; se consideraron tres escenarios o tipos de distribuciones
de probabilidad multivariada para cada grupo: normal multivariada, log-normal
y chi-cuadrada; en cada uno de estos escenarios se consideraron distribuciones
bivariadas y trivariadas. El tama√±o de muestra fue siempre de 50 observaciones
para cada uno de los dos grupos. El objetivo b√°sico del estudio fue comparar
el desempe√±o de las dos t√©cnicas usando la tasa de clasificaci√≥n err√≥nea para el
conjunto de entrenamiento y para un nuevo conjunto de validaci√≥n obtenidos de
la misma distribuci√≥n.
    Para el caso de dos grupos provenientes de una distribuci√≥n normal multiva-
riada (2 √≥ 3 variables) se encontr√≥ que cuando hay igualdad entre las matrices de
covarianzas LDA tiene tasas de clasificaci√≥n err√≥neas como m√°ximo 1 % mejores
que las de NDA. Se encontr√≥ tambi√©n que a medida que las matrices de covarianza
difieren entre s√≠, la ventaja de LDA disminuye hasta el punto que NDA obtiene
menor tasa de clasificaci√≥n err√≥nea para el caso extremo de matrices de covarianza.
Para el caso de grupos provenientes de una distribuci√≥n log-normal se encontr√≥ que
NDA es muy superior que LDA; el desempe√±o de NDA estuvo por encima de LDA
en 16 % para conjuntos de entrenamiento y 14 % para conjuntos de validaci√≥n.
Para este mismo caso se hall√≥ que, a medida que los par√°metros de la distribuci√≥n
log-normal para cada grupo difieren, el desempe√±o de NDA mejora sobre el de
LDA. Para el caso de grupos provenientes de una distribuci√≥n chi-cuadrado se en-
contr√≥ que NDA tuvo un desempe√±o similar a NDA; las diferencias entre las tasas
de clasificaci√≥n err√≥neas fueron 1 % a favor de NDA; se observ√≥ tambi√©n que la
ventaja de NDA sobre LDA se incrementaba ligeramente a medida que disminu√≠an
los grados de libertad de la distribuci√≥n.
    Choulakian & Almhana (2001) realizaron una comparaci√≥n entre LDA y NDA
usando tres conjuntos de datos: poultry data, encontrado en Raveh (1983), confor-
mado por diez grupos con cuatro variables; wolf skull data, encontrado en Morrison
(1990), conformado por cuatro grupos y nueve variables, y feelings data, encon-
trado en Hand (1989), conformado por cuatro grupos y veinticinco variables. En
cada una de estas tres aplicaciones se construy√≥ la funci√≥n discriminante NDA con
base en la funci√≥n discriminante de LDA y se encontr√≥ que NDA clasifica mejor
el conjunto de entrenamiento, tambi√©n se hall√≥ un aumento en el coeficiente de
discriminaci√≥n Disco para cada una de las aplicaciones: para la primera aplicaci√≥n

                                      Revista Colombiana de Estad√≠stica 32 (2009) 247‚Äì265

254                         Freddy Hern√°ndez Barajas & Juan Carlos Correa Morales

Disco pas√≥ de 0.9915 a 0.9935, para la segunda Disco cambi√≥ de 0.987 a 1 y para
la √∫ltima Disco pas√≥ de 0.9615 a 0.9832.


2. Objetivo del estudio
    La tarea de clasificar sujetos u objetos en grupos preestablecidos dado un con-
junto de caracter√≠sticas siempre ha sido un problema a resolver en diferentes √°m-
bitos; algunos ejemplos son: asignaci√≥n de cr√©ditos, determinaci√≥n del estado de
enfermedad de un paciente, env√≠o de publicidad para clientes de una empresa,
entre otros. Las caracter√≠sticas o variables observadas en los sujetos a clasificar
pueden ser cuantitativas y/o cualitativas, y en caso de que existan variables cuali-
tativas las condiciones de normalidad multivariada no est√°n aseguradas; por tanto,
es importante estudiar y comparar t√©cnicas que puedan incorporar la informaci√≥n
de variables cualitativas, lo cual es el caso de An√°lisis Discriminante no M√©trico
y Regresi√≥n Log√≠stica. Por otra parte, como se observa en la secci√≥n anterior, no
existen en la literatura estad√≠stica estudios de comparaci√≥n entre LDA, NDA y
MLR para el caso de m√°s de dos grupos.
    El objetivo principal de este art√≠culo es estudiar mediante simulaci√≥n el desem-
pe√±o de las tres t√©cnicas LDA, NDA y MLR en el proceso de clasificaci√≥n para el
caso de m√°s de dos grupos bajo diferentes distribuciones de probabilidad multiva-
riada.
    Se decidi√≥ incluir LDA en el estudio para extender los trabajos de Raveh (1989)
a m√°s de dos grupos y los ejemplos de Choulakian & Almhana (2001) ya que la
funci√≥n discriminante NDA se basa en el primer discriminante de LDA.


2.1. Metodolog√≠a
    La comparaci√≥n de las t√©cnicas NDA y MLR se llev√≥ a cabo por medio de simu-
laciones en las cuales se consideraron tres tipos de distribuciones de probabilidad
multivariada para los grupos, diferentes par√°metros y varios tama√±os de muestra.

2.1.1. Escenarios

   Los escenarios considerados en el estudio fueron cuatro:

Escenario 1. Tres grupos normales bivariados (denotados por 1, 2 y 3), matrices
    de covarianzas iguales donde œÉ1 = œÉ2 = 1 con valores de correlaci√≥n œÅ de 0.1,
    0.3, 0.5, 0.7 y 0.9. Se consideraron cinco situaciones de alejamiento gradual
    para los grupos; el grupo 1 siempre fue el de referencia y estuvo ubicado en
    el origen del plano cartesiano mientras que las ubicaciones de los otros dos
    grupos cambiaron sobre los ejes. A representa la situaci√≥n m√°s cercana de
    vectores de medias y E la m√°s lejana.
        ‚Ä¢ Situaci√≥n A: ¬µ1 = (0, 0), ¬µ2 = (1, 0), ¬µ3 = (0, 1).
        ‚Ä¢ Situaci√≥n B: ¬µ1 = (0, 0), ¬µ2 = (1, 0), ¬µ3 = (0, 2).

                                    Revista Colombiana de Estad√≠stica 32 (2009) 247‚Äì265

Comparaci√≥n entre tres t√©cnicas de clasificaci√≥n                                         255

         ‚Ä¢ Situaci√≥n C: ¬µ1 = (0, 0), ¬µ2 = (1, 0), ¬µ3 = (0, 3).
         ‚Ä¢ Situaci√≥n D: ¬µ1 = (0, 0), ¬µ2 = (2, 0), ¬µ3 = (0, 2).
         ‚Ä¢ Situaci√≥n E: ¬µ1 = (0, 0), ¬µ2 = (2, 0), ¬µ3 = (0, 3).
      Los tama√±os muestrales utilizados fueron de 20, 50, 100 y una combinaci√≥n
      de estos √∫ltimos.
Escenario 2. Este escenario es similar al anterior con respecto a las situaciones
    de media para cada grupo y tama√±os muestrales; la diferencia est√° en que se
    consideraron matrices de covarianzas diferentes para cada grupo.
      La matriz de covarianzas del grupo 1 se caracteriz√≥ por œÉ1 = œÉ2 = 1 con
      diferentes valores de correlaci√≥n œÅ de 0.1, 0.3, 0.5, 0.7 y 0.9. La matriz de
      covarianzas de los grupos 1, 2 y 3 se denotan por Œ£1 , Œ£2 y Œ£3 . Los casos
      considerados fueron:
         ‚Ä¢ Œ£2 = 2Œ£1 y Œ£3 = 2Œ£1 .
         ‚Ä¢ Œ£2 = 2Œ£1 y Œ£3 = 3Œ£1 .
         ‚Ä¢ Œ£2 = 3Œ£1 y Œ£3 = 3Œ£1 .
         ‚Ä¢ Œ£2 = 2Œ£1 y Œ£3 = 4Œ£1 .
Escenario 3. Siete grupos distribuidos normal trivariada. Se consideraron tres
    situaciones de alejamiento gradual para los grupos; el grupo 1 siempre fue
    el de referencia y estuvo ubicado en el origen mientras que las ubicaciones
    de los otros grupos cambiaron de manera gradual. A representa la situaci√≥n
    m√°s cercana y C la m√°s lejana.
         ‚Ä¢ Situaci√≥n A: ¬µ1 = (0, 0, 0), ¬µ2 = (0, 0, 2), ¬µ3 = (0, 0, ‚àí2), ¬µ4 = (0, 2, 0), ¬µ5 =
           (0, ‚àí2, 0), ¬µ6 = (2, 0, 0), ¬µ7 = (‚àí2, 0, 0).
         ‚Ä¢ Situaci√≥n B: ¬µ1 = (0, 0, 0), ¬µ2 = (0, 0, 2), ¬µ3 = (0, 0, ‚àí2), ¬µ4 = (0, 3, 0), ¬µ5 =
           (0, ‚àí3, 0), ¬µ6 = (3, 0, 0), ¬µ7 = (‚àí3, 0, 0).
         ‚Ä¢ Situaci√≥n C: ¬µ1 = (0, 0, 0), ¬µ2 = (0, 0, 2), ¬µ3 = (0, 0, ‚àí2), ¬µ4 = (0, 4, 0), ¬µ5 =
           (0, ‚àí4, 0), ¬µ6 = (4, 0, 0), ¬µ7 = (‚àí4, 0, 0).

      El tama√±o de las muestras tomadas de cada poblaci√≥n fue de 10, 20 y 50.
      Las matrices de covarianzas de los grupos 1, 2, . . . , 7 se denotan por Œ£1 ,
      Œ£2 ,. . . , Œ£7 . Se consideraron matrices de covarianza diferentes y la matriz de
      covarianzas del grupo 1 (de referencia) se caracteriz√≥ porque las varianzas
      de las tres variables fueron œÉ1 = œÉ2 = œÉ3 = 1. Se consideraron valores de
      correlaci√≥n entre pares de variables œÅ de 0.1, 0.3, 0.5, 0.7 y 0.9. Las estructuras
      generales de las matrices de covarianzas fueron:
         ‚Ä¢ Estructura 1: Œ£1 = Œ£2 = Œ£3 , Œ£4 = Œ£5 = 2Œ£1 , Œ£6 = Œ£7 = 4Œ£1 .
         ‚Ä¢ Estructura 2: Œ£1 = Œ£2 = Œ£3 , Œ£4 = Œ£5 = 4Œ£1 , Œ£6 = Œ£7 = 8Œ£1 .
Escenario 4. Tres grupos con funciones de densidad especiales bivariadas. En
    este escenario se llev√≥ a cabo la comparaci√≥n de las t√©cnicas usando tres
    funciones de densidad especiales: Lognormal, Sinh‚àí1 -normal y Logit-normal.
    Para crear las muestras se generaron muestras de observaciones normales

                                       Revista Colombiana de Estad√≠stica 32 (2009) 247‚Äì265

256                          Freddy Hern√°ndez Barajas & Juan Carlos Correa Morales

      bivariadas con los par√°metros del escenario 1 y luego se aplic√≥ el sistema
      de transformaci√≥n sugerido por Johnson (1987) para obtener observaciones
      provenientes de cada distribuci√≥n.

2.1.2. Evaluaci√≥n de las funciones de clasificaci√≥n

    La evaluaci√≥n de los desempe√±os de las t√©cnicas de clasificaci√≥n se llev√≥ a cabo
utilizando la Tasa de Clasificaci√≥n Err√≥nea (TCE) que se define de la siguiente
manera:
                                          N CE
                                 T CE =
                                         N OBS
donde N CE corresponde al n√∫mero de clasificaciones erradas por la t√©cnica en
el conjunto de validaci√≥n y N OBS corresponde al n√∫mero de observaciones en el
conjunto de validaci√≥n.

2.1.3. Procedimiento de comparaci√≥n

   Las tres t√©cnicas de clasificaci√≥n se compararon llevando a cabo el siguiente
conjunto de pasos:

   1. Se simul√≥ una muestra por cada una de las g poblaciones con las cuales se
      forma el conjunto de entrenamiento.

   2. Con el conjunto de entrenamiento se construyeron las funciones de discrimi-
      naci√≥n para LDA, NDA y MLR.

   3. Se generaron nuevas muestras como en el paso 1 y con ellas se form√≥ un nuevo
      conjunto llamado de validaci√≥n. Todas las observaciones de este conjunto se
      clasificaron mediante las funciones obtenidas en el paso 2.

   4. Se calcul√≥ la tasa de clasificaci√≥n err√≥nea para el conjunto de validaci√≥n del
      paso anterior para cada t√©cnica.

   5. Los pasos 3 y 4 se repitieron mil veces y se calcul√≥ la tasa promedio de
      clasificaci√≥n err√≥nea para cada uno de los procedimientos de clasificaci√≥n.

Las simulaciones se programaron y generon en R (R Development Core Team
2008), se us√≥ la funci√≥n lda() para an√°lisis discriminante lineal y multinom() de
la librer√≠a nnet para regresi√≥n log√≠stica multinomial; la funci√≥n para an√°lisis discri-
minante no m√©trico se program√≥ usando el procedimiento sugerido por Choulakian
& Almhana (2001) y estar√° disponible a petici√≥n del lector.


2.2. Resultados
    Escenario 1: tres grupos normales bivariados con matrices de covarianza igua-
les. En este escenario se encontr√≥ que las TCE para LDA y MLR difieren en m√°-
ximo 1 %, lo cual se puede observar en las figuras 1 y 2 por medio de las l√≠neas a
trazos que se superponen. La l√≠nea continua corresponde a la TCE para NDA y
siempre qued√≥ ubicada en las figuras 1 y 2 por encima de las l√≠neas asociadas a
las otras dos t√©cnicas; sin embargo, para valores de œÅ ‚â• 0.7 las diferencias en TCE
para las tres t√©cnicas fueron de m√°ximo 2 %. Otro patr√≥n esperado que se encontr√≥
en este escenario fue que la TCE disminuy√≥ a medida que se presentaron las tres
situaciones siguientes: aumento de la distancia entre los vectores de medias de los
grupos (situaci√≥n A a E), aumento del coeficiente de correlaci√≥n y aumento en el
tama√±o de muestra.

    Escenario 2: tres grupos normales bivariados con matrices de covarianza di-
ferentes. En las figuras 3 y 4 se observan dos casos particulares del escenario y se
pueden apreciar nuevamente resultados similares y los mismos patrones menciona-
dos en el escenario anterior. Al considerar diferentes matrices de covarianzas para
los grupos, los desempe√±os de las t√©cnicas se vieron afectados, el aumento promedio
en la tasa de clasificaci√≥n err√≥nea fue de 6 %. Se observ√≥ tambi√©n que cuando las
matrices de covarianzas eran diferentes con tama√±os muestrales de 50 las tasas de
clasificaci√≥n err√≥neas eran pr√°cticamente las mismas que cuando se tiene matrices
de covarianzas iguales, es decir, los desempe√±os de las t√©cnicas fueron similares
cuando los tama√±os muestrales fueron de 50 sin importar si se cumpl√≠a el supuesto
de igualdad de matriz de covarianzas. En promedio LDA y MLR tuvieron tasas
de clasificaci√≥n err√≥nea 3.5 % menos que NDA.
    Escenario 3: siete grupos normal trivariado con matrices de covarianzas di-
ferentes. En el presente escenario se consideraron dos estructuras de matrices de
covarianzas y al pasar de la primera estructura a la segunda se afect√≥ la TCE
aument√°ndola. Se observ√≥ nuevamente que al aumentar el valor de œÅ disminuye-
ron las tasas de clasificaci√≥n err√≥nea; adicionalmente, a medida que se alejan las
poblaciones, la TCE disminuy√≥ en promedio 12 %. En este escenario se comprob√≥
otra vez que al aumentar los tama√±os muestrales se favorece el desempe√±o de las
t√©cnicas; en particular se encontr√≥ que TCE disminuy√≥ en promedio 3 %. En las
figuras 5, 6 y 7 se observa que el desempe√±o de LDA y MLR es similar y que
clasifican mejor que NDA en el presente escenario.
    Escenario 4: tres grupos con funciones de densidad especiales bivariadas. En la
figura 8 se pueden observar las TCE para las t√©cnicas cuando se us√≥ la distribuci√≥n
lognormal con tama√±os de muestra de 20. El patr√≥n observado en las l√≠neas de esta

figura es igual al encontrado cuando se usaron tama√±os de muestra de 50 y 100; por
tanto, para este caso el aumento en el tama√±o de muestra no mejor√≥ el desempe√±o
de las t√©cnicas. Se encontr√≥ que LDA es la t√©cnica con el desempe√±o m√°s bajo y por
lo menos el 30 % de las veces clasific√≥ incorrectamente. Los mejores desempe√±os
los obtuvo MLR, y a medida que aumentaba la separaci√≥n entre los grupos, la
diferencia con LDA y NDA fue mayor. A partir de la situaci√≥n de alejamiento C la
t√©cnica LDA mantuvo su tasa de clasificaci√≥n err√≥nea independiente del coeficiente
de correlaci√≥n œÅ mientras que para NDA y LDA se observ√≥ que el aumento en œÅ
mejor√≥ el desempe√±o.
    En la figura 9 se pueden observar las TCE para las t√©cnicas en el caso de
distribuci√≥n Sinh‚àí1 -normal. Nuevamente en este caso la t√©cnica LDA present√≥ el

              Figura 8: Distribuci√≥n lognormal con n1 = n2 = n3 = 20.

desempe√±o m√°s bajo de las tres t√©cnicas seguido por NDA mientras que la mejor
t√©cnica fue MLR. Los patrones mencionados anteriormente para cada una de las
t√©cnicas en el caso de la distribuci√≥n lognormal se repiten nuevamente aqu√≠; el
desempe√±o de LDA no mejora cuando se aumenta œÅ a partir de la situaci√≥n D,
mientras que para NDA y MLR el impacto es favorable al aumentar œÅ. Se observa
tambi√©n que al aumentar la distancia entre los vectores de medias de los grupos
las t√©cnicas MLR y NDA mejoran el desempe√±o.

    En la figura 10 se encuentran los resultados para el caso de la distribuci√≥n
logit-normal para tama√±o muestral de 20. El patr√≥n de las l√≠neas de desempe√±o
TCE para las figuras asociadas a tama√±os muestrales de 50 y 100 es el mismo que
el de figura 10; se encontr√≥ que al aumentar el tama√±o de muestra de 20 a 50,
en promedio las TCE disminuyeron 3 %, mientras que al aumentar el tama√±o de
muestra de 50 a 100, la disminuci√≥n en TCE fue insignificante. Se observ√≥ tambi√©n
que la t√©cnica NDA fue la de peor desempe√±o mientras que LDA y MLR tuvieron
desempe√±os similares, la m√°xima diferencia de TCE entre ellas fue de 2.5 %; este
√∫ltimo patr√≥n fue com√∫n denominador para los escenarios 1, 2, 3 y distribuci√≥n
logit-normal.

           Figura 10: Distribuci√≥n Logit-normal con n1 = n2 = n3 = 20.
2.3. Discusi√≥n
    Las t√©cnicas de clasificaci√≥n LDA y RLM tuvieron mejor desempe√±o al mo-
mento de clasificar en tres grupos bivariados y siete grupos trivariados; adem√°s,
su desempe√±o fue tan similar que en la mayor√≠a de las figuras las l√≠neas de TCE
asociadas se superponen, lo cual muestra que las diferencias son m√≠nimas. En el
estudio la t√©cnica NDA nunca obtuvo TCE menores que MLR; los √∫nicos casos en
que NDA fue mejor que LDA fue cuando se consideraron poblaciones distribuidas
lgnormal y Sinh‚àí1 -normal.


3. Conclusiones
   Hasta el momento en la literatura estad√≠stica se han reportado solamente com-
paraciones v√≠a simulaci√≥n entre pares de las t√©cnicas an√°lisis discriminante lineal,

                                                                                       Revista Colombiana de Estad√≠stica 32 (2009) 247‚Äì265

Comparaci√≥n entre tres t√©cnicas de clasificaci√≥n                                     263

regresi√≥n log√≠stica multinomial y an√°lisis discriminante no m√©trico. En el presente
estudio de simulaci√≥n se compararon las tres t√©cnicas y se encontr√≥ que an√°lisis
discriminante lineal y regresi√≥n log√≠stica multinomial tuvieron tasas de clasifica-
ci√≥n err√≥nea muy similares y m√°s bajas que an√°lisis discriminante no m√©trico en la
mayor√≠a de los escenarios estudiados; esta √∫ltima t√©cnica present√≥ un mejor desem-
pe√±o solo cuando la distribuci√≥n poblacional fue lognormal y Sinh‚àí1 -normal.
    En situaciones pr√°cticas donde se presente un problema de clasificaci√≥n de
nuevas observaciones a grupos ya definidos, teniendo en cuenta varias variables
explicativas, se recomienda utilizar principalmente la t√©cnica RLM seguida de la
LDA, siempre y cuando la distribuci√≥n de probabilidad de los datos sea cercana
a una situaci√≥n de normalidad multivariada; el supuesto de homogeneidad de va-
rianzas puede violarse ligeramente y los resultados obtenidos con cualquiera de las
t√©cnicas ser√°n similares; adicionalmente se sugiere utilizar este criterio siempre y
cuando todas las variables explicativas sean de tipo cuantitativo.
    Posibles trabajos futuros podr√≠an encaminarse a comparar el desempe√±o de
las t√©cnicas considerando otro tipo de escenarios en los cuales se pueden estudiar
aspectos como: mayor n√∫mero de grupos a clasificar, tama√±os muestrales mayores,
estructuras de matrices de covarianzas diferentes, otros tipos de distribuciones para
los grupos, medidas de desempe√±o diferentes a la tasa de clasificaci√≥n err√≥nea y
algoritmos de b√∫squeda para determinar el vector de clasificaci√≥n en la t√©cnica
NDA.
Referencias
Anderson, J. (1972), ‚ÄòSeparate Sample Logistic Discrimination‚Äô, Biometrica 23, 19‚Äì35.
Carroll, R. & Pederson, S. (1993), ‚ÄòOn Robustness in the Logistic Regression Model‚Äô, Journal of the Royal Statistical Society 55, 693‚Äì706.
Cheng, T., Pia, M. & Feser, V. (2002), ‚ÄòHigh-Breakdown Estimation of Multivariate Mean and Covariance with Missing Observations‚Äô, British Journal of Mathematical and Statistical Psychology 55, 317‚Äì335.
Choulakian, V. & Almhana, J. (2001), ‚ÄòAn Algorithm for Nonmetric Discriminant Analysis‚Äô, Computational Statistics & Data Analysis 35, 253‚Äì264.
Clunies, C. & Riffenburgh, R. (1960), ‚ÄòGeometry and Linear Discrimination‚Äô, Biometrics 47, 185‚Äì189.
Cornfield, J. (1962), ‚ÄòJoint Dependence of the Risk of Coronary Heart Disease on Serum Cholesterol and Systolic Blood Pressure: A Discriminant Function Analysis‚Äô, Proceedings of the Federal American Society of Experimental Biology 21, 58‚Äì61.
Cox, D. (1966), Some Procedures Associated with the Logistic Qualitative Response Curve, John Wiley & Sons, New York, United States.
Crawley, D. (1979), ‚ÄòLogistic Discrimination as an Alternative to Fisher‚Äôs Linear Function‚Äô, New Zealand Statistician 14, 21‚Äì25.
Croux, C. & Dehon, C. (2001), ‚ÄòRobust Linear Discriminant Analysis Using S Estimators‚Äô, Canadian Journal of Statistics/Revue Canadienne de Statistique 29, 473‚Äì493.
Day, N. & Kerridge, D. (1967), ‚ÄòA General Maximum Likelihood Discriminant‚Äô, Biometrics 23, 313‚Äì323.
Efron, B. (1975), ‚ÄòThe Efficiency of Logistic Regression Compared to Normal Discriminant Analysis‚Äô, Journal of American Statistical Association 70, 892‚Äì898.
Fisher, R. A. (1936), ‚ÄòThe Use of Multiple Measurements in Taxonomic Problems‚Äô,Annual Eugenics 7, 179‚Äì188.
Guttman, L. (1998), ‚ÄòEta, disco, odisco and F.‚Äô, Psychometrika 53, 393‚Äì405.
Hand, D. (1989), Discriminant Analysis for Psychiatric Screening, 2 edn, John Wiley & Sons, New York, United States.
Harrell, F. E. & Lee, K. L. (1985), A comparison of the discrimination of discriminant analysis and logistic regression under multivariate normality, in P. K.Sen, ed., ‚ÄòBiostatistics: Statistics in Biomedical, Public Health and Environmental Sciences‚Äô, North-Holland, New York, United States, pp. 333‚Äì343.
Hawkins, D. & McLachan, J. (1997), ‚ÄòHigh-Breakdown Linear Discriminant Analysis‚Äô, Journal of American Statistical Asociation 92, 136‚Äì146.
Johnson, M. (1987), Multivariate Statistical Simulation, John Wiley & Sons, New York, United States.
Little, R. & Smith, P. (1987), ‚ÄòEditing and Imputing for Quantitative Survey Data‚Äô,Journal of the American Statistical Association 82, 58‚Äì68.
Morrison, D. (1990), Multivariate Statistical Methods, 3 edn, McGraw-Hill, New York, United States.
Pohar, M., Blas, M. & Turk, S. (2004), ‚ÄòComparison of Logistic Regression and Linear Discriminant Analysis: A Simulation Study‚Äô, Metodolski Zvezki 1, 143‚Äì161.
Pregibon, D. (1981), ‚ÄòLogistic Regression Diagnostics‚Äô, The Annals of Statistics 9, 705‚Äì724.
R Development Core Team (2008), R: A Language and Environment for Statistical Computing, R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0.*http://www.R-project.org
Rao, C. (1948), ‚ÄòThe Utilization of Multiple Measurements in Problems of Biological Classification‚Äô, Journal of the Royal Statistical Society: Series B 10, 159‚Äì193.
Raveh, A. (1983), ‚ÄòPreference Structure Analysis: A Nonmetric Approach‚Äô, Patter Recognition 16, 253‚Äì259.
Raveh, A. (1989), ‚ÄòA Nonmetric Approach to Linear Discriminant Analysis‚Äô, Journal of the American Statistical Association 84, 176‚Äì183.
Rencher, A. (1998), Multivariate Statistical Inference and Applications, John Wiley & Sons, New York, United States.
Shelley, B. & Donner, A. (1987), ‚ÄòThe Efficiency of Multinomial Logistic Regression Compared with Multiple Group Discriminant Analysis‚Äô, Journal of American Statistical Association 82, 1118‚Äì1122.
Trevor, F. & Ferry, G. (1991), ‚ÄòRobust Logistic Discrimination‚Äô, Biometrika 78, 841‚Äì849.
Welch, B. (1939), ‚ÄòNote on Discriminant Functions‚Äô, Biometrika 31, 218‚Äì220.