Comparación de metodologı́as para el análisis de datos de degradación para trayectorias lineales
Universidad Nacional de Colombia
Resumen
Se usarán conceptos de análisis de degradación en relación con la confiabilidad de un producto. Muchos mecanismos de falla pueden detectarse a través de procesos de degradación. La degradación es una debilidad que eventualmente puede causar la falla. Existen varios métodos de análisis para datos de degradación en confiabilidad. En este artı́culo se comparan los resultados de la metodologı́a de análisis de degradación explı́cita y de la metodologı́a de análisis de degradación aproximada. Particularmente se diseña un estudio de simulación para el caso en que las trayectorias de degradación son de tipo lineal, para estudiar qué tan diferentes pueden ser las estimaciones de la función de distribución acumulativa del tiempo de vida dadas por cada una de las metodologı́as; se encontró que las estimaciones son competitivas para este caso. Se ilustra con datos sobre vida útil de algunos dispositivos láser tomados de Meeker & Escobar (1998).
Palabras clave: teorı́a de confiabilidad, modelo de efectos mixtos, simulación.
Introducción
Tomando como base la teorı́a de confiabilidad, se define “falla”1 como el even-
to en el cual una unidad (e. g., una parte, pieza o componente individual de una
máquina), dispositivo (e. g., un artefacto que puede desempeñar una función par-
ticular, como un circuito integrado, un láser, etc.) o un sistema compuesto de
diferentes dispositivos y unidades deja de funcionar correctamente o deja de cum-
plir apropiadamente con su objetivo.
     En algunos experimentos es difı́cil observar fallas, pues las unidades no dejan de
funcionar en ningún momento del experimento. En este último caso no se presentan
fallas según la definición inicial, pero se puede obtener información de cada unidad
o dispositivo midiendo la evolución de algunas propiedades fı́sicas que puedan
indicar su estado general de deterioro.
     Bajo este último esquema se puede entonces definir otra forma de falla, la que
se conoce como “falla suave” en la que la unidad o dispositivo no deja de funcio-
nar como tal, pero algunas de las propiedades fı́sicas que se están monitoreando
sobre el tiempo, alcanzan niveles en los cuales, por diversos motivos, puede no
ser conveniente continuar la operación con esta unidad. La cuantificación de este
deterioro sobre el tiempo o alguna medida de uso es lo que se conoce en el lengua-
je de confiabilidad como “datos de degradación” (e. g., el desgaste que sufren los
neumáticos).
     En este artı́culo se comparan los resultados de la metodologı́a de análisis de
degradación explı́cita y de la metodologı́a de análisis de degradación aproximada.
Particularmente se diseña un estudio de simulación para el caso en que las trayec-
torias de degradación son de tipo lineal, para estudiar qué tan diferentes pueden
ser las estimaciones de la función de distribución acumulativa del tiempo de vida
F (t) dadas por cada una de las metodologı́as; se encontró que las estimaciones son
competitivas para este caso.
     Para muchos productos no se puede medir fı́sicamente la degradación, pero
sı́ se pueden tener medidas de la evolución del desempeño que es una medida
indirecta de degradación. Por ejemplo, la degradación de una lámpara fluorescente
puede medirse por el porcentaje de pérdida de capacidad lumı́nica con respecto a
la capacidad lumı́nica inicial.
     Sobre la vida de algunos dispositivos láser, la degradación causa disminución
de la luz de salida. Algunos láseres, sin embargo, tienen un mecanismo de re-
troalimentación que puede mantener aproximadamente constante la luz de salida
al incrementar la corriente de operación mientras el láser se degrada (Meeker &
Escobar 1998). Cuando la corriente alcanza un valor demasiado alto, se considera
que el dispositivo ha fallado, aunque no haya dejado de funcionar. Los recursos
necesarios para que este funcione han aumentado bastante y puede resultar menos
costoso reemplazar el dispositivo. La figura 1 muestra el incremento de la corrien-
te de operación (en porcentaje con respecto a la inicial) en el tiempo, para una
muestra de 15 láseres de GaAs (Galio-Arsénico); estos datos se recolectaron en 17

  1 Algunos autores las llaman“fallas duras” (Meeker & Escobar 1998, pp. 328).




                                       Revista Colombiana de Estadı́stica 29 (2006) 133–151

Degradación para trayectorias lineales                                                            135


instantes de tiempo, que van desde cero hasta 4000 horas (datos tomados de Mee-
ker & Escobar (1998, pp. 642)). La lı́nea horizontal es el umbral máximo permisible
que corresponde a un incremento del 10 % en la corriente inicial de operación. A
partir de este punto se considera que tiene una “falla suave”.

     Figura 1: Gráfico de la corriente de operación del láser en función del tiempo.

2.                         Análisis aproximado de degradación
    Un tipo de análisis que se puede utilizar para tratar los datos de degradación
se conoce con el nombre de “análisis de degradación aproximado”, el cual tiene
en cuenta la forma del comportamiento de la degradación en el tiempo, ajustando
un modelo con el fin de extrapolar para cada unidad más allá del tiempo final
del experimento y ası́ obtener lo que se conoce como seudotiempos de falla (esta
metodologı́a no considera censura).
    Los seudotiempos de falla son tiempos estimados de falla para las unidades
que no fallaron antes del final del experimento y estos tiempos se asumen como
verdaderos para realizar un análisis de tiempos de vida según las metodologı́as de la
teorı́a de confiabilidad. En resumen, se obtienen los tiempos de falla para cada una
de las unidades, ya sea mirando el tiempo en que la unidad supera el nivel máximo
de degradación permisible o extrapolando la trayectoria de degradación de una
unidad, hasta que esta supere el nivel de degradación definido como falla. Ya con
los seudotiempos de falla para cada unidad, simplemente se busca la distribución
de probabilidad que mejor se ajuste a los datos y esta se asume como la distribución
del tiempo de vida para ese tipo de unidades (Meeker & Escobar 1998).


                                                   Revista Colombiana de Estadı́stica 29 (2006) 133–151

136                                            Sergio Yáñez & Ronald Andrés Granada


3.     Análisis de degradación explı́cito

    La otra metodologı́a que se considerará en este artı́culo para el análisis de da-
tos de degradación es el “análisis de degradación explı́cito” que fue propuesto en
Lu & Meeker (1993) y más extensamente explicado en Meeker & Escobar (1998,
cap. 13). Esta metodologı́a va de la mano con la teorı́a que describe la evolución
del proceso de degradación de interés, utilizando modelos fı́sicos. Los modelos que
dan cuenta de estos fenómenos son, en general, de efectos mixtos, cuya estimación
se hace por máxima verosimilitud aproximada utilizando los algoritmos de estima-
ción planteados por Pinheiro & Bates (2000).

    Teniendo como base las estimaciones de los parámetros en los modelos de efec-
tos mixtos, se estiman la función de distribución acumulativa F (t) para el tiempo
de falla de las componentes y los intervalos de confianza para algunos cuantiles de
interés, todo esto usando métodos basados en simulación Monte Carlo y la metodo-
logı́a bootstrap para la construcción de intervalos de confianza (Yáñez et al. 2003).

    Según los modelos de tipo determinista, todas las unidades iguales que hayan
sido manufacturadas idénticamente, bajo unas mismas condiciones ambientales y
de producción, falları́an exactamente en el mismo tiempo cuando alcancen un um-
bral máximo de degradación particular. Lo que ocurre en realidad es que siempre
existe algún grado de variabilidad en todos los factores del modelo y también en
los factores que no están incluidos en el modelo. La mezcla de todos estos factores
causa variabilidad en las curvas de degradación de unidad a unidad y por supuesto
en los tiempos de falla de cada una de estas; por ello se propone el uso de modelos
de efectos mixtos (Yáñez et al. 2003).

    La literatura especializada dispone de un gran número de modelos que dan
cuenta de curvas de degradación, desarrollados por ingenieros y fı́sicos a partir
de los principios básicos que describen los procesos de degradación. Usualmen-
te dichos modelos empiezan con una descripción determinista de los procesos de
degradación, en la forma de ecuaciones diferenciales o sistemas de ecuaciones dife-
renciales (Meeker & Escobar 1998, Chao 1999). La aleatoriedad puede introducirse
apropiadamente usando distribuciones de probabilidad para describir la variabili-
dad de los parámetros del modelo.

    La variabilidad en las condiciones iniciales de degradación de una unidad y la
posterior forma de su trayectoria de degradación son causadas, entre otros factores,
por la no homogeneidad de los materiales con que se producen las unidades, las
condiciones de operación de la unidad, las condiciones ambientales, defectos de
manufactura o intervenciones externas no controladas o no clasificadas, etc. Todo
esto justifica el uso de los modelos de efectos mixtos, pues si se dispone de un
“buen modelo” fı́sico-quı́mico, se puede comprender mejor los mecanismos que
generan las fallas, si todos o algunos de los parámetros asociados a este modelo se
consideran como variables aleatorias de unidad a unidad (Granada 2004).


                                      Revista Colombiana de Estadı́stica 29 (2006) 133–151

Degradación para trayectorias lineales                                                      137


4.     Simulación para trayectorias de degradación
       lineal
    Se compararán las estimaciones obtenidas por las metodologı́as de análisis de
degradación explı́cita y análisis de degradación aproximado de la función de dis-
tribución acumulativa F (t) para el tiempo de vida de un dispositivo cualquiera,
con trayectorias de degradación de tipo lineal.
    Se conjetura que la metodologı́a aproximada para análisis de datos de de-
gradación presenta resultados competitivos con la metodologı́a explı́cita para la
estimación de la función de distribución acumulativa.
    Se definen las trayectorias de degradación de tipo lineal con el siguiente modelo:

         yij = Dij + εij , i = 1, . . . , n, j = 1, . . . , m,   con Dij = β1i + β2i tij     (1)

donde:

       yij : degradación muestral acumulada del dispositivo i al tiempo tj .

       tj : medida de tiempo o de uso asociada con cada medida de degradación
       muestral.

       Dij : trayectoria de degradación lineal.

       n: número de dispositivos probados.

       m: número de tiempos en los que se observa la degradación muestral de cada
       dispositivo.

       β1i : intercepto de la trayectoria lineal para el dispositivo i. Se considera
       variable de dispositivo a dispositivo (unidad a unidad).

       β2i : pendiente de la trayectoria lineal para el dispositivo i. Se considera
       variable de dispositivo a dispositivo (unidad a unidad).

       εij: perturbaciones aleatorias que se presentan en las observaciones de de-
       gradación muestral con respecto a la trayectoria de degradación lineal.

    Los parámetros de la trayectoria de degradación lineal β1 y β2 (el intercepto y
la pendiente, respectivamente) se generarán asumiendo que son aleatorios con una
distribución de probabilidad normal bivariada, ası́:
                                 "     #      "                                   #!
                                                 σβ21
                      
                      
                   β1              µβ1                                 ρσβ1 σβ2
                β=      ∼ N µβ =         , Σβ
                   β2              µβ2         ρσβ1 σβ2                  σβ22

     donde:

       µβ1 y µβ2 son las medias del intercepto y la pendiente, respectivamente, en
       el modelo lineal,


                                             Revista Colombiana de Estadı́stica 29 (2006) 133–151

138                                            Sergio Yáñez & Ronald Andrés Granada


       ρ es el coeficiente de correlación entre los dos betas y
       σβ1 , σβ2 son las desviaciones estándar asociadas a cada uno de los parámetros
       aleatorios mencionados.

    Para generar los parámetros que sean necesarios en la simulación se utilizarán
las rutinas de generación de normales multivariadas “mvrnorm” desarrolladas por
Venables & Ripley (2002) en su librerı́a MASS, disponible para los programas
S-PLUS y R. Se asume también que las perturbaciones aleatorias εij tendrán una
distribución normal con media cero y varianza constante σε2 , donde las εij son
independientes de β1 y β2 . Estas perturbaciones pueden generarse fácilmente en
los paquetes mencionados con la función “rnorm”.
    Se denotará por Df el umbral o lı́mite máximo de degradación acumulada,
donde se considera que la unidad o dispositivo falla.


4.1.     Metodologı́a de simulación
    Se controlarán los factores que tienen que ver con la estructura de covarianza
de los parámetros (Σβ ), para estudiar su efecto en la estimación final de F (t).
Estos parámetros son los que afectan la variabilidad de las trayectorias muestra-
les de unidad a unidad, caracterı́stica que se quiere estudiar. Se considerarán 45
escenarios de simulación con las siguientes caracterı́sticas:

       Para el coeficiente de correlación ρ entre β1 y β2 del modelo, se consideran
       cinco niveles con los siguientes valores [−0.75, −0.35, 0, 0.35, 0.75].
       Se tomarán n = 15 y m = 20 valores usuales de los experimentos referencia-
       dos en la literatura especializada de confiabilidad para datos de degradación.
       Los valores de Df y σε se definen con base en las escalas de µβ , de tal mane-
       ra que los datos simulados representen un experimento usual de degradación
       (Granada 2004, capı́tulo 2) y acorde a estos valores se definen también los
       tiempos de observación de la degradación. Para esta simulación se tomarán
       µβ1 = 1 y µβ2 = 0.5, Df = 11 y σε = 0.2 y los tiempos de observación de la
       degradación estarán en el intervalo [0, 19].
       Para σβ1 y σβ2 se tomarán 3 niveles para cada uno, de acuerdo con unos
       valores definidos para el coeficiente de variación (CV ) de los parámetros en
       β. Sea X una variable aleatoria con media µ y varianza σ, su CV se define
       como:
                                                 σ
                                          CV =                                     (2)
                                                 µ

   Se considera que valores de CV entre:

       0 y 15 % indican una población homogénea,
       15 % y 30 %, homogeneidad moderada y
       mayores de 30 % pueden indicar una heterogeneidad significativa.


                                      Revista Colombiana de Estadı́stica 29 (2006) 133–151

Degradación para trayectorias lineales                                                     139


   De la ecuación (2) se obtiene:
                                      (l)
                                     σβj = CVl ∗ µβj

con i = 1, . . . , 5, l = 1, 2, 3, j = 1, 2
   Para este diseño se definirán tres niveles del coeficiente de variación, uno dentro
de cada intervalo expresado anteriormente:

      CV1 = 0.1 comportamiento homogéneo de los parámetros,
      CV2 = 0.2 heterogeneidad media, y
      CV3 = 0.3 heterogeneidad significativa.

    Para cada nivel de ρ se tienen nueve escenarios diferentes de simulación, como
se puede observar en la tabla 1.

Tabla 1: Combinaciones de parámetros para cada nivel del coeficiente de correlación ρ.

                                                      ρi
                                     CV1 (β2 )     CV2 (β2 )   CV3 (β2 )
                                      (1) (1)       (1) (2)     (1) (3)
                        CV1 (β1 )    σβ1 σβ2       σβ1 σβ2     σβ1 σβ2
                                       (2) (1)       (2) (2)    (2) (3)
                        CV2 (β1 )    σβ σβ         σβ σβ       σβ σβ
                                        1     2       1    2     1     2

                                       (3) (1)       (3) (2)    (3) (3)
                        CV3 (β1 )    σβ σβ         σβ σβ       σβ σβ
                                        1     2       1    2     1     2



   El superı́ndice indica que el valor de la desviación estándar se calculó con base
en el nivel l del coeficiente de variación que se define como:

                                    CVl (βj ) = CVl ∗ µβj

     Para cada escenario se simularán K = 400 experimentos bajo las condiciones
del escenario, y en cada experimento simulado se aplicarán las metodologı́as de
análisis de degradación explı́cita y aproximada para estimar F (t). En la tabla 2 se
presentan los parámetros de simulación utilizados en cada uno de los 45 escenarios
y también la proporción teórica esperada de trayectorias que cruzan el umbral de
falla al tiempo final de experimento (P c).
     La comparación de las metodologı́as tiene sentido cuando esta proporción no
es cercana a uno, pues se tendrı́a suficiente información para realizar un análisis
clásico de confiabilidad y ajustar una distribución de probabilidad a los tiempos
de cruce observados. Observe, además, que la P (β2 > 0) es siempre mayor de 0.95,
pues para este caso µβ2 = 0.5 y σβ2 varı́a de 0.05 a 0.15
     Se utiliza la siguiente notación:

      FR (t) : función de distribución acumulativa verdadera para el tiempo de vida
      de los dispositivos, calculada para cada escenario de simulación utilizando
      los parámetros verdaderos.


                                            Revista Colombiana de Estadı́stica 29 (2006) 133–151

140                                               Sergio Yáñez & Ronald Andrés Granada


Tabla 2: Combinaciones de parámetros utilizados en cada uno de los escenarios de si-
         mulación para generar los 400 experimentos por cada escenario y proporción
         teórica de trayectorias que cruzan el umbral de falla al tiempo final del expe-
         rimento (P c) para cada escenario.

      F̂R (t)k : k-ésima estimación de FR (t) proporcionada por las metodologı́as de
      degradación explı́cita o aproximada en un escenario, ası́:

         • FD (t)k : k-ésima estimación de FR (t) obtenida utilizando el método
           explı́cito de análisis de datos de degradación para cada experimento
           (k = 1, . . . , K).
         • FAL (t)k : k-ésima estimación de FR (t) utilizando el método aproximado
           de análisis de datos de degradación ajustando una distribución lognor-
           mal para cada experimento (k = 1, . . . , K).
         • FAW (t)k : k-ésima estimación de FR (t) utilizando el método aproximado
           de análisis de datos de degradación ajustando una distribución Weibull
           para cada experimento (k = 1, . . . , K).

   Se escogen las distribuciones lognormal y Weibull en el método aproximado
por ser de uso muy frecuente en análisis de confiabilidad. Según Meeker & Escobar
(1998), son los workhorses en el modelamiento paramétrico de la confiabilidad.
   Para cada escenario se podrá asociar una función de distribución acumulada del
tiempo de vida de las unidades, que se considerará la “verdadera” FR (t), calculada
con base en los parámetros de simulación ası́:
                                             "                                  #
                                                     µβ1 + µβ2 t − Df
        P (T ≤ t) ≈ P (Df ≤ β1 + β2 t) = Φ                                    1
                                               (σβ21 + σβ22 t2 + 2tρσβ1 σβ2 ) 2

    Se considerará que el modelo de degradación inducido es el “verdadero”, pues es
el que tiene en cuenta la teorı́a fı́sico-quı́mica asociada a cada problema particular
y por lo tanto es el que mejor explica el fenómeno.
    Para la FR (t) se calculan en cada escenario algunos cuantiles de interés
(t0.05 , t0.25 , t0.5 , t0.75 , t0.95 ) y se tiene en cada escenario para cada tp un error
cuadrático medio (ECMp ).
    Ası́, para cada metodologı́a de análisis de datos de degradación y para cada
cuantil tp , el ECMp , en un escenario de simulación, se define como:
                                  h                    2 i
                          ECMp = E F̂R (tp ) − FR (tp )                                   (3)

   y se puede estimar como:
                                     K 
                                     P                       2
                                        F̂R (tp )k − FR (tp )
                         \ p = k=1
                         ECM
                                                 K
                                     K 
                                     P                2
                                        F̂R (tp )k − p
                                                                                          (4)
                                  = k=1
                                                 K




                                          Revista Colombiana de Estadı́stica 29 (2006) 133–151

142                                           Sergio Yáñez & Ronald Andrés Granada


donde:

      tp es el cuantil p × 100 % de la distribución verdadera del tiempo de vida de
      los dispositivos.
      K es el número de experimentos en cada escenario (K = 400).

    Entonces para cada combinación de parámetros (cada escenario) se tendrán
errores cuadráticos medios de cada metodologı́a en cada cuantil de interés. Esta
medida permitirá comparar las metodologı́as de análisis de datos de degradación
para cada combinación de parámetros que se genere y por tanto plantear unas
conclusiones sobre las diferencias observadas en el caso de trayectorias de tipo
lineal.
    El criterio utilizado para comparar las metodologı́as es: a menor ECMp , mejor
la estimación de FR (tp ).


5.     Resultados de la simulación
     Los resultados se obtuvieron utilizando los programas estadı́sticos R y
S-PLUS. En el programa R (R Development Core Team 2006) se generaron los da-
tos de cada experimento en cada escenario, utilizando para ello la librerı́a “MASS”
de Venables y Ripley, y algunas funciones de la librerı́a base.
     Con los parámetros que se utilizaron para emular los experimentos en cada
escenario, se obtienen los percentiles teóricos de la distribución verdadera en cada
escenario, utilizando el algoritmo Monte Carlo descrito en Yáñez et al. (2003). Es-
tos percentiles servirán para calcular el error cuadrático medio de las metodologı́as
explı́cita y aproximada utilizando la ecuación (4).
     Una vez se tienen los datos de cada uno de los escenarios se utiliza la librerı́a
“nlme” (Pinheiro & Bates 2000). Como parte del proceso de estimación de la
metodologı́a explı́cita se obtienen los parámetros del modelo de efectos mixtos,
estimados para cada uno de los experimentos dentro del escenario, y se generan
los seudotiempos de falla que se utilizarán para el proceso de estimación de F (t)
con la metodologı́a aproximada.
     Con estos parámetros estimados se tienen estimaciones de F (t) evaluadas en
cada percentil de interés para cada experimento dentro de cada escenario y utili-
zando (4) se calculan los ECM para cada percentil en cada escenario.
     Los seudotiempos de falla generados en la primera parte se exportan al pro-
grama S-PLUS y en este se utilizan las funciones programadas por Meeker &
Escobar (2000) en el Módulo SPLIDA (S-PLUS Life Data Analysis), para estimar,
vı́a máxima verosimilitud aproximada, los parámetros asociados a los modelos de
probabilidad Weibull y lognormal, como se sugirió en la sección anterior. Utili-
zando los percentiles de la distribución teórica y (3) se calculan los ECM para la
metodologı́a aproximada ajustando la distribución lognormal y la Weibull.
     Para obtener una medida adimensional que permita comparar las metodologı́as
y mirar qué tanto difieren, se define una medida que indica el grado de hetero-


                                     Revista Colombiana de Estadı́stica 29 (2006) 133–151

Degradación para trayectorias lineales                                                   143


geneidad en las estimaciones de F (tp ) obtenidas por las metodologı́as en cada
percentil de interés. Esta medida está relacionada con el coeficiente de variación
CV , denotada por CV ? .
   Para un percentil de interés, el CV ? se define de la siguiente manera:
                             √
                         ?    ECM pi
                     CVi =             × 100 %, i = 1, 2, . . . , 45             (5)
                                 p

donde:

      p indica la probabilidad asociada al percentil de la distribución verdadera
      (que cumple que F (tp ) = p).

      i es un ı́ndice que señala el escenario para el que se calcula.

   Entonces para comparar las metodologı́as en cada percentil se define una
medida promedio del CV ? calculada sobre todos los escenarios que se denotará
    ?
CVprom   , ası́:
   Para cada escenario de simulación se denota el promedio de los coeficientes de
                   ?
variación como CVprom y se calcula de la siguiente manera:
                                                P45      ?
                                    ?             i=1 CVi
                                 CVprom =                                                 (6)
                                                    45

    A continuación se presentan las tablas con los ECM para cada percentil y en
                                                                                  ?
cada uno de los escenarios de simulación y al final de cada tabla el CVprom           para
cada metodologı́a.
    La tabla 3 muestra los errores cuadráticos para cada escenario de simulación y
para cada metodologı́a de análisis en el percentil 5 %. Se puede observar claramente
que los ECM asociados a la metodologı́a explı́cita son menores que los errores de
cualquiera de las dos distribuciones ajustadas por la metodologı́a aproximada en
cualquier escenario. Igualmente se puede ver que para el método aproximado, los
ECM asociados a la distribución lognormal son menores que los ECM asociados a la
distribución Weibull en cualquier escenario. Esta tabla muestra cómo los escenarios
que están asociados con un coeficiente de variación del 10 % para la pendiente del
modelo de efectos aleatorios (escenarios 1, 4, 7, 10, 13, . . . , 40, 43), es decir, cuando
las pendientes son bastante homogéneas, los ECM son más pequeños en todas las
metodologı́as y son parecidos de metodologı́a a metodologı́a.
                        ?
    Si se miran los CVprom   al final de la tabla 3 para cada metodologı́a, se podrı́a
pensar que los valores indican una heterogeneidad muy alta en las estimaciones de
F (t), pero esto se debe a la división por p = 0.05, la cual infla los CV ? de cada
                                                                           ?
escenario; entonces la interpretación debe ser cuidadosa. Los CVprom           corroboran
que la metodologı́a explı́cita es la mejor, pero la aproximada con ajuste lognormal
está cerca de la explı́cita si se considera la escala que se maneja; esto sugiere
que en este caso las metodologı́as explı́cita y aproximada lognormal pueden ser
competitivas.

Tabla 3: Errores cuadráticos medios calculados por simulación para las metodologı́as
         de degradación explı́cita y degradación aproximada en el percentil 5 % de la
         distribución verdadera en cada escenario.

    La tabla 4 presenta los errores cuadráticos para cada escenario de simulación y
para cada metodologı́a de análisis en el percentil 25 %. En esta tabla se expone algo
diferente: los ECM más pequeños están asociados a la metodologı́a aproximada
con ajuste Weibull, seguidos por los ECM de la metodologı́a aproximada con la
distribución lognormal.
                             ?
    Si se observan los CVprom   , al final de la tabla 4, se nota cómo la metodologı́a
aproximada con ajuste Weibull tiene el menor grado de heterogeneidad 27.3 %
mientras que la explı́cita y la aproximada con ajuste lognormal presentan valores
más cercanos entre ellas; para estos niveles de heterogeneidad la diferencia con la
Weibull es pequeña. Las anteriores diferencias nos indican que aunque la meto-
dologı́a explı́cita no es la mejor, la metodologı́a aproximada con ajuste lognormal
presenta ECM muy cercanos a los de la explı́cita y se puede pensar que para este
caso las dos metodologı́as son competitivas en la calidad de estimación. Se sigue
presentando que los ECM son mı́nimos cuando CV (β1 ) = 0.1, lo que indica que
la homogeneidad de la pendiente del modelo de efectos mixtos es importante a la
hora de estimar F (t).
                                                    ?
    La tabla 5 presenta un resumen de los CVprom          para cada percentil trabaja-
do. Las tablas completas con los ECM para cada percentil en cada escenario de
simulación se pueden ver en Granada (2004). El análisis de estos valores es si-
milar al realizado con los valores de las tablas anteriores. Para el percentil 50 %,
se puede notar cómo las tres metodologı́as de estimación presentan valores muy
cercanos del orden de 22 %, lo que sugiere claramente que para este caso las tres
metodologı́as son competitivas; lo mismo se presenta para los percentiles 75 % y
                                                                              ?
95 % donde, aunque baja el grado de heterogeneidad medido por el CVprom             para
cada metodologı́a, se nota cómo las diferencias no son muy altas en cada percentil
trabajado.
                                                                           ?
    Se observan al final de la tabla los valores más pequeños de CVprom       , lo que
indica que las estimaciones de F (t) en este percentil son más homogéneas que en
                                                                   ?
los casos anteriores. Las metodologı́as que tienen menores CVprom       son la explı́cita
(4.1 %) y la aproximada con ajuste lognormal (5 %), lo que claramente muestra la
competitividad de estas dos metodologı́as.

6.     Aplicación de las metodologı́as de análisis para
       la degradación de los dispositivos láser
    Retomando el ejemplo referenciado en la sección 1, se tratará un ejemplo sobre
la degradación de algunos dispositivos láser, originalmente abordado en Meeker
& Escobar (1998). En la figura 1 se puede apreciar que un posible modelo que
describe la evolución del incremento en la corriente de operación en el tiempo es
un modelo lineal con intercepto constante y pendiente aleatoria; pues se puede
observar cómo todos los dispositivos parten de una corriente de operación igual
pero la rapidez con que se incrementa la corriente de operación varı́a de unidad a
unidad. Por lo anterior se quiere estimar un modelo de la siguiente forma:
       yij = Dij + εij , i = 1, . . . , 15, j = 1, . . . , 17, donde Dij = β1 + β2i tij   (7)


                                          Revista Colombiana de Estadı́stica 29 (2006) 133–151

146                                             Sergio Yáñez & Ronald Andrés Granada


Tabla 4: Errores cuadráticos medios calculados por simulación para las metodologı́as
         de degradación explı́cita y degradación aproximada en el percentil 25 % de la
         distribución verdadera en cada escenario.

                            ?
            Tabla 5: Resumen de los CVprom , para cada percentil tratado.
                                               ?
                                           CVprom
         Percentil   Método explı́cito   Método aproximado     Método aproximado
                                               lognormal               Weibull
           5%           76.80540 %             93.7437 %             189.87400 %
           25 %         37.07660 %             35.5192 %              27.27290 %
           50 %         21.90950 %             22.7696 %              22.87200 %
           75 %         13.35190 %             14.9356 %              18.76220 %
           95 %          4.06818 %              5.0284 %               7.02805 %


     Es un modelo donde se tienen 15 dispositivos y a cada uno se le tomó una
medida de la corriente de operación en 17 instantes de tiempos diferentes medidos
en horas y que van desde cero horas hasta 4000 horas. Se observa gráficamente
que β1 es igual a cero, pero que en β2i la pendiente del modelo lineal es diferente
para cada dispositivo y por tanto se asume que para toda la población de láseres
la pendiente es una variable aleatoria; además, que esta es independiente de los
εij . Se asume, también, que la distribución de la pendiente es normal.
     Para estimar los parámetros en la ecuación (7), se utilizan las funciones im-
plementadas por Pinheiro & Bates (2000) en S-PLUS y en R. En este caso la
función lme proporciona las estimaciones de máxima verosimilitud aproximada de
los parámetros.
     Esta función genera entonces las siguientes estimaciones:
                                     β̂1 = 9.4937 × 10−3
                                  µ̂β2 = 2.0432 × 10−3
                                   σ̂β2 = 4.62169 × 10−4
                                     σ̂ε = 1.9926 × 10−1

   La estimación de F (t) es bastante sencilla pues podemos llegar fácilmente a
una forma cerrada de la siguiente manera:
                                                                         
                                           D f − β1        β1 + µβ2 t − Df
   P (T < t) = P [D(t) ≥ Df ] = P β2 ≥                =Φ
                                               t                 σβ2 t

    Donde Φ(·) es la función de probabilidad acumulada para la distribución nor-
mal estándar y Df = 10 % es el incremento en la corriente de operación que define
el nivel especificado de falla. Para esta aplicación, al igual que en el ejemplo ante-
rior, se utilizan los parámetros estimados por el modelo de efectos aleatorios para
estimar F (t) (figura 2).


6.1.    Comparación con el análisis aproximado de
        degradación
    Se puede utilizar una metodologı́a alterna para obtener la función de probabi-
lidad acumulada del tiempo de vida (distribución del tiempo de falla), estimando

para cada dispositivo un modelo lineal y obteniendo los tiempos de cruce (tiem-
pos en los que el dispositivo alcanza el nivel máximo de degradación permisible
Df = 10 %), también llamados seudotiempos de falla (figura 3).
Figura 2: F (t) estimada por la metodologı́a de degradación e intervalos de confianza
          bootstrap del 95 %. Los cı́rculos representan el estimador no-paramétrico de
          F (t), basado en los seudotiempos de falla.

Figura 3: Extrapolación de las trayectorias de degradación para los datos de los láseres
          de GaAs.

   Utilizando los seudotiempos de falla obtenidos con cada una de las estimacio-
nes independientes de cada trayectoria, se ajustan varias distribuciones de pro-
babilidad. La tabla 6 presenta la distribución de probabilidad y los respectivos
parámetros estimados para cada una, con los intervalos de confianza del 95 % para
cada estimación.
Tabla 6: Estimaciones de máxima verosimilitud para tres distribuciones basadas en los
         seudotiempos de falla obtenidos por extrapolación.

    La figura 4 muestra las diferentes estimaciones de F (t), tanto por el método
de degradación explı́cito cómo por el método aproximado con el cual se presentan
las tres distribuciones que mejor se ajustan. En la figura 5 se grafican las diferen-
cias simples de cada distribución estimada por el análisis aproximado, contra la
estimación por el método de degradación explı́cito y se puede ver cómo la F (t)
lognormal estimada con el método aproximado es la que presenta las menores di-
ferencias y la diferencia más amplia, en valor absoluto, es del 5 %. Mirando ambos
gráficos, se aprecia cómo las estimaciones Weibull y normal se alejan un poco más
de la estimación por degradación explı́cita. Podemos notar que en este ejemplo los
resultados obtenidos por las dos metodologı́as a evaluar siguen siendo similares, si
se observa además que las estimaciones por el método aproximado siguen estando
dentro del intervalo de confianza bootstrap del 95 % calculado para la estimación
por degradación explı́cita (ver figura 2).

7.      Conclusiones
     De la sección anterior se puede concluir que:
       Para trayectorias de degradación lineal, las metodologı́as de análisis de da-
       tos de degradación explı́cita y aproximada con ajuste lognormal, presentan
       resultados competitivos.
       Las diferencias entre los ECM asociados a las metodologı́as eran mı́nimas
       cuando la pendiente del modelo lineal no tenı́a una variabilidad muy alta
       vista en términos de un coeficiente de variación del 10 %. Esto muestra que
       la metodologı́a explı́cita es pertinente cuando hay alta variabilidad de unidad
       a unidad en el experimento de degradación.
       La metodologı́a explı́cita mostró tener menor ECM para la mayorı́a de los
       percentiles evaluados; por ello se recomienda que para fines de extrapolación
       se utilice esta metodologı́a.

    En el caso no lineal, los resultados para los casos concretos (Meeker & Escobar
1998, Granada 2004) muestran que el método explı́cito es mucho mejor. Se explo-
rará esta hipótesis a futuro.

      Figura 4: Estimaciones de F (t) obtenidas por las diferentes metodologı́as.

Figura 5: Diferencias simples entre la estimación del método de degradación explı́cito
          y las estimaciones del método aproximado en su orden lognormal, Weibull y
          normal.

Agradecimientos
Investigación patrocinada por la Dirección de Investigación, Universidad Nacional de Colombia, sede Medellı́n (DIME). Proyecto código 0308022737.
Queremos dar gracias por las juiciosas y oportunas observaciones de los jurados, quienes ayudaron a dar forma definitiva a este trabajo.
Referencias
Chao, M. T. (1999), Degradation Analysis and Related Topics: some Thoughts and a Review, Vol. 23, National Science Council, R. O. C., Taipei, Taiwan.
Granada, R. A. (2004), Modelos y análisis para datos de degradación, Tesis de maestría en estadı́stica, Universidad Nacional de Colombia, sede Medellín.
Lu, C. J. & Meeker, W. Q. (1993), ‘Using Degradation Measures to Estimate a Time-to-Failure Distribution’, Technometrics 35, 161–174.
Meeker, W. Q. & Escobar, L. A. (1998), Statistical Methods for Reliability Data, Wiley, New York.
Meeker, W. Q. & Escobar, L. A. (2000), SPLIDA (S-PLUS Life Data Analysis), Iowa State University and Louisiana State University.
Pinheiro, J. C. & Bates, D. M. (2000), Mixed-Effects Models in S and S-PLUS, Springer, New York.
R Development Core Team (2006), R: A Language and Environment for Statistical Computing, R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0. http://www.R-project.org
Venables, W.Ñ. & Ripley, B. D. (2002), Modern Applied Statistics with S, 4th edn, Springer, New York. http://www.stats.ox.ac.uk/pub/MASS4
Yáñez, S., Granada, R. A. & Jaramillo, M. (2003), ‘Modelos y análisis para datos de degradación’, Revista Colombiana de Estadı́stica 26(1), 41–59.