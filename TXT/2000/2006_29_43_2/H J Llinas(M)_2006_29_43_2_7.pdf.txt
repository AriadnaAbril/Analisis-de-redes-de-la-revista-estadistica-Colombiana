Precisiones en la teoría de los modelos logísticos
Universidad del Norte
Resumen
Se estudian los modelos logísticos, como una clase de modelos lineales generalizados (MLG). Se demuestra un teorema sobre la existencia y unicidad de las estimaciones de máxima verosimilitud (abreviadas por ML) de los parámetros logísticos y el método para calcularlas. Con base en una teoría asintótica para estas ML-estimaciones y el vector score, se encuentran aproximaciones para las diferentes desviaciones −2 log L, siendo L la función de verosimilitud. A partir de ellas se obtienen estadísticas para distintas pruebas de hipótesis, con distribución asintótica chi-cuadrada. La teoría asintótica se desarrolla para el caso de variables independientes y no idénticamente distribuidas, haciendo las modificaciones necesarias para la conocida situación de variables idénticamente distribuidas. Se hace siempre la distinción entre datos agrupados y no agrupados.
Palabras clave: variable de respuesta binaria, modelo lineal generalizado, teoría asintótica.
Introducción
Los modelos logísticos son adecuados para situaciones donde se quiere explicar la probabilidad p de ocurrencia de un evento de interés por medio de los valores de ciertas variables “explicativas”. Si se asocia al evento de interés una variable dicotómica, entonces esta es una variable de Bernoulli con esperanza condicional p. En cambio, los modelos lineales cubren situaciones completamente diferentes. Estos quieren explicar la esperanza condicional de una variable aleatoria continua. Ambos tipos son casos particulares de los MLG. Además, con base en una teoría asintótica para las ML-estimaciones, se han encontrado aproximaciones para diferentes desviaciones, es decir, para (-2) veces los logaritmos de las ML-funciones. Estas se usan para obtener diferentes pruebas de hipótesis estadísticas que tienen distribuciones asintóticas chi-cuadrado. Por una parte, en algunos libros se mencionan estos resultados dando solo pocos detalles. Esto ocurre en el libro básico sobre MLG de Mc Cullagh & Nelder (1983). En Agresti (1990) ya se encuentran más detalles con mayor enfoque para el caso de variables explicativas categóricas. De todas formas, falta el desarrollo detallado de la teoría asintótica de ML-estimaciones para el caso de variables independientes y no idénticamente distribuidas. En los libros “clásicos” de Estadística Matemática, como Rao (1973) o Zacks (1971), se detalla solo el caso de variables independientes e idénticamente distribuidas. Esto último no se presenta en MLG. Por otra parte, muchos artículos originales, como Wedderburn (1974), Wedderburn (1976) o Mc Cullagh (1983), enfocan el concepto más general de funciones de “cuasi-verosimilitud”, las cuales son relevantes para modelos logísticos en casos como problemas con mediciones repetidas. Además, es bien conocido que, para modelos lineales, existe una teoría exacta tanto para las estimaciones como para las pruebas de hipótesis. Por lo tanto, considerando la importancia de los modelos logísticos para muchas aplicaciones, este artículo desarrolla los detalles de los temas arriba mencionados. El artículo va más allá de ser un “estudio descriptivo” acerca de los modelos logísticos porque, como se dijo anteriormente, la teoría que se ha desarrollado no aparece así detallada en la literatura sino, en gran parte, solo esbozada. El artículo está compuesto de cinco secciones en las cuales se presenta un análisis teórico detallado sobre los modelos logísticos describiendo los supuestos básicos, propiedades y características que tienen dichos modelos y presentando y demostrando los resultados más importantes que se conocen sobre las estimaciones de sus parámetros, distribuciones asintóticas y pruebas de comparación de modelos; se dan los detalles que, así reunidos, no se
encuentran comúnmente en la literatura.


2.     Modelos logísticos y modelos relacionados
2.1.    El modelo de Bernoulli
   Supongamos que la variable de interés Y es de Bernoulli. En símbolo,
Y ∼ B(1, p), siendo p := E(Y ) = P (Y = 1) la probabilidad de que ocurra Y .
Haciendo n observaciones independientes de Y , se obtienen los datos yi ∈ {0, 1},


                                    Revista Colombiana de Estadística 29 (2006) 239–265

Precisiones en la teoría de los modelos logísticos                                        241


i = 1, . . . , n, donde yi es un posible valor de la variable muestral Yi , las cuales
son independientes entre sí. De esta forma, se llega a un modelo estadístico de
Bernoulli:
                       Yi = pi + ei ∼ B(1, pi ), i = 1, . . . , n

    Fijando y = (y1 , . . . , yn )T obtenemos la función de verosimilitud en el paráme-
tro p = (p1 , . . . , pn )T :
                                         Yn
                                  L(p) =    [pyi i (1 − pi )1−yi ]
                                           i=1

y el logaritmo de la función de máxima verosimilitud será:
                                       n
                                       X
                  L(p) := log L(p) =         [yi log pi + (1 − yi ) log(1 − pi )]         (1)
                                       i=1


   Como 0 ≤ f (y, p) ≤ 1, se tiene que −∞ ≤ L(p) ≤ 0. Hay varias situaciones
que se pueden presentar en un modelo de Bernoulli. Se dice que este se puede
identificar como alguno de los siguientes modelos: completo, nulo o saturado.


2.2.    Los modelos completo y nulo
     El modelo completo se caracteriza por el supuesto de que todos
pi , i = 1, . . . , n se consideran como parámetros.

Teorema 1. En el modelo completo, las ML-estimaciones de pi son p̂i = Yi con
valores p̂i = yi , i = 1, . . . , n. Además, Lc := L(y) = 0.

Demostración. Considerando la ecuación (1) se tiene:
                                   X                  X
                          L(p) =           log pi +           log(1 − pi )
                                     i                  i
                                   yi =1              yi =0


              !
Ahora, L(p) = 0 si y solo si pi = yi , para cada i = 1, . . . , n.

    Esto demuestra la existencia de las ML-estimaciones. Si para algún i se tiene
que pi 6= yi , entonces, L(p) < 0. Esto último demuestra que las ML-estimaciones
son únicas porque, si pe es un vector que tiene por lo menos una componente pi
                                             p) < Lc (ya que al reemplazar pi = yi
diferente de yi , entonces, se tendría que L(e
en L(p) se obtiene que Lc = 0).

    El modelo nulo se caracteriza por el supuesto de que todos los pi , i = 1, . . . , n
se consideran iguales; es decir, se tiene un solo parámetro p = pi , i = 1, . . . , n. En
este caso, (1) será:

                        L(p)   =   n[y log p + (1 − y) log(1 − p)]                        (2)


                                           Revista Colombiana de Estadística 29 (2006) 239–265

242                                                                         Humberto Jesús Llinás


Teorema 2. En el modelo nulo, la ML-estimación de p es p̂ = Y con valor p̂ = y
  Además, Lo := L(y) < 0 si y solo si 0 < y < 1.


Demostración. De (2) se tiene, para y = 0, que L(p) = 0 si y solo si p = 0 y
para y = 1, que L(p) = 0 si y solo si p = 1. Ahora, supongamos que 0 < y < 1.
    De la ecuación (2) se puede demostrar que p̂ = y y que es única. Además, log y
y log(1 − y) son cantidades negativas, por lo tanto, Lo < 0.


2.3.     El modelo saturado y supuesto
   El modelo saturado se caracteriza por los siguientes supuestos:

  1. Se supone que:

        a) Se tienen K variables explicativas X1 , . . . , XK (algunas pueden ser nu-
           méricas y otras categóricas) con valores x1i , . . . , xKi para i = 1, . . . , n
           (fijadas u observadas por el estadístico, según sean variables determi-
           nísticas o aleatorias).

        b) Entre las n K-uplas (x1i , . . . , xKi ), i = 1, . . . , n de los valores de las
           variables explicativas X1 , . . . , XK haya J K-uplas diferentes, definiendo
           las J poblaciones. Por tanto, J ≤ n.


       Notación. Para cada población j = 1, . . . , J se denota:

            El número de observaciones Yij en cada población j por nj , siendo
            n1 + . . . + nJ = n;
                                                               nj
                                                               P
            La suma de las nj observaciones Yij en j por Zj :=    Yij con valor
                                                                              i=1

                              nj                     J              nj
                                                                  J X               n
                              X                      X            X                 X
                       zj =         yij ,   siendo         zj =             yij =         yi
                              i=1                    j=1          j=1 i=1           i=1


       Para mayor simplicidad en la escritura, se abreviará la j-ésima población
       (x1j , . . . , xKj ) por el símbolo ?.

  2. Para cada población j = 1, . . . , J y cada observación i = 1, . . . , n en j, se
     supone que:

            (Yij |?) ∼ B(1, pj )
            Las variables (Yij |?) son independientes entre sí
            pj = P (Yij = 1|?) = E(Yij |?) y            vj := V (Yij |?) = pj (1 − pj )


                                            Revista Colombiana de Estadística 29 (2006) 239–265

Precisiones en la teoría de los modelos logísticos                                        243


      A continuación se desarrolla el símbolo ?.

      El supuesto 2 implica:
        a) Todos los pij , i = 1, . . . , n dentro de cada población j son iguales. Es
           decir, se tiene como parámetro el vector p = (p1 , . . . , pJ )T .
        b) Para cada población j = 1, . . . , J se tiene:
                Zj ∼ B(nj , pj )
                Las variables Zj son independientes entre las poblaciones
                mj := E(Zj ) = nj pj y Vj := V (Zj ) = nj vj
            Escrito en forma vectorial, Z := (Z1 , . . . , ZJ )T con valores reunidos en
            z := (z1 , . . . , zJ )T , tiene:
                m := E(Z) = (n1 p1 , . . . , nJ pJ )T
                V := Cov(Z) = diag{n1 v1 , . . . , nJ vJ }, matriz diagonal de tamaño
                J × J.

   En el modelo saturado, el logaritmo de la función de máxima verosimilitud será
                        J      nj                                       !
                       X      X
            L(p) =                [yij log pj + (1 − yij ) log(1 − pj )]
                           j=1     i=1
                           J
                           X
                      =          [zj log pj + (nj − zj ) log(1 − pj )]
                           j=1

                                                                                      Z
Teorema 3. En el modelo saturado, las ML-estimaciones de pj son p̃j = njj , con
              zj
valores p̃j =    , j = 1, . . . , J.
              nj
    Además, Ls := L(p̃) < 0 para 0 < p̃j < 1.

Demostración. Si 0 < p̃j < 1, se tiene que:

                ∂L    zj        nj − z j                        zj
                    =    + (−1)          = 0 si y solo si p̃j =
                ∂pj   pj        1 − pj                          nj

   Por consiguiente, si 0 < zj < nj , se tiene
                                        "                #
                       ∂2L                n2j     n2j
                                  =−          +            <0
                        ∂p2j              zj    nj − z j
                                 pj =p̃j


   Falta analizar los dos casos extremos:
                             ∂L        nj
      Si zj = 0, entonces        =−         decrece en pj . En este caso, L decrece
                             ∂pj     1 − pj
      en pj ; es decir, se hace maximal L(p) para pj = 0.


                                           Revista Colombiana de Estadística 29 (2006) 239–265

244                                                                      Humberto Jesús Llinás

                             ∂L       n
       Si zj = nj , entonces ∂pj
                                 = pjj decrece en pj . En este caso, L crece en pj ; es
       decir, se hace maximal L(p) para pj = 1.
   En el modelo saturado, se puede obtener el valor de L reemplazando, en la
ecuación (3), cada pj por p̃j , j = 1, . . . , J. Por lo tanto:
                             J
                             X
                   Ls   =          nj [p̃j log p̃j + (1 − p̃j ) log(1 − p̃j )]
                             j=1

   Bajo la condición 0 < p̃j < 1 se puede afirmar que log p̃j y log(1 − p̃j ) son
cantidades negativas. Por consiguiente, la suma del lado derecho de la ecuación
anterior es también negativa.


2.4.     El modelo logístico
   Se hacen los supuestos 1 y 2 de la sección 2.3, donde adicionalmente se supone
que la matriz de diseño
                                                     
                                  1 x11 . . . xK1
                                                  .. 
                           C =  ...   ..
                                        .
                                            ..
                                               .    . 
                                       1 x1J       . . . xKJ
tiene rango completo Rg(C) = 1 + K ≤ J. Para llegar a un modelo logístico se
hace el supuesto adicional
                             K
                               X
                      pj
              log            =    βk xkj , con βo = δ, xoj = 1           (3)
                    1 − pj
                                       k=0

   Sea α = (δ, β1 , . . . , βK )T el vector de parámetros en el modelo. Nótese que el
supuesto sobre Rg(C) = 1 + K hace identificable al parámetro α.


2.5.     Score e información para los modelos saturado
         y logístico
Teorema 4. En el modelo saturado se tiene:

  a) El vector (aleatorio) score de la muestra es
                                            Z −n p 
                                                         1     1 1
                                              v1      
                                       ∂L      ..     
                               S(p) :=    =
                                                .
                                                       
                                                       
                                       ∂p  Z − n p 
                                             J     J J
                                               vJ
       donde L = L(p) es el “logaritmo de la función de máxima verosimilitud”
       aleatorio; es decir, entran las variables aleatorias Yi en lugar de los valores
       yi . Además, E(S(p)) = 0.


                                          Revista Colombiana de Estadística 29 (2006) 239–265

Precisiones en la teoría de los modelos logísticos                                         245


   b) La matriz de información de la muestra es

                        =(p) := Cov(S(p)) = diag{n1 /v1 , . . . , nJ /vJ },
                                                                ˜ := =(p).
      la cual es definida positiva. Para mayor simplicidad, sea =

  c) S(p) = =˜ · (p̃ − p).
        2 
  d) E − ∂∂pL2 = =,  ˜ siendo ∂ 2 L2 la matriz de las segundas derivadas parciales de
                              ∂p
      L = L(p).
                                                                    Zj −nj pj
Sea Z ∗ = (Z1∗ , . . . , ZJ∗ )T el vector de las variables Zj∗ :=    √
                                                                       nj vj . Entonces,


   e) Z ∗ = V −1/2 (Z − m)

   f ) S(p) = =˜ 1/2 Z ∗ o Z ∗ = =                       ˜ 1/2 es la matriz definida
                                    ˜ −1/2 · S(p), donde =
                         ˜ ) · (=
       por la propiedad (= 1/2   ˜ ) = =,
                                  1/2     ˜ siendo = ˜ la matriz diagonal de J × J
       definida en 4 y cuyos elementos diagonales nj /vj son positivos.
                                                  p
       El j-ésimo elemento diagonal de = ˜ 1/2 es nj /vj y = ˜ −1/2 es la inversa de
       ˜ 1/2
       = .

Demostración. Solo debemos aplicar resultados básicos del cálculo y del álgebra
lineal y, obviamente, de la estadística (Dobson 2002).
                                         nj
Teorema 5. Supóngase que lı́m                  = σ12 > 0 existe para cada j = 1, . . . , J.
                                  nj →∞ n·vj         j

Entonces, para el modelo saturado, valen las siguientes afirmaciones asintóticas
(n → ∞), considerando naturalmente J fijo:
        2 
                     a   ˜ siendo ∂ 2 L2 la matriz de las segundas derivadas parciales
  a) n1 − ∂∂pL2 = n1 =,             ∂p
     de L = L(p) y =      ˜ como en la parte d) del teorema 4. Es decir, para cada
     j = 1, . . . , J se tiene que
                                                        !
                                   1 ∂2L    1     ∂2L P
                                          − E             →0
                                                          −
                                   n ∂p2j   n      ∂p2j

           d                            d
   b) Z ∗ −
          → NJ (0, I)      y    √1 S(p) −
                                        → NJ (0, Ξ̃), siendo
                                  n
                                                                      
                                                         1         1
                                Ξ̃ := Ξ(p) = diag            ,..., 2
                                                         σ12      σJ

      una matriz definida positiva de tamaño J × J.

           a                                             P
                                             →, convergencia en probabilidad y
  Aquí, = significa equivalencia asintótica; −
d
→, convergencia en distribución.
−


                                         Revista Colombiana de Estadística 29 (2006) 239–265

246                                                                               Humberto Jesús Llinás


Demostración. Considerando el teorema 4d) se tiene, para cada j = 1, . . . , J,
que

  a)
                                            !                                                
            1 ∂2L   1                ∂2L                   Zj                 nj      1 − 2pj       P
                   − E                          = −           − pj       ·          ·               →0
                                                                                                    −
            n ∂p2j  n                ∂p2j                  nj                n · vj      vj
                                                                                      
                                                                             Zj            P
       ya que, por la ley débil de los grandes números,                      nj − pj       → 0, si nj → ∞
                                                                                           −
       y, por el supuesto,

                                  nj      1 − 2pj nj →∞ 1 1 − 2pj
                                        ·         −−−−→ 2 ·
                                 n · vj      vj         σj   vj

  b) Como las variables Zj∗ son estandarizadas, entonces Zj∗ −→
                                                              d
                                                                N1 (0, 1), cuando
                                            d
       nj → ∞. Por tanto, Z ∗ − → NJ (0, I), cuando n → ∞, y J fijo. Ahora, se
       demostrará la otra parte del inciso b). Considerando el teorema 4f, se tiene
                      1/2
                         ˜
       que √1n S(p) = n1 =     Z ∗ . Entonces, por el supuesto y la parte b) de este
       teorema,                                             !
                                     1/2
                                nj             d         1
                                           Zj∗ −
                                               → N1 0, 2
                               n · vj                    σj
                             1/2
                        1 ˜             d
       Por tanto,       n=           Z∗ −
                                        → NJ (0, Ξ̃), cuando n → ∞, y J fijo.

   Algunas observaciones importantes son las siguientes:

  1. El supuesto del teorema 5 implica que
                                                1˜     n→∞
                                                  =   −−−−→      Ξ̃                                      (4)
                                                n

  2. Los vectores score Si (p) correspondientes a las observaciones i son inde-
     pendientes, pero sus distribuciones no son idénticas ya que dependen de la
     población j a la cual i pertenece.
                           n                             o
  3. La matriz =˜ i = diag 0, . . . , 0, 1 , 0, . . . , 0 , que se refiere a una observación
                                         vj
     Yi en j, no es definida positiva. En cambio, la matriz =          ˜ se refiere a toda la
     muestra y siempre es definida positiva.
  4. Cuando se trabaja con el modelo saturado, se tiene el caso de utilizar datos
     agrupados porque las observaciones Yi , i = 1, . . . , n se reúnen en J gru-
     pos (poblaciones) de tamaño nj , j = 1, . . . , J. En el caso especial nj = 1,
     j = 1, . . . , J (lo que implica que J = n) se habla de datos no agrupados.
     La distinción entre datos agrupados y no agrupados es importante por dos
     razones:


                                                Revista Colombiana de Estadística 29 (2006) 239–265

Precisiones en la teoría de los modelos logísticos                                    247


            Algunos métodos de análisis apropiados para datos agrupados no son
            aplicables a datos no agrupados.
            Las aproximaciones asintóticas para datos agrupados pueden basarse
            en uno de estos dos casos distintos: nj → ∞ (n → ∞) o J → ∞. El
            último caso es apropiado únicamente para datos no agrupados.
   5. El supuesto del teorema 5 se puede interpretar de la siguiente manera: “la
      velocidad” de cada nj → ∞ debe ser la misma que la de n → ∞. Por ejemplo,
      en un diseño balanceado todas las nj son iguales. En este caso, nj = Jn . Por lo
                              n      1
      tanto, la cantidad n1 · vjj = J·vj
                                         es fija; es decir, no depende de n. Utilizando
      las notaciones del teorema anterior, se tendría que σj2 = J · vj porque, en
      este caso, la expresión (4) se convierte en una igualdad de la forma n1 =  ˜ = Ξ̃.

   6. En la práctica se puede suponer que el supuesto del teorema 5 siempre se
      cumple. Pero es importante resaltar que J debe ser fijo. Esta situación se
      presenta cuando se tienen datos agrupados con J fijo. Por esta razón, debe
      tomarse como “base” el modelo saturado. Es decir, se empieza con el score
      de la muestra usando los vectores Zj , donde j = 1, . . . , J.
      Además, si J → ∞ (por ejemplo, si J = n), entonces en el modelo satu-
      rado no se puede considerar a J como fijo. Obsérvese que esta situación se
      presenta cuando se tienen datos no agrupados. En este caso, no se puede to-
      mar como “base” el modelo saturado. Ahora se empezaría con el score de la
      muestra utilizando, de una vez, las observaciones Yi , i = 1, . . . , n. De ahora
      en adelante, cuando se trabaje con aproximaciones asintóticas para:
            Datos agrupados y no agrupados, se utilizará únicamente la notación
            n → ∞.
            Datos agrupados, esta misma expresión pero acompañada de la ex-
            presión J es fijo. Esto es con el fin de enfatizar que el tamaño de la
            población J es fijo.
            Datos no agrupados, la notación J → ∞ en vez de n → ∞. Esto es con
            el fin de enfatizar que el tamaño de la población J no es fijo.
Teorema 6. Considerando las notaciones z, m y V de la sección 2.3 y C de la
sección 2.4, se tiene en un modelo logístico:

  a) El vector (aleatorio) score de la muestra es S(α) := ∂L    T
                                                          ∂α = C (Z − m), un
     vector columna de tamaño 1 + K. Además, E(S(α)) = 0.
                                                                  
  b) La matriz de información de la muestra es =(α) := Cov ∂L  ∂α   = C T V C,
     matriz de (1 + K) × (1 + K). Para mayor simplicidad, sea = := =(α).
        2 
         ∂ L       ∂2L
  c) E          =       = −=.
         ∂α2       ∂α2

    Para el caso particular de datos no agrupados, en donde nj = 1, ∀j y J = n,
se tiene que: Z = Y , m = (p1 , . . . , pn )T , C es la matriz de diseño original de
n × (1 + K) y V = diag{v1 , . . . , vn }.


                                       Revista Colombiana de Estadística 29 (2006) 239–265

248                                                                 Humberto Jesús Llinás


Demostración. Solo debemos aplicar resultados básicos del cálculo y del álgebra
lineal y, obviamente, de la estadística. Cada uno de estos resultados se relaciona
con Dobson (2002).
Teorema 7. Considerando los supuestos de las secciones 2.3 y 2.4, se tiene:

  a) La matriz = es de rango completo Rg(=) = 1 + K y definida positiva.
                              nj
  b) Supóngase que lı́m               = σ12 > 0 existe. Entonces, existe una matriz Ξ,
                       nj →∞ n·vj         j

      definida positiva y de tamaño (1 + K) × (1 + K), tal que
                         1     d
                        √ S(α) −
                               → N1+K (0, Ξ),           n → ∞,     J fijo
                          n

    Para el caso de datos no agrupados, en donde J = n, el supuesto dado en b)
no tiene sentido porque, como se explicó al final de la sección anterior, J no es
fijo. Por lo tanto, en vez de esa condición, se debe suponer la existencia de una
matriz definida positiva Ξ tal que J1 = J→∞
                                        −→ Ξ. De esta forma, se tiene

                           1     d
                          √ S(α) −
                                 → N1+K (0, Ξ),          J →∞
                            J
Demostración.

  a) Se sabe que la matriz = es de tamaño (1 + K) × (1 + K).
      Entonces,

                  Rg(=) = Rg([V 1/2 C]T · [V 1/2 C]) = Rg(V 1/2 C) = 1 + K

      Por consiguiente, = es de rango completo 1 + K. Demostremos que = es
      definida positiva. En efecto, para cualquier vector columna u 6= 0, de tamaño
      (1 + K) siempre se cumple que

                    0 ≤ (V 1/2 Cu)T · (V 1/2 Cu) = uT (C T V C)u = uT =u

      Ahora, considerando el hecho de que V 1/2 es invertible y C de rango com-
      pleto,

         (V 1/2 Cu)T · (V 1/2 Cu) = 0 ⇐⇒ V 1/2 Cu = 0 ⇐⇒ Cu = 0 ⇐⇒ u = 0.

      Esto contradice el hecho de que u 6= 0. Por tanto, = es definida positiva.
  b) Se considera cualquier vector λ := (λo , . . . , λK )T de números reales. Se de-
     mostrará, a continuación, que
                                                  n
                                       1        1 X T
                           λT ·       √ S(α) = √       λ · Si (α)
                                        n        n i=1


                                         Revista Colombiana de Estadística 29 (2006) 239–265

Precisiones en la teoría de los modelos logísticos                                         249


      tiene distribución asintótica normal 1-dimensional. Para ello se aplicará el
      teorema de Lindberg (véase, por ejemplo, Rao 1973 pp. 123(xi), 128(iii),
      128(iv))

      En efecto,
            E(λT · Si (α)) = 0
            La matriz de información =i (para una observación Yi en la población
            j) correspondiente al modelo logístico viene dada por:
                     =i     = Cov{Si (α)} = E{Si (α) · [Si (α)]T } = C ∗ · vj
            siendo vj = V (Yi ) y
                                                                            
                                      x2oj         xoj x1j     . . . xoj xKj
                                    x1j xoj        x21j       . . . x1j xKj 
                                                                            
                            C ∗ :=     ..            ..       ..        ..  
                                        .             .           .      .  
                                                                        2
                                     xKj xoj       xKj x1j     ...    xKj
            A diferencia del modelo saturado, la matriz de información
                                                              P         =i sí es
            definida positiva puesto que = también lo es y = = =i . Por tanto,
                                                                            i
                            T                T                          T
                       V (λ · Si (α)) = λ · Cov(Si (α)) · λ = λ · =i · λ > 0
                                           ˜ ∗ , donde V ∗ = diag{v1 , . . . , vJ }. Por
            Es fácil verificar que V = V ∗ =V
            consiguiente, por el teorema 6b, la ecuación (4) y la expresión anterior:
                          1             1˜          n→∞
                            = = CT V ∗ · = · V ∗ C −−−−→ C T V ∗ · Ξ̃ · V ∗ C
                          n             n
            siendo Ξ̃ como en el teorema 5b. Es decir,
                                                 1 n→∞
                                                   = −−−−→ Ξ                               (5)
                                                 n
            donde Ξ := C T V ∗ · Ξ̃ · V ∗ C. La expresión que aparece en la ecuación
                                                      n
            (5) se cumple siempre y cuando lı́m n·vjj = σ12 > 0 exista. La matriz
                                                    nj →∞           j

            Ξ también es de rango completo porque
                 Rg(Ξ) = Rg(C T · V ∗
                                  | {zΞ̃V ∗} ·C) = Rg([D1/2 C]T [D1/2 C]) = 1 + K
                                         D

            Mediante un razonamiento análogo a la demostración de la parte a)
            del teorema 7, se obtiene que Ξ es definida positiva.1 Considerando la
            ecuación (5) y sabiendo que Ξ es definida positiva, se tiene que
                    n                           
                 1X        T             T    1         n→∞
                       V (λ · Si (α)) = λ ·     = · λ −−−−→ λT · Ξ · λ > 0
                 n i=1                        n
   1 Para el caso no agrupado, debe suponerse en seguida la existencia de una matriz Ξ definida
                     J →∞
positiva tal que J1 = −−−−→ Ξ.


                                         Revista Colombiana de Estadística 29 (2006) 239–265

250                                                                    Humberto Jesús Llinás


           Se verificará la condición de Lindberg. Es decir, para cada ε > 0,

                  1X
                        n                                               
                                                2                          n→∞
                        EλT ·S (α) [λT · Si (α)] · 1{[λT ·S (α)]2 >ε2 ·n} −−−−→ 0
                  n i=1       i                            i




           donde EλT ·S (α) significa que se calcula la esperanza con base en la
                        i                hP              i
                                             K
           distribución de λT · Si (α) =     k=0 k kj (Yi − pj ). Suponiendo que
                                                  λ  x
           las xkj están acotadas uniformemente con respecto a j (que no parece
           restricción alguna para la práctica), entonces existe un N = N (ε) tal
                                   hP            i2
                                      K
           que para cada n ≥ N ,      k=0 λk xkj    (yi − pj )2 ≤ ε2 n. De esta forma,
           se cumple la condición de Lindberg, porque

                  1X
                        N                      2
                                                                         
                                                                           n→∞
                        EλT ·S (α) [λT · Si (α)] · 1{[λT ·S (α)]2 >ε2 ·n} −−−−→ 0
                  n i=1       i                            i




           ya que la suma anterior no depende de n.
      Aplicando el teorema de Lindberg, se tiene
                                     n
                           1       1 X T             d
               λT ·       √ S(α = √                  → N1 (0, λT · Ξ · λ),
                                          λ · Si (α) −
                            n       n i=1

      Por consiguiente, al aplicar el teorema central del límite multivariado (véa-
      se, por ejemplo, Rao 1973 pp. 123(xi), 128(iii), 128(iv)), se concluye que
      √1 S(α) −→
               d
                 N1+K (0, Ξ), cuando n → ∞.
        n



3.    Existencia y cálculos de parámetros logísticos
    El método que se propone para calcular las ML-estimaciones en un modelo
logístico es el método iterativo de Newton-Raphson, como se muestra en el siguiente
teorema:
Teorema 8 (Teorema de existencia). Las ML-estimaciones α̂ de α existen, son
únicas y se calculan según la siguiente fórmula de recursión:

           α̂(0) = 0,        α̂(t+1) := α̂(t) + (C T V̂ (t) C)−1 · C T (z − m̂(t) )

  Además, asintóticamente se tiene
                                       
√             a         1 ∂L          1 ∂L     √
 n · (α̂ − α) = Cov −1 √ ·         · √ ·      = n · (C T V C)−1 · C T (z − m)
                         n ∂α          n ∂α

   Para el caso particular de datos no agrupados, en donde J = n, se tiene que:
z = y y m, C y V son como se explicó al final del teorema 6.


                                         Revista Colombiana de Estadística 29 (2006) 239–265

Precisiones en la teoría de los modelos logísticos                                             251


Demostración. En la parte a) del teorema 7 se demostró que la matriz
∂2 L
∂α2 = −= es de rango completo 1 + K. Por lo tanto, existen únicamente las
ML-estimaciones α̂ como soluciones de las 1 + K ecuaciones ∂L(α)
                                                               ∂α = 0. O, lo que
es lo mismo por la parte a) del teorema 6, teniendo C T (z − m) = 0. Por lo tanto,
debe cumplirse que
                                    ∂L(α̂)
                                           =0                                  (6)
                                     ∂α
   Ahora, del teorema 6, se tiene para k = 0, . . . , K fijo, que
                                 XJ                           
                           ∂L                          1
                               =     xkj zj − nj
                           ∂βk   j=1
                                                 1 + exp{−~j }

donde ~j es la suma del lado derecho de la ecuación (3). Esta última expresión
no es lineal en los parámetros βk . Por tanto, se requiere un método aproximativo
que se motiva por la siguiente aproximación de Taylor. Si α1 es un punto que está
entre α y α̂, entonces
                           ∂L(α)         ∂L(α̂) ∂ 2 L(α1 )
                                     =         +           · (α − α̂)
                            ∂α            ∂α       ∂α2
   Considerando la ecuación (6), esta expresión se puede reescribir como
                                 −1
                       ∂ 2 L(α1 )       ∂L(α)
       α̂ − α =      −                ·       = (C T V1 C)−1 · C T (z − m)
                          ∂α2            ∂α
siendo V1 := V (α1 ).
    Como α1 es un punto del segmento de línea que une a α y α̂, entonces α1 =
tα̂+(1−t)α, para un t ∈ [0, 1]. Bajo el supuesto de que α̂ es fuertemente consistente
                                                    c.s
para α (es decir, por componentes se cumple que α̂ −→    α, cuando n → ∞), se
              c.s                                      P
tiene que α1 −→ α. Por lo tanto, por componentes, α1 −→ α cuando n → ∞.
    Esto implica que, por componentes,

                                                                       #
                −1                             −1
     T 1                1 T             T  1               1 T          P
 C · V1 · C          · √ C (z − m) − C · V · C          · √ C (z − m) − →0
        n                n                 n                n
  |                {z              }
                    √
               =     n·(α̂−α)


    Por tanto,
                −1                                                    −1
         1              1                       a               1                 1
    C T · V1 · C     · √ C T (z − m)            =        CT ·     V ·C         · √ C T (z − m)
         n               n                                      n                  n
  |               {z               }                 |                      {z               }
                    √                                           √
                =       n·(α̂−α)                            =       n·(C T V C)−1 ·C T (z−m)


   De este resultado se obtiene una forma aproximada para:
                                α̂ ∼
                                   = α + (C T V C)−1 · C T (z − m)

                                           Revista Colombiana de Estadística 29 (2006) 239–265

252                                                                  Humberto Jesús Llinás


   Reemplazando en el lado derecho α por la t-ésima aproximación α̂(t) de α se
obtiene la fórmula de recursión que da la (t + 1)-ésima aproximación α̂(t+1) de α̂,
como se indica en la formulación del teorema.


4.    Distribuciones asintóticas
                                           n
Teorema 9. Supóngase que lı́m n·vj j = σ12 > 0 existe. Entonces existe una
                 n                onj →∞         j

                   1            1
matriz Ξ̃ := diag σ2 , . . . , σ2 definida positiva tal que
                     1       J

                    √          d
                               → NJ (0, Ξ̃−1 ),
                     n(p̃ − p) −                        n → ∞,    J fijo,
siendo p̃ la estimación de p en el modelo saturado.

                                                            √            −1/2
Demostración. Considerando el teorema 4c), 4f),                             ˜
                                                             n(p̃ − p) = n1 =   Z ∗.
   Ahora, mediante un procedimiento análogo al de la demostración de la parte
                               −1/2
                                 ˜
b) del teorema 5, se tiene que 1 =    Z ∗ −→
                                           d
                                             NJ (0, Ξ̃−1 ), cuando n → ∞ y J
                                    n
fijo.
     Con este resultado y la igualdad anterior, el teorema queda demostrado.

   El teorema 9 no es válido para datos no agrupados, en donde J = n. Esto se
debe a que, si J no es fijo, no tiene sentido hablar de una aproximación asintóti-
camente normal (J-dimensional), cuando J → ∞.
Corolario 1. Para datos no agrupados (aquí se supone la existencia de una matriz
= definida positiva tal que J1 = J→∞
                                 −→ =), son válidas las siguientes afirmaciones:

  a) Si X ∗ := =−1/2 · C T (Y − m), entonces X ∗ −→
                                                  d
                                                    N1+K (0, I),             J → ∞.
     √
  b) J(α̂ − α) −→d
                     N1+K (0, Ξ−1 ), J → ∞.

  c) =1/2 (α̂ − α) −→
                    d
                      N1+K (0, I),         J → ∞.
     β̂k −βk
  d) √        d
             −→ N1 (0, 1),       k = 0, 1, . . . , K,   βo = δ,   J → ∞.
        V̂ (β̂k )

donde C, m, = son como se describieron en la observación del teorema 6; V̂ (β̂k ) la
varianza estimada (por eso V̂ ) de β̂k y corresponde al k-ésimo elemento diagonal
de la matriz de covarianzas estimada de α̂, Ĉov(α̂). Recuerde que la expresión
“estimada” significa que en V (β̂k ), los parámetros se reemplazan por estimaciones
consistentes.
    Este teorema también es válido para datos agrupados, en donde J es fijo. En
                                        n
este caso debe suponerse que lı́m n·vjj = σ12 > 0 existe y, además, se tiene que:
                                 nj →∞             j


   Y = Z, m = (n1 p1 , . . . , nJ pJ )T , C es la matriz de diseño de J × (1 + K)
= = C T V C con V = diag{n1 v1 , . . . , nJ vJ }.


                                         Revista Colombiana de Estadística 29 (2006) 239–265

Precisiones en la teoría de los modelos logísticos                                      253


Demostración.

  a) De la observación del teorema 6, para datos no agrupados (en donde J = n),
     se tiene que

                                                                −1/2
                                                           1                1
                         X ∗ = =−1/2 · S(α) =                =           · √ S(α)
                                                           J                 J
      Al considerar la ecuación (4), se tiene que
                                             −1/2
                                        1             J→∞
                                          =           −−−−→ Ξ−1/2
                                        J

      Además, por la observación del teorema 7, es válido que

                         1     d
                        √ S(α) −
                               → N1+K (0, Ξ),                  cuando J → ∞
                          J
      Entonces,
                                d
                           X∗ −
                              → N1+K (0, I),               cuando        J →∞

  b) De los teoremas 8 y 6a se tiene que
                                               −1        
                             √          a   1          1
                              J(α̂ − α) =     =       √ S(α)
                                            J           J

      Al considerar la parte b) del teorema 7 y resultados conocidos de la teoría
      de la probabilidad y de la normal multivariada, se puede concluir, mediante
      un razonamiento análogo al elaborado en la demostración del teorema 9, que
                     √          d
                                → N1+K (0, Ξ−1 ),
                      J(α̂ − α) −                                  cuando    J →∞

  c) Se obtiene inmediatamente de a) y del teorema 8.

                                                  β̂k − βk
  d) Se sigue de c), considerando el hecho de que q           tiene media cero y
                                                     V (β̂k )
      varianza 1 y que, al reemplazar en V (β̂k ) los parámetros por estimaciones
      consistentes, el resultado queda válido asintóticamente.

    Obsérvese que los resultados que se dieron en el teorema 9 y en el corolario 1b)
son bastante similares, lo que significa que tienen la misma interpretación tanto
en los modelos logísticos como en los saturados.
                                               nj
Corolario 2. Supóngase que lı́m                     = σ12 > 0 existe. Entonces, asintótica-
                                    nj →∞ n·vj             j
mente tenemos
  b) Para cada subparámetro γ de α, de dimensión s,
                                                          d
                                                        → χ2 (s),
                    (γ̂ − γ)T · Ĉov −1 (γ̂) · (γ̂ − γ) −            n→∞


       siendo Ĉov(γ̂) la matriz de covarianzas estimada de la estimación γ̂.
  c)
                (β̂k − βk )2 d 2
                             → χ (1),
                             −            k = 0, 1, . . . , K,   βo = δ,   n→∞
                   V̂ (β̂k )
Observación. Todos los resultados del teorema se cumplen para datos no agru-
pados, a pesar de que el supuesto no es válido, como se explicó al final de la
observación que aparece antes del teorema 6.

Demostración. Considerando la parte b) del corolario 1, para el   √ caso de datos
no agrupados, se tiene que la matriz de covarianzas asintótica de n · α̂ es Ξ−1 .
   El resultado quedará válido si se pasa a la matriz de covarianzas estimada
                                        √
                                   Ĉov( n · α̂)

                       √           √                                               d
   Sea B := Ĉov −1/2 ( n · α̂) · [ n · (α̂ − α)], por tanto, del corolario 1b), B −
                                                                                   →
N1+K (0, I), cuando n → ∞ ya que se obtiene a Ξ1/2 · Ξ−1 · Ξ1/2 = I como matriz
de covarianzas asintótica.
   Por consiguiente,

         (α̂ − α)T · Ĉov −1 (α̂) · (α̂ − α) = B T B −→ χ2 (1 + K),        n → ∞,

ya que es la suma de los cuadrados de 1 + K variables normales estandarizadas e
independientes de un grado de libertad. De esta forma, se obtiene el resultado a).
La parte b) es un caso particular de a) y la c), de b).


5.     Pruebas de comparación de modelos y selección
       de modelos
    Con base en la teoría asintótica para las ML-estimaciones y el vector score, en
esta sección se encuentran aproximaciones para las diferentes desviaciones −2L(θ̂).
A partir de ellas se obtienen estadísticas para distintas pruebas de comparación
de modelos (logístico vs. saturado, submodelo vs. logístico y nulo vs. logístico),
con distribución asintótica chi-cuadrada. Estas pruebas de hipótesis sirven como
criterios para escoger uno o varios submodelos de un modelo logístico sin perder
información estadísticamente significativa.


                                        Revista Colombiana de Estadística 29 (2006) 239–265

Precisiones en la teoría de los modelos logísticos                                           255


5.1.    Comparación de un modelo logístico con el modelo
        saturado correspondiente
                                                  nj
Teorema 10. Supóngase que lı́m                          = σ12 > 0   existe. Entonces
                                    nj →∞ n·vj              j


                                                 T                 
                                    a        ∂L           −1   ∂L      ∂L
                2[L(p̃) − L(p)] =                   · Cov           ·
                                             ∂p                ∂p      ∂p

donde p̃ es la ML-estimación de p en el modelo saturado.

Demostración. Se sabe por la aproximación de Taylor que, para cualquier par
de puntos p̃ y p, existe un p1 , que está entre ellos dos, tal que:
                                       T
                               ∂L(p̃)                  1            ∂ 2 L(p1 )
         L(p) = L(p̃) +                      (p − p̃) + (p − p̃)T ·            · (p − p̃)
                                ∂p                     2               ∂p2

   Pero ∂L(
         ∂p
            p̃)
                = 0 porque la ML-estimación p̃ es una solución del sistema de
ecuaciones ∂L(p)
            ∂p    = 0. Por consiguiente, de la expresión anterior y de la parte c)
del teorema 4, se obtiene que
                                        c.s.     Z
   Por la ley fuerte de los grandes números, p̃j = njj −→   pj , cuando nj → ∞, para
                                                                            c.s.
cada j. Por el supuesto adicional, vale lo mismo para n → ∞, es decir, p̃ −→     p,
                                                    c.s                      
   Por otra parte, por los teoremas 9 y 4,

   Teniendo en cuenta las ecuaciones (7), (8) y el resultado anterior, se obtiene

   En el siguiente teorema se encontrará, para datos no agrupados (en donde
J = n), una expresión que tiene la misma estructura que la del teorema 10. A
pesar de ello, el teorema anterior es válido solo cuando J es fijo. Por eso, falla el
caso de datos no agrupados.
Teorema 11. Para los casos de datos agrupados y no agrupados, tenemos:
                                 T                   
                           a   ∂L           −1   ∂L      ∂L
           2[L(α̂) − L(α)] =          · Cov           ·
                               ∂α                ∂α      ∂α
donde α̂ es una ML-estimación consistente de α en el modelo logístico.

Demostración. Se siguen los pasos de la demostración del teorema 10 sin deta-
llarlos todos. Así, para cualquier par de puntos α̂ y α, existe un α2 , que está entre
ellos dos, tal que
                                T
                          ∂L(α̂)              1            ∂ 2 L(α2 )
        L(α) = L(α̂) +              (α − α̂) + (α − α̂)T ·            · (α − α̂).
                           ∂α                 2               ∂α2
                                   2         
                                       L(α2 )
   Pero 2[L(α̂)−L(α)] = (α̂−α)T · − ∂ ∂α 2      ·(α̂−α) porque α̂ es ML-estimación
de α. Aquí se necesita hacer el supuesto de que α̂ sea consistente para α. Así se
cumple que α2 también es consistente para α. Con lo anterior se obtiene que
                       2                2          
                    1     ∂ L(α2 ) a 1        ∂ L(α) a 1
                        −           =      −             = =
                    n       ∂α2        n         ∂α2       n

   Usando ahora el teorema 8 y el corolario 1, con n en lugar de J, se obtiene el
resultado en plena analogía con la demostración del teorema 10.
Teorema 12. La LR-estadística de prueba (según el método de cocientes de fun-
ciones de verosimilitud) para la hipótesis H0 : el modelo logístico (con X1 , . . . , XK ),
vs. la alternativa H1 : el modelo saturado correspondiente con sus J poblaciones,
es equivalente a la llamada desviación que tiene el modelo logístico del modelo
saturado:                                   


con las siguientes características:
               a
  a) D∗ (M ) = (Z ∗ )T (IJ − PJ )(Z ∗ ), bajo H0 .

  b) D∗ (M ) −→
              d
                χ2 [J − (1 + K)],        bajo H0 ,    n → ∞,      J fijo.

donde p̃ y α̂ son los vectores de las ML-estimaciones de los modelos saturado y
logístico, respectivamente; Z ∗ es el vector definido en el teorema 4 y PJ es la matriz
definida en el teorema 3. Además, se hacen los supuestos de los teoremas 10 y 11.

Observación. Nótese que aquí se requiere que J > 1 + K. Para el caso en que
J = 1+K, el análisis en un modelo logístico es el mismo que en el modelo saturado.
Esta prueba únicamente se cumple para datos agrupados porque J es fijo, lo que
no sucede para el caso de datos no agrupados.

Demostración.    a) Se puede demostrar que ∂L          T 1/2 ∗
                                                ∂α = C V      Z es un vector
   columna de tamaño 1 + K. Ahora, bajo H0 , vale que L(p) = L(α). Por lo
   tanto, bajo H0 , y teniendo en cuenta los teoremas 4b), 4f), 6b), 10 y 11, se
   tiene que

  b) Como la matriz IJ − PJ es una proyección con rango J − (1 + K), entonces
     (ver Rao (1973, cáp. 1)) existe una matriz ortogonal U de m × J tal que
     IJ − PJ = U T U con m = J − (1 + K). Considerando lo anterior y el teo-
     rema 5b) y resultados conocidos relacionados con la normal multivariada,
      se tiene que U Z ∗ −→   d
                                  Nm (0, I), cuando n → ∞ y J fijo. Por consiguiente,
                        ∗ T
      se tiene (U Z ) (U Z ) −→ ∗  d
                                       χ2 (m), cuando n → ∞ y J fijo, sabiendo que
      m = J − (1 + K) y que las componentes (U Z ∗ )l del vector (U Z ∗ ), con
      l = 1, . . . , m, son independientes entre sí. La parte b) queda completamente
      demostrada si se considera que bajo H0 se cumple que

    Se espera que la prueba del teorema 12 no rechace H0 (p-valor alto), o sea
que los datos obtenidos no estén en contra del modelo logístico. Es decir, al pasar
del modelo saturado al modelo logístico no se pierde información estadísticamente
significativa.


5.2.     Comparación de un modelo logístico con algún
         submodelo
Teorema 13. Para la situación de hacer la hipótesis H0 : un submodelo logístico
con X1 , . . . , XK̃ , vs. la alternativa H1 : el modelo logístico con X1 , . . . , XK con
K̃ < K, son válidas las siguientes afirmaciones:

  b) Cov       ∂αo       = T T T es una matriz cuadrada de tamaño 1 + K̃,

siendo T := To =1/2 una matriz de (1 + K̃) × (1 + K); α el vector de los 1 + K
parámetros en el modelo logístico y αo el vector α restringido bajo H0 : β1+K̃ =
. . . = βK = 0.

Demostración. Solo debemos considerar la regla de la cadena y el teorema 6b).


Teorema 14. La LR-estadística de prueba para la situación señalada en el teorema
13 es equivalente a la estadística
                                                        
                            ∗                  L(α̂)
                          D (L) := 2 log                     = 2[L(α̂) − L(α̂o )]
                                               L(α̂o )
   con las siguientes características:
                a
  a) D∗ (L) = (X ∗ )T (I1+K − P1+K )(X ∗ ), bajo H0 .

  b) D∗ (L) −→
             d
               χ2 [K − K̃],          bajo H0 ,          J →∞
       donde:
           α̂ = (δ̂, β̂1 , . . . , β̂K )T es la ML-estimación en el modelo logístico de la
           alternativa H1 ;
           α̂o = (δ̂o , β̂o1 , . . . , β̂oK̃ )T es la ML-estimación en el submodelo logístico
           de la hipótesis Ho ;
           X ∗ es el vector definido en el corolario 1a;
           P1+K := T T · (T · T T )−1 · T y T es la matriz definida en el teorema 13.


  Para la situación anterior, una estadística asintóticamente equivalente es la de
Wald:
                          d
                              → χ2 [K − K̃], bajo H0 , J → ∞ donde γ̂ es la estimación
  c) γ̂ T · Ĉov −1 (γ̂) · γ̂ −
     de γ, que es la parte (K − K̃)-dimensional del vector α que se anula bajo H0
     y Ĉov(γ̂) es la matriz de covarianzas estimada de γ̂.

Observación. Nótese que la hipótesis de la primera parte del teorema es equi-
valente a la hipótesis Ho : γ = 0. Es importante señalar que esta prueba que
presenta el teorema utiliza datos no agrupados. Aunque también es posible reali-
zarla teniendo en cuenta el modelo saturado. Pero como en la prueba únicamente
se considera el modelo logístico, no tiene mucho sentido comparar este con un
submodelo teniendo que pasar por el modelo saturado.

Demostración.         a) En forma análoga al teorema 11 se tiene que
                                                    T                               
                                      a       ∂L                  −1       ∂L        ∂L
                  2[L(α̂o ) − L(αo )] =                   · Cov                   ·          (9)
                                              ∂αo                          ∂αo       ∂αo

      Por lo tanto,

                D∗ (L)    = 2[L(α̂) − L(α)] − 2[L(α̂o ) − L(αo )]
                                T                                     
                              ∂L       −1         T     T −1         ∂L
                          =            = − (To ) (T T ) To ·
                              ∂α                                      ∂α
                                T                                     
                              ∂L                                      ∂L
                          =           =−1/2 ·[I1+K − P1+K ] · =−1/2
                              ∂α                                      ∂α
                            |      {z     }                   |    {z     }
                                    (X ∗ )T                                        (X ∗ )


  b) Se puede demostrar que la matriz I1+K − P1+K es una proyección con rango
     K − K̃. Por consiguiente (Rao 1973, cáp.1), existe una matriz ortogonal U
     de m × J tal que I1+K − P1+K = U T U con m = K − K̃. Ahora, considerando
      el corolario 1a), se tiene que U X ∗ −→
                                            d
                                              Nm (O, I), bajo H0 , cuando J → ∞.
      Por tanto, (U X ∗ )T (U X ∗ ) −→d
                                         χ2 (m), bajo H0 , cuando J → ∞, sabiendo
      que m = K − K̃ y que las componentes (U X ∗ )l del vector (U X ∗ ), con
      l = 1, . . . , m, son independientes entre sí. La parte b) queda completamente
      demostrada si se considera que bajo H0 ,
                           a
                   D∗ (L) = (X ∗ )T (I1+K − P1+K )(X ∗ ) = (U X ∗ )T (U X ∗ )
                                     |    {z   }
                                              UT U


  c) Se sabe que γ es un subparámetro de α con dimensión K − K̃. Entonces, por
     el corolario 2b), se tiene (γ̂ − γ)T · Ĉov −1 (γ̂) · (γ̂ − γ) −→
                                                                     d
                                                                       χ2 [K − K̃], cuando
     J → ∞. Bajo H0 : γ = 0, esta expresión quedará reducida a la que aparece
     en el teorema y con esto queda demostrada la parte c).


                                          Revista Colombiana de Estadística 29 (2006) 239–265

260                                                                   Humberto Jesús Llinás


    Si en el trabajo práctico se ha llegado a un submodelo del modelo logístico
inicial mediante un proceso de eliminación de variables explicativas, entonces se
espera que la prueba dada en el teorema 14 no rechace Ho (p-valor alto). Se espera
esto para poder reemplazar el modelo inicial por el submodelo. En caso contrario,
la reducción significaría una pérdida de información estadísticamente significativa.


5.3.     Comparación de un modelo logístico con el nulo
Corolario 3. Para la hipótesis H0 : el modelo nulo (sólo con el intercepto), vs. la
alternativa H1 : el modelo logístico (con X1 , . . . , XK ), se puede tomar, alternati-
vamente, una de las dos estadísticas de pruebas siguientes:
                   
             L(α̂)
   a) 2 log L( δ̂ )
                                            d
                      = 2[L(α̂) − L(δ̂o )] −→ χ2 [K], bajo H0 , J → ∞
                 o


       donde δ̂o = logit(Y ) es la estimación de δ en el modelo nulo.

  b) β̂ T · Ĉov −1 (β̂) · β̂ −→
                               d
                                 χ2 [K],    bajo H0 ,   J → ∞,

       donde β̂ es la ML-estimación de β = (β1 , . . . , βK )T que corresponde a la
       parte de α que se anula bajo H0 y Ĉov(β̂) es la matriz de covarianzas esti-
       mada de β̂.

Observación. Nótese que la hipótesis de la primera parte del teorema es equiva-
lente a la hipótesis Ho : β = 0. Como esta prueba es un caso particular del teorema
14, también se está considerando el caso de datos no agrupados.

Demostración. La parte a) es un caso particular del teorema 14 con K̃ = 0. En
este caso, α̂o = δ̂o . Ahora se demostrará la parte b). β es un subparámetro de α con
dimensión K. Por el corolario 2b), se tiene (β̂ − β)T · Ĉov −1 (β̂) · (β̂ − β) −→
                                                                                 d
                                                                                   χ2 [K],
cuando J → ∞. Bajo H0 : β = 0, esta expresión quedará reducida a la que aparece
en el teorema y con esto queda demostrada la parte b).

   En el trabajo práctico se espera que la prueba del teorema 3 sí rechace Ho (p-
valor bajo). Es decir, que las K variables explicativas del modelo logístico tienen,
en su conjunto, una explicación más informativa que solo el intercepto. En caso
contrario, que no es muy común en problemas prácticos, se tendría que chequear
otro modelo logístico con más o con otras variables.


5.4.     Comparación de un modelo logístico con un submodelo
         que tiene una variable explicativa menos
Corolario 4. Para la hipótesis H0 : el submodelo con una variable menos (es decir,
con X1 , . . . , Xk−1 , Xk+1 , . . . , XK ), vs. la alternativa H1 : el modelo logístico (con
X1 , . . . , XK ), se puede tomar, alternativamente, una de las dos estadísticas de
pruebas siguientes:
       siendo β̂k la estimación de βk , para cada k = 0, 1, . . . , K en el modelo
       H1 ) con su varianza estimada V̂ (β̂k ) y su error estándar SE(β̂k ) =                             V̂ (β̂k ).

Observación. Como esta prueba es un caso particular del teorema 14, también
se está considerando el caso de datos no agrupados.

Demostración. La parte a) es un caso particular del teorema 14b) con K̃ = K−1.
Es decir, con K − K̃ = 1. La parte b) es un caso particular del corolario 2c).

Observación. Con base en todas las “pruebas parciales” del teorema 4, para cada
k = 0, 1, . . . , K se eliminará la variable explicativa que menor aporte individual
tenga en la explicación. Es decir, la variable que tenga p-valor parcial más alto. Así,
se sigue eliminando variable tras variable hasta que se rechacen todas las pruebas
parciales (todas las que tengan p-valores bajos).


5.5.     Análisis de desviaciones (ANODEV)
    Con el fin de analizar la bondad de ajuste de un modelo logístico fijo (con
variables explicativas X1 , . . . , XK que definen J poblaciones), se lo compara hacia
los dos lados, es decir, con el modelo saturado correspondiente (con estas J pobla-
ciones) y con el modelo nulo (con solo el intercepto). Para esta situación, puede
orientarse en la llamada tabla de ANODEV (en inglés: ANalysis Of DEViance),
en analogía a la ANOVA para modelos lineales (tabla 1).

                        Tabla 1: ANODEV del modelo logístico vs. saturado.

       Teorema                                           DF                       Estadística
        5.3.1                 Diferencia de               K          D ∗ (0) − D ∗ (M ) = 2[L(α̂) − L(δ̂o )]
                              desviaciones
                          Desviación del
        5.1.5                                     J − (1 + K)             D ∗ (M ) := 2[L(p̃) − L(α̂)]
                          modelo logístico

                         Desviación total
                                                        J −1              D ∗ (0) := 2[L(p̃) − L(δ̂o )]
                        (del modelo nulo)



    Una estadística de desviación relativa con respecto al modelo saturado es la
siguiente, propuesta en la mayor parte de la literatura:

                                              D∗ (0) − D∗ (M )    D∗ (M )
                                   RDS :=            ∗
                                                               =1− ∗
                                                   D (0)          D (0)

                                                       Revista Colombiana de Estadística 29 (2006) 239–265

262                                                                 Humberto Jesús Llinás


    Esta desviación relativa la analizaron por Theil (1970) y Goodman (1971) y
tiene algunas características:

  a) 0 ≤ RDS ≤ 1.


  b) Cuando el modelo logístico no mejora en ajuste con respecto al modelo nulo:
     RDS = 0 si y solo si 0 < D∗ (0) = D∗ (M ) si y solo si L(p̃) > L(δ̂o ) = L(α̂).


  c) Cuando el modelo logístico se ajusta tan bien como el modelo saturado:
     RDS = 1 si y solo si 0 = D∗ (M ) < D∗ (0) si y solo si L(p̃) = L(α̂) > L(δ̂o ).


  d) Al eliminar una variable explicativa (disminuir K) se disminuye el numera-
     dor, pero, en general, se disminuye también el denominador (ya que disminu-
     ye J). De esta forma, RDS puede disminuir o aumentar al eliminar variables
     explicativas, siendo esto último el gran defecto que tiene la desviación rela-
     tiva.

    Ahora, si se quiere comparar dos o más modelos logísticos, ya no se puede hacer
según lo mencionado en la nota que sigue al teorema 4 por el defecto señalado
arriba. Por eso se propone analizar la bondad de ajuste de un modelo logístico fijo
(con variables explicativas X1 , . . . , XK que definen J poblaciones), al compararlo
con el modelo completo (que no se basa en poblaciones) y con el modelo nulo (solo
con el intercepto). En este caso, la desviación del modelo logístico será
                      D∗ (M ) := 2[L(p̂) − L(α̂)] = −2[L(α̂)]
ya que L(p̂) = 0. Esta situación se orienta en la tabla 5.5 de ANODEV.


               Tabla 2: ANODEV del modelo logístico vs. completo.
  Ahora se sugiere como una estadística de desviación relativa (con respecto al
modelo completo) la propuesta por C.F & McFadden (1974):
                                 D(0) − D(M )     D(M )
                      RDC :=                  =1−
                                     D(0)         D(0)
   Esta desviación relativa tiene las características:


                                        Revista Colombiana de Estadística 29 (2006) 239–265

Precisiones en la teoría de los modelos logísticos                                    263


  a) 0 ≤ RDC ≤ RDS ≤ 1.

  b) RDC = 0 si y solo si 0 < D(M ) = D(0) si y solo si 0 > L(α̂) = L(δ̂o ).

  c) RDC = 1 si y solo si 0 = D(M ) < D(0) si y solo si 0 = L(α̂) > L(δ̂o ).

  d) Al eliminar una variable explicativa (disminuir K) se disminuye el numera-
     dor, pero el denominador D(0) no cambia de valor porque no depende de
     las variables explicativas. Por lo tanto, RDC sí disminuye al eliminar varia-
     bles explicativas. Esto quiere decir que sí se puede comparar las desviaciones
     relativas RDC entre un modelo logístico y cualquier submodelo.


5.6.     Criterio para la escogencia de un buen submodelo
         logístico
5.6.1.    El análisis de un modelo logístico M fijo

    Se compara este modelo logístico (con sus 1 + K parámetros) con el modelo
saturado correspondiente (con J poblaciones/parámetros) y con el modelo nulo
(con 1 parámetro, el intercepto) como se hizo en las secciones 5.1 y 5.3, respecti-
vamente. Para que el modelo logístico pueda considerarse como “aceptable”, debe
estar “cerca” del modelo saturado y “lejos” del modelo nulo. Es decir,

       No debe ser rechazado el modelo logístico vs. el saturado según la prueba del
       teorema 12, y

       Sí debe ser rechazado el modelo nulo vs. el logístico según la prueba del
       teorema 3.

    Lo anterior debe reflejar un valor “grande” de la desviación relativa con respecto
al modelo saturado, RDS, según se señaló en la sección 5.5. Es importante tam-
bién recalcar que la desviación relativa RDC es una estadística que tiene un valor
“grande” si el modelo logístico, adicionalmente a lo anterior, también está cerca
del modelo completo. Todas estas situaciones pueden visualizarse claramente en
la figura 1.

5.7.     De un modelo logístico M hacia un buen submodelo
         logístico Mo
   Para escoger el mejor submodelo logístico, se compara cada submodelo con:

   1. Su modelo saturado (el número de las poblaciones baja) y con el modelo
      nulo. Como criterio sirve el teorema 4.

   2. El modelo inicial, según la hipótesis dada en el teorema 13 y con la estadística
      señalada en el teorema 14.

               Figura 1: Gráfica para el análisis de un modelo logístico M fijo.
  3. La sucesión de todos los submodelos anteriores. En este caso, se debe calcular
     la RDC para cada submodelo. De esta manera, se obtiene una sucesión (fini-
     ta) decreciente de valores que sirve como un criterio (junto al anterior) para
     decidir cuándo y por qué se detiene el proceso de eliminación. Es decir, para
     decidir cuál submodelo puede ser mejor que los anteriores, incluso mejor que
     el modelo inicial. Esta decisión es siempre un compromiso entre aceptar una
     pérdida de explicación global, que se expresa en un menor valor RDC (análo-
     gamente al coeficiente de ajuste R2 para modelos lineales), pero que no sea
     estadísticamente significativa la pérdida; y ganar mejor explicación parcial,
     que se expresa en el rechazo de todas las hipótesis parciales Ho : βk = 0,
     k = 0, 1, . . . , K, para cada variable que quedó en el modelo final, según el
     teorema 4. La situación final puede visualizarse en la figura 2.

          Figura 2: Gráfica para escoger el mejor submodelo logístico Mo .
Referencias
Agresti A.Categorical Data Analysis.(1990).John Wiley and Sons.New York.
C F M,McFadden D.Frontiers in Econometrics Applications.(1974).MIT Press.Cambridge.
Dobson A J.An Introduction to Generalized Linear Models.(2002).Chapman & Hall.London.
Goodman L.The Analysis of Multidimensional Contingency Tables: Stepwise Procedures and Direct Estimation Methods for Building Models for Multiple Classifications.(1971).Technometrics.
Mc Cullagh P.Quasi-likelihood Functions.(1983).Annals of Statistics.
Mc Cullagh P,Nelder J.Generalized Linear Models.(1983).Chapman and Hall.London.
Rao C.Linear Statistical Inference and its Applications.(1973).John Wiley and Sons.New York.
Theil H.On the Estimation of Relationships Involving Qualitative Variables.(1970).American Journal of Sociology.
Wedderburn R.Quasi-likelihood Functions, Generalized linear models and the Gauss-Newton Method.(1974).Biometrika.
Wedderburn R.On The Existence and Uniqueness of the Maximum Likelihood Estimates for Certain Generalized Linear Models.(1976).Biometrika.
Zacks S.The Theory of Statistical Inference.(1971).John Wiley and Sons.New York.