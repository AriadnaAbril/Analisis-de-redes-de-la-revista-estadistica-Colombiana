On Cumulative Residual Renyi's . Sobre la entropÃ­a residual acumulada de Renyi
University of Mohaghegh Ardabili, Ardabil, Iran
Abstract
At the entropy measures and their generalization path, in the direction of statistics and information science, recently, Sunoj & Linu (2012) proposed the cumulative residual Renyi's entropy of order Î± and its dynamic version and studied its main properties. In this paper, we introduce an alternative measure of cumulative residual Renyi's entropy (CRRE) of order Î± which, unlike the mentioned one, is positive for all distributions and all values of Î±. We also consider its dynamic version and study their main properties in the context of reliability theory and stochastic orders. We give an estimator of the proposed CRRE and investigate its exact and asymptotic distribution. Numerous examples illustrating the theory are also given.
Key words : Aging classes; Cumulative residual entropy, Mean residual lifetime, Stochastic orders, Shannon entropy, Tsallis entropy.
Resumen
En las medidas de entropÃ­a y su camino de generalizaciÃ³n, en la direcciÃ³n de las estadÃ­sticas y la ciencia de la informaciÃ³n, recientemente, Sunoj & Linu (2012) propuso el residual acumulativo la entropÃ­a de Renyi de orden Î± y su versiÃ³n dinÃ¡mica y se estudiaron sus principales propiedades. En este artÃ­culo presentamos una medida alternativa de la entropÃ­a residual acumulada de Renyi (CRRE) de orden Î± que, a diferencia de la mencionada, es positiva para todas las distribuciones y todos los valores de Î±. TambiÃ©n consideramos su versiÃ³n dinÃ¡mica y estudiamos sus principales propiedades en el contexto de la teorÃ­a de la conabilidad y los Ã³rdenes estocÃ¡sticos. Damos un estimador del CRRE propuesto e investigamos su distribuciÃ³n exacta y asintÃ³tica. TambiÃ©n se dan numerosos ejemplos que ilustran la teorÃ­a.
Palabras clave : Clases de envejecimiento; EntropÃ­a residual acumulada; EntropÃ­a de Shannon, EntropÃ­a de Tsallis; Vida Ãºtil residual media; Ã“rdenes estocÃ¡sticas.

1. Introduction

    It is well-known that the approach in Shannon (1948) was one of the rst
works for mathematically quantifying of the information entropy that employed the
probability and chance notion and linked between these two notions. Considering
dierent point of views, various generalizations of Shannon's entropy have been
given by many researchers. For a comprehensive entropy-related works review and
the history of the derivation of Shannon's entropy and its dierent generalizations,
we refer the reader to Nanda & Chowdhury (2019). It is worth to mention that
Shannon's entropy and its dierent versions were rstly introduced for the discrete
probability spaces. The continuous versions of the entropy measures have been
usually given by replacing the sum notation with the integral, straightforwardly.
Among the several generalizations of Shannon's entropy, RÃ©nyi (1961) has
introduced an important one which for a non-negative continuous random variable
X with density function f (x) is given by
                                     Z âˆž
                             1
                 EÎ² (X) =        log(    f Î² (x)dx), Î² Ì¸= 1, Î² > 0,
                           1âˆ’Î²        0

where log is the natural logarithm. The dynamic version of EÎ² (X) has been
studied by Abraham & Sankaran (2005). However, the density function is not
necessarily exist for all random variables. On the other hand, the obtained entropy
measures by the density function may does not have the required main properties
of an information measure. For example, they may take negative values (see,
Figure 1 for a plot of EÎ² (X)). Regarding these and other limitations, alternative
generalizations of entropy measures have been introduced by researchers through
replacing the density function with the survival and distribution functions. This
kind of generalization has been started by Rao et al. (2004) when they proposed
the generalized version of Shannon's entropy as
                                      Z âˆž
                            E(X) = âˆ’       FÌ„ (x)log FÌ„ (x)dx,
                                       0

where FÌ„ (x) = 1 âˆ’ F (x) is the survival function of random variable X with
distribution function F . Recently, motivated by the usefulness of Renyi's entropy
and Rao et al.'s cumulative residual entropy (CRE) measure, Sunoj & Linu (2012)
have introduced cumulative residual Renyi's entropy (CRRE) of order Î² as
                                   Z âˆž
                           1
                 Î³(Î²) =        log(    FÌ„ Î² (x)dx), Î² Ì¸= 1, Î² > 0.
                         1âˆ’Î²        0

    They also considered the dynamic version of the CRRE (DCRRE, by extending
it to the residual lifetime variable) and studied its main properties useful in
reliability modelling.
   As Figure 1 depicts, Î³(Î²) has still the drawback that it may take negative
values for some distributions. It is also worth to recall that the Tsallis entropy is
one of the well-known entropy measures which its cumulative version of order Î±
(CTE) has been introduced by Rajesh & Sunoj (2019) as


                 Revista Colombiana de EstadÃ­stica - Theorical Statistics 45 (2022) 257-273

Cumulative Residual Renyi's Entropy                                                    259




                        Figure 1: Plots of EÎ² (X) and Î³(Î²).

                               Z âˆž
                         1
             TÎ± (X) =                [FÌ„ (x) âˆ’ FÌ„ Î± (x)]dx, Î± Ì¸= 1, Î± > 0.             (1)
                        Î±âˆ’1     0

Here, TÎ± (X) is always positive.
    In this paper, by re-parametrization of Î³(Î²) (replacing Î² by Î± + 1) and by
normalizing it, we propose an alternative measure of the CRRE of order Î± by the
following:                           R âˆž Î±+1            !
                             1            FÌ„      (x)dx
                   Î³Î± (X) = âˆ’ log     0R
                                         âˆž                , Î± > 0.          (2)
                             Î±          0
                                             FÌ„ (x)dx

    It is clear that Î³Î± (X) is always positive. The rest of the paper is organized
as follows. In Section 2, we rst give the main properties of Î³Î± (X). Comparing
values of the CRRE under various stochastic orders between random variables
are also studied in this section. Section 3 is devoted to the dynamic CRRE
and its properties. The estimation of the proposed CRRE and its properties are
investigated in Section 4. Finally, some conclusions are given in Section 5.
   Before proceeding to give the main results of the paper, we overview some
preliminary concepts of ageing and stochastic orders (For more details of these
concepts see, for example, Shaked & Shanthikumar, 2007).
   Let X and Y be non-negative random variables with the distribution functions
F and G, survival functions FÌ„ = 1 âˆ’ F and GÌ„ = 1 âˆ’ G, hazard functions
                             d                           d
                Î»X (t) = âˆ’      log FÌ„ (t) and Î»Y (t) = âˆ’ log GÌ„(t),
                             dt                          dt
and mean residual life functions
                          Z âˆž                            Z âˆž
                      1                               1
         mX (t) =             FÌ„ (x)dx and mY (t) =          GÌ„(x)dx,
                    FÌ„ (t) t                        GÌ„(t) t

                 Revista Colombiana de EstadÃ­stica - Theorical Statistics 45 (2022) 257-273

260                                                                                Vali Zardasht

respectively. Throughout this paper we assume that these functions all exist and
increasing (decreasing) means non-decreasing (non-increasing).
Denition 1. The random variable X is said to be:
(i) increasing (decreasing) failure rate in average, IFRA (DFRA), if âˆ’ 1t log FÌ„ (t)
        is increasing (decreasing) in t,
(ii) new better (worse) than used, NBU(NWU), if FÌ„ (x + t) â‰¤ (â‰¥)FÌ„ (x)FÌ„ (x), for
        all x, t > 0,
(iii) new better (worse) than used in expectation, NBUE(NWUE), if mX (t) â‰¤ (â‰¥
        ) mX (0), for all t > 0,
(iv) increasing (decreasing) mean residual life (IMRL(DMRL)) if mX (t) is
        increasing (decreasing) in t,
(v) smaller than Y in the usual stochastic order (denoted by X â‰¤st Y ) if
        FÌ„ (t) â‰¤ GÌ„(t) for all t,
(vi) smaller than Y in the hazard rate ordering (denoted by X â‰¤hr Y ) if
        Î»X (t) â‰¥ Î»Y (t) for all t,
(vii) smaller than Y in the mean residual lifetime ordering (denoted by X â‰¤mrl Y )
        if mX (t) â‰¤ mY (t) for all t,

(viii) smaller than Y in the DMRL order (denoted by X â‰¤dmrl Y ) if m
                                                                                                 âˆ’1
                                                                   m (G                 Y             (u))
                                                                     (F                          âˆ’1 (u))
                                                                                        X
        is increasing in u âˆˆ [0, 1],

(ix) smaller than Y in the NBUE order (denoted by X â‰¤nbue Y ) if m
                                                                                            âˆ’1
                                                                   (F                            (u))
                                                                 m (G
                                                                                    X
                                                                                    Y
                                                                                            âˆ’1 (u))     â‰¤
        E[X]
        E[Y ] for all u âˆˆ [0, 1],

(x) smaller than Y in the increasing convex order (denoted by X â‰¤icx Y ) if
        E[Ï•(X)] â‰¤ E[Ï•(Y )], for all increasing convex functions Ï•,
(xi) smaller than Y in the dispersive order (denoted by X â‰¤disp Y ) if F âˆ’1 (Î²) âˆ’
        F âˆ’1 (Î±) â‰¤ Gâˆ’1 (Î²) âˆ’ Gâˆ’1 (Î±), whenever 0 < Î± â‰¤ Î² < 1,
(xii) smaller
           R than Y              in the Lorenz order (denoted by X                  â‰¤L           Y ) if
               u âˆ’1             R u âˆ’1
         1
        E(X)   0
                            1
                 F (v)dv â‰¥ E(Y ) 0 G   (v)dv for all u âˆˆ [0, 1].


2. Some Properties of                        Î³Î± (X)
      Note that Î³Î± (X) can also be written as
                                               1
                                  Î³Î± (X) = âˆ’     log(E[FÌ„ Î± (Xe )]),                                  (3)
                                               Î±
where, Xe is the equilibrium random variable corresponding to X with density
                                      Râˆž
function fe (x) = FÌ„ Âµ(x) , Âµ = E[X] = 0 FÌ„ (x)dx. The following theorem gives the
main properties of the CRRE Î³Î± (X) for Î± > 0.


                        Revista Colombiana de EstadÃ­stica - Theorical Statistics 45 (2022) 257-273

Cumulative Residual Renyi's Entropy                                                       261

Theorem 1. (a) Î³Î± (X) â‰¥ 0.
(b) Î³Î± (X) is decreasing in Î±.
(c) limÎ±â†’0 Î³Î± (X) = E(X)
                      Âµ .

(d) If Y = aX , a > 0, then Î³Î± (Y ) = Î³Î± (X).
(e) T Âµ(X) â‰¤ Î³Î± (X) â‰¤ E(X)
       Î±+1
                        Âµ .


(f) Î³Î± (X) â‰¥ âˆ’ Î±1 log
                                              
                                                   , for 0 < Î± < 1.
                                        1
                            Î±+1 1âˆ’Î±
                            2Î± Âµ E  [X 1âˆ’Î± ]

Proof . We only give the proof of parts (b), (e) and (f). For a proof of (b), use
(3) and the Lyapounov inequality. To prove (e), rst note that Î³Î± (X) can also be
expressed as

                                      Î± âˆž
                                       Z                        
                           1
             Î³Î± (X) = âˆ’ log 1 âˆ’              mX (x)FÌ„ Î± (x)dF (x)             (4)
                          Î±           Âµ 0
                           1         Î±
                    = âˆ’ log(1 âˆ’ TÎ±+1 (X)),                                    (5)
                          Î±          Âµ

where the last equation follows from equation (7) in Rajesh & Sunoj (2019). Now,
applying the inequality âˆ’ log(1âˆ’x) â‰¥ x, 0 â‰¤ x â‰¤ 1, implies that Î³Î± (X) â‰¥ TÎ±+1Âµ(X) .
On the other hand, using the log-sum inequality we have
                        R âˆž Î±+1            !          Râˆž               !
                1        0R
                             FÌ„      (x)dx    1        0
                                                            FÌ„ (x)dx       E(X)
    Î³Î± (X) = âˆ’ log          âˆž                = log R âˆž Î±+1               â‰¤      .
                Î±          0
                                FÌ„ (x)dx      Î±      0
                                                         FÌ„      (x)dx       Âµ

This completes the proof.

   To prove part (f), the integration by parts gives that

                                     Î±+1 âˆž
                                          Z                   
                              1
                 Î³Î± (X) = âˆ’ log                 xFÌ„ Î± (x)dF (x) .                         (6)
                              Î±        Âµ    0

The result now follows from Holder's inequality with p = Î±1 and q = 1âˆ’Î±
                                                                     1
                                                                        .

Example 1. Let X be a random variable with a Weibull distribution and survival
                            Î²
function FÌ„ (x) = eâˆ’(Î»x) . Then

                                                1
                                   Î³Î± (X) =       log(Î± + 1).
                                               Î±Î²

Example 2. Let X be distributed as Pareto with survival function FÌ„ (x) =
     a
  b
 b+x       , x, b > 0, a > 1. Then
                                                                     
                                             1             aâˆ’1
                                Î³Î± (X) = âˆ’     log                        .
                                             Î±           aÎ± + a âˆ’ 1

                    Revista Colombiana de EstadÃ­stica - Theorical Statistics 45 (2022) 257-273

262                                                                                Vali Zardasht

    The following theorem compares the CRREs of two random variables when one
is smaller than the other in some stochastic orders.

Theorem 2. (i) If X â‰¤st Y , then Î³Î± (X) â‰¥ Î³Î± (Y ) âˆ’ Î±1 log E[X]
                                                                 
                                                            E[Y ]
                                                                    , for all Î± > 0.

(ii) If X â‰¤icx Y and E[X] = E[Y ], then Î³Î± (X) â‰¤ Î³Î± (Y ), for all Î± > 0.
(iii) If X â‰¤nbue Y , then Î³Î± (X) â‰¤ Î³Î± (Y ), for all Î± > 0.
(iv) If X â‰¤L Y , then Î³Î± (X) â‰¤ Î³Î± (Y ), for all Î± > 0.

Proof .
      For part (ii), rst note that the CTE given in (1) can also be rewritten as
                                     Z âˆž
                             1
                   TÎ± (X) =                F âˆ’1 (u)[1 âˆ’ Î±(1 âˆ’ u)Î±âˆ’1 ]du.                    (7)
                            Î±âˆ’1        0

This along with Theorem 4.A.4 in Shaked & Shanthikumar (2007, p. 183) implies
that if X â‰¤icx Y , then TÎ±+1 (X) â‰¤ TÎ±+1 (Y ). Part (ii) now follows from (5) and
the hypothesis E[X] = E[Y ].
      To prove part (iii), one can see that equation (4) can also be given as
                                        Z 1                         
                           1          Î±
              Î³Î± (X) = âˆ’     log 1 âˆ’         mX (F âˆ’1 (u))(1 âˆ’ u)Î± du .
                           Î±         E[X] 0

      The result now follows from the fact that X â‰¤nbue Y is equivalent to that

                      mX (F âˆ’1 (u))   mY (Gâˆ’1 (u))
                                    â‰¤              , 0 â‰¤ u â‰¤ 1,
                         E[X]            E[Y ]

(see Shaked & Shanthikumar, 2007).
   Finally, for the proof of (iv), using the integration by parts, we obtain from
equation (6) that
                                                  Z 1
                                  1                        F âˆ’1 (u)
                Î³Î± (X)   =    âˆ’     log((Î± + 1)                     (1 âˆ’ u)Î± du)
                                  Î±                0        E(X)
                                                   Z 1
                            1
                         = âˆ’ log(Î±(Î± + 1)                  (1 âˆ’ u)Î±âˆ’1 LX (u)du),            (8)
                            Î±                          0
                      R u âˆ’1
where LX (u) = E(X)
                  1
                       0
                         F (v)dv is the Lorenz curve corresponding to X . The
result now follows from the fact that under the hypothesis, LX (u) â‰¥ LY (u), for
all u âˆˆ [0, 1].
   It is readily seen from (8) that, for any integer k â‰¥ 1, Î³k (X) = âˆ’ k1 log(1 âˆ’ Gk ),
where Gk is the k th Gini index (see Farris, 2010).


                   Revista Colombiana de EstadÃ­stica - Theorical Statistics 45 (2022) 257-273

Cumulative Residual Renyi's Entropy                                                         263

Remark 1. Note that the CTE can also be given by
                                        Z âˆž
                             TÎ± (X) =          Ï†F (x; Î±)dF (x),
                                         0
                        Rx
where Ï†F (x; Î±) = Î±âˆ’1 1
                         0
                           [1 âˆ’ FÌ„ Î±âˆ’1 (y)]dy is an increasing function of x for all
Î± > 0. Additionally, X â‰¤st Y if and only if E[Ï•(X)] â‰¤ E[Ï•(Y )] for all increasing
function Ï•RShaked & Shanthikumar  R âˆž (2007, p. 4). Thus, X â‰¤st Y implies that
             âˆž
TÎ± (X) â‰¤ 0 Ï†F (x; Î±)dG(x) â‰¤ 0 Ï†G (x; Î±)dG(x) = TÎ± (Y ), for Î± > 0. Since
limÎ±â†’1 TÎ± (X) = E(X), we get that if X â‰¤st Y , then E(X) â‰¤ E(Y ). This improves
the inequality given in Proposition 2.1 in Navarro et al. (2010).
Remark 2. One can see from the proof of part (ii) in the above theorem that
if X â‰¤icx Y , then TÎ±+1 (X) â‰¤ TÎ±+1 (Y ). Letting Î± goes to zero implies that if
X â‰¤icx Y , then E(X) â‰¤ E(Y ). This relation has been used in Zardasht (2015) to
test the increasing convex order hypothesis.


3. Dynamic Cumulative Residual Renyi's Entropy

     If random variable X has survival function FÌ„ , then, the corresponding residual
lifetime variable Xt = X âˆ’ t | X > t has survival function FÌ„t (x) = FÌ„FÌ„ (x)  (t)
                                                                                   , x > t.
Indeed, if X is the failure time of a new product or an engineering system, Xt
is that of the product or system at its age t. The probability distribution of Xt
and its properties play an important role in reliability and life testing studies. By
replacing FÌ„ (x) with FÌ„t (x) in (2), the dynamic CRRE can be dened as
                                           Râˆž                    Î±+1 !
                                   1        tR
                                                  FÌ„ (x)/ FÌ„ (t)       dx
                Î³Î± (X; t) = âˆ’ log               âˆž                  
                                   Î±           t
                                                     FÌ„ (x)/ FÌ„ (t)  dx
                                             R âˆž Î±+1                !
                                   1              FÌ„      (x)dx
                            = âˆ’ log           t     Râˆž                , Î± > 0.          (9)
                                   Î±       FÌ„ Î± (t) t FÌ„ (x)dx
Example 3. Let X be a random variable with a Weibull distribution and survival
                         Î²
function FÌ„ (x) = eâˆ’(Î»x) . Then
                                                                        Î²
                                                                            !
                                1            FÌ„g ((Î± + 1)(Î»t)Î² )eÎ±(Î»t)
                   Î³Î± (X; t) = âˆ’ log                    1                       ,
                                Î±              (Î± + 1) Î² FÌ„g ((Î»t)Î² )
where FÌ„g (.) is the survival function of the gamma distribution with shape and
scale parameters Î²1 and 1, respectively.
Example 4. Let X be distributed as power with distribution function F (x) = xÎ² ,
0 â‰¤ x â‰¤ 1, Î² > 0. Then
                                      Fb (1 âˆ’ tÎ² , Î± + 2, Î²1 )B(Î± + 2, Î²1 )
                                                                                    !
                           1
              Î³Î± (X; t) = âˆ’ log                                                         ,
                           Î±         (1 âˆ’ tÎ² )Î± Fb (1 âˆ’ tÎ² , 2, Î²1 )B(2, Î²1 )
where, Fb (x, a, b) is the distribution function of a beta random variable with
parameters a and b, and B(a, b) = Î“(a)Î“(b)
                                      Î“(a+b) is the beta function.


                  Revista Colombiana de EstadÃ­stica - Theorical Statistics 45 (2022) 257-273

264                                                                             Vali Zardasht

      Figure 2 shows the plot of Î³Î± (X; t) for the above two examples.




             Figure 2: Plot of Î³Î± (X; t) for Power and Weibull distribution.

   It is clear that Î³Î± (X; 0) = Î³Î± (X). Analog to equations (4), (5) and (6), the
DCRRE can also be rewritten as
                                              Râˆž                          !
                              1             Î± t mX (x)FÌ„ Î± (x)dF (x)
             Î³Î± (X; t) = âˆ’ log 1 âˆ’                      Râˆž                            (10)
                              Î±                 FÌ„ Î± (t) t FÌ„ (x)dx
                                                         
                              1             Î±TÎ±+1 (X; t)
                        = âˆ’ log 1 âˆ’                         ,                         (11)
                              Î±                mX (t)
                                               Râˆž                          !
                              1       (Î± + 1) t (x âˆ’ t)FÌ„ Î± (x)dF (x)
                        = âˆ’ log                                              ,        (12)
                              Î±                 FÌ„ Î±+1 (t)mX (t)
                        Râˆž
where, mX (t) = FÌ„ 1(t) t FÌ„ (x)dx is the mean residual lifetime function of X
and equation (11) is obtained using equation (12) in Rajesh & Sunoj (2019), an
equivalent equation for the CRTE which is given by
                                  Râˆž Î±             ! Râˆž
                   1                t
                                       FÌ„ (x)dx           t
                                                             mX (x)FÌ„ Î±âˆ’1 (x)dF (x)
    TÎ± (X; t) =          mX (t) âˆ’                      =                            .
                 Î±âˆ’1                  FÌ„ Î± (t)                     FÌ„ Î± (t)

The following theorem gives some results for Î³Î± (X; t) when X belongs to some
reliability aging classes.
Theorem 3. (a) If X âˆˆ IF RA(DF RA), then
                                                                               
                                       1           mX ((Î± + 1)t)FÌ„ ((Î± + 1)t)
                   Î³Î± (X; t) â‰¤ (â‰¥) âˆ’     log                                        .
                                       Î±            (Î± + 1)mX (t)FÌ„ Î±+1 (t)

(b) If X âˆˆ N BU (N W U ), then Î³Î± (X; t) â‰¥ (â‰¤)Î³Î± (X) + Î±1 log( m Âµ(t) ). X




                   Revista Colombiana de EstadÃ­stica - Theorical Statistics 45 (2022) 257-273

Cumulative Residual Renyi's Entropy                                                     265

(c) If X âˆˆ N BU E(N W U E), then Î³Î± (X; t) â‰¤ (â‰¥) âˆ’ Î±1 log 1 âˆ’ (Î±+1)m
                                                                                   
                                                                     (t) .
                                                                  Î±Âµ
                                                                                X


(d) If X âˆˆ DM RL(IM RL), then Î³Î± (X; t) â‰¤ (â‰¥) Î±1 log(Î± + 1).
Proof . To prove part (a), it is not hard to show that X âˆˆ IF RA(DF RA) is
equivalent to FÌ„ a (t) â‰¥ (â‰¤)FÌ„ (at), for a â‰¥ 1. Thus, for Î± > 0,
            Z âˆž                            Z âˆž
                     Î±+1
                  FÌ„     (x)dx â‰¥ (â‰¤)            FÌ„ ((Î± + 1)x)dx
             t                              t
                                                   Z âˆž
                                              1
                                    =                     FÌ„ (x)dx
                                           Î± + 1 (Î±+1)t
                                              1
                                    =             mX ((Î± + 1)t)FÌ„ ((Î± + 1)t).
                                           Î±+1
The result now follows from equation (9). For part (b), the hypothesis implies
that
                                Z âˆž               Î±+1 !
                          1            FÌ„ (x + t)           1
     Î³Î± (X; t)   =      âˆ’ log                           dx + log(mX (t))
                          Î±      0        FÌ„ (t)            Î±
                               Z âˆž                
                          1                            1
               â‰¥ (â‰¤) âˆ’ log          FÌ„ Î±+1 (x)dx + log(mX (t))
                          Î±      0                     Î±
                                1      mX (t)
                 =      Î³Î± (X) + log(           ).
                                Î±          Âµ
This completes the proof. Parts (c) and (d) similarly follow from equation (10).

   As an application in reliability theory, let X1 , X2 , . . . , Xn be the independent
random lifetimes of the components of a series system which are copies of X . Then,
the lifetime of the system is X1:n = min{X1 , X2 , . . . , Xn }. It is not dicult to see
from equation (9) that

      Î±Î³Î± (X1:n ; t) = (n(Î± + 1) âˆ’ 1)Î³n(Î±+1)âˆ’1 (X; t) âˆ’ (n âˆ’ 1)Î³nâˆ’1 (X; t).             (13)

This reveals that the DCRRE of series systems is straightly related to that of its
components.
     Consider also another series systems with Y1 , Y2 , . . . , Yn being its components
lifetime which are independent and are copies of Y . For these series systems, the
following result gives that, if Î³Î± (X; t) â‰¤ Î³Î± (Y ; t), for t â‰¥ 0, then under a condition,
the DCRREs of the systems are also ordered.
Theorem 4. If Î³Î± (X; t) â‰¤ Î³Î± (Y ; t), for all Î± > 0, t â‰¥ 0, and if X âˆˆ IM RL and
Y âˆˆ DM RL, then Î³Î± (X1:n ; t) â‰¤ Î³Î± (Y1:n ; t), for t â‰¥ 0.

Proof . Under the rst assumption, and using equation (13) we obtain that
          Î±[Î³Î± (Y1:n ; t) âˆ’ Î³Î± (X1:n ; t)] â‰¥ (n âˆ’ 1)[Î³nâˆ’1 (X; t) âˆ’ Î³nâˆ’1 (Y ; t)].

The second assumption along with Theorem 3(d) now gives the result.


                  Revista Colombiana de EstadÃ­stica - Theorical Statistics 45 (2022) 257-273

266                                                                                           Vali Zardasht

      Furthermore, using equation (9), for n â‰¥ 1, we have
                                        1     mX1:n+1 (t)
                          Î³n (X; t) = âˆ’ log(              ),                                          (14)
                                        n       mX (t)
which ensures that X1:n â‰¤mrl X .
    The following theorem shows that when the components of two series systems
are ordered in the sense of the mean residual lifetime and the DCRREs, then their
mean residual lifetime functions are also ordered.
Theorem 5. If X â‰¤mrl Y and Î³nâˆ’1 (X; t) â‰¥ Î³nâˆ’1 (Y ; t), for t â‰¥ 0, then
X1:n â‰¤mrl Y1:n .

Proof . Using equation (14) Î³nâˆ’1 (X; t) â‰¥ Î³nâˆ’1 (Y ; t) is equivalent to that
mX1:n (t) mY1:n (t)
 mX (t) â‰¤ mY (t) .   The result now follows from the fact that X â‰¤mrl Y means
that mX (t) â‰¤ mY (t), for t â‰¥ 0.

   In the sequel, we give some results comparing the DCRRE of two random
variables which are stochastically ordered in some notions.

Theorem 6. If X â‰¤hr Y , then Î³Î± (X; t) â‰¥ Î³Î± (Y ; t) âˆ’ Î±1 log m
                                                                    
                                                              mY (t)
                                                               X (t)  , for t > 0.

Proof . The hypothesis is equivalent to FÌ„FÌ„(x)
                                            (t)
                                                â‰¤ GÌ„(x)
                                                  GÌ„(t)
                                                        , for all t â‰¤ x (cf. Shaked &
Shanthikumar, 2007, p. 16). The result now follows from (9).
Theorem 7. If X â‰¤dmrl Y , then Î³Î± (X; F âˆ’1 (p)) â‰¤ Î³Î± (Y ; Gâˆ’1 (p)), for 0 < p < 1.
Proof . First, using equation (10) we have
                                                      R1
                                     âˆ’1              Î± p mX (F âˆ’1 (u))(1 âˆ’ u)Î± du
                 1 âˆ’ eâˆ’Î±Î³Î± (X;F           (p))
                                                 =                                        .           (15)
                                                       (1 âˆ’ p)Î±+1 mX (F âˆ’1 (p))
                                                                       âˆ’1                       âˆ’1
On the other hand, the hypothesis implies that m   X (F  (u))    mY (G (u))
                                                 mX (F âˆ’1 (p)) â‰¤ mY (Gâˆ’1 (p)) , for
p â‰¤ u. The result now, follows from the above equation.
Theorem 8. If X and Y are non-negative random variables with common
left endpoint zero, and if X â‰¤disp Y , X âˆˆ DM RL, then Î³Î± (X; F âˆ’1 (p)) â‰¤
Î³Î± (X; Gâˆ’1 (p)), for 0 < p < 1.

Proof . Using equation (15) and by applying inequalities 3.C.5 and 3.C.9, and
Theorem 3.b.13(a) in Shaked & Shanthikumar (2007), pp. 165, 166 and 154,
respectively, we obtain that
                     âˆ’1                                                     âˆ’1
   (1 âˆ’ eâˆ’Î±Î³Î± (X;F        (p))
                                 )mX (F âˆ’1 (p)) â‰¤          (1 âˆ’ eâˆ’Î±Î³Î± (X;G       (p))
                                                                                        )mX (Gâˆ’1 (p))
                                                                            âˆ’1
                                                       â‰¤   (1 âˆ’ eâˆ’Î±Î³Î± (X;G       (p))
                                                                                        )mX (F âˆ’1 (p)).

Hence, we get that Î³Î± (X; F âˆ’1 (p)) â‰¤ Î³Î± (X; Gâˆ’1 (p)), for 0 < p < 1. This completes
the proof.


                   Revista Colombiana de EstadÃ­stica - Theorical Statistics 45 (2022) 257-273

Cumulative Residual Renyi's Entropy                                                          267

    Dierentiating equation (2) with respect to t, we have

                   Î±mX (t)Î³Î±â€² (X; t) = eÎ±Î³Î± (X;t) âˆ’ Î±mâ€²X (t) âˆ’ (Î± + 1).                      (16)

   A new class of distributions can be considered based on the mathematical
behaviour of the DCRRE.
Denition 2. A random variable X is said to be increasing (decreasing) DCRRE,
denoted by IDCRRE(DDCRRE), if Î³Î± (X; t) is an increasing (decreasing) function
of t.

    Note that, equation (16) implies that X âˆˆ IDCRRE(DDCRRE) if

                          Î±Î³Î± (X; t) â‰¥ (â‰¤) log(Î±mâ€²X (t) + Î± + 1),

or equivalently, if

                         Î±Î³Î± (X; t) â‰¥ (â‰¤) log(Î±mX (t)Î»X (t) + 1).

Example 5. If X has a weibull distribution with survival function FÌ„ (x) = e(Î»x) ,
                                                                                               Î²


then X âˆˆ IDCRRE(DDCRRE) if Î² > (<)1.


     6. Let X be distributed uniformly on (0, Î²).
Example                                                                         Then, Î³Î± (X; t) =
âˆ’ Î±1 log    2
           Î±+2   , which is a constant function of t.

     Sunoj & Linu (2012) have characterized some distributions using the
relationship between their own version of the DCRRE and the mean residual
lifetime function. Rajesh & Sunoj (2019) have also obtained characterizations
for some distributions using the DCTE and mean residual lifetime function. The
following theorem gives a characterization of some distributions using the same
relationship between the mean residual lifetime function and Î³Î± (X; t).
Theorem 9. Let X be a non-negative random variable with continuous survival
function FÌ„ (x) and the mean residual lifetime function mX (x). Suppose that the
relationship Î±Î³Î± (X; t) = log(c(t)mX (t)) holds, for a nonnegative function c(t).
Then, for t â‰¥ 0
                                                   Z t
                                  1                   âˆ’(Î± + 1) p      C(x)
                 mX (t) = p             C(t)
                                               [                 c(x)e 2Î± dx + k],           (17)
                              c(t)e      2Î±         0   2Î±

where, C â€² (t) = c(t), and k is a constant.

Proof . Under the given relationship, equation (16) implies that
                                      Î±câ€² (t) âˆ’ c2 (t)
                                                        
                                                                        âˆ’(Î± + 1)
                      mâ€²X (t) +                              mX (t) =            ,
                                          2Î±c(t)                          2Î±

which is a dierential equation and has a solution in the form of (17).


                    Revista Colombiana de EstadÃ­stica - Theorical Statistics 45 (2022) 257-273

268                                                                                 Vali Zardasht

    The following result gives a characterization result similar to that of Theorem
2.3 in Sunoj & Linu (2012).
Theorem 10. Let Î±Î³Î± (X; t) = log(C). Then, X has
(i) Pareto distribution with survival function FÌ„ (x) = (1 + bx)âˆ’a , x, b > 0, a > 1,
(ii) exponential distribution with survival function FÌ„ (x) = eâˆ’Î»x , x, Î» > 0,
(iii) nite range distribution with survival function FÌ„ (x) = (1 âˆ’ bx)a , 0 < x <
      1
      b , a, b > 0   ,
                               >
                               =
according as Câˆ’(Î±+1)
                Î±    < 0.

Proof . Under the above relationship, equation (16) follows that mâ€²X (t) =
Câˆ’(Î±+1)
   Î±      = k , a constant. This is equivalent to mX (t) = kt + d, where d is
                                                                                              >
                                                                                              =
also a constant, which characterizes the distributions (i)-(iii) according as k < 0.
The converse part is easy to prove.

     Note that the uniform distribution in Example 3 is just the distribution in part
(iii) of the above theorem with b = Î²1 and a = 1.


4. The estimation of CRRE

   In this section, we propose an estimator of Î³Î± (X) and investigate its exact and
asymptotic distribution under some conditions. Let X1 , . . . , Xn be independent
positive random sample from the population of X with continuous distribution
functions F and corresponding order statistics X(1) , . . . , X(n) . Let also Fn be the
empirical distribution function of X .
   Regarding equation (6), Î³Î± (X) can also be given through an L-functional (cf.
Shao, 2003, p. 343) by
                                     Râˆž                    !
                              1          xJÎ± (F (x))dF (x)
                   Î³Î± (X) = âˆ’ log     0   Râˆž                 ,             (18)
                              Î±            0
                                              xdF (x)

where, JÎ± (u) = (Î± +1)(1âˆ’u)Î± . Now, by replacing F in (18) with Fn , an estimator
of Î³Î± (X) can be given by the following:
                                     Z âˆž                       
                              1       1
                 Î³Î± (Fn ) = âˆ’ log            xJÎ± (Fn (x))dFn (x)
                              Î±       XÌ„ 0
                                           n
                                                        !                    (19)
                              1        1 X        i
                          = âˆ’ log             JÎ± ( )X(i) .
                              Î±       nXÌ„ i=1     n

The following theorem gives the exact distribution of the above estimator under a
random sample from an exponential population.


                         Revista Colombiana de EstadÃ­stica - Theorical Statistics 45 (2022) 257-273

Cumulative Residual Renyi's Entropy                                                          269

Theorem 11. Let X1 , . . . , Xn be a random sample from an exponential
distribution with an arbitrary hazard rate. Then, the survival function of Î³Î± (Fn )
is given by
                                       n Y
                                         n
                                                                 !
                                                           âˆ’Î±x
                                       X          dÎ±
                                                   i,n âˆ’ e                     âˆ’Î±x
             P [Î³Î± (Fn ) > x] =                                      I(dÎ±
                                                                        i,n > e    ),
                                       i=1 j=1
                                                  dÎ±       Î±
                                                    i,n âˆ’ dj,n
                                           jÌ¸=i


where, dÎ±i,n = nâˆ’i+1                   , cÎ±i,n = (Î± + 1)(1 âˆ’ ni )Î± and I(.) is the indicator
                 1
                        Pn       Î±
                            j=i cj,n
function.

Proof . First note that, (19) can also be expressed as
                                                    Pn Î±           !
                                        1            i=1 ci,n X(i)
                            Î³Î± (Fn ) = âˆ’ log         Pn              .
                                        Î±              i=1 X(i)

On the other hand, by using the normalized spacings Di = (nâˆ’i+1)(X(i) âˆ’X(iâˆ’1) ),
i = 1, . . . , n ( X(0) â‰¡ 0), one can see easily that
            n
            X                      n X
                                   X   n
                  dÎ±
                   i,n Di    =       (   cÎ±
                                          j,n )(X(i) âˆ’ X(iâˆ’1) )
            i=1                    i=1 j=i
                                     j
                                   n X
                                   X                                     n
                                                                         X
                             =               cÎ±
                                              j,n (X(i) âˆ’ X(iâˆ’1) ) =           cÎ±
                                                                                j,n X(j) ,
                                   j=1 i=1                               j=1

which implies that Î³Î± (Fn ) can also be given by
                                                     Pn Î±         !
                                        1             i=1 ei,n Di
                            Î³Î± (Fn ) = âˆ’ log          Pn            .
                                        Î±               i=1 Di

The result now follows by applying the Theorem 3.1 in Belzunce et al. (2005).

   It is clear from the almost sure convergence property of the L-estimators (see
Example 1 and 2 in Wellner, 1977 and Helmers, 1977) and the continuous mapping
theorem (cf. Theorem 1.10 in Shao, 2003, p. 59) that as n â†’ âˆž,

                                        Î³Î± (Fn ) â†’ Î³Î± (X),

with probability one, provided that the population mean is nite. The following
theorem gives the asymptotic distribution of Î³Î± (Fn ) under some mild conditions.
Theorem 12. Assume that E(X 2 ) < âˆž and
                             Z âˆžZ âˆž
          ÏƒÎ±2 (F, J) = 2                 F (x)FÌ„ (y)JÎ± (F (x))JÎ± (F (y))dydx > 0.            (20)
                              0     x

Then, as n â†’ âˆž,
                            âˆš                           d
                                n[Î³Î± (Fn ) âˆ’ Î³Î± (X)] âˆ’â†’ N (0, Ïƒ 2 ),


                   Revista Colombiana de EstadÃ­stica - Theorical Statistics 45 (2022) 257-273

270                                                                           Vali Zardasht


where, âˆ’â†’ denotes convergence in distribution, N (0, Ïƒ 2 ) stands for the normal
          d
                                                     2
                                                       2 (X) ,
random variable with mean zero and variance Ïƒ 2 = Î±Ïƒ2Î±Î³(F,J)
                                                                 1Î±

                                               Z âˆž
                         Î³1Î± (X) = (Î± + 1)           xFÌ„ Î± (x)dF (x).
                                                0



Proof . We have Î³Î± (Fn ) = âˆ’ Î±1 log( Î³ XÌ„(F ) ) where Î³1Î± (Fn ) = n1 Pni=1
                                          1Î±    n
                                                                       âˆš JÎ± ( n )X(i) .
                                                                              i

It follows from Theorem 2 and 3 in Stigler (1974) that as n â†’ âˆž, n[Î³1Î± (Fn ) âˆ’
Î³1Î± (X)] converges in distribution to a normal random variable with mean zero and
             R âˆžOn the other hand, XÌ„ is a consistent estimator of the population
variance (20).
mean, Âµ = 0 FÌ„ (x)dx. Applying the Slutsky and Delta-method theorems now
gives the result.


   It is worth to mention that a consistent estimator of the asymptotic variance
can be obtained by replacing F in (20) with Fn .
   Crescenzo & Longobardi (2009) have used the following data sets to apply their
cumulative entropy for analyzing the lifetime data. As an example, we use these
data and compute the estimators of the CRRE Î³Î± (X) and Î³(Î²) in Sunoj & Linu
(2012). Since, it has not been proposed any estimator of Î³(Î²) in Sunoj & Linu
(2012), we rewrite it as
                                               Z âˆž
                                  1
                      Î³(Î²) =         log(Î²           xFÌ„ Î²âˆ’1 (x)dF (x)),
                                 1âˆ’Î²           0

and consider the corresponding estimator by the following
                                             n
                                  1       Î²X        i
                       Î³Ì‚(Î²) =       log(       (1 âˆ’ )Î²âˆ’1 X(i) ).
                                 1âˆ’Î²      n i=1     n

Example 7. The data set analyzed in Crescenzo & Longobardi (2009) includes
43 sample lifetime data which are as follows.

      7, 47, 58, 74, 177, 232, 273, 285, 317, 429, 440, 445, 455, 468, 495, 497, 532,

      571, 579, 581, 650, 702, 715, 779, 881, 900, 930, 968, 1077, 1109, 1314, 1334,

      1367, 1534, 1712, 1784, 1877, 1886, 2045, 2056, 2260, 2429, 2509.

For these data, Crescenzo & Longobardi (2009) obtained the estimate of their
cumulative entropy as 572.3. Figure 3 depicts the plot of the estimators Î³Î± (Fn )
and Î³Ì‚(Î²) for dierent values of Î± and Î² . One can see from the plot that the
estimator Î³Ì‚(Î²) takes negative values for Î² > 1.



                   Revista Colombiana de EstadÃ­stica - Theorical Statistics 45 (2022) 257-273

Cumulative Residual Renyi's Entropy                                                    271




                    Figure 3: Plots of Î³Î± (Fn ) (left) and Î³Ì‚(Î²)(right).



5. Conclusion


    In this paper, we have proposed an alternative measure of cumulative residual
Renyi's entropy (CRRE) of order Î± which unlike the one by Sunoj & Linu (2012)
preserves the main property of an information measure and is always positive.
We have investigated the main properties of the proposed measure and studied its
relation with other entropy measures. Assuming some well-known stochastic orders
between two random variables, the imposed orders between their corresponding
CRRE were revealed. The dynamic version of the CRRE was also considered and
its main properties and its relation to Tsallis's Entropy were studied. The dynamic
CRREs of the stochastically ordered random variables were also compared. The
estimator of the proposed CRRE and its exact distribution under a random sample
from an exponential distribution and also its asymptotic distribution were studied.
Throughout the paper numerous examples and plots illustrating the theory were
given.




Acknowledgements


   The author thank two anonymous reviewers for helpful comments and
suggestions.


                
                 Received: June 2021  Accepted: February 2022
                 Revista Colombiana de EstadÃ­stica - Theorical Statistics 45 (2022) 257-273

272                                                                         Vali Zardasht

References
Abraham B, Sankaran P. Renyi's entropy for residual lifetime distribution.(2005). Statistical Papers.
Belzunce F, Pinar J F, Ruiz J M. On testing the dilation order and HNBUE alternatives.(2005). Annals of the Institute of Statistical Mathematic.
Crescenzo A D, Longobardi M. On cumulative entropies and lifetime estimations in International Work-Conference on the Interplay Between  Natural and Articial Computation.(2009). Springer.
Farris F A. The Gini Index and Measures of Equitability.(2010). American Mathematical Monthly.
Helmers R. A strong law of large numbers for linear combinations of order statistics.(1977). Mathematisc Centrum.
Nanda A K, Chowdhury S. Shannon's entropy and Its Generalizations towards Statistics Reliability and Information Science during 1948-2018.(2019). arXiv:1901.09779[stat.OT] .
Navarro J, del Aguila Y, Asadi M. Some new results on the cumulative residual entropy.(2010). Journal of Statistical Planning and Inference.
Rajesh G, Sunoj S M. Some properties of cumulative Tsallis entropy of order Î±.(2019). Statistical papers.
Rao M, Chen Y, Vemuri B. Cumulative residual entropy: a new measure of information.(2004). IEEE Transactions on Information Theory.
RÃ©nyi A. On measures of entropy and information in Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability - Contributions to the Theory of Statistics.(1961). University of California.
Shaked M, Shanthikumar J G. Stochastic Orders.(2007). Springer.
Shannon C. A mathematical theory of communication.(1948). The Bell System Technical Journal.
Shao J. Mathematical Statistics.(2003).Springer.
Stigler S M. Linear functions of order statistics with smooth weight functions.(1974). Annals of Statistics.
Sunoj S M, Linu M N. Dynamic cumulative residual Renyi's entropy.(2012). Statistics: A Journal of Theoretical and Applied Statistics.
Wellner J A. A Gelivenko-Cantelli theorem and strong laws of large numbers for functions of order statistics.(1977). Annals of Statistics.
Zardasht V. A test for the increasing convex order based on the cumulative residual entropy.(2015). Journal of the Korean Statistical Society.