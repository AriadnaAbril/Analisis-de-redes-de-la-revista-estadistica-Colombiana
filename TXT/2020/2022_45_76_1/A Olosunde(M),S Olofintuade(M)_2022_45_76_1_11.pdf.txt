Some Inferential Problems from Log Student’s T-distribution and its Multivariate Extension. Algunos problemas inferenciales a partir de la distribucion T de Student y su extension multivariante
Obafemi Awolowo University, Ile-Ife, Nigeria
Abstract
Assumption of normality in statistical analysis had been a common practice in many literature, but in the event where small sample is obtainable, then normality assumption will lead to erroneous conclusion in the statistical analysis. Taking a large sample had been a serious concern in practice due to various factors. In this paper, we further derived some inferential properties for log student’s t-distribution (simply log-t distribution) which makes it more suitable as substitute to log-normal when carrying out analysis on right-skewed small sample data. Mathematical and Statistical properties such as the moments, cumulative distribution function, survival function, hazard function and log-concavity are derived. We further extend the results to case of multivariate log-t distribution; we obtained the marginal and conditional distributions. The parameters estimation was done via maximum likelihood estimation method, consequently its best critical region and information matrix were derived in order to obtain the asymptotic conﬁdence interval. The applications of log-t distribution and goodness-of-ﬁt test was carried out on two dataset from literature to show when the model is most appropriate.
Key words: best critical region; log-t distribution; maximum likelihood estimation; Multivariate log-t distribution; Shannon entrop.
Resumen
La suposicion de normalidad en el analisis estadistico habia sido una pratica comun en mucha literatura, pero en el caso de que se pueda obtener una muestra pequena, la suposicion de normalidad conducira a conclusiones erroneas en el analisis estadistico. En la practica, la toma de una muestra grande habia sido una gran preocupacion debido a varios factores. En este articulo, obtuvimos ademas algunas propiedades inferenciales para la distribucion t de log student (simplemente distribucion log-t) que la hace mas adecuada como sustituto de log-norma al realizar analisis en datos de muestras pequenas con sesgo a la derecha. Se derivan propiedades matematicas y estadisticas como los momentos, la funcion de supervivencia, la funcion de riesgo y la concavidad logaritmica. ampliamos aun mas  el resultado al caso de distribucion log-t multivariante; obtuvimos las distribuciones marginales y condicionales. La estimacion de los parametros se realizo mediante el metodo de estimacion de maxima verosimilitud, por lo que se derivo su mejor region critica y matriz de informacion para obtener el intervalo de conﬁanza asintotico. Las aplicaciones de la distribucion log-t y la prueba de bondad de ajuste se llevaron a cabo en dos conjuntos de datos de la literatura para mostrar cuando el modelo es mas apropiado.
Palabras clave: distribucion log-t; distribucion log-t multivariante; estimacion de maxima verosimilitud; entropia de Shannon; mejor region critica.



1. Introduction
    The student’s t-distribution is a continuous probability distribution that arises
when estimating the normally distributed population mean with unknown variance
for a small sample size. It has the degree of freedom parameter which regulates it
tails and generalizes Cauchy and normal distributions when varied, Gosset (1908)
and Fisher (1925). Some univariate and multivariate extensions of the student’s
t-distribution have been studied by Lin (1972), Kotz & Nadarajah (2004), Kibria
& Joarder (2006), Cassidy (2016) and Hassan & Assar (2016).
    However, in lifetime data analysis, Saw et al. (2002) introduced the
log-Exponential Inverse Gaussian distribution (log-EIG) by adopting the
transformation that exist between the lognormal and normal distribution. The
study showed that the log-EIG model outperformed other lifetime models such
has the gamma, Weibull, lognormal and inverse Gaussian distributions that have
been widely used. Mitzenmacher & Tworetzky (2003) also introduced the log-
t distribution as a new model and method for ﬁle size distributions and it
was remarked that the log-t was suitable just as the hybrid lognormal-pareto
distribution owing to its fewer parameters and also outperformed the lognormal
distribution.
    Moreover, the log-t distribution is a positively skewed distribution derived from
the transformation of the random variable that has a student’s t-distribution. It
possesses the same parameters as that of the student’s t but on a positive real line
like other lifetime distributions studied by Cassidy et al. (2013), Butt & Habibullah
(2016). However, several distributions such as gamma, lognormal, Weibull, log-
logistic and so on have been used in modelling lifetime data irrespective of the
sample size. Whereas, the log-t distribution which tends to perform better than
other distributions in this class due to its degree of freedom parameter which

                   Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

Some Inferential Problems from Log Student’s T-distribution...                         211

could be varied to generalize the log-Cauchy and lognormal distributions, and
more importantly when analyzing relatively small sample size data (n < 30) has
been under-utilized in literature as far as we know. It is on this basis, we examine
some inferential statistics of the log-t distribution and its multivariate extension
which may make it suitable in situations where researchers do not have large
enough data to assume lognormality.
   Finally, we applied the log-t distribution two dataset of diﬀerent sample sizes:
bladder cancer and acute leukamia data as presented by Lee & Wang (2003).
    The paper is organized as follows: In section 1, the introduction of the study is
presented. In section 2, the distribution is derived, some mathematical properties
are obtained. In section 3, the moment of the distribution is investigated. In
section 4, the Shannon entropy is obtained. In section 5, the log-concavity and
monotonicity of the distribution are investigated. In section 6, the special cases
of the distribution are studied. In section 7, the distribution parameters are
estimated by the maximum likelihood estimation (MLE) method. In section 8, the
information matrix and asymptotic conﬁdence interval are obtained. In section 9,
the distribution’s multivariate version, marginal and conditional distributions, log-
concavity, bivariate densities and contours are studied. Finally, in section 10, the
application of the distribution is investigated on two datasets of diﬀerent sample
sizes.


2. Log Student’s T-distribution
   We derive the log-t distribution using a transformation of the student’s t
density function. This is presented in proposition 2.1.
Proposition 1. Suppose that a random variable W follows the univariate
student’s t-distribution. Then it has a density function given as
                                        [        (    )2 ]−( k+1
                                                              2 )
                                Γ( k+1
                                    2  ) 1 + k1 w−µ σ
            f (w; µ, σ 2 , k) =                    √              ; w∈R (1)
                                          Γ( k2 )σ kπ
where µ ∈ R is the mean of the distribution, σ 2 > 0 is the variance and k > 0 is
the degree of freedom parameter which regulates its tails.
A random variable U is said to have log-t distribution if
                                       [       (        )2 ]−( k+1
                                                                2 )
                                             1 ln(u)−µ
                              Γ( k+1
                                  2  )   1 + k      σ
          f (u; µ, σ 2 , k) =                       √               ; u>0     (2)
                                           uΓ( k2 )σ kπ
where µ ∈ R+ , is the mean of the distribution, σ 2 > 0 is the variance and k > 0
is the degree of freedom.

Proof . The proof is obtained by simply applying the transformation U = eW
FU (u) = P r(U ≤ u) = P r(eW ≤ u), =⇒ P r(W ≤ ln(u)) = FW (ln(u)),
 d             1
   FW (ln(u)) = fW (ln(u)). Then Equation (1) becomes Equation (2).
du             u

                   Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

212                                            Akinlolu Olosunde & Sylvester Oloﬁntuade


    Henceforth equation (2) will be referred to as the Log-t Distribution (LTD).
We denote this distribution as U ∼ LTD (µ, σ, k). We shall proof in subsequent
section that when k = 1 Equation (2) reduces to log-Cauchy distribution and
approaches log-normal distribution as the degree of freedom parameter k grows
large. The plot of the density function (2) is given in ﬁgure 1 for some values of
the degree of freedom.




Figure 1: Log-T Distribution density plot for n=5, and k taking the values 2, 12 and
          99.


Proposition 2. Let U be a random variable having log-t distribution given in
Equation (2). Then the cumulative distribution function (CDF), the survival and
the hazard function are respectively;
                                             (     )
                                      1 1      1 k
                           F (a; k) = + Ra      ,    ;                       (3)
                                      2 2      2 2
                                                   (      )
                                               1 1    1 k
                      S(t) = 1 − F (a) =        − R a; ,                              (4)
                                               2 2    2 2
and
                                             t− 2 [1 + t]−( 2 )
                                                   1           k+1
                               f (t)
                    h(t) =             =                                              (5)
                             1 − F (a)   B( 12 , k2 )[1 − R(a; 21 , k2 )]
Where Ba (λ, τ ) is the incomplete beta function and R(a; λ, τ ) is the regularized
incomplete beta function.
                         2
Proof . Let t = (ln(u)−µ)
                    kσ 2  in Equation (2) then (ln(u) − µ)2 = tkσ 2 which implies
that                                       √
                                u = e(µ+σ tk)
and                                      √
                                     σ       k (µ+σ√tk)
                                du =           e        dt
                                     2       t

                  Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

Some Inferential Problems from Log Student’s T-distribution...                                        213

The density function (2) can be written as

                                                −( 2 )        k+1     √
                                Γ( k+1
                                    2 √)[1 + t]        σ                    k (µ+σ√tk)
                       2
              f (t; µ, σ , k) =                   √                           e        dt.             (6)
                                e(µ+σ tk) Γ( k )σ kπ 2
                                                  2
                                                                            t

Equation (6) can be written as

                                                     t− 2
                                                               1
                                         1
                         f (t; k) =       1 k             k+1 ;             t>0                        (7)
                                      2B( 2 , 2 ) (1 + t) 2

where B(.) is the beta function deﬁned by B(λ, τ ) = Γ(λ)Γ(τ )
                                                      Γ(λ+τ ) ,                           λ, τ ∈ R+ and
k > 0 is the degree of freedom parameter.

    Since the log-t distribution is a special case of the generalized beta distribution
of the second kind (GB2) Hence, the CDF of the log-t distribution given as:
                                                      ∫ b
                                                                    t− 2
                                                                       1
                                         1
                           F (b; k) =                                            dt                    (8)
                                      2B( 12 , k2 )
                                                                           k+1
                                                          0    (1 + t) 2
           a
where b = 1−a , t > 0, k > 0
                                                          (      )
                                           1                1 k
                             F (a; k) =               .Ba    ,                                         (9)
                                        2B( 12 , k2 )       2 2
                                                        (      )
                                         1 1              1 k
                               F (a; k) = + Ra             ,                                         (10)
                                         2 2              2 2
                                                                                 (1 k)
where Ba ( 12 , k2 ) is the incomplete beta function and Ra                       2 , 2 is the regularized
incomplete beta function.

The survival and the hazard follow by substituting (2) and (10) in each deﬁnition.
Hence                                            (       )
                                        1 1          1 k
                     S(t) = 1 − F (a) = − R a; ,           ,                  (11)
                                        2 2          2 2
and
                                                   t− 2 [1 + t]−( 2 )
                                                        1         k+1
                             f (t)     f (t)
                h(t) =               =       =                                                       (12)
                           1 − F (a)   S(t)    B( 12 , k2 )[1 − R(a; 12 , k2 )]
Some of the mathematical properties of the log-t distribution are presented below.


2.1. The Moment
   To investigate the ﬁnite moment of the log-t distribution, let u ∼ LT D(k) for
r = 1, 2, 3, . . . The rth non-central moments is given as
                                             ∫ ∞
                               µr = E[ur ] =     ur f (u)du
                                                      0


                   Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

214                                            Akinlolu Olosunde & Sylvester Oloﬁntuade


For r = 1, the ﬁrst moment is given as
                                      [             ]−( k+1
                                                         2 )
                          ∫ ∞ Γ( k+1 ) 1 + (ln(u))2
                                  2            k
                   E[u] =    u                 √             du                      (13)
                           0          uΓ( k2 )σ kπ

                                      [             ]−( k+1
                                                         2 )
                          ∫ a Γ( k+1 ) 1 + (ln(u))2
                                  2             k
                    = lim                      √             du                      (14)
                     a→∞ 0             Γ( k2 )σ kπ
                                          ∫ a[                ]−( k+1
                                                                   2 )
                       1                            (ln u)2
                  =√                lim          1+                      du          (15)
                     kB( 12 , k2 ) a→∞     0           k
The approximate solution of the integral is obtained by the series expansion given
as
           [            ]− k+1          [ (         )] [         ]
                (ln(u))2 ( 2 )                k+1        (ln x)2
             1+                   =1+ −                            + ···      (16)
                    k                           2           k
                                  (k + 1)(ln(u))2
                            =1−                   + ···
                                         2k
Integrating the result with respect to u gives,
                      [                                      ]
                           k+1
                 lim u −         (2u − 2u ln(u) + u(ln(u))2 ) = ∞                    (17)
                u→∞          2k

Hence, E[u] = ∞
  Remark: Since the mean of the log-t distribution is inﬁnite, thus higher
moments of the log-t distribution do not exist.


3. The Shannon Entropy
   An entropy provides a superior tool to quantify the amount of information (or
uncertainty) contained in a random observation regarding its parent distribution
(population). A large value of entropy implies greater uncertainty in the data.
The two most popular entropies are Renyi (1960) and ? entropies measures. The
Renyi entropy of a random variable U with density function f (u) is deﬁned by
                                 ∫ ∞
                        1
                IR =        log(     f α (u)du); α > 0, α ̸= 1.             (18)
                      1−α         0

While the Shannon entropy of a random variable U is deﬁned by:
                                        ∫
                hu = E[− ln(fu (u))] = − fu (u) ln(fu (u))du
                                                  S

where S = {u : fu (u) > 0} It is a special case of Renyi entropy as α ↑ 1.

                  Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

Some Inferential Problems from Log Student’s T-distribution...                                             215

   Now, from the transformation that resulted to equation (8), the Shannon
entropy of the log-t distribution is given by
                             ∫ ∞
                     ht = −      f (t; µ, σ 2 , k) ln f (t; µ, σ 2 , k)dt (19)
                                         0

to have
                                         ∫ ∞                        [                            ]
                                                    t− 2                         t− 2
                                                       1                            1
                            1
            h(k) = −                                       k+1 ln                                    dt   (20)
                         2B( 12 , k2 )
                                                                                           k+1
                                             0   (1 + t) 2              2B( 12 , k2 )(1 + t) 2
        [             ]∫ ∞                                               ∫ ∞
                                              t− 2                                    t− 2
                                                 1                                       1
                 1 k              1                            1
 = − ln 2B( , )                                        dt +                  ln(t)         k+1 dt
                             2B( 12 , k2 ) (1 + t) 2
                                                   k+1
                 2 2     0                                  4B( 12 , k2 ) 0        (1 + t) 2
                ∫ ∞
                                   t− 2
                                       1
    k+1
 +                   ln(1 + t)           k+1 dt
   4B( 12 , k2 ) 0             (1 + t) 2
                                                                                             (21)
the ﬁrst integral gives by the property of PDF over entire domain gives
          [          ]
                1 k
  = ln 2B( , ) +
                2 2
              [∫ ∞                    (    )              ∫ ∞                        (    ) ]
  k+1                     −1         − k+1                                1
                                                                         −2         − k+1
                   ln(t)t  2 (1 + t)    2    dt + (k + 1)     ln(1 + t)t    (1 + t)    2   dt             (22)
 4B( 12 , k2 ) 0                                           0


setting y = 1 + t, t = y − 1, dt = dy 1 < y < ∞ in the third integral, we have
                                         ∫ ∞
                                                 (y − 1)− 2 ln(y)
                                                             1


                                                           k+1           dy                               (23)
                                             1       y( 2 )

From Gradshteyn & Ryzhik (1965), formula 4.255(1) pp. 541.
     ∫ ∞
            (y − u)θ−1 ln(y)
                             dy = uθ−λ B(λ − θ, θ)[ln(u) + ψ(λ) − ψ(λ − θ)]                               (24)
       u           yλ

provided 0 < Re(θ) < Re(λ), B(·, ·) is the beta function and ψ(·) is the digamma
function. Hence,
                   ∫ ∞                                     [                ]
                          (y − 1)− 2 ln(y)
                                         1
                                                         k     k+1       k
                                   k+1            dy = B( ) ψ(     ) − ψ( )                               (25)
                    1           y( 2 )                   2      2        2

where u = 1, θ = 12 , λ = k+1
                           2 and Gradshteyn & Ryzhik (1965), formula 4.253(1)
pp 538 gives
             ∫ 1                                             (          )[                           ]
                                                     1           θ              θ      θ
                   y θ−1 (1 − y)λ ln(y)dy =             B          ,λ         ψ( ) − ψ( + λ)              (26)
               0                                     s2          s              s      s

Re(θ), Re(λ), s > 0.

                        Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

216                                                        Akinlolu Olosunde & Sylvester Oloﬁntuade


      Hence the ﬁrst integral gives
            ∫ ∞                                  (     )[                 ]
                       − 21      −( k+1 )          1 k      1       k+1
                 ln(t)t (1 + t)      2    dt = B    ,     ψ( ) − ψ(     )                       (27)
             0                                     2 2      2        2
where θ = 12 , λ = k2 and s = 1 The resultant equation gives
                (       )
                    1 k    1 1   1 k+1    (k + 1)B( k2 ) k + 1    (k + 1)B( k2 ) k
 = ln(2)+ln B        ,    + ψ( )− ψ(   )+     (     ) ψ(       )−     (     ) ψ( ) (28)
                    2 2    4 2   4   2     4B 1 , k        2       4B 1 , k      2
                                                                  2   2             2   2

Hence, the Shannon entropy of the log-t distribution is given as
                                                               (k+1)B( k )
                               {     ( 1 k ) 1 ψ( 1 ) 4B( 1 , k2) ψ( k+1
                                                                      2 )}
                                   2B 2 , 2 e 4 2 e       2 2
                        log2                            (k+1)B( k )
                                                                             bits               (29)
                                                                 2 ψ( k )
                                                                      2
                                           1    1
                                         e 4 ψ( 2 ) e      (
                                                         4B 1 , k
                                                            2 2   )


4. Log-concavity and Monotonicity
    Bagnoli & Bergstrom (2005) deﬁned the log-concavity of twice-diﬀerentiable
real-valued function, g whose domain is an interval on the extended real line
as a function that satisﬁes the condition: (ln g(x))′′ < 0. Log-concavities of
distributions have important properties in modelling. Based on this property,
the log-concavity of the log-t distribution is presented in proposition 3.3.1.
Proposition 3. The Log-t distribution with probability density function, f (u) is
either log-concave or log-convex on its entire domain. It depends on the values of
the random variable U and the degree of freedom k.

Proof . Given the density in equation (2), without loss of generality when µ = 0
and σ = 1, the natural logarithm is given as
              [ (     ])     [    ]            (     ) [              ]
                  k+1           k       √        k+1     k + (ln(u))2
 ln f (u) = ln Γ
                   2
                         − ln Γ( ) − ln( kπ) −
                                2                 2
                                                      ln
                                                               k
                                                                        − ln(u)                 (30)
The ﬁrst order derivative of equation (30) with respect to u gives
                                (       )[                 ]
                 d ln f (u)       k+1          2 ln(u)         1
                            =−                          2
                                                             −                                  (31)
                     du             2      u[k + (ln(u)) ]     u
The second order derivative of equation (30) with respect to u gives
                  (       )[                                       ]
   d2 ln f (u)      k+1      2k(1 − ln(u)) − 2(ln(u))2 − 2(ln(u))3     1
               =−                                                    + 2                        (32)
      du2             2               [u + k + (ln(u))2 ]2            u
                                                                                            2
Hence, from equation (32), the log-concavity and log-convexity of d [ln   f (u)]
                                                                        du2      over
positive real line depends on the values of random variable U and degree of freedom
k. For example, suppose u ∈ (0, 1.7] and k > 1 then, [ln f (u)]′′ < 0, this implies
log-concavity and the log-t density function is monotonically increasing on the
real line because it is easy to see that f (u + ϵ) − f (u) > 0 for any k > 0. The
case is reversed when on the interval (1.7, ∞) for k > 1, it is log-convex and
monotonically decreasing. Therefore, we conclude that equation (32) is neither
strictly log-concave nor log-convex on its entire domain.

                       Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

Some Inferential Problems from Log Student’s T-distribution...                              217

5. The Special Cases of The Log-t Distribution
Proposition 4. Suppose U is a random variable having equation (2) as its density
function then;

  1. for k = 1, we have the log-Cauchy distribution; and

  2. as k → ∞ we have the lognormal distribution

Proof . The proof is trivial, by substituting k = 1 into the density function of
log-t distribution in equation (2) and noting

                                             1   √
                                           Γ( ) = π
                                             2
We have,                                 [                  ]
                                       1          σ
                        f (u; µ, σ) =                         ;                 u>0        (33)
                                      uπ σ 2 + (ln(u) − µ)2
where µ ∈ R and σ > 0 which is the density function of the log-Cauchy distribution.
For the second part and without loss of generality from equation (2), let µ = 0
and σ = 1. (2) becomes the standardized probability density function of the log-t
distribution given by
                                          [               ]−( k+1
                                                               2 )
                                                 (ln(u))2
                                 Γ( k+1
                                     2  )   1 +      k
                      f (u; k) =                   √               ;0 < u < ∞              (34)
                                           uΓ( k2 ) kπ
                  2
Let p1 = (ln(u))
             k   and k = p(ln(u))2 . The principal part of the numerator can be
written as                                         (              )
                  [             ] k+1
                               2 −( 2 )
                                         [       ]− p(ln(u)) 2 +1

                       (ln(u))                 1         2
                    1+                  = 1+
                           k                   p
                                 [       ]− p(ln(u)) 2
                                                          [          ]− 12
                                    1           2
                                                              1
                               = 1+                      × 1+
                                    p                         p
              [      ]p
Recall, limp→∞ 1 + p1 = e

                                  [      ]p −(ln(u)) 2
                                                                 [              ] −1
                                   1            2
                                                                            1     2
                         = lim 1 +                       × lim       1+
                          p→∞      p                      p→∞               p
                 −(ln(u))2
which gives, e       2       × 1. Taking the other part of (34)

                         Γ( k+1
                              2 )      Γ( k+1
                                           2 )       Γ( k+1
                                                         2 )    1
                            k
                               √   =       √      =      √ √
                        uΓ( 2 ) kπ      k     k2π     k     k u 2π
                                     uΓ( )   2      Γ( )
                                                     2           2      2


                        Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

218                                                 Akinlolu Olosunde & Sylvester Oloﬁntuade

                 Γ( k+1
To show that         2 )√ k = 1we apply the asymptotic formula given by
                Γ( k
                   2)     2

                                              √
                                                  2πe−λz (λz)λz+τ − 2
                                                                         1
                              Γ(λz + τ ) ∼                                              (35)

where λ = 12 , τ = 12 and z = k respectively in the numerator while λ = 21 , τ = 0
and z = k respectively in the denominator.
      Substituting this values we have
                                       √            k
                                         2πe− 2 k2 2
                                               k
                    Γ( k+12 )   1                             1
                          √ × √    ∼√                   1 ×
                                                             √
                              u 2π
                                                k
                                                  − 1
                                                      +     u 2π
                                      2πe− 2 k2 2 2 2
                                           k
                   Γ( k2 ) k2

                                               1
                                          ∼1× √
                                             u 2π
Combining these results we obtain the standardized lognormal distribution deﬁned
by
                                1
                      f (u) = √ e− 2 (ln(u)) ; 0 < u < ∞
                                       1     2
                                                                             (36)
                              u 2π
Hence, the standardized log-t distribution generalizes the standardized lognormal
distribution as the degree of freedom parameter k → ∞.


6. Maximum Likelihood Estimate of the Parame-
   ters
   Given a random sample of n observations u1 , u2 , . . . , un from log-t distribution,
the likelihood function is given as:

                                                   ∏
                                                   n
                              L(ui ; µ, σ, k) =          f (ui ; µ, σ, k),              (37)
                                                   i=1

The log-likelihood function gives
                                          (   )           ( ) ∑    n
                                         k+1               k
               ℓ(ui ; µ, σ, k) = n ln Γ          − n ln Γ       −     ln(ui )
                                          2                2      i=1
                                          n           n
                               − n ln(σ) − ln(k) − ln(π)                                (38)
                                          2           2
                                 (       ) n     [ 2                    ]
                                    k+1 ∑         kσ + (ln(ui ) − µ)2
                               −              ln
                                      2   i=1
                                                           kσ 2

Diﬀerentiating equation (38) with respect to each parameter to have

                                               ∑n [                       ]
                   ∂                                     ln(ui ) − µ
                     ℓ(ui ; µ, σ, k) = (k + 1)                                          (39)
                  ∂µ                           i=1
                                                    kσ 2 + (ln(ui ) − µ)2


                     Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

Some Inferential Problems from Log Student’s T-distribution...                          219
                                        (         )∑
                                                   n [                            ]
            ∂                   −n          k+1               (ln(ui ) − µ)2
              ℓ(ui ; µ, σ, k) =    +                                                   (40)
           ∂σ                   σ            σ      i=1
                                                          kσ 2 + (ln(ui ) − µ)2
                          [ (       )      ] (          ) n [                       ]
   ∂                    n       k+1      1       k+1 ∑            (ln(ui ) − µ)2
      ℓ(ui ; µ, σ, k) =     ψ         −      +
   ∂k                   2         2      k         2k    i=1
                                                              kσ 2 + (ln(ui ) − µ)2
                                 [ 2                  ]
                        1∑
                           n
                                  kσ + (ln(ui ) − µ)2
                      −       ln
                        2 i=1            kσ 2
                                                                                       (41)

       ∂ ln Γ( k+1
                2 )
                          (    )     ∂ ln Γ( k2 )       ( )
where               = 12 ψ k+1
                            2    and              = 12 ψ k2 ψ(, ) is the digamma
            ∂k                           ∂k
function.
    Setting equations (39), (40) and (41) equals zero and solving simultaneously
does not give a closed form solution. Hence, we adopt a numerical approach
(Newton-Raphson Method) which gives the approximate value for each parameter
estimated from the sample data.


7. Information Matrix and Asymptotic Conﬁdence
   Interval
   In statistical inference, the inverse of the Fishers information matrix is often
used to construct the conﬁdence interval and in testing hypotheses.
    For the asymptotic inference of the parameter space Φ = (µ, σ, k), the Fisher
information matrix I(Φ) is required such that its inverse is known to be the
asymptotic variance matrix of the maximum likelihood estimators.
   The Fisher information matrix for the log-t distribution is presented in the
proposition given below

Proposition 5. Given a random variable, u that follows log-t distribution, let Φ
be the parameter space µ, σ and k, then the second order partial derivatives of the
log-likelihood function form the elements of the Fisher information matrix
                                                             
                                        Iµµ        Iµσ    Iµk
                             I(Φ) = −  Iσµ        Iσσ    Iσk 
                                        Ikµ        Ikσ    Ikk

Proof . The elements of I(Φ) are

                                          ∑n {                            }
                         ∂ 2 l∗                −kσ 2 + [ln(ui ) − µ̂]2
                Iµµ =           = (k + 1)                                              (42)
                         ∂ µ̂2            i=1
                                               [kσ 2 + [ln(ui ) − µ̂]2 ]2

                                                 ∑n {                             }
                   ∂ 2 l∗     ∂ 2 l∗                    −2kσ̂[ln(ui ) − µ̂]
           Iµσ =            =          = (k + 1)                                       (43)
                   ∂ µ̂∂ σ̂   ∂ σ̂∂ µ̂           i=1
                                                      [kσ̂ 2 + [ln(ui ) − µ̂]2 ]2


                    Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

220                                                                  Akinlolu Olosunde & Sylvester Oloﬁntuade


                                                n {
                                                ∑                       }                n {
                                                                                         ∑                                    }
          ∂ 2 l∗          ∂ 2 l∗                    −σ 2 [ln(ui ) − µ̂]                                 [ln(ui ) − µ̂]
Iµk =               =               = (k + 1)                                       +                                             (44)
         ∂ µ̂∂ k̂        ∂ k̂∂ µ̂               i=1      k̂σ 2 + [ln(ui ) − µ̂]2         i=1      k̂σ 2 + [ln(ui ) − µ̂]2

                    (              )∑
                                    n {                                   } (       ) n {                         }
        ∂ 2 l∗          k+1                           −2kσ̂                   k+1 ∑            [ln(ui ) − µ̂]         n
Iσσ =          =                                                           −                                        + 2
        ∂ σ̂ 2           σ̂          i=1
                                            [kσ̂ 2 + [ln(ui ) − µ̂]2 ]2        σ̂ 2  i=1  k̂σ 2 + [ln(ui ) − µ̂]2    σ̂
                                                                                                                                  (45)

                                        (          n {
                                                  )∑                                     }              {                                }
                                                                                                 1 ∑
                                                                                                   n
         ∂ 2 l∗         ∂ 2 l∗              k+1                  [ln(ui ) − µ̂]2                                  [ln(ui ) − µ̂]2
Iσk =               =              =−                                                        +
         ∂ σ̂∂ k̂       ∂ k̂∂ σ̂             σ̂            [kσ̂ 2 + [ln(ui ) − µ̂]2 ]2           σ̂         [kσ̂ 2 + [ln(ui ) − µ̂]2 ]
                                                   i=1                                            i=1

                                                                                                                                  (46)
                    [ (             )        ( )          ] (        ) n {                                  }
           ∂ 2 l∗ n     ′    k̂ + 1       ′    k̂     2n       k+1 ∑                  σ̂[ln(ui ) − µ̂]2
  Ikk =             =Ψ                −Ψ           +       −
          ∂ k̂2   4             2              2      k̂2        2k̂     i=1
                                                                                [kσ̂ 2 + [ln(ui ) − µ̂]2 ]2
                  {                           }           {                           }
           1 ∑                                     1 ∑
                n                                      n
                         [ln(ui ) − µ̂]2                        [ln(ui ) − µ̂]2
        −                                       +
          2k̂2 i=1 [kσ̂ 2 + [ln(ui ) − µ̂]2 ]     2k̂ i=1 [kσ̂ 2 + [ln(ui ) − µ̂]2 ]
                                                                                                                                  (47)
              ′
Where Ψ (·) is the trigamma function.

   Consequently, let parameter vector Φ = (µ, σ, k) and the corresponding
maximum likelihood estimate of ϕ as ϕ̂ = (µ̂, σ̂, k̂), the asymptotic normality
results can be written as
                                                (ϕ̂ − ϕ) → N3 (0, (I(ϕ))−1 )                                                       (48)
where I(ϕ) is the Fishers information matrix. Therefore, under certain regularity
conditions of asymptotic properties of the maximum likelihood estimation ensure
that                     √
                           n(Φ̂ − Φ) →d N3 (0, I(Φ)−1 )
where →d means the convergence in distribution, with mean 0 = (0, 0, 0, )T and
3X3 variance covariance matrix I(Φ)−1 .
      Hence, the 100(1 − α) conﬁdence interval for Φ ≡ (µ, σ, k) becomes:
                                          √
                                 Φ̂ ± C 2 var(Φ̂)
                                        α



where C α2 is the standard normal at the signiﬁcance level α2 and var(·)’s denote
the diagonal elements of I(Φ)−1 corresponding to the model’s parameters.


8. The Best Critical Region for the Mean
   µ Parameter
    The best critical region is obtained by the Neymann Pearson lemma deﬁned
as follows:

                                   Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

Some Inferential Problems from Log Student’s T-distribution...                                                   221

    Neyman-Pearson Lemma: Let u1 , u2 , . . . , un be a random sample from
f (u, θ) where θ is one of the known values θ0 and θ1 . Let 0 < α < 1 be ﬁxed, q is
a positive constant and A is a subset ∀ x ∈ α that satisfy
  (i) P r [u1 , u2 , . . . , un ∈ A|H0 ] = α
          1 ,u2 ,...,un ∈A|H0 )
 (ii) L(u
      L(u1 ,u2 ,...,un ∈A|H1 ) ≤ q;              0<q<1

   The Neyman-Pearson lemma demonstrates that the likelihood ratio test is the
most powerful test. The likelihood function of the log-t distribution is given by
                ( k+1 )n (               )n               ∏n [        (            )2 ]−( k+1
                                                                                            2 )
                 Γ( 2 )             1              1                1 ln(ui ) − µ
L(u; µ, σ, k) =                    √         . ∑ n             1 +
                  Γ( k2 )         σ kπ           i=1 ui i=1         k       σ
                                                                                           (49)
To test simple null hypothesis (H0 ) against simple alternative hypothesis (H1 )
denoted by
                             H0 : µ0 = 0 vs H1 : µ1 = 1
                 (          )n                                  [       (          )2 ]−( k+1
                                                                                           2 )
                   Γ( k+1
                        2 )           2 −n        1
                                                          ∏n          1 ln(ui )−µ0
                        k       .(kπσ   )  2 . ∑ n      .         1 +
                    Γ( 2 )                           u      i=1       k     σ
 L(u; µ0 = 0)                                    i=1 i
               =(           )                                   [
 L(u; µ1 = 1)                 n
                                                          ∏n            (          )2 ]−( k+1
                                                                                           2 )
                   Γ( k+1
                        2 )           2 )− n      1                   1 ln(ui )−µ1
                                .(kπσ      2 . ∑n       .         1 +
                    Γ( k )     2
                                                     ui     i=1 i=1   k     σ

                                                                                                                (50)
                               [            (             )2 ]−( k+1
                                                                  2 )
                    ∏n                          ln(ui )
                         i=1       1 + k1         σ
                =          [                                             ≤ q;     f or   0<q≤1                  (51)
                    ∏n                  (                  )2 ]−( k+1
                                                                   2 )
                                            ln(ui )−1
                     i=1       1 + k1           σ

Taking the natural logarithm of both sides of equation (51) to have
     (         )∑    [      (      ) ] (    ) n    [      (          ) ]
                                         k+1 ∑
                n
         k+1              1 ln(ui ) 2                   1 ln(ui ) − 1 2
 −                 ln 1 +             +          ln 1 +                  ≤ ln(q) (52)
          2    i=1
                          k   σ           2  i=1
                                                        k     σ
         (         )∑
                    n
                      [            [            (             )2 ]       [        (                 )2 ] ]
             k+1                  1                 ln(ui )                   1       ln(ui ) − 1
 =−                        ln 1 +                                    − ln 1 +                                ≤ ln(q)
              2     i=1
                                  k                   σ                       k           σ
                                                                                                                (53)
               (         )∑
                          n            [                              ]
                   k+1        kσ 2 + (ln(ui ))2          kσ 2
         =−                ln                                           ≤ ln(q)                                 (54)
                    2  i=1
                                    kσ 2        kσ 2 + (ln(ui ) − 1)2
                       (      ) n      [                        ]
                         k+1 ∑             kσ 2 + (ln(ui ))2
                    =−              ln                            ≤ ln(q)                                       (55)
                            2   i=1
                                         kσ 2 + (ln(ui ) − 1)2
The exponential of both sides gives
                     ∑n [                        ]
                             kσ 2 + (ln(ui ))2
                                                   ≥ qe−( k+1 )
                                                           2
                                                                                                                (56)
                     i=1
                           kσ + (ln(ui ) − 1)
                              2                2


Hence, we have a test whose critical point of acceptance or rejection of H0 or H1
is at q.

                         Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

222                                                Akinlolu Olosunde & Sylvester Oloﬁntuade


9. Multivariate Extension of Log-t Distribution
Proposition 6. A random vector w = (w1 , . . . , wp )′ with p ≥ 1 has a p-
dimensional log-t distribution with mean vector µ = (µ1 , . . . , µp )′ , positive deﬁnite
symmetric matrix Σpxp and degree of freedom parameter k ∈ (0, ∞) if its density
is                                                                            k+p

                             2 ){1 + k [ln(w) − µ] Σ
                         Γ( k+p       1             ′ −1
                                                         [ln(w) − µ]}−( 2 )
      fw (w; µ, Σ, k) =               ∏p              p  p       1

                                        i=1 wi Γ( 2 )k π |Σ|
                                                  k   2  2       2




Proof . From the univariate log-t distribution we derive its multivariate
counterpart by some transformations of random variable given as follows: Let
w = Φ(u) where Φ is a smooth and bijective function.

                            Pw (w) = Pu (Φ−1 (w))|detJΦ−1 (w)|

where JΦ−1 is the Jacobian matrix of the inverse transformation.
   By deﬁnition, if u = ln(w) ∼ t(µ, σ, k) then w = eu ∼ LT D(µ, σ, k)
                        −1
u = Φ−1 (w) = ln(w), dΦdw = | w |, JΦ (w) = diag( w1 , . . . , wn ).
                               1
                                     −1
                                                    1           1
                                                                 ∏n
   Since one of the properties of a diagonal matrix is |D| = i=1 di . then,

                                            ∏
                                            n
                                                                    1
                        |detJΦ−1 (w)| =           wi−1 =
                                            i=1
                                                           w1 , w 2 , . . . , w n

                                                           dΦ−1 (w)
                             fw (w) = fu [Φ−1 (w)].|                |
                                                             dw
Also, by the Mahalanobis distance approach,
                    (               )2
                        ln(w) − µ
                                         = [ln(w) − µ]′ (σ 2 )−1 [ln(w) − µ]
                            σ

                                [ln(w) − µ]′ Σ−1 [ln(w) − µ]
Thus, the density function of the multivariate log-t distribution is given by
                                                                                    k+p

                               2 ){1 + k (z(w) − µ) Σ
                           Γ( k+p      1             ′ −1
                                                          (z(w) − µ)}−( 2 )
        f (w; µ, Σ, k) =              ∏p               p  p   1                           (57)
                                         i=1 wi Γ( 2 )k π |Σ|
                                                   k   2  2   2



where p ≥ 1, Σ is positive deﬁnite symmetric matrix, k ∈ (0, ∞), µj > 0,
j = 1, 2, 3, . . . , p and z = z(w) = [ln(w1 ), . . . , ln(wp )]′ and µ = [µ1 , . . . , µp ]′
   The degree of freedom parameter k is also referred to as the shape parameter
because the peakedness of equation (57) may be decrease, preserved or increased
by varying k.
      The distribution is said to be central if µ = 0; otherwise it is non-central.
   If p = 1, µ = 0 and Σ = 1, then equation (57) is the density function of the
univariate log-t distribution with degree of freedom k.

                     Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

Some Inferential Problems from Log Student’s T-distribution...                                223

    If k = 1, then equation (57) is the p-variate log-Cauchy distribution and the
limiting of equation (57) as k → ∞ is the joint PDF of the p-variate lognormal
distribution with mean vector µ and covariance matrix Σ.



9.1. Marginal Distributions
   Let w be p-variate log-t distribution with degree of freedom k, mean vector µ,
covariance matrix Σ. Consider the partition using the notation in equation (57)
     (      )       (      )            (             )
         z1           µ1                   Σ11 Σ12
z=            ,µ=            and Σ =                     where z1 is p1 x1 and Σ11 is
         z2           µ2                   Σ21 Σ22
p1 x p1 . Then, z1 has p1 -variate log-t distribution with degree of freedom k, mean
vector µ1 , covariance matrix Σ11 and with joint PDF given by
                                             [                                  ]−( k+p 1
                                                                                          )
                     Γ( k+p
                          2 )
                            1
                                          1                                           2
 f (z1 ) =              p1        1 X 1 +   (z1 − µ1 )′ Σ−1
                                                         11 (z1 − µ1 )                        (58)
           w1 Γ( 2 )(kπ) 2 |Σ11 | 2
                 k                        k

Similarly, z2 also has the (p − p1 )-variate log t distribution with degree of freedom
k, mean vector µ2 , covariance matrix Σ22 and with joint PDF given by
                                         [                                    ]−( k+p−p 1
                                                                                          )
                   Γ( k+p−p 1
                              )                 1                                    2
 f (z2 ) =               2
                           p1        1       1 + (z2 − µ2 )′ Σ−1
                                                              22 (z2 − µ2 )                   (59)
             w2 Γ( k2 )(kπ) 2 |Σ22 | 2          k


9.2. Conditional Distributions
                                                                          1             1       1
   Considering central log-t such that µ = 0, let w = w1 w2 , |Σ| 2 = |Σ11 | 2 |Σ22 | 2
                                                    f (z1 , z2 )
                                    f (z2 |z1 ) =
                                                      f (z1 )
                                                  1 [                 ]−( k+p   )
                                                                              1
                             w1 Γ( k+p
                                    2 )    |Σ11 | 2 1 + k1 z′1 Σ−1
                                                                11 z1
                                                                            2

             f (z2 |z1 ) =                                                                    (60)
                                            |Σ| 2 [1 + 1 z′ Σ−1 z]−( 2 )
                                p1              1                        k+p
                           w(kπ) 2 Γ( k+p
                                       2 )
                                                           k     11

Since |Σ| = |Σ11 ||Σ22 − Σ21 Σ−1
                              11 Σ12 | and

                           z′ Σ−1 z = z′1 Σ−1       ′    −1
                                           11 z1 + z2.1 Σ22.1 z2.1


9.3. Log-concavity of Multivariate Log-t Distribution
   A non-negative function ψ : Rm → R is log-concave if for all u,w ∈ Rm and
β ∈ (0, 1) we have
                         ψ(βu + (1 − β)w) ≥ [ψ(u)]β [ψ(w)]1−β
If ψ(u) > 0 for all u ∈ Rm then,
                   ln ψ(βu + (1 − β)w) ≥ β ln ψ(u) + (1 − β) ln ψ(w)

                      Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

224                                                  Akinlolu Olosunde & Sylvester Oloﬁntuade


Suppose Ψ(u) = − ln ψ(u) then the deﬁnition is equivalent to ▽2 Ψ(u ≥ 0)
provided Ψ(·) is twice diﬀerentiable and the Hessian elements given by

                                             ∂ 2 Ψ(u)
                           ▽2 Ψ(u)i,j =               , i, j = 1, . . . , n
                                              ∂ui uj

exist.

Proposition 7. Let U = (U1 , . . . , Un ) be a random vector with density function
Ψ(u) = f (u1 , . . . , un ) of the multivariate log-t distribution given equation (57) then
Ψ(u) is neither log-concave nor log-convex in its entire domain.


Proof . The multivariate log-t distribution is
                                       [                 2
                                                             ]−( k+1
                                                                  2 )
                                           1 + (ln(u))
                                                   k
                         ψ(u; k) = C                                    ;   u>0                (61)
                                                     u
              Γ( k+1 )
where C = Γ( k )2√kπ
               2

                                                 (             )     [             ]
                                                     k+1                  (ln(u))2
         Ψ(u) = − ln Ψ(u) = − ln(C) +                              ln 1 +            + ln(u)   (62)
                                                      2                       k

The ﬁrst order partial derivative of equation (62) with respect to u is

                                              (k + 1) ln(u)   1
                               Ψ′ (u) =                   2
                                                            +                                  (63)
                                             u[k + (ln(u)) ] u

The second order partial derivative of equation (62) with respect to u is

                           (k + 1)[k − k ln(u) − (ln(u))2 − (ln(u))3 ]   1
              Ψ′′ (u) =                               2 2
                                                                       − 2                     (64)
                                       [uk + u(ln(u)) ]                 u

Thus,
                                           √                     √
                   ∂ 2 Ψ(u)   (k + p)[k − k z ′ z − z ′ z − z ′ z z ′ z]   1
                            =                       ′    2
                                                                         − 2                   (65)
                    ∂ui uj               [kui + ui z z]                   ui
for i ̸= j = 1, . . . , n. The elements of the Hessian matrix A are given by the above
second order partial derivatives and the corresponding quadratic form becomes
                                                [            ]
                                                    ∂ 2 Ψ(u)
                               Z′ AZ = Z′                      Z≥0                             (66)
                                                     ∂ui uj

Therefore, the multivariate log-t density is neither strictly log-concave nor log-
convex on its entire domain, but its log-concavity or log-convexity depends on the
random vectors Ui and their respective values of degree of freedom ki .

                      Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

Some Inferential Problems from Log Student’s T-distribution...                                                                     225

9.4. Bivariate Log-t Distribution
   Considering the two random variables u1 and u2 in terms of their individual
parameters: µu1 = E(u1 ), µu2 = E(u2 ), σ11 = var(u1 ), σ22 = var(u2 ),
ρ12 = √σσ1112σ22 = corr(z(u1 ), z(u2 )) and a positive semi-deﬁnite 2 × 2 variance-
covariance matrix:
                (                              )   (                   )
                    var(u1 )     cov(u1 , u2 )         σu2 1  ρσu1 σu2
         Σ=                                      =
                  cov(u1 , u2 )    var(u1 )          ρσu1 σu2    σu2 2
                     (               )        (                 )
                         z(u1 )                   ln(u1 )
where z(u) =                              =                         .
                         z(u2 )                   ln(u2 )
   Then the joint PDF becomes
                                                                                                                              (      )
           {                  [(                )2       (                )2           (                )(                )]}− k+2
                                   z(u1 )−µu1                z(u2 )−µu2                    z(u1 )−µu1        z(u2 )−µu2         2
 Γ( k+2
     2 )
                      1
               1 + k(1−ρ 2)           √
                                        σ11          +          √
                                                                  σ22          − 2ρ           √
                                                                                                σ11
                                                                                                                √
                                                                                                                  σ22
                                                                  √
                                              u1 u2 Γ( k
                                                       2 )(kπ)          σ11 σ22 (1 − ρ2 )
                                                                                                                                  (67)
If the random variable u1 and u2 are uncorrelated so that ρ = 0, the joint
density can be written as the product of two univariate log-t densities of the
form f (u1 , u2 ) = f (u1 )f (u2 ) where u1 and u2 are independent.
   Thus, equation (67) becomes

                                     {            [(                     )2        (                )2 ]}−( k+2
                                                                                                             2 )
                                                         z(u1 )−µu1                    z(u2 )−µu2
                                         1 + k1             √
                                                              σ11              +          √
                                                                                            σ22
                 f (u1 , u2 ) =                                               √                                                   (68)
                                                             u1 u2 Γ( k2 )(kπ) σ11 σ22

The densities and contour plots for diﬀerent degrees of freedom parameter k are
shown below It is observed that, as the degree of freedom increases the kurtosis of
the densities increase. Also, increase in the value of k ﬂattens the contours. The
contour plots show the asymmetric nature of the bivariate log-t distribution. One
desirable property of this distribution is its adaptivity to both peakedness and
ﬂatness in the dataset by varying the value of the degree of freedom k. Therefore,
the distribution is ﬂexible enough to capture the heavy-tail behaviour of large
datasets.



10. Applications
    In this section, we provide applications to two datasets presented by Lee &
Wang (2003) to illustrate the performance of the lognormal and log-t distributions.
The goodness-of-ﬁt of statistics for these distributions are compared and the
maximum likelihood estimations of their parameters are also provided. The log-
likelihood and Akaike information criterion (AIC) are compared for the ﬁtted
distributions. However, the smaller these values the better the ﬁt.

                              Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

226                                          Akinlolu Olosunde & Sylvester Oloﬁntuade




      Figure 2: Densities and contour plots for the bivariate log-t distribution.


10.1. Remission Times of Bladder Cancer Patients
    The ﬁrst dataset is the remission times (months) of 128 bladder cancer patients
as shown below: 0.08, 2.09, 3.48, 4.87, 6.94, 8.66, 13.11, 23.63, 0.20, 2.23, 0.52,
4.98, 6.97, 9.02, 13.29, 0.40, 2.26, 3.57, 5.06, 7.09, 0.22, 13.80, 25.74, 0.50, 2.46,
3.64, 5.09, 7.26, 9.47, 14.24, 0.82, 0.51, 2.54, 3.70, 5.17, 7.28, 9.74, 14.76, 26.31,
0.81, 0.62, 3.82, 5.32, 7.32, 10.06, 14.77, 32.15, 2.64, 3.88, 5.32, 0.39, 10.34, 14.38,
34.26, 0.90, 2.69, 4.18, 5.34, 7.59, 10.66, 0.96, 3.66, 1.05, 2.69, 4.23, 5.41, 7.62,
10.75, 16.62, 43.01, 0.19, 2.75, 4.26, 5.41, 7.63, 17.12, 46.12, 1.26, 2.83, 4.33, 0.66,
11.25, 17.14, 79.05, 1.35, 2.87, 5.62, 7.87, 11.64, 17.36, 0.40, 3.02, 4.34, 5.71, 7.93,
11.79, 18.10, 1.46, 4.40, 5.85, 0.26, 11.98, 19.13, 1.76, 3.25, 4.50, 6.25, 8.37, 12.02,
2.02, 0.31, 4.51, 6.54, 8.53, 12.03, 20.28, 2.02, 3.36, 6.76, 12.07,0.73, 2.07, 3.36,
6.93, 8.65, 12.63, 22.69, 5.49


Table 1: Comparison of lognormal distribution with log-t distribution in the analysis of
         bladder cancer patents data.
                         Parameter        Lognormal      Log-T
                         µ̂                1.75345      1.76490
                         σ̂                12.16384     0.37002
                         k̂                 ******      3.80275
                         Log-likelihood   -228.3571    -228.3563
                         AIC               460.7142    462.7126




                   Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

Some Inferential Problems from Log Student’s T-distribution...                         227

10.2. Remission Times of Acute Leukamia Patients
    The second dataset is the remission times (in weeks) of 21 acute leukamia
patients as shown below: 1, 1, 2, 2, 3, 4, 4, 5, 5, 6, 8, 8, 9, 10, 10, 12, 14, 16, 20,
24, 34.

Table 2: Comparison of lognormal distribution with log-t distribution in the analysis of
         acute leukamia patents data.
                         Parameter        Lognormal      Log-T
                         µ̂                1.84886      1.84903
                         σ̂                4.36282      0.00605
                         k̂                 ******      3.37290
                         Log-likelihood   -41.71817    -34.90832
                         AIC               87.43634    75.81664




10.3. Discussion
    The probability density plot shown in Figure(1) reveals how increase in the
degree of freedom parameter of the log-t distribution regulates its tails. Moreover,
the ﬂexibility and heaviness of its tails accommodate more data. Figure(2) shows
the bivariate densities and contour plots. Table 1. shows the comparison of
log-normal distribution with log-t distribution on bladder cancer patients data
with relatively large sample size (n = 128). The comparison was done using the
AIC values. Using this data, the maximum Likelihood estimate of parameters of
lognormal distribution are µ̂ = 1.75345 and σ̂ = 12.16384. While the parameters
of the log-t distribution are µ̂ = 1.76490, σ̂ = 0.37002 and k̂ = 3.80275. More so,
the AIC values of the lognormal and log-t distributions are 460.7142 and 462.7126
respectively. The lognormal distribution which has the smaller value of AIC is
considered to ﬁt the data better. In the same vein,Table 2. shows the comparison
of lognormal distribution with log-t distribution on acute leukamia patients data
with relatively small sample size (n = 21). The comparison was done using the
AIC values. Using this data, the maximum Likelihood estimate of parameters of
lognormal distribution are µ̂ = 1.84886 and σ̂ = 4.36282. While the parameters
of the log-t distribution are µ̂ = 1.84903, σ̂ = 0.00605 and k̂ = 3.37290. More so,
the AIC values of the lognormal and log-t distributions are 87.43634 and 75.81664
respectively.The log-t distribution which has the smaller value of AIC is considered
to ﬁt the data better. Therefore, the log-t distribution outperforms the lognormal
distribution for relatively small sample size (n < 30).


11. Conclusion
   This study examined some inferential statistics of the log-t distribution which
has degree of freedom parameter that regulates it tails. It generalizes both log-
Cauchy and lognormal distributions. It ﬁts better relatively small sample size data
than the lognormal distribution. The multivariate log-t distribution is unique and

                   Revista Colombiana de Estadística - Applied Statistics 45 (2022) 209–229

228                                           Akinlolu Olosunde & Sylvester Oloﬁntuade


generalizes the multivariate log-Cauchy and multivariate lognormal diatributions.
Therefore, we advocate its application in survival analysis in situations where
researchers could not get large enough data to assume lognormality.
            [                                                                ]
                Received: September 2020 — Accepted: December 2021


References
Bagnoli, M. & Bergstrom, T. (2005), ‘Log-concave probability and its applications’, Economic theory 26(2), 445–469.
Butt, A. & Habibullah, S. (2016), On the derivation of the sia-log student’s t distribution, in ‘Proceedings of the 2nd International Multi-Disciplinary Conference’, Gujrat, Pakistan.
Cassidy, D. T. (2016), ‘A multivariate student’s t-distribution’, Open Journal of Statistics 6(1), 443–450.
Cassidy, D. T., Hamp, M. J. & Ouyed, R. (2013), ‘log student’s t-distribution-based option sensitivities: greek for the gosset formulae’, Quantitative ﬁnance 13(1), 1289–1302.
Fisher, R. A. (1925), ‘Application of “Students” distribution’, Metron 5(1), 90–104.
Gosset, W. S. (1908), ‘The probable error of a mean’, Biometrika 6(1), 1–25.
Gradshteyn, I. S. & Ryzhik, I. M. (1965), Tables of Integral Series and Products, Academic Press, New York.
Hassan, A. & Assar, S. (2016), ‘A compound class of lifetime distributions’, Institute of statistical Studies and Research (doi:10.13140/RG.2.2.24870.93760).
Kibria, B. M. & Joarder, A. H. (2006), ‘A short review of multivariate t- distribution’, Journal of Statistical Research 40(1), 59–72.
Kotz, S. & Nadarajah, S. (2004), Multivariate T Distributions and their Applications, University Press, Cambridge.
Lee, E. T. & Wang, J. W. (2003), Statistical Methods for Survival Data Analysis, 3 edn, John Wiley & Sons, Ltd, New York.
Lin, P.-E. (1972), ‘Some characterizations of the multivariate t distribution’, Journal of Multivariate Analysis 2(3), 339–344.
Mitzenmacher, M. & Tworetzky, B. (2003), New models and methods for ﬁle size distributions, in ‘Proceedings of the Annual Allerton Conference on Communication Control and Computing’, Vol. 41, The University; 1998, pp. 603–612.
Renyi, A. (1960), ‘On measures of entropy and information’, Berkeley Symposium on Mathematical Statistics and Probability 1(1), 547–561.
Saw, S. L., Balasooriya, U. & Tan, K. C. (2002), ‘The log-eig distribution: A new probability model for lifetime data’, Communications in Statistics-Theory and Method 31(1), 11–23.
