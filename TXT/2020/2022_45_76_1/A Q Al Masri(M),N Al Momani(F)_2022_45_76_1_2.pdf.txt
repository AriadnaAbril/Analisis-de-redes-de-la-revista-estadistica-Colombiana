 Bahadur’s Stochastic Comparison of Combining inﬁnitely Independent Tests in Case of Extreme Value Distribution. Comparación estocástica de Bahadur de la combinación de pruebas inﬁnitamente independientes en caso de distribución de valor extremo
Yarmouk University, Irbid, Jordan
Abstract
For simple null hypothesis, given any non-parametric combination method which has a monotone increasing acceptance region, there exists a problem for which this method is most powerful against some alternative. Starting from this perspective and recasting each method of combining p-values as a likelihood ratio test, we present theoretical results for some of the standard combiners which provide guidance about how a powerful combiner might be chosen in practice. In this paper we consider the problem of combining n independent tests as n → ∞ for testing a simple hypothesis in case of extreme value distribution (EV(θ,1)). We study the six free-distribution combination test producers namely; Fisher, logistic, sum of p-values, inverse normal, Tippett’s method and maximum of p-values. Moreover, we studying the behavior of these tests via the exact Bahadur slope. The limits of the ratios of every pair of these slopes are discussed as the parameter θ → 0 and θ → ∞. As θ → 0, the logistic procedure is better than all other methods, followed in decreasing order by the inverse normal, the sum of p-values, Fisher, maximum of p-values and Tippett’s procedure. Whereas, θ → ∞ the logistic and the sum of p-values procedures are equivalent and better than all other methods, followed in decreasing order by Fisher, the inverse normal, maximum of p-values and Tippett’s procedure.
Key words: Bahadur eﬃciency; Bahadur slope; combining independent tests; extreme value distribution.
Resumen
Para hipótesis nulas simples, dado cualquier método de combinación no paramétrico que tenga una región de aceptación creciente monótona, existe un problema para el cual este método es más poderoso frente a alguna alternativa. Partiendo de esta perspectiva y reformulando cada método de combinación de valores p como una prueba de razón de verosimilitud, presentamos resultados teóricos para algunos de los combinadores estándar que brindan orientación sobre cómo se podría elegir un combinador poderoso en la práctica. En este artículo consideramos el problema de combinar pruebas independientes de n como n → ∞ para probar una hipótesis simple en el caso de una distribución de valor extremo (EV (θ, 1)). Estudiamos los seis productores de prueba de combinación de distribución gratuita, a saber; Fisher, logística, suma de valores p, normal inversa, método de Tippett y máximo de valores p. Además, estudiamos el comportamiento de estaspruebas a través de la pendiente exacta de Bahadur. Los límites de las razones de cada par de estas pendientes se analizan como el parámetro θ → 0 y θ → ∞. Como θ → 0, la logística El procedimiento es mejor que todos los demás métodos, seguido en orden decreciente por el inverso normal, la suma de valores p, Fisher, el máximo de valores p y el procedimiento de Tippett. Considerando que, θ → ∞ la logística y la suma de los procedimientos de valores p so equivalentes y mejores que todos los demás métodos, seguidos en orden decreciente por Fisher, la inversa normal, máxima de valores p y procedimiento de Tippett.
Palabras clave: combinación de pruebas independientes; distribución de valor extremo; eﬁciencia Bahadur; pendiente Bahadur.


1. Introduction
    We consider the asymptotic relative eﬃciency (ARE) of two test procedures
in which the probabilities of the two types of error change with increasing sample
size n, and with respect to the alternative behavior. Abu-Dayyeh & El-Masri
(1994) studied six methods of combining inﬁnitely number of independent tests
in case of triangular distribution. These methods are sum of p-values, inverse
normal, logistic, Fisher, minimum of p-values and maximum of p-values. They
showed that the sum of p-values is the best of all other methods. Abu-Dayyeh
et al. (2003) combined inﬁnity number of independent tests for testing simple
hypotheses against one-sided alternative for normal and logistic distributions, they
used four methods of combining (Fisher, logistic, sum of p-values and inverse
normal). Al-Masri (2010) studied six methods of combining independent tests.
He showed under conditional shifted Exponential distribution that the inverse
normal method is the best among six combination methods. Al-Talib et al. (2020)
considered combining independent tests in case of conditional normal distribution
with probability density function X|θ ∼ N (γθ), θ ∈ [a, ∞], a ≥ 0 when θ1 , θ2 , . . .
have a distribution function (DF) Fθ . They concluded that the inverse normal
procedure is better than the other procedures. Al-Masri (2021a) considered
combining n independent tests of simple hypothesis, vs one-tailed alternative as n
approaches inﬁnity, in case of Laplace distribution L(γ, 1). He showed that the sum

                Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 193–208

Bahadur’s Stochastic Comparison in Case of Extreme Value Distribution                  195

of p-values procedure is better than all other procedures under the null hypothesis,
and the inverse normal procedure is better than the other procedures under the
alternative hypothesis. Al-Masri & Al-Momani (2021) considered combining n
independent tests of simple hypothesis, vs one-tailed alternative as n approaches
inﬁnity, in case of log-logistic distribution. They showed that the sum of p-values
procedure is better than all other procedures under the null hypothesis and under
the alternative hypothesis. Al-Masri (2021b) considered the problem of combining
n independent tests as n → ∞ for testing a simple hypothesis in case of log-normal
distribution. He showed that as ξ → 0, the maximum of p-values is better than
all other methods, followed in decreasing order by the inverse normal, logistic,
the sum of p-values, Fisher and Tippett’s procedure. Also, as ξ → ∞ the worst
method the sum of p-values and the other methods remain the same, since they
have the same limit.



2. Extreme Value (Gumbel) Distribution
   The extreme value Gumbel distribution (EV(θ,1)) is used to model maximums
and minimums. For example, it has been used to predict earthquakes, ﬂoods and
other natural disasters, as well as modeling operational risk in risk management
and the life of products that quickly wear out after a certain age.
    Extreme value distributions are the limiting distributions for the minimum
or the maximum of a very large collection of random observations from the same
arbitrary distribution. Extreme value distributions for the minimum are frequently
encountered. For example, if a system consists of n identical components in series,
and the system fails when the ﬁrst of these components fails, then system failure
times are the minimum of n random component failure times. In extreme value
theory, independent of the choice of component model, the system model will
approach a Weibull as n becomes large. The same reasoning can also be applied
at a component level, if the component failure occurs when the ﬁrst of many similar
competing failure processes reaches a critical level.
   The EV (θ, 1) distribution with location parameter θ, has distribution function
(DF) and probability density function (pdf) are given, respectively, by
                                             −(x−θ)
                            F (x; θ) = e−e            , x ∈ R, θ ∈ R                   (1)

                                  −(x−θ)
           f (x; θ) = e−(x−θ)−e            = −F (x; θ) ln F (x; θ), x ∈ R, θ ∈ R       (2)



3. The Basic Problem
   Consider testing the hypothesis

                      (i)                             (i)
                    H0 : ηi = η0i ,        vs,   H1 : ηi ∈ Ωi − {η0i }                 (3)

               Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 193–208

196                                                          Abedel-Qader Al-Masri & Noor Al-Momani

                 (i)
such that H0 becomes rejected for large values of some real valued continuous
random variable T (i) , i = 1, 2, . . . , n. The n hypotheses are combined into one as

       (i)                                                    (i)
  H0 : (η1 , . . . , ηn ) = (η01 , . . . , η0n ), vs , H1 : (η1 , . . . , ηn )
                                                                 { n                                }
                                                                     ∏
                                                              ∈            Ωi − {(η0 , . . . , η0 )}
                                                                                   1            n
                                                                                                         (4)
                                                                          i=1


For i = 1, 2, . . . , n, the p-value of the i-th test is given by

                                            (         )
                             Pi (t) = PH (i) T (i) > t = 1 − FH (i) (t)                                  (5)
                                             0                                  0



                                                             (i)                                        (i)
where FH (i) (t) is the DF of T (i) under H0 . Note that Pi ∼ U (0, 1) under H0 .
             0


  As a special case where ηi = θ and η0i = θ0 for i = 1, . . . , n, and assume that
 (1)
T , . . . , T (n) are independent, then (3) reduces to

                            H0 : θ = θ 0 ,       vs,     H1 : θ ∈ Ω − {θ0 }                              (6)

It follows that the p-values P1 , . . . , Pn are also iid rv’s that have a U (0, 1)
distribution under H0 , and under H1 have a distribution whose support is a
subset of the interval (0, 1) and is not a U (0, 1) distribution. Therefore, if f is
the probability density function (pdf) of P , then (6) is equivalent to

                         H0 : P ∼ U (0, 1),            vs,     H1 : P  U (0, 1)                         (7)

where P has a pdf f with support a subset of the interval (0, 1).
    This study considers the case: ηi = 0, i = 1, . . . , n. Also we are assuming that
T (1) , T (2) , . . . , T (n) are independent. Then (6) reduced to

                                   H0 : θ = 0,           vs,        H1 : θ > 0                           (8)

Thus, under H0 , the p-values P1 , P2 , . . . , Pn are iid rv’s distributed with a uniform
distribution U (0, 1) which is given by (8).
                                                   (i)
   By suﬃciency we may{ assume    } ni = 1 and T = Xi for i = 1, . . . , n. Then
                              (n)
we consider the sequence T         of independent test statistics that is we will take
a random sample X1 , . . . , Xn of size n and let n → ∞ and compare the four
non-parametric methods via exact Bahadur slope (EBS).
   The producers will be used in this paper are Fisher, logistic, sum of p-values,
inverse normal, Tippett’s method and maximum of p-values. These producers are

                       Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 193–208

Bahadur’s Stochastic Comparison in Case of Extreme Value Distribution                                197

based on p-values of the individual statistics Ti , and reject H0 if
                                      ∑
                                      n
                     ΨF isher = − 2         ln(Pi ) > χ22n,α ,
                                      i=1
                                    ∑
                                    n          (            )
                                                     Pi
                    Ψlogistic = −         ln                    > bα ,
                                    i=1
                                                   1 − Pi
                                    ∑n
                                                            √
                    Ψnormal = −           Φ−1 (Pi ) >           nΦ−1 (1 − α),
                                    i=1
                                    ∑
                                    n
                       Ψsum = −           Pi > C α ,
                                    i=1
                                                       1
                       Ψmax = − max Pi < α n ,
                                                                     1
                         ΨT = − min Pi < 1 − (1 − α) n .

where Φ is the DF of standard normal distribution.


4. Deﬁnitions
   This section lays out some tools basic to Bahadur’s stochastic comparison
theory as used in this article
Deﬁnition 1 (Serﬂing 2009, Bahadur eﬃciency and exact Bahadur slope (EBS)).
Let X1 , . . . , Xn be i.i.d. from a distribution with a probability density function
                                                                                   {     }
                                                                                     (1)
f (x, θ), and we want to test H0 : θ = θ0 vs H1 : θ ∈ Θ − {θ0 }. Let Tn
      {      }
         (2)
and Tn           be two sequences of test statistics for testing H0 . Let the signiﬁcance
                                         (      )             (      )        (        )
                   (i)      (i)             (i)                  (i)             (i)
attained by Tn be Ln = 1 − Fi Tn , where Fi Tn                         = P H 0 T n ≤ ti ,
i = 1, 2. Then there exists a positive valued function Ci (θ) called the exact
                                  (i)
Bahadur slope of the sequence {Tn } such that
                                               ( )
                          Ci (θ) = lim −2n−1 ln Lin
                                      θ→∞
                                                                                  {          }
                                                                                       (1)
with probability 1 (w.p.1) under θ and the Bahadur eﬃciency of                        Tn         relative
   {      }
      (2)
to Tn       is given by eB (T1 , T2 ) = C1 (θ)/C2 (θ).

Theorem 1 (Serﬂing 2009, Large deviation   ∑ntheorem). Let X1 , X2 , . . . , Xn be
i.i.d., with distribution F and put Sn =      i=1( Xi . ) Assume existence of the
moment generating function (mgf) M (z) = EF ezX , z real, and put m(t) =
inf z e−z(X−t) = inf z e−zt M (z). The behavior of large deviation probabilities
P (Sn ≥ tn ) , where tn → ∞ at rates slower than O(n). The case tn = tn, if
                                        n
−∞ < t ≤ EY, then P (Sn ≤ nt) ≤ [m(t)] , the

                −2n−1 ln PF (Sn ≥ nt) → −2 ln m(t)                   a.s.   (Fθ ) .

                Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 193–208

198                                          Abedel-Qader Al-Masri & Noor Al-Momani


Theorem 2 (Bahadur 1960, Bahadur theorem). Let {Tn } be a sequence of test
statistics which satisﬁes the following:

    1. Under H1 : θ ∈ Θ − {θ0 }:

                               n− 2 Tn → b(θ)
                                   1
                                                    a.s.    (Fθ ) ,

      where b(θ) ∈ R.

    2. There exists an open interval I containing {b(θ) : θ ∈ Θ − {θ0 }} , and a
       function g continuous on I, such that
                              [             ]                [               ]
           lim −2n−1 log sup 1 − Fθn (n 2 t) = lim −2n−1 log 1 − Fθn (n 2 t)
                                         1                              1

            n              θ∈Θ0                         n

                                                  = g(t),     t ∈ I.

If {Tn } satisﬁed (1)-(2), then for θ ∈ Θ − {θ0 }

                −2n−1 log sup [1 − Fθn (Tn )] → C(θ)         a.s.      (Fθ ) .
                           θ∈Θ0

Theorem 3 (Al-Masri 2010). Let X1 , . . . , Xn be i.i.d. with probability density
function f (x,∑θ), and we √   want to test H0 : θ = 0 vs H1 : θ > 0. For j = 1, 2,
                  n
let Tn,j =           f (x
                  i=1 i i  )/   n be a sequence of statistics such that H0 will be
rejected for large values of Tn,j and let φj be the test based on Tn,j . Assume
Eθ (fi (x)) > 0, ∀θ ∈ Θ, E0 (fi (x)) = 0, Var(fi (x)) > 0 for j = 1, 2. Then
1. If the derivative b′j (0) is ﬁnite for j = 1, 2, then
                                                    [        ]2
                            C1 (θ)   Varθ=0 (f2 (x)) b′1 (0)
                        lim        =                            ,
                        θ→0 C2 (θ)   Varθ=0 (f1 (x)) b′2 (0)

where bi (θ) = Eθ (fj (x)), and Cj (θ) is the EBS of test φj at θ.
2. If the derivative b′j (0) is inﬁnite for j = 1, 2, then
                                                  [           ]2
                         C1 (θ)   Varθ=0 (f2 (x))      b′ (θ)
                     lim        =                   lim 1′       .
                     θ→0 C2 (θ)   Varθ=0 (f1 (x)) θ→0 b2 (θ)
                                       (1)        (2)
Theorem 4 (Serﬂing 2009). If Tn and Tn are two test statistics for testing
                                                          (1)       (2)
H0 : θ = 0 vs H1 : θ > 0 with distribution functions F0 and F0 under H0 ,
                         (1)                          (2)
respectively, and that Tn is at least as powerful as Tn at θ for any α, then if φj
                       (j)
is the test based on Tn , j = 1, 2, then

                                   Cφ(1)
                                      1
                                         (θ) ≥ Cφ(2)
                                                  2
                                                     (θ)

.

Corollary 1 (Serﬂing 2009). If Tn is the uniformly most powerful test for all α,
then it is the best via EBS.

                Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 193–208

Bahadur’s Stochastic Comparison in Case of Extreme Value Distribution                    199

Theorem 5 (Al-Masri 2010).
                           2t ≤ mS (t) ≤ et, ∀ : 0 ≤ t ≤ 0.5,
where
                                                    ez − 1
                                mS (t) = inf e−zt          .
                                          z>0          z
Theorem 6 (Al-Masri 2010).            1. mL (t) ≥ 2te−t ,      ∀t ≥ 0,
  2. mL (t) ≤ te1−t , ∀t ≥ 0.852,
               ( 2 )3
  3. mL (t) ≤ t 1+t
                  t
                     2 e1−t , ∀t ≥ 4,
        where mL (t) = inf z∈(0,1) e−zt πz csc(πz) and csc is an abbreviation for
        cosecant function.
Theorem 7 (Al-Masri 2010). For x > 0,
                        [       ]
                          1   1                ϕ(x)
                   ϕ(x)     −     ≤ 1 − Φ(x) ≤      .
                          x x3                  x
Where ϕ is the pdf of standard normal distribution.
Theorem 8 (Al-Masri 2010). For x > 0,
                                                  ϕ(x)
                                  1 − Φ(x) >        √ .
                                                x + π2

Lemma 1 (Al-Masri 2010).             1. mL (t) ≥ inf e−zt = e−t
                                                  0<z<1
                            (    )
              e−t /(t+1) t+1
                    2
                           πt

  2. mL (t) ≤         (    )
                        πt
                 sin t+1
     {                   −zt   −z              −zt
       ms (t) = inf z>0 e (1−e    )
                                    ≤ inf z>0 e z ≤ −et,                 t<0
  3.                         z
       ms (t) ≥ −2t,                                              − 12 ≤ t ≤ 0.


5. Deviation of the EBS for EV(θ, 1)
    In this section we will study testing problem (8). We will compare the six
methods Fisher, logistic, sum of p-values, the inverse normal, Tippett’s method
and maximum of p-values via EBS.
Let X1 , . . . , Xn be iid with probability density function (2) and we want to test
(8). Then by (1), the P-value is given by
                                                                −x
                         Pn (Xn ) = 1 − F H0 (Xn ) = 1 − e−e                             (9)

   The next three lemmas give the EBS for Fisher (CF ), logistic (CL ), inverse
normal (CN ), and sum of p-values (CS ), Tippett’s method (CT ) and maximum of
p-values (Cmax )methods.

                 Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 193–208

200                                              Abedel-Qader Al-Masri & Noor Al-Momani


Lemma 2. The exact Bahadurs slope (EBSs) result for the tests, which is given
in Section 2, are as follows:

 B1. Fisher method. CF (θ) = bF (θ) − 2 ln(bF (θ)) + 2 ln(2) − 2,
     where                            (                   )
                          bF (θ) = −2 ψ(1) − ψ(eθ + 1) ,
                    ′
                    (x)
       and ψ(x) = ΓΓ(x) is the digamma function.

 B2. Logistic method. CL (θ) = −2 ln(m(bL (θ))), where

                               mL (t) = inf e−zt πz csc(πz)
                                             z∈(0,1)

       and
                              bL (θ) = ψ(eθ + 1) − e−θ − ψ(1).

 B3. Sum of p-values method. CS (θ) = −2 ln(m(bS (θ))), where

                                                           1 − e−z
                                       mS (t) = inf e−zt
                                                z>0           z
       and                                        (       )−1
                                        bS (θ) = − eθ + 1     .

 B4. Inverse Normal method. CN (θ) = −2 ln(m(bN (θ))) = b2N (θ).
     Where                                       (             )
                    bN (θ) = −eθ EBeta(eθ −1,1) ϕ Φ−1 (1 − W )

Proof of B1. By Theorem (2)
                                          [        −x
                                                      ]
                                     ∑n ln 1 − e−e
                             TF = −2         √          .
                                     i=1
                                               n

By the strong law of large number (SLLN)
                        T w.p.1                   [        −x
                                                              ]
                        √F −−−→ bF (θ) = −2 EH1 ln 1 − e−e
                         n

then                           ∫         [           ]
                                                  −x             −(x−θ)
                 bF (θ) = −2           ln 1 − e−e      e−(x−θ)−e        dx.
                                   R
                                                −θ
               −(X−θ)
Now, let U = e         , and Z = 1 − e−e U , then
  ∫      [          ]                          ∫ 1
                 −x             −(x−θ)
      ln 1 − e−e      e−(x−θ)−e                    ln(z)(1 − z)e −1 dz
                                                                θ
                                       dx = eθ
       R                                               0
                                                = EBeta(1,eθ ) ln Z = ψ(1) − ψ(eθ + 1).
                (                )
Then bF (θ) = −2 ψ(1) − ψ(eθ + 1) .

                Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 193–208

Bahadur’s Stochastic Comparison in Case of Extreme Value Distribution                                       201

                                                             −zt
                               ( 1, we have
   Now under H0 , then by Theorem        ) mS (t) = inf z>0 e MS (z), where
                                                              −x                                       −z
MS (z) = EF (ezX ). Under H0 : − 1 − e−e                               ∼ U (−1, 0), so MS (z) = 1−ez        , by
part (2) of Theorem 2 we complete the proof, that is
                                                   (                  )
                                                     bF (θ) 1− bF (θ)
              CF (θ) = − 2 ln(mF (bF (θ))) = −2 ln         e     2
                                                       2
                         = bF (θ) − 2 ln(bF (θ)) + 2 ln(2) − 2.



Proof of B2.                                                  [                 ]
                                                                           −x
                                                                  1−e−e
                                                       ∑
                                                       n ln
                                                                   e−e−x
                                          TL = −                   √                .
                                                       i=1
                                                                       n
By the strong law of large number (SLLN)
                                                 [         −x
                                                              ]
                          TL w.p.1                 1 − e−e
                          √ −−−→ bL (θ) = − E ln
                                             H1
                           n                         e−e−x

then
                    ∫     [     −x
                                   ]
                       1 − e−e                 −(x−θ)
       bL (θ) = −   ln     −e−x      e−(x−θ)−e        dx
                  R      e
               ∫   [          ]                        ∫
                           −x             −(x−θ)                       −(x−θ)
              = ln 1 − e−e      e−(x−θ)−e        dx −    e−x e−(x−θ)−e        dx.
                R                                                                   R

Now,                              ∫
                                                             −(x−θ)
                                          e−x e−(x−θ)−e                dx = e−θ ,
                                      R
                          ∫         [           ]
                                             −x            −(x−θ)
and from Proof (B1),              ln 1 − e−e      e−(x−θ)−e       dx = ψ(1)−ψ(eθ +1). Then
                              R

                                  bL (θ) = ψ(eθ + 1) − e−θ − ψ(1)



Proof of B3.
                                                        ∑n
                                                            1 − e−e
                                                                    −x

                                           TS = −              √       .
                                                        i=1
                                                                 n
By the strong law of large number (SLLN)
                           T w.p.1               (        −x
                                                             )
                           √S −−−→ bS (θ) = − EH1 1 − e−e
                            n

then                      ∫ (                      )
                                              −x                    −(x−θ)            (       )−1
            bS (θ) = −            1 − e−e              e−(x−θ)−e                dx = − eθ + 1     .
                           R


                    Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 193–208

202                                                     Abedel-Qader Al-Masri & Noor Al-Momani

                                         −zt
                     ) mS (t) = inf z>0 e MS (z), where MS (z) = EF (e ).
                                                                      zX
            ( 1, we have
Now, by Theorem
                               −x                                         −z
Under H0 : − 1 − e−e                 ∼ U (−1, 0), so MS (z) = 1−ez             , by part (2) of Theorem
2 we complete the proof, that is CS (θ) = −2 ln(mS (bS (θ))).

Proof of B4.                                     (           )
                                                          −x
                                           ∑n Φ−1 1 − e−e
                                    TN = −         √           .
                                           i=1
                                                     n
By the strong law of large number (SLLN)
                T w.p.1                         (        −x
                                                            )
                √N −−−→ bN (θ) = − EH1 Φ−1 1 − e−e            ,
                   n
                            ∫       (          )
                                            −x        −(x−θ)
              bN (θ) = −e θ
                                Φ−1 1 − e−e      e−x−e       dx,
                              R
                (        −x
                            )
then put U = Φ−1 1 − e−e       to get
                     ∫                                            ∫
                                                eθ −1                  dϕ(u)            eθ −1
      bN (θ) = −eθ           uϕ(u) (1 − Φ(u))           du = eθ              (1 − Φ(u))       du,
                         R                                            R du

                  d
where −uϕ(u) =      ϕ(u).
                 du
Now, by using integration by parts and put W = 1 − Φ (U ) to get
                                     ∫
                            (      ) 1 eθ −2 ( −1           )
              bN (θ) = − eθ eθ − 1        w   ϕ Φ (1 − w) dw
                                       0
                                            (             )
                     = − eθ EBeta(eθ −1,1) ϕ Φ−1 (1 − W )
                                          ()
        (           )          Φ−1 (1 − w)
where ϕ2 Φ−1 (1 − w) = √12π ϕ      √         .
                                     2
                                                  −zt
Now, by Theorem 1, we( have m   N)(t) = inf z>0 e     MN (z), where MN (z) =
                               −x
                            −e                                 2
EF (e ). Under H0 : − 1 − e
     zX
                                    ∼ N (0, 1), so MN (z) = ez /2 , by part (2)
of Theorem 2, CN (θ) = −2 ln(mN (bN (θ))) = b2N (θ).
Theorem 9 (Abu-Dayyeh & El-Masri 1994). Let U1 , U2 , . . . be i.i.d. like U with
probability density function f and suppose that we want to test H0 : Ui ∼ U (0, 1)
vs H1 : Ui ∼ f on (0, 1) but not U (0, 1). Then Cmax (f ) = −2 ln (ess.supf (u))
where ess.Supf (u) = sup {u : f (u) > 0} w.p.1 under f.
Lemma 3.
                                              Cmax (θ) = 0.

Proof . By Theorem (9) Cmax (f ) = −2 ln (ess.supf (u)) where ess.Supf (u) =
Sup {u : f (u) > 0} w.p.1 under θ.
                         −(x−θ)                          −x              (      )
   For f (x) = e−(x−θ)−e        , x, θ ∈ R, let Y = 1−e−e , then Y ∼ Beta eθ , 1 .
Then ess.supf (u) = 1.
      Therefore, Cmax (θ) = −2 ln (1) = 0.

                  Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 193–208

Bahadur’s Stochastic Comparison in Case of Extreme Value Distribution                       203

Theorem 10 (Abu-Dayyeh & El-Masri 1994). If ∆(ln ∆)2 f (∆) → 0 as ∆ → 0,
then CT (f ) = 0.

Lemma 4.
                                         CT (θ) = 0.

Proof . By Theorem (10)
                                                           −(∆−θ)
      lim ∆(ln ∆)2 f (∆) = lim ∆(ln ∆)2 e−(∆−θ)−e
                                                                          θ
                                                                    = eθ+e lim ∆(ln ∆)2 .
   ∆→0                      ∆→0                                               ∆→0

Clearly, by L’Hopital rule twice, lim∆→0 ∆(ln ∆)2 = 0 which implies CT (θ) =
0.


5.1. Comparison of the EBSs when θ → 0
   Now, we will compare the EBSs that obtained in Section (4). We will ﬁnd the
limit of the ratio of the EBSs of any two methods when θ → 0.

Corollary 2. The limits of ratios of diﬀerent tests are as follows:

       CT (θ)   Cmax (θ)
C1.           =          = 0, where CD (θ) ∈ {CF (θ), CL (θ), CS (θ), CN (θ)} .
       CD (θ)    CD (θ)
C2. eB (TS , TF ) → 1.80314

C3. eB (TL , TF ) → 1.97729

C4. eB (TN , TF ) → 1.96121

C5. eB (TL , TN ) → 1.0082

C6. eB (TN , TS ) → 1.08764

C7. eB (TL , TS ) → 1.09656

Proof of C2.                           (                )
                            bF (θ) = −2 ψ(1) − ψ(eθ + 1) .
Therefore
                                  b′F (θ) = 2eθ ψ1 (1 + eθ ),
                d
where ψ1 (x) = dx ψ(x) is the trigamma function.
                                               (           )
                                                   π2
                             lim b′F (θ) = 2          −1       < ∞.
                            θ→0                    6

Also                                        (       )−1
                                  bS (θ) = − eθ + 1     ,

                 Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 193–208

204                                                 Abedel-Qader Al-Masri & Noor Al-Momani


then
                                                                 −1
                       lim b′S (θ) = lim 21 (cosh(θ) + 1)             = 14 < ∞.
                      θ→0             θ→0
                             [        −x
                                         ]                     (        −x
                                                                           )
Now under H0 : hF (x) = −2 ln 1 − e−e      ∼ χ22 and hS (x) = − 1 − e−e      ∼
                                                                      b′ (0)
U (−1, 0), so Varθ=0 (hF (x)) = 4 and Varθ=0 (hS (x)) = 12 1
                                                             , also, ′S       =
                                                                     bF (0)
( 2       )−1
  8π                                                   CS (θ)         27
      −8      . By applying Theorem 3 we can get lim           =            2 =
   6                                               θ→0 CF (θ)     (π − 6)
                                                                    2
1.80314. Similarly we can prove the other parts.



5.2. The Limiting ratio of the EBS for diﬀerent tests when
     θ→∞
   Now, we compare the limit of the ratio of the EBSs of any two methods when
θ → ∞.

Corollary 3. The limits of ratios for diﬀerent tests are as follows:
D1. eB (TL , TF ) → 1

D2. eB (TS , TF ) → 1

D3. eB (TN , TS ) → 0

D4. lim {CF (θ) − CL (θ)} ≤ 0
       θ→∞

D5. eB (TN , TF ) → 0, eB (TN , TL ) → 0, eB (TL , TS ) → 1.




Proof of D1. By Lemma 1 part (1) CL (θ) ≤ 2bL (θ). So

                          CL (θ)                 2bL (θ)
                                 ≤                                     .
                          CF (θ)   bF (θ) − 2 ln(bF (θ)) + 2 ln(2) − 2

                                     2bL (θ)
It is suﬃcient to obtain lim                 .
                               θ→∞   bF (θ)
      Therefore,

                       2bL (θ)         ψ(eθ + 1) − e−θ − ψ(1)
                    lim        = − lim                        = 1.
                   θ→∞ bF (θ)     θ→∞     ψ(1) − ψ(eθ + 1)

Then,
                                                 CL (θ)
                                         lim              ≤ 1.
                                        θ→∞ CF (θ)


                   Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 193–208

Bahadur’s Stochastic Comparison in Case of Extreme Value Distribution                     205

Also, by Theorem 6 part (2), we have CL (θ) ≥ 2bL (θ) − 2 ln (bL (θ)) − 2. So
                      CL (θ)           2bL (θ) − 2 ln (bL (θ)) − 2
                  lim        ≥ lim                                    .
                  θ→∞ CF (θ)  θ→∞ bF (θ) − 2 ln(bF (θ)) + 2 ln(2) − 2

                                                2bL (θ)
It is suﬃcient to obtain the limit of lim               .
                                          θ→∞   bF (θ)
     Therefore,

                       2bL (θ)         ψ(eθ + 1) − e−θ − ψ(1)
                   lim         = − lim                        = 1.
                   θ→∞ bF (θ)     θ→∞     ψ(1) − ψ(eθ + 1)
Then,
                                          CL (θ)
                                      lim        ≥1
                                      θ→∞ CF (θ)

                                          CL (θ)
By pinching theorem, we have lim                   = 1.
                                    θ→∞ CF (θ)


Proof of D2. By Lemma 1 part (3) CS (θ) ≤ −2 ln(2) − 2 ln(−bS (θ)). So
                      CS (θ)           −2 ln(2) − 2 ln(−bS (θ))
                  lim        ≤ lim                                    .
                  θ→∞ CF (θ)  θ→∞ bF (θ) − 2 ln(bF (θ)) + 2 ln(2) − 2

                                                −2 ln(−bS (θ))
It is suﬃcient to obtain the limit of lim                      .
                                          θ→∞       bF (θ)
     Then
                                  (       )                (       )
       −2 ln(−bS (θ))           ln 1 + eθ                ln 1 + eθ
   lim                = − lim                 = − lim                    .
  θ→∞      bF (θ)        θ→∞ ψ(1) − ψ(eθ + 1)    θ→∞ ψ(1) − ψ(eθ ) − e−θ

Now, by using Gauss’s integral for asymptotic expansion of ψ
                                   ∫ ∞(                )
                             1          1 1        1
              ψ(z) = ln z −     −         − + t          e−tz dt,
                            2z      0   2    t   e −1
we get

                 (       )      1
  ψ(1 + eθ ) = ln 1 + eθ −             −
                           2 (1 + eθ )
                     ∫ ∞(                 )
                           1 1          1                  (      )
                                            e−t(1+e ) dt ln 1 + eθ as θ → ∞.
                                                   θ
                             − + t
                       0   2    t    e −1
Therefore,
                                                  (       )
                       −2 ln(−bS (θ))           ln 1 + eθ
                   lim                = − lim                    = 1.
                  θ→∞      bF (θ)        θ→∞ ψ(1) − ln (1 + eθ )

So
                                            CS (θ)
                                      lim            ≤ 1.
                                     θ→∞ CF (θ)


                  Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 193–208

206                                              Abedel-Qader Al-Masri & Noor Al-Momani


Also, by Theorem Lemma 1 part (3), we have CS (θ) ≥ −2 − 2 ln(−bS (θ)). So, in
the same manner, we get
                                  CS (θ)
                             lim         ≥ 1.
                            θ→∞ CF (θ)

                                          CS (θ)
By pinching theorem, we have lim                 = 1.
                                      θ→∞ CF (θ)


Proof of D3. From B4 we have
                            [                (            )]2
                CN (θ) = e2θ EBeta(eθ −1,1) ϕ Φ−1 (1 − W ) .

By Lemma 1 part (3) CS (θ) ≥ −2 − 2 ln(−bS (θ)). So
                                      [                (             )]2
                      CN (θ)       e2θ EBeta(eθ −1,1) ϕ Φ−1 (1 − W )
                  lim        ≤ lim
                 θ→∞ CS (θ)    θ→∞         −2 − 2 ln(−bS (θ))
                                      [                (             )]2
                                   e2θ EBeta(eθ −1,1) ϕ Φ−1 (1 − W )
                             = lim                                       .
                               θ→∞          −2 + ln (1 + eθ )
                                                 (         )
   Now by using reﬂection symmetry, then W ∼ Beta eθ − 1, 1 then 1 − W ∼
    (         )
Beta 1, eθ − 1 .
      Now we will ﬁnd the limiting distribution for Hθ = eθ Wθ when eθ → ∞.
                                         [              ]       (     )
         GHθ (hθ ) = Pθ [Hθ ≤ hθ ] = Pθ Wθ ≤ e−θ hθ = FWθ e−θ hθ
                             ∫ e−θ hθ                          [      ]eθ −1
                                                eθ −2              hθ
                   = (e − 1)
                       θ
                                      (1 − wθ )       dwθ = 1 − 1 − θ        .
                               0                                   e

                                                  [       ] eθ
                                        limeθ →∞ 1 − heθθ
                    lim GHθ (hθ ) = 1 −             [       ] = 1 − e−h .
                   eθ →∞                  limeθ →∞ 1 − heθθ
Then, lim eθ Beta(1, eθ − 1) = Exponential(1). Then,
         eθ →∞
                                      [          (         )]2
                           CN (θ)       EExp(1) ϕ Φ−1 (W )
                       lim        ≤                            = 0.
                      θ→∞ CS (θ)    limθ→∞ {−2 + ln (1 + eθ )}
Then
                                           CN (θ)
                                        lim       = 0.
                                       θ→∞ CS (θ)




Proof of D4. By Theorem 6 (2), we have

          CF (θ) − CL (θ) ≤ bF (θ) − 2 ln bF (θ) + 2 ln(2) + 2 ln bL (θ) − 2bL (θ)
                                                (        )
                                                  bL (θ)
                     = bF (θ) − 2bL (θ) + 2 ln             + 2 ln(2).
                                                  bF (θ)

                    Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 193–208

Bahadur’s Stochastic Comparison in Case of Extreme Value Distribution                          207

Now,
                                bF (θ) − 2bL (θ) = 2e−θ .

Also,
                     bL (θ)         ψ(eθ + 1) − e−θ − ψ(1)  1
                     lim    = − lim                        = .
                 θ→∞ bF (θ)    θ→∞ 2 (ψ(1) − ψ(e + 1))
                                                  θ         2

Then,
                                                                              (            )
                                                                                  bL (θ)
        lim (CF (θ) − CL (θ)) ≤ lim (bF (θ) − 2 ln bF (θ)) + 2 lim ln
        θ→∞                       θ→∞                              θ→∞            bF (θ)
                                  + 2 ln(2) = 0 − 2 ln(2) + 2 ln(2) = 0.

So, CF (θ) ≤ CL (θ) for large θ


Proof of D5. By using D1-D3


5.3. Comparison of the EBS for the Four Combination
     Procedures
    From the relations in section (4.1) we conclude that locally as θ → 0, the
logistic procedure is better than all other procedures since it has the highest EBS,
followed in decreasing order by the inverse normal, sum of p-values procedure and
the Fisher’s procedure. The worst are the Tippett’s and the maximum of p-values
procedures, i.e.,

               CL (θ) > CN (θ) > CS (θ) > CF (θ) > CT (θ) = Cmax (θ).

     Whereas, from result of Section (4.2) as θ → ∞ the worst methods are
Tippett’s and the maximum of p-values, the logistic and sum of p-values methods
remain the same, they are better than all other procedures since it has the highest
EBS, followed in decreasing order by Fisher’s and the inverse normal procedures,
i.e.,
               CL (θ) = CS (θ) > CF (θ) > CN (θ) > CT (θ) = Cmax (θ).



Acknowledgement
   We are very grateful to the Editor and the reviewer for useful comments and
suggestions on an early version of this paper.

                 [                                                        ]
                     Received: July 2021 — Accepted: December 2021

                 Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 193–208

208                                         Abedel-Qader Al-Masri & Noor Al-Momani


References
Abu-Dayyeh W, Al-Momani M, Muttlak H. Exact bahadur slope for combining independent tests for normal and logistic distributions.(2003). Applied Mathematics and Computation.
Abu-Dayyeh W, El-Masri A. Combining independent tests of triangular distribution.(1994). Statistics and Probability Letters.
Al-Masri A. Combining independent tests of conditional shifted exponential distribution.(2010). Journal of Modern Applied Statistical Methods.
Al-Masri A. Exact bahadur slope for combining independent tests in case of laplace distribution.(2021). Jordan Journal of Mathematics and Statistics.
Al-Masri A. On combining independent tests in case of log-normal distribution.(2021). American Journal of Mathematical and Management Sciences. https://doi.org/10.1080/01966324.2021.1997676
Al-Masri A, Al-Momani N. On combining independent tests in case of log-logistic distribution.(2021). Electronic Journal of Applied Statistical Analysis.
Al-Talib M, Al-Kadiri M, Al-Masri A. On combining independent tests in case of conditional normal distribution.(2020). Communications in Statistics - Theory and Methods.
Bahadur R. Stochastic comparison of tests.(1960). The Annals of Mathematical Statistics.
Serﬂing R. Approximation theorems of mathematical statistics.(2009). John Wiley and Sons.