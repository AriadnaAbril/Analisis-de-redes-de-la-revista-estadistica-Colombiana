Wavelet Shrinkage Generalized Bayes Estimation for Multivariate Normal Distribution Mean Vectors with unknown Covariance Matrix under Balanced-LINEX Loss. Contracci√≥n de la ond√≠cula Estimaci√≥n de Bayes generalizada para vectores medios de distribuci√≥n normal multivariante con matriz de covarianza desconocida con p√©rdida de LINEX equilibrada
Persian Gulf University, Bushehr, Iran
Abstract
In this paper, the generalized Bayes estimator of mean vector parameter for multivariate normal distribution with Unknown mean vector and covariance matrix is considered. This estimation is performed under the balanced-LINEX error loss function. The generalized Bayes estimator by using wavelet transformation is investigated. We also prove admissibility and minimaxity of shrinkage estimator and we present the simulation study and real data set for test validity of new estimator.
Key words: admissibility; generalized bayes estimator; balanced-linex loss; minimaxity; multivariate normal distribution; soft wavelet shrinkage estimator.
Resumen
En este trabajo, se considera el estimador de Bayes generalizado del par√°metro de vector medio para distribuci√≥n normal multivariante con vector de media desconocido y matriz de covarianza. Esta estimaci√≥n se realiza bajo la funci√≥n de p√©rdida de error LINEX balanceada. Se investiga el estimador de Bayes generalizado mediante la transformaci√≥n de ond√≠culas. Tambi√©n probamos la admisibilidad y minimaxidad del estimador de contracci√≥n y presentamos el estudio de simulaci√≥n y el conjunto de datos reales para comprobar la validez de la prueba del nuevo estimador.
Palabras clave: admisibilidad; estimador de Bayes generalizado; estimador de contracci√≥n de ondas suaves; distribuci√≥n normal multivariante; minimaxidad; p√©rdida de LINEX equilibrada.

1. Introduction
   Many univariate tests and conÔ¨Ådence intervals are based on the univariate
normal distribution. Similarly, the majority of multivariate procedures have the
multivariate normal distribution as their underpinning. For more information
about this issue refer to Rencher & Christensen (2012).
    Given the importance of the multivariate normal distribution, estimating the
parameters of this distribution is very important. In this paper, our goal is to
estimate the mean vector of this distribution. We suppose that the random vector
X = (X1 , . . . , Xp ) has a multivariate normal distribution with unknown mean
vector Œ∏ = (Œ∏1 , . . . , Œ∏p ) and unknown covariance matrix Œ£ (X ‚àº Np (Œ∏, Œ£)). For
this purpose, using the generalized Bayes estimator, we estimated mean vector
under a combination of balanced loss function and asymmetric linear exponential
(LINEX) loss function.
   Balanced loss functions and their role in estimation have captured the interest
of many researchers. The balanced loss function was introduced by Zellner (2009)
to reÔ¨Çect two criteria: goodness of Ô¨Åt and precision of estimation. In Zellner‚Äôs
framework, the target estimator was least-squares, but such a target can be viewed
more broadly (e.g., Jozani, Marchand & Parsian 2006, 2014). For more details
about the use of this loss, we refer to Jozani et al. (2012), Cao & He (2017),
Zinodiny et al. (2017), Karamikabir & Arashi (2018), Karamikabir et al. (2020),
Marchand & Strawderman (2020), and Karamikabir & Afshari (2020, 2021) to
mention a few.
    Suppose that X is a random vector having a multivariate normal distribution
with mean vector parameter Œ∏. The balanced-type loss function is deÔ¨Åned as
follows:

      L‚àóœâ,Œ¥0 (Œ∏, Œ¥) = œâœÅ (Œ¥0 (X), Œ¥(X)) + (1 ‚àí œâ)œÅ (Œ∏, Œ¥(X)) ,          0 ‚â§ œâ < 1.      (1)

where Œ¥0 is a target estimator of Œ∏ obtained for instance using the criterion
of maximum likelihood, least-squares, unbiasedness etc. œÅ(¬∑) is an arbitrary
multivariate loss function and Œ¥(X) is an estimator of p-vector parameter Œ∏ based
on the random vector X.
   In the balanced-type loss function (1), with the change of multivariate loss
function œÅ(¬∑), we can deÔ¨Åne diÔ¨Äerent types of the balanced-type
                                                               loss functions. If
                                                                                
                                                                  T
we consider multivariate LINEX loss function œÅ(l, Œ¥) = b ea (Œ¥‚àíl) ‚àí aT (Œ¥ ‚àí l) ‚àí 1
with the scale parameter b and the p-vector shape parameter a = (a1 , . . . , ap )T ,
then we have the balanced-LINEX loss function as follows:
                      T                                      
   Lœâ,Œ¥0 (Œ∏, Œ¥) = bœâ ea (Œ¥(X)‚àíŒ¥0 (X)) ‚àí aT (Œ¥(X) ‚àí Œ¥0 (X)) ‚àí 1
                              T                             
                   +b(1 ‚àí œâ) ea (Œ¥(X)‚àíŒ∏) ‚àí aT (Œ¥(X) ‚àí Œ∏) ‚àí 1 , 0 ‚â§ œâ < 1.(2)

If œâ = 0, then the balanced-LINEX loss function becomes the basic case of LINEX
loss function. For more information about this loss function see Jozani et al.
(2012).

                Revista Colombiana de Estad√≠stica - Theoretical Statistics 45 (2022) 107‚Äì123

Wavelet Shrinkage Generalized Bayes Estimation                                         109

    Shrinking and truncating the data directly or the coeÔ¨Écients in their Fourier
series expansions is an old technique in signal and image processing. For non-local
bases, such as trigonometric, shrinking the coeÔ¨Écients can aÔ¨Äect the global shape
of the reconstructed function and introduce unwanted artifacts. In the context
of function estimation by wavelets, the shrinkage has an additional feature; it is
connected with smoothing (denoising) because the measures of smoothness of a
function depend on the magnitudes of its wavelet coeÔ¨Écients (Vidakovic, 2009).
Wavelet methods are usually employed as a form of nonparametric regression, and
the techniques take on many names such as wavelet shrinkage, curve estimation,
or wavelet regression. Two diÔ¨Äerent kinds of threshold in denoising is the hard
threshold and the soft threshold. Donoho & Johnstone (1994) deÔ¨Åne the hard and
soft thresholding functions. Given a wavelet coeÔ¨Écient X and a threshold value
Œª > 0, the hard threshold value is given by
                             Œ¥ hard (X) = XI(|X| ‚â• Œª),
and the soft thresholding wavelet shrinkage estimation is given by
                     Œ¥ sof t (X) = sign(X)(|X| ‚àí Œª)I(|X| ‚â• Œª).                         (3)
where I(¬∑) is an indicator function.
    We try to make a connection between the generalized Bayes estimator and
the wavelet shrinkage estimator. For this purpose, we will Ô¨Ånd a threshold value
for the soft thresholding wavelet shrinkage estimator using the generalized Bayes
estimator.
   In this paper, we also generalized the paper of Karamikabir & Afshari (2019)
by changing loss function and the covariance matrix and Karamikabir & Afshari
(2020) by changing the diagonal covariance matrix œÉ 2 Ip to the unknown covariance
matrix Œ£. Recently, the problem of estimating a mean vector parameter has
received several new developments. For example, we can refer to Pal et al. (2007),
Jiang & Zhang (2009), Tsukuma & Kubokawa (2015), Fourdrinier & Strawderman
(2015), Joly & Oliveira (2017), Karamikabir & Arashi (2018) and Karamikabir &
Afshari (2020).
   Finally, we present a method to select the threshold value in wavelet
regularization. For this purpose, the threshold value is selected using the
generalized Bayes estimator and the method described in Section 3.
    The paper is outlined as follows. In Section 2, we Ô¨Ånd the generalized Bayes
estimator when X ‚àº Np (Œ∏, Œ£) under a balanced-LINEX loss function. In Section
3, we discuss the shrinkage Wavelet generalized Bayes estimation and threshold
value, and in Section 4 the numerical performance of the proposed estimator using
a simulation study. In Section 5 we investigate a real example, and in Section 6
concludes the paper.


2. Main Result
   In this section, we investigate the point estimation of the mean vector Œ∏
when the covariance matrix Œ£ is unknown. For this purpose, we suppose that

               Revista Colombiana de Estad√≠stica - Theoretical Statistics 45 (2022) 107‚Äì123

110                                             Hamid Karamikabir & Mahmud Afshari


X ‚àº Np (Œ∏, Œ£) and we Ô¨Ånd the generalized Bayes estimator for Œ∏ with respect
to the improper prior œÄ(Œ∏) = 1 under the balanced-LINEX loss function. Also
suppose that X|Œ∏ ‚àº Np (Œ∏, Œ£) and Œ¥(X) = (Œ¥(X1 ), . . . , Œ¥(Xp ))T be an estimator for
Œ∏. In this regard, we need the following theorem.
Theorem 1 (Rudin 1976, Chapter 7, pp. 148). Suppose limn‚Üí‚àû fn (x) = f (x)
where x ‚àà H. Put Mn = supx‚ààH |fn (x) ‚àí f (x)|. Then fn ‚Üí f uniformly on H if
and only if Mn ‚Üí 0 as n ‚Üí ‚àû.
Theorem 2 (Rudin 1976, Chapter 7, pp. 167). Suppose fn (x) is Riemann
integrable on [a, b], for n = 1, 2, . . . , and suppose fn (x) ‚Üí f (x) uniformly on
[a, b]. Then f (x) is Riemann integrable on [a, b], and
                           Z b                  Z b
                               f (x)dx = lim        fn (x)dx.
                           a              n‚Üí‚àû    a

   Now, in the following theorem, we Ô¨Ånd the generalized Bayes estimator for Œ∏
under the balanced-LINEX loss function in (2).
Theorem 3. Suppose that X ‚àº Np (Œ∏, Œ£), Œ∏ ‚àà Rp , under the balanced-LINEX loss
function, the generalized Bayes estimator for Œ∏ with respect to the improper prior
œÄ(Œ∏) = 1, is the following:
                                                                          
             ‚àía                                                     1 T
  Œ¥ ‚àó (X) =      log   œâ exp ‚àía T
                                  Œ¥ 0 (X)   + (1 ‚àí œâ) exp   ‚àía T
                                                                 X +   a Œ£a    .
            ‚à•a‚à•2                                                     2

Proof . For œÄ(Œ∏) = 1, the posterior distribution is œÄ(Œ∏|X) ‚àº Np (X, Œ£). It is easy
to check that the posterior loss function of an arbitrary estimator Œ¥(X) is given
by
                            Z
     ‚àó
    r (œÄ(Œ∏|X), Œ¥(X)) =          Lœâ,Œ¥0 (Œ∏, Œ¥)œÄ(Œ∏|X)dŒ∏
                                T
                              Œò
                                                                           
                       = E bœâ ea (Œ¥(X)‚àíŒ¥0 (X)) ‚àí aT (Œ¥(X) ‚àí Œ¥0 (X)) ‚àí 1
                                         T                            
                            +b(1 ‚àí œâ) ea (Œ¥(X)‚àíŒ∏) ‚àí aT (Œ¥(X) ‚àí Œ∏) ‚àí 1 X .

The generalized Bayes estimator is obtained by the following:
‚àÇr‚àó (œÄ(Œ∏|X), Œ¥(X))                T                          T
                                                                              
                    = bE œâaT ea (Œ¥(X)‚àíŒ¥0 (X)) + (1 ‚àí œâ)aT ea (Œ¥(X)‚àíŒ∏) ‚àí aT X
      ‚àÇŒ¥(X)
                               T                          T
                                                                             
                    = b œâaea (Œ¥(X)‚àíŒ¥0 (X)) + (1 ‚àí œâ)aea Œ¥(X) MŒ∏|X (‚àíaT ) ‚àí a
                      = 0,

then the generalized Bayes estimator as follows:
                                                           
   ‚àó        ‚àía                                         1 T
  Œ¥ (X) =       log œâ exp ‚àía Œ¥0 (X) + (1 ‚àí œâ) exp ‚àía X + a Œ£a .
                              T                     T
           ‚à•a‚à•2                                         2



               Revista Colombiana de Estad√≠stica - Theoretical Statistics 45 (2022) 107‚Äì123

Wavelet Shrinkage Generalized Bayes Estimation                                            111

    In Theorem 3, we obtained the generalized Bayes estimator Œ¥ ‚àó (X) with respect
to the target estimator Œ¥0 (X). By changing the target estimator Œ¥0 (X) and the
covariance matrix Œ£, the generalized Bayes estimator Œ¥ ‚àó (X) takes on diÔ¨Äerent
types. In this regard, we have the following corollaries by Theorem 3:

Corollary 1. Suppose that X ‚àº Np (Œ∏, œÉ 2 Ip ), then the generalized Bayes estimator
is
                                                                             
   ‚àó          ‚àía                                                    1 2
  Œ¥ (X) =         log œâ exp ‚àía Œ¥0 (X) + (1 ‚àí œâ) exp ‚àía X + œÉ ‚à•a‚à•
                               T                               T             2
                                                                                  .
             ‚à•a‚à•2                                                    2

Also, if that œâ = 0, then the generalized Bayes estimator is

                                                           œÉ2
                                    Œ¥ ‚àó (X)      =   X‚àí       a.
                                                           2
Corollary 2. Suppose that X ‚àº Np (Œ∏, Œ£) and Œ¥0 (X) = X, then the generalized
Bayes estimator is
                                                           
                          a                           1 T
           Œ¥ ‚àó (X) = X ‚àí      log   œâ + (1 ‚àí œâ) exp     a Œ£a    .        (4)
                         ‚à•a‚à•2                         2

Also, if that Œ£ = œÉ 2 Ip , then the generalized Bayes estimator is
                                                 2      
              ‚àó          a                        œÉ
            Œ¥ (X) = X ‚àí      log œâ + (1 ‚àí œâ) exp     ‚à•a‚à•2
                                                             .                            (5)
                        ‚à•a‚à•2                       2

   The following proposition shows that the results of Huang (2002), Torehzadeh
& Arashi (2014) and Karamikabir & Afshari (2019) are only a special case of
Theorem 3.

Proposition 1. The special case of Theorem 3 is the following:

  1. Suppose that X ‚àº N (Œ∏, œÉ 2 Ip ). The generalized Bayes estimator for œâ = 0,
     aT = (0, . . . , ai , . . . , 0) and b = n1 is as follows:

                                                               ai œÉ 2
                                            Œ¥ ‚àó (Xi ) = Xi ‚àí          .
                                                                 2
      See Huang (Huang, 2002).

  2. Suppose that X ‚àº SM N (Œ∏, œÉ 2 , G) (the covariance matrix of multivariate
     normal distribution). The generalized Bayes estimator for œâ = 0, aT =
     (0, . . . , ai , . . . , 0) and b = n1 is as follows:

                                                         ln Œ±(a2i , œÉ 2 )
                                    Œ¥ ‚àó (Xi ) = Xi ‚àí                      ,
                                                              ai
                              R‚àû    a2
                                     iœÉ
                                        2

      where Œ±(a2i , œÉ 2 ) =   0
                                   e 2t dG(t). See Torehzadeh & Arashi (2014).

                  Revista Colombiana de Estad√≠stica - Theoretical Statistics 45 (2022) 107‚Äì123

112                                              Hamid Karamikabir & Mahmud Afshari


  3. Suppose that X ‚àº Np (Œ∏, Œ£), The generalized Bayes estimator for œâ = 0,
     aT = (0, . . . , ai , . . . , 0) and b = n1 is as follows:
                                                       ai œÉii
                                    Œ¥ ‚àó (Xi ) = Xi ‚àí          .
                                                         2
      See Karamikabir & Afshari (2019).

    Now, we want to Ô¨Ånd minimax and admissible estimator based on general Bayes
estimator. Suppose that X ‚àº Np (Œ∏, Œ£) where Œ∏ and Œ£ is unknown. Also suppose
that œÄ(Œ∏) be an arbitrary proper prior, under the balanced-LINEX loss function,
the Bayes risk of the estimator Œ¥ ‚àó (X) is given by
                         Z
    r(œÄ(Œ∏), Œ¥ ‚àó (X)) =       R(Œ∏, Œ¥ ‚àó (X))œÄ(Œ∏)dŒ∏
                         Z Œò      Z       T                                        
                     =       œÄ(Œ∏)      bœâ ea (Œ¥(x)‚àíŒ¥0 (x)) ‚àí aT (Œ¥(x) ‚àí Œ¥0 (x)) ‚àí 1
                           Œò        X                                 
                                         T
                         +b(1 ‚àí œâ) ea (Œ¥(x)‚àíŒ∏) ‚àí aT (Œ¥(x) ‚àí Œ∏) ‚àí 1 dx dŒ∏.             (6)

The target estimator obtained using the criterion of the maximum likelihood, least-
squares, unbiasedness and etc. We have selected target estimator as the maximum
likelihood Œ¥0 (X) = X for the balanced-LINEX loss function (see Corollary 2).
     In the following theorem, we want to Ô¨Ånd the the Bayes risk of the estimator
Œ¥ ‚àó (X) by using the equation (6). For this purpose, we Ô¨Årst obtain the risk
R(Œ∏, Œ¥ ‚àó (X)) and then integrate it with respect to Œ∏. If the the Bayes risk
r(œÄ(Œ∏), Œ¥ ‚àó (X)) is constant value, the generalized Bayes estimator Œ¥ ‚àó (X) estimator
is Minimax.

Theorem 4. Suppose that X ‚àº Np (Œ∏, Œ£), œÄ(Œ∏) be an arbitrary proper prior and
Œ¥0 (X) = X. Under the balanced-LINEX loss function in (2) the Bayes risk of the
estimator Œ¥ ‚àó (X) in (4) is given by
                                                            
                       ‚àó                               1 T
              r(œÄ(Œ∏), Œ¥ (X)) = b log œâ + (1 ‚àí œâ) exp     a Œ£a    .          (7)
                                                       2

Also the generalized Bayes estimator is minimax.

Proof . By using Corollary 2, we have the generalized Bayes estimator Œ¥ ‚àó (X) in
(4). In this case, the risk function calculated as follows.

R(Œ∏, Œ¥ ‚àó (X)) =     [Lœâ,Œ¥0 (Œ∏, Œ¥(X))]
                   E(
                                                                              
                                              a                       1 T
               =   E bœâ exp aT X ‚àí                log   œâ +  (1 ‚àí œâ)e 2 a Œ£a   ‚àí X
                                             ‚à•a‚à•2
                                                                         
                                 a                       1 T
                   ‚àíaT X ‚àí           log   œâ + (1 ‚àí œâ)e  2 a Œ£a   ‚àí X    ‚àí  1
                              ‚à•a‚à•  
                                   2
                                                                                     
                                                  a                       1 T
                   +b(1 ‚àí œâ) exp aT X ‚àí               log    œâ + (1 ‚àí œâ)e 2 a Œ£a   ‚àí Œ∏
                                                ‚à•a‚à•2

                Revista Colombiana de Estad√≠stica - Theoretical Statistics 45 (2022) 107‚Äì123

Wavelet Shrinkage Generalized Bayes Estimation                                               113

                                                                    )
                               a                      1 T
                  ‚àía T
                          X‚àí       log œâ + (1 ‚àí œâ)e 2   a  Œ£a
                                                                ‚àíŒ∏ ‚àí1
                              ‚à•a‚à•2
                                          ‚àí1                              
                                     1 T                             1 T
              =   bœâ œâ + (1 ‚àí œâ)e 2 a Œ£a        + log œâ + (1 ‚àí œâ)e 2 a Œ£a ‚àí 1
                                                 ‚àí1
                                                      e‚àía Œ∏ MX|Œ∏ (aT )
                                            1 T            T
                  +b(1 ‚àí œâ) œâ + (1 ‚àí œâ)e 2 a Œ£a
                            h                                  1 T
                                                                                 i
                  +b(1 ‚àí œâ) E ‚àíaT X|Œ∏ + log œâ + (1 ‚àí œâ)e 2 a Œ£a + aT Œ∏ ‚àí 1
                                  1 T
                                          ‚àí1                       1 T
                                                                           
              =   bœâ œâ + (1 ‚àí œâ)e 2 a Œ£a       + b log œâ + (1 ‚àí œâ)e 2 a Œ£a ‚àí b
                                           1 T
                                                  ‚àí1 1 T
                  +b(1 ‚àí œâ) œâ + (1 ‚àí œâ)e 2 a Œ£a       e 2 a Œ£a
                                     1 T
                                            
              =   b log œâ + (1 ‚àí œâ)e 2 a Œ£a .                                    (8)

By using the risk R(Œ∏, Œ¥ ‚àó (X)) in (8), we have the following Bayes risk
                                        Z
                 r(œÄ(Œ∏), Œ¥ ‚àó (X)) =         R(Œ∏, Œ¥ ‚àó (X))œÄ(Œ∏)dŒ∏
                                          Œò
                                                            1 T
                                                                 
                                   = b log œâ + (1 ‚àí œâ)e 2 a Œ£a .

Since Bayes risk r(œÄ(Œ∏), Œ¥ ‚àó (X)) is a constant value, so Œ¥ ‚àó (X) is a minimax
estimator.

Corollary 3. In Theorem 4, suppose that X ‚àº Np (Œ∏, œÉ 2 Ip ), then the Bayes risk
of the estimator Œ¥ ‚àó (X) in (5) is given
                                                                                  
                     ‚àó                                                    œÉ2
             r(œÄ(Œ∏), Œ¥ (X))      =   b log œâ + (1 ‚àí œâ) exp                   ‚à•a‚à•2        .
                                                                          2

Also, if œâ = 0, then the Bayes risk of the estimator Œ¥ ‚àó (X) in (4) is given by

                                                   b T
                              r(œÄ(Œ∏), Œ¥ ‚àó (X)) =     a Œ£a.
                                                   2
And the Bayes risk of the estimator Œ¥ ‚àó (X) in (5) is given by

                                                  b œÉ2
                             r(œÄ(Œ∏), Œ¥ ‚àó (X)) =        ‚à•a‚à•2 .
                                                    2

    In the following Lemma, we Ô¨Ånd the posterior distribution and generalized
Bayes estimators a multivariate normal distribution Np (Œ∏, Œ£) with conjugate prior
distribution Np (0, œ±2k Œ£).

Lemma 1. Suppose that X|Œ∏ ‚àº Np (Œ∏, Œ£) and the prior distribution œÄk (Œ∏) =
Np (0, œ±2k Œ£), then, we have the following results.
                                                                           
                                                           œ±2k        œ±2k
  ‚Ä¢ The posterior distribution œÄ(Œ∏|X) is Np              œ±2k +1
                                                                  X, œ±2 +1 Œ£ .
                                                                      k




               Revista Colombiana de Estad√≠stica - Theoretical Statistics 45 (2022) 107‚Äì123

114                                                Hamid Karamikabir & Mahmud Afshari


   ‚Ä¢ Under the balanced-LINEX loss function in (2), when Œ¥0 (X) = X, the Bayes
     estimator for Œ∏ is:
                                                                          
                       a                           1             œ±2k
     Œ¥ (X) = X ‚àí
      œÄk
                         log œâ + (1 ‚àí œâ) exp            T
                                                       a X+              T
                                                                        a Œ£a . (9)
                    ‚à•a‚à•2                       œ±2k + 1       2(œ±2k + 1)

Proof . The posterior distribution œÄ(Œ∏|X) is obtained as follows.

 œÄ(Œ∏|X)      ‚àù       œÄ(Œ∏)
               f (œâ|Œ∏)                                                
                       1     T ‚àí1        T ‚àí1       T ‚àí1    1 T ‚àí1
             ‚àù exp ‚àí       x Œ£ x + Œ∏ Œ£ Œ∏ ‚àí 2Œ∏ Œ£ x + 2 Œ∏ Œ£ Œ∏
                     2                                 œ±k
                       1 T      ‚àí1     1 ‚àí1        T ‚àí1
             ‚àù exp ‚àí Œ∏        Œ£ + 2Œ£           Œ∏+Œ∏ Œ£ x
                     2              œ±k                                  
                       1      ‚àí1    1 ‚àí1              T ‚àí1   ‚àí1    1 ‚àí1 ‚àí1
             = exp ‚àí        Œ£ + 2Œ£            Œ∏ Œ∏ ‚àí 2Œ∏ Œ£ (Œ£ + 2 Œ£ ) x
                                               T

                    ( 2            œ±k
                                                         T 
                                                                   œ±k
                                                                           ‚àí1
                         1 h               1 ‚àí1 ‚àí1 ‚àí1                 1 ‚àí1
                                     ‚àí1                         ‚àí1
             ‚àù exp ‚àí          Œ∏ ‚àí (Œ£ + 2 Œ£ ) Œ£ x              Œ£ + 2Œ£
                         2                 œ±k                         œ±k
                                                i )
                                  1
               √ó Œ∏ ‚àí (Œ£‚àí1 + 2 Œ£‚àí1 )‚àí1 Œ£‚àí1 x
                                 œ±k
                                                                  
                                 1                       1
               ‚àº Np (Œ£‚àí1 + 2 Œ£‚àí1 )‚àí1 Œ£‚àí1 X, (Œ£‚àí1 + 2 Œ£‚àí1 )‚àí1
                      2         œ±k                     œ±k
                          œ±k         œ±2k
               = Np            X, 2       Œ£ .
                       œ±2k + 1     œ±k + 1
Now similar to Theorem 3, a generalized Bayes estimator is obtained by the
following:
‚àÇr ‚àó (œÄ(Œ∏|X), Œ¥(X))
                            T                         T
                                                                      
                       =    bE œâaT ea (Œ¥(X)‚àíŒ¥0 (X)) + (1 ‚àí œâ)aT ea (Œ¥(X)‚àíŒ∏) ‚àí aT X
      ‚àÇŒ¥(X)
                                    T                              T
                                                                                              
                       =    b œâaT ea (Œ¥(X)‚àíŒ¥0 (X)) + (1 ‚àí œâ)aT ea Œ¥(X) MŒ∏|X (‚àíaT ) ‚àí aT
                       =    0,

then we have the following Bayes estimator with respect to the target estimator
Œ¥0 (X).
                                              2              2         
              ‚àía                                    œ±             œ±k
                   log œâe‚àía Œ¥0 (X) + (1 ‚àí œâ) exp ‚àí 2 k aT X +
                           T
Œ¥ œÄk (X) =                                                               aT Œ£a            .
             ‚à•a‚à• 2                                œ±k + 1      2(œ±2k + 1)

Finally by replacing Œ¥0 (X) = X in the Bayes estimator Œ¥ œÄk (X), we have the
following Bayes estimator.
                                                                                 
                     a                               1               œ±2k
 Œ¥ œÄk (X) = X ‚àí          log   œâ + (1 ‚àí œâ) exp           a T
                                                             X +            a T
                                                                                Œ£a    .
                   ‚à•a‚à•2                          œ±2k + 1         2(œ±2k + 1)



     Now, in the following theorem, we prove that the generalized Bayes estimators
Œ¥ ‚àó (X) is admissible and minimax under the balanced-LINEX loss function.

                  Revista Colombiana de Estad√≠stica - Theoretical Statistics 45 (2022) 107‚Äì123

Wavelet Shrinkage Generalized Bayes Estimation                                           115

Theorem 5. Suppose that X ‚àº Np (Œ∏, Œ£) and Œ¥0 (X) = X, then under the balanced-
LINEX loss function, Œ¥ ‚àó (X) in (4) is admissible and a minimax estimator.


Proof . We know R(Œ∏, Œ¥) is continuous in Œ∏ for any Œ¥. Suppose that Œ¥ ‚àó is not
admissible. Then, there exists an estimator Œ¥ such that R(Œ∏, Œ¥) < R(Œ∏, Œ¥ ‚àó ), with
strict inequality for some Œ∏, say Œ∏0 . Since R(Œ∏, Œ¥) and R(Œ∏, Œ¥ ‚àó ) are continuous in
Œ∏, there exist strictly positive constants c1 and c2 such that

               R(Œ∏, Œ¥) < R(Œ∏, Œ¥ ‚àó ) ‚àí c1     f or   Œ∏ ‚àà [Œ∏ : |Œ∏ ‚àí Œ∏0 | < c2 ],

Consider a sequence of priors œÄk (Œ∏) = Np (0, œ±2k Œ£), with limk‚Üí‚àû œ±2k = ‚àû, and
uniformly, limk‚Üí‚àû œÄk (Œ∏) ‚Üí œÄ(Œ∏), where œÄ(Œ∏) < ‚àû is a proper distribution. Using
the technique of minimizing posterior expected loss, under the balanced-LINEX
loss function by using Theorem 1, we have the Bayes estimator Œ¥ œÄk (X) in (9). By
using the balanced-LINEX loss function Lœâ,Œ¥0 (Œ∏, Œ¥(X)) in (2), the risk R(Œ∏, Œ¥ œÄk (X))
as follows:
            (                                                                    
                                                    1                  œ±2k
      bE      œâ exp ‚àí log œâ + (1 ‚àí œâ) exp                 T
                                                         a X+                   T
                                                                              a Œ£a
                                                œ±2k + 1           2(œ±2k + 1)
                                                                          
                                          1                  œ±2k
            +œâ log œâ + (1 ‚àí œâ) exp            a T
                                                  X   +             aT Œ£a ‚àí 1
                           (          œ±2k + 1            2(œ±2k + 1)
                                           h                      1
            +(1 ‚àí œâ) exp aT X ‚àí aT log œâ + (1 ‚àí œâ) exp 2                   aT X
                                                                  œ±k + 1
                                      )               "
                  œ±2k         i
            +            a Œ£a ‚àí a Œ∏ ‚àí (1 ‚àí œâ) aT X ‚àí aT Œ∏
                          T        T
              2(œ±2k + 1)
                                                                          #)
                                        1                  œ±2k
            ‚àí log œâ + (1 ‚àí œâ) exp             T
                                             a X+                   T
                                                                   a Œ£a
                                    œ±2k + 1            2(œ±2k + 1)
               "                                                          ‚àí1
                                         1                   œ±2k
       = bE œâ œâ + (1 ‚àí œâ) exp                  T
                                              a X+                   T
                                                                    a Œ£a
                                     œ±2k + 1            2(œ±2k + 1)
                                                                              #
                                                              2
                                         1                  œ±
            + log œâ + (1 ‚àí œâ) exp            aT X +           k
                                                                   aT Œ£a       ‚àí1
                                     œ±2k + 1            2(œ±2k + 1)
                                  ‚àía Œ∏  T

             " ‚àí œâ)MX|Œ∏ (a X)e
                            T
           +b(1
                                                                          ‚àí1 #
                                      1                    œ±2k
           E    œâ + (1 ‚àí œâ) exp               T
                                            a X+                     T
                                                                   a Œ£a
                                  œ±2k + 1              2(œ±2k + 1)
                              
              " ‚àíœâ)E a X|Œ∏ + 
           ‚àíb(1          T
                                   b(1 ‚àí œâ)aT Œ∏
                                                                              ‚àí1
                                         1                     œ±2k
      =    bE œâ œâ + (1 ‚àí œâ) exp                 a T
                                                    X  +               a T
                                                                           Œ£a
                                      œ±2 + 1             2(œ±2k + 1)
                                  k                                        
                                         1                    œ±2k
           + log œâ + (1 ‚àí œâ) exp               a T
                                                   X  +               a T
                                                                          Œ£a     ‚àí1
                                     œ±2k + 1             2(œ±2k + 1)
                                                                                ‚àí1
                                               1                   œ±2k
           +(1 ‚àí œâ) œâ + (1 ‚àí œâ) exp                 a T
                                                        X  +               a T
                                                                               Œ£a
                                           œ±2k + 1             2(œ±2k + 1)

                Revista Colombiana de Estad√≠stica - Theoretical Statistics 45 (2022) 107‚Äì123

116                                                 Hamid Karamikabir & Mahmud Afshari

                                #
                        1 T
            √ó exp         a Œ£a        .                                                (10)
                        2

By using Theorem 1 for equation (10), we can obtain the values of risk as follows.
                                       Z
                r(œÄk (Œ∏), Œ¥ œÄk (X)) =      R(Œ∏, Œ¥ œÄk (x))œÄ(Œ∏)dŒ∏
                                       ZŒò Z
                                    =         gk (x)dx dŒ∏.
                                                Œò    œá

where
                                                                        ‚àí1
                                       1                   œ±2k
  gk (x) =     bœâ œâ + (1 ‚àí œâ) exp    2 +1    a T
                                                 X   +             a T
                                                                       Œ£a
                                   œ±
                                     k                 2(œ±2k + 1)           
                                           1                   œ±2k
               + log œâ + (1 ‚àí œâ) exp               T
                                                 a X+                   T
                                                                       a Œ£a    ‚àí1
                                       œ±2k + 1            2(œ±2k + 1)
                                                                              ‚àí1
                                                 1                 œ±2k
               +(1 ‚àí œâ) œâ + (1 ‚àí œâ) exp                T
                                                      a X+                  T
                                                                           a Œ£a
                                           œ±k2 + 1           2(œ±2k + 1)
                       1 T
               √ó exp     a Œ£a .
                       2

As a result
                                                    ‚àí1
                                               1 T
      lim gk (x) =       bœâ œâ + (1 ‚àí œâ) exp     a Œ£a
      k‚Üí‚àû
                                              2         
                                                   1 T
                         +b log œâ + (1 ‚àí œâ) exp      a Œ£a    ‚àíb
                                                   2
                                                            ‚àí1              
                                                       1 T                1 T
                         +b(1 ‚àí œâ) œâ + (1 ‚àí œâ) exp       a Œ£a       exp     a Œ£a
                                                     2                2
                                                 1 T
                    =    b log œâ + (1 ‚àí œâ) exp     a Œ£a     = g(x).
                                                 2

Again, we put that Mk = supx‚ààRp |gk (x) ‚àí g(x)|, then we can write
                                                               
                                                          1 T
     lim sup |gk (x) ‚àí g(x)| = b log œâ + (1 ‚àí œâ) exp        a Œ£a
    k‚Üí‚àû x‚ààRp                                              2
                                                                 
                                                            1 T
                                 ‚àíb log œâ + (1 ‚àí œâ) exp       a Œ£a    = 0.
                                                            2

By Theorem 1, since limk‚Üí‚àû Mk = 0, then limk‚Üí‚àû gk (x) = g(x) uniformly on
Rp . As a result by Theorem 2, we can write
                                                                  
             lim r(œÄk (Œ∏), Œ¥ œÄk (X)) = b log œâ + (1 ‚àí œâ)œà œÉ 2 aT a .
              k‚Üí‚àû

Now, according to r(œÄ(Œ∏), Œ¥ ‚àó (X)) in equation (7) we have

                    lim {r(œÄ(Œ∏), Œ¥ ‚àó (X)) ‚àí r(œÄk (Œ∏), Œ¥ œÄk (X))} = 0.                  (11)
                k‚Üí‚àû


                Revista Colombiana de Estad√≠stica - Theoretical Statistics 45 (2022) 107‚Äì123

Wavelet Shrinkage Generalized Bayes Estimation                                               117
                      R
Let c3 = limk‚Üí‚àû inf |Œ∏‚àíŒ∏0 |<c2 œÄk (Œ∏)dŒ∏. Since limk‚Üí‚àû œ±2k = ‚àû, then c3 > 0.
Therefore, for k is large enough

     r(œÄk , Œ¥ ‚àó ) ‚àí r(œÄk , Œ¥ œÄk )   ‚â•   r(œÄk , Œ¥ ‚àó ) ‚àí r(œÄk , Œ¥)
                                        Z
                                    =       (R (Œ∏, Œ¥ ‚àó ) ‚àí R (Œ∏, Œ¥)) œÄk (Œ∏)dŒ∏ > c1 c2 > 0.
                                         Rp

This contradicts with equation (11). As a result, Œ¥ ‚àó (X) is an admissible estimator.
The minimaxity of Œ¥ ‚àó (X) follows from its admissibility and the constant risk
phenomenon (7).


3. Shrinkage Wavelet Generalized Bayes Estima-
   tion
    In this section, the goal is to Ô¨Ånd a particular type of the soft wavelet estimator
using the generalized Bayes estimator. In the issue, Huang (2002) investigated
the shrinkage wavelet estimation problem in the multivariate normal by diagonal
covariance matrix œÉ 2 Ip and Torehzadeh & Arashi (2014) extended his result for
a scale mixture of multivariate normal distributions. Finally Karamikabir &
Afshari (2019) investigated the shrinkage wavelet estimation problem in the class
of elliptically distribution, in LINEX loss function.
   Consider the following model:

                                           X = Œ∏ + Œµ,

where X = (X1 , . . . , Xp )T are the p √ó 1 random vector, Œµ = (Œµ1 , . . . , Œµp )T
are independent identical distribution Np (0, Œ£), and Œ∏ = (Œ∏1 , . . . , Œ∏p )T are the
p-vector mean vectors. Again, suppose that X|Œ∏ ‚àº Np (Œ∏, Œ£) and Œ¥(X) =
(Œ¥(X1 ), . . . , Œ¥(Xp ))T be an estimator for Œ∏.
    For deonising or shrinkage coeÔ¨Écients, one of the most important concepts
in wavelets and deionising is using thresholds. Shrinkage of the empirical wavelet
coeÔ¨Écients works best in problems where the underlying set of the true coeÔ¨Écients
of f is sparse and the remaining few large coeÔ¨Écients explain most of the functional
form in f . By shrinking, the empirical coeÔ¨Écients towards zero, the smaller ones
which contain primarily noise may be reduced to negligible levels, hence denoising
the signal.
   Let Y1 , . . . , Yn are observed data from model,

                                         Y = f (Z) + Œ∑,

where the {Œ∑i } is some noise and {Zi } is some points from domain of f . Typically
n is an integer power of 2.
   Note that the observations are sampled from distribution f but with some noise
and we are interested to remove noises. To achieve this aim, observations or noisy
data are converted to wavelet coeÔ¨Écients. Denoised coeÔ¨Écients are returned to
the Y domain by the inverse discrete wavelet transformation.

                  Revista Colombiana de Estad√≠stica - Theoretical Statistics 45 (2022) 107‚Äì123

118                                              Hamid Karamikabir & Mahmud Afshari


   In this section, we suppose that X ‚àº Np (Œ∏, œÉ 2 Ip ). In this regard, we use the
condition of corollary 2, under the balanced-LINEX loss function with Œ¥0 (X) = X.
Consider the generalized Bayes estimator in (4). For aT = (0, . . . , ai , . . . , 0) (ith
element is ai ), the ith element of Œ¥ ‚àó (X) is
                                                            
                              1                        1 2 2
              Œ¥ ‚àó (Xi ) = Xi ‚àí log œâ + (1 ‚àí œâ) exp       ai œÉ     .
                              ai                       2

   We again suppose that aT = (0, . . . , ai , . . . , 0) in the balanced-LINEX loss
function. We consider speciÔ¨Åcally ai values depending on signs of Œ∏i ‚Äôs
                        (
                          c   f or, Œ∏i > 0,
                   ai =                          i = 1, . . . n.
                          ‚àíc f or, Œ∏i < 0,
where c > 0 is some constant. Such an error criterion discourages estimators from
over-estimation in magnitude (i.e., in absolute value) and results in shrinkage
estimation towards zero. In other words, we can be considered this issue as a
regularization problem that regularizes or shrinks the wavelet coeÔ¨Écient estimates
towards zero.
     Under such a loss criterion the generalized Bayes estimator in   (4) is given by
Œ¥ ‚àó (Xi ) = Xi ‚àí sign(Œ∏i )Œªi , where Œªi = 1c log œâ + (1 ‚àí œâ) exp œÉ2ii c2 .
    The wavelet estimation problem can be treated via the estimation of the
mean vector Œ∏ from a elliptical distribution X|Œ∏ ‚àº Np (Œ∏, Œ£). Often the signs of
parameters Œ∏i ‚Äôs are not known. A natural approach is to use sign(Xi ) to estimate
sign(Œ∏i ) and make truncation at zero. In conclusion, we have the empirical version
of Œ¥ ‚àó in the following Proposition.
Proposition 2. According to Œ¥ sof        t
                                        (Xi ) in (3), by choosing the threshold value of
Œªi = c log œâ + (1 ‚àí œâ) exp 2 c , the soft wavelet shrinkage estimator for Œ∏ by
       1                          œÉii 2

using Œ¥ sof t (Xi ) can be obtained as follows.
                    (
                      (Xi ‚àí Œªi ) ‚à® 0 f or, Xi ‚â• 0,
    Œ¥ sof t (Xi ) =                                      = sign(Xi )(|Xi | ‚àí Œªi )+ .
                      (Xi + Œªi ) ‚àß 0 f or, Xi < 0,


4. Simulation
    In this section, we checked theoretical outcomes with the numerical
computation and simulation to investigate the performance of the soft wavelet
shrinkage estimator in Section 3.
    We compare the new threshold method to the three commonly used shrinkage
strategies, i.e, hard and soft thresholding with the universal threshold and false
discovery rate (FDR). To assess the performance, we calculated the average mean
squared error (AMSE) from the m = 1000 simulations. The value of AMSE is
obtained as follows:
                                 Xm X N                     2
                        m‚àí1 N ‚àí1          f (xi ) ‚àí fÀÜ(xi,j ) ,
                                   j=1 i=1


                Revista Colombiana de Estad√≠stica - Theoretical Statistics 45 (2022) 107‚Äì123

Wavelet Shrinkage Generalized Bayes Estimation                                             119

where f (xi ) is the true signal and fÀÜ(xi,j ) is the estimate of the function from
simulation j. In general, lower values of AMSE represent the accuracy of the     
                                                                             2 1
estimate. We suppose that X ‚àº N2 (Œ∏, Œ£) where Œ∏ = (0, 0) and Œ£ =                    .
                                                                             1 3
Tables 1 and 2 represent the AMSE with respect to c and œâ for wavelet estimator
based on the hard and soft universal threshold, FDR threshold and new threshold
for the X ‚àº N2 (Œ∏, Œ£). As shown in Tables 1 and 2, the AMSE amount obtained
in the new method is lower than that of the methods. Also, by increasing of the
value of œâ, the estimation AMSE increases and by increasing the N and c, the
AMSE of the all method decreases.
   Table 1: AMSE for hard and soft universal, FDR and new threshold for c = 25.
     N                 New threshold                Universal    Universal      FDR
           œâ = 0.2        œâ = 0.5       œâ = 0.8        Soft        Hard
    128   0.01615664    0.01616252     0.01617411   0.04245191   0.22163175   0.11313481
    256   0.00931865    0.00932817     0.00934689   0.03439703   0.19397322   0.11066790


   Table 2: AMSE for hard and soft universal, FDR and new threshold for c = 35.
     N                 New threshold                Universal    Universal      FDR
           œâ = 0.2        œâ = 0.5       œâ = 0.8        Soft        Hard
    128   0.01550174    0.01550226     0.01550329   0.04245191   0.22163175   0.11313481
    256   0.00798796    0.00798963     0.00799290   0.03439703   0.19397322   0.11066790


    In general, as the amount of œâ increases, the risk increases. The reason for
this is the value of threshold (Œª). As the amount of œâ increases, the amount of
Œª decreases. Also, by increasing the c or œÉii , the AMSE decreases. Because the
amount of c or œÉii increases, the amount of Œª increases.
    Now, we checked theoretical outcomes with the numerical computation and
simulation to investigate the performance of the soft wavelet shrinkage estimator
and generalized Bayes estimator. All calculations in this section are done using R
software.
   To investigate the risk of estimators, a Monte Carlo simulation study was
performed to compare the risk values estimators for the N8 (Œ∏, Œ£)    ‚àöwhere Œ£ is       
randomly generated using Wishart distribution and Œ∏ is selected as      k, 0, . . . , 0
                                                            P
                                                            p
and k = 0, 0.1, 0.2, . . . , 10. In this case, ‚à•Œ∏‚à• = Œ∏T Œ∏ =   Œ∏i 2 = k. These risk
                                                                  i=1
values have been obtained using the 1000 Monte Carlo simulation replications
and plotted in Figure 1 for p = 8, c = 25, diÔ¨Äerent values of œâ, the soft wavelet
shrinkage estimator and generalized Bayes estimator.
    In Figure 1 the soft wavelet shrinkage estimator risk curve is lower that of the
generalized Bayes estimator, i.e., the soft wavelet shrinkage estimator dominates
the generalized Bayes estimator. As the value of œâ increases, the superiority of the
soft wavelet shrinkage estimator over the generalized Bayes estimator is increases.



                Revista Colombiana de Estad√≠stica - Theoretical Statistics 45 (2022) 107‚Äì123

120                                                                                   Hamid Karamikabir & Mahmud Afshari

                                  œâ=0.2                                                                                 œâ=0.5




                                                                                      35
                      The soft wavelet shrinkage estimator                                                 The soft wavelet shrinkage estimator




         35
                      Generalized Bayes estimator                                                          Generalized Bayes estimator




                                                                                      30
         30




                                                                                      25
         25




                                                                                      20
         20
  Risk




                                                                            Risk

                                                                                      15
         15




                                                                                      10
         10




                                                                                      5
         5




              0   2           4               6       8            10                          0       2            4            6        8       10

                                     ||Œ∏||                                                                               ||Œ∏||
                                                                            œâ=0.8
                                         25




                                                              The soft wavelet shrinkage estimator
                                                              Generalized Bayes estimator
                                         20
                                         15
                                  Risk

                                         10
                                         5




                                                  0       2             4                  6       8           10

                                                                              ||Œ∏||

Figure 1: Risk plot for the soft wavelet shrinkage estimator and generalized Bayes
          estimator with p = 8 for selected values of œâ.



5. 3D Road Network Data Set
    In this section, we further investigate the average risk value of the soft wavelet
shrinkage estimator for real data set. For this sake, we use the 3D road network
data set from Guo et al. (2012). This dataset was constructed by adding elevation
information to a 2D road network in North Jutland, Denmark (covering a region of
185√ó135 km2 ). Elevation values where extracted from a publicly available massive
Laser Scan Point Cloud for Denmark. This 3D road network was eventually used
for benchmarking various fuel and CO2 estimation algorithms. This dataset can be
used by any applications that require to know very accurate elevation information
of a road network to perform more accurate routing for eco-routing, cyclist routes
etc. The dataset contains 4 variables and 434873 observations.
    We have implemented a bootstrap analysis to evaluate the risk functions. Table
3 lists the average risk value of the soft wavelet shrinkage estimator for diÔ¨Äerent
values of œâ. As shown in Table 3, by increasing of the value of œâ, the average
risk value decreases. In the case of œâ = 0, the balanced-LINEX loss function

                      Revista Colombiana de Estad√≠stica - Theoretical Statistics 45 (2022) 107‚Äì123

Wavelet Shrinkage Generalized Bayes Estimation                                               121

is the basic case of LINEX loss function and it has the most average risk value.
Therefore, it can be concluded that the risk values can be reduced by using the
balanced-LINEX loss function.

Table 3: Average risk value of the soft wavelet shrinkage estimator for 3D road network
         data set.
                 Œ¥           œâ=0       œâ = 0.2     œâ = 0.4     œâ = 0.6      œâ = 0.8
           Œ¥ sof t (X)     1.610567    1.397633   1.185041    0.9731351    0.7630683




6. Conclusion
    In this paper, we consider the generalized Bayes shrinkage estimator of mean
vector for multivariate normal distribution under balanced-LINEX loss function.
We assume that the random vector X having Np (Œ∏, Œ£) distribution with the
unknown covariance matrix Œ£. We Ô¨Ånd minimax and admissible estimator of mean
vector based on generalized Bayes estimator. Theoretical Ô¨Åndings of this paper are
further supported by some numerical analyses. In this regard, the performance
evaluation of the proposed class of estimators is checked through a simulation
study and real data set.


Acknowledgements
   The authors would like to thank the research committee of Persian Gulf
University. Also, the authors would like to thank the editors and reviewers for
their valuable comments, which greatly improved the readability of this paper.
                                                                                
                 Received: December 2020 ‚Äî Accepted: November 2021


References
Cao, M. X. & He, D. (2017), ‚ÄòAdmissibility of linear estimators of the common mean parameter in general linear models under a balanced loss function‚Äô, Journal of Multivariate Analysis 153, 246‚Äì254.
Donoho, D. L. & Johnstone, I. M. (1994), ‚ÄòIdeal spatial adaptation by wavelet shrinkage‚Äô, Biometrika 81, 425‚Äì455.
Fourdrinier, D. & Strawderman, W. E. (2015), ‚ÄòRobust minimax stein estimation under invariant data-based loss for spherically and elliptically symmetric distributions‚Äô, Metrika 78(4), 461‚Äì484.
Guo, C., Ma, Y., Yang, B., S., J. C. & Kaul, M. (2012), ‚ÄòEcoMark: Evaluating models of vehicular environmental impact‚Äô, SIGSPATIAL/GIS pp. 269‚Äì278.
Huang, S. Y. (2002), ‚ÄòOn a Bayesian aspect for soft wavelet shrinkage estimation under an asymmetric linex loss‚Äô, Statistics and Probability Letters 56, 171‚Äì175.
Jiang, W. & Zhang, C. H. (2009), ‚ÄòGeneral maximum likelihood empirical Bayes estimation of normal means‚Äô, The Annals of Statistics 37(4), 1647‚Äì1684.
Joly, E. Lugosi, G. & Oliveira, R. I. (2017), ‚ÄòOn the estimation of the mean of a random vector‚Äô, Electronic Journal of Statistics 11, 440‚Äì451.
Jozani, J. M., Leblanc, A. & Marchand, E. (2014), ‚ÄòOn continuous distribution functions, minimax and best invariant estimators, and integrated balanced loss functions‚Äô, Canadian Journal of Statistics 42, 470‚Äì486.
Jozani, M. J., Marchand, √â. & Parsian, A. (2006), ‚ÄòOn estimation with weighted balanced-type loss function‚Äô, Statistics and Probability Letters 76, 733‚Äì780.
Jozani, M. J., Marchand, √â. & Parsian, A. (2012), ‚ÄòBayesian and Robust Bayesian analysis under a general class of balanced loss functions‚Äô, Statistical Papers 53, 51‚Äì60.
Karamikabir, H. & Afshari, M. (2019), ‚ÄòWavelet Shrinkage Generalized Bayes Estimation for Elliptical Distribution Parameters under LINEX Loss‚Äô, International Journal of Wavelets, Multiresolution and Information Processing 14(1), 1950009.
Karamikabir, H. & Afshari, M. (2020), ‚ÄòGeneralized Bayesian Shrinkage and Wavelet Estimation of Location Parameter for Spherical Distribution under Balance-type Loss: Minimaxity and Admissibility,‚Äô, Journal of Multivariate Analysis 177(1), 104583.
Karamikabir, H. & Afshari, M. (2021), ‚ÄòNew wavelet SURE thresholds of elliptical distributions under the balance loss‚Äô, Statistica Sinica 31(4), 1829‚Äì1852.
Karamikabir, H. Afshari, M. & Arashi, M. (2018), ‚ÄòShrinkage estimation of non-negative mean vector with unknown covariance under balance loss‚Äô, Journal of Inequalities and Applications 2018, 331.
Karamikabir, H., Afshari, M. & Lak, F. (2020), ‚ÄòWavelet threshold based on Stein‚Äôs unbiased risk estimators of restricted location parameter in multivariate normal‚Äô, Journal of Applied Statistics 48(10), 1712‚Äì1729.
Marchand, E. & Strawderman, W. E. (2020), ‚ÄòOn shrinkage estimation for balanced loss functions‚Äô, Journal of Multivariate Analysis 175, 104558.
Pal, N., Sinha, B. K., Chaudhuri, G. & Chang, C. H. (2007), ‚ÄòEstimation Of A Multivariate Normal Mean Vector And Local Improvements‚Äô, Statistics 26(1), 1‚Äì17.
Rencher, A. C. & Christensen, W. F. (2012), Methods of Multivariate Analysis, third edition edn, John Wiley & Sons.
Rudin, W. (1976), Principle of Mathematical Analysis, MacGraw-Hill.
Torehzadeh, S. & Arashi, M. (2014), ‚ÄòA note on shrinkage wavelet estimation in bayesian analysis‚Äô, Statistics and Probability Letters 84, 231‚Äì234.
Tsukuma, H. & Kubokawa, T. (2015), ‚ÄòEstimation of the mean vector in a singular multivariate normal distribution‚Äô, Journal of Multivariate Analysis 140(4), 245‚Äì258.
Vidakovic, B. (2009), Statistical Modelling by Wavelets, John Wiley and Sons.
Zellner, A. (2009), Bayesian and non-bayesian estimation using balanced loss functions, in J. O. Berger & S. S. Gupta, eds, ‚ÄòStatistical decision theory and related topics V‚Äô, Springer, New York.
Zinodiny, S., Rezaei, S. & Nadarajah, S. (2017), ‚ÄòBayes minimax estimation of the mean matrix of matrix-variate normal distribution under balanced loss function‚Äô, Statistics and Probability Letters 125, 110‚Äì120.
