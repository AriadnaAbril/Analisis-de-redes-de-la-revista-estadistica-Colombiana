Wavelet Shrinkage Generalized Bayes Estimation for Multivariate Normal Distribution Mean Vectors with unknown Covariance Matrix under Balanced-LINEX Loss. Contracción de la ondícula Estimación de Bayes generalizada para vectores medios de distribución normal multivariante con matriz de covarianza desconocida con pérdida de LINEX equilibrada
Persian Gulf University, Bushehr, Iran
Abstract
In this paper, the generalized Bayes estimator of mean vector parameter for multivariate normal distribution with Unknown mean vector and covariance matrix is considered. This estimation is performed under the balanced-LINEX error loss function. The generalized Bayes estimator by using wavelet transformation is investigated. We also prove admissibility and minimaxity of shrinkage estimator and we present the simulation study and real data set for test validity of new estimator.
Key words: admissibility; generalized bayes estimator; balanced-linex loss; minimaxity; multivariate normal distribution; soft wavelet shrinkage estimator.
Resumen
En este trabajo, se considera el estimador de Bayes generalizado del parámetro de vector medio para distribución normal multivariante con vector de media desconocido y matriz de covarianza. Esta estimación se realiza bajo la función de pérdida de error LINEX balanceada. Se investiga el estimador de Bayes generalizado mediante la transformación de ondículas. También probamos la admisibilidad y minimaxidad del estimador de contracción y presentamos el estudio de simulación y el conjunto de datos reales para comprobar la validez de la prueba del nuevo estimador.
Palabras clave: admisibilidad; estimador de Bayes generalizado; estimador de contracción de ondas suaves; distribución normal multivariante; minimaxidad; pérdida de LINEX equilibrada.

1. Introduction
   Many univariate tests and conﬁdence intervals are based on the univariate
normal distribution. Similarly, the majority of multivariate procedures have the
multivariate normal distribution as their underpinning. For more information
about this issue refer to Rencher & Christensen (2012).
    Given the importance of the multivariate normal distribution, estimating the
parameters of this distribution is very important. In this paper, our goal is to
estimate the mean vector of this distribution. We suppose that the random vector
X = (X1 , . . . , Xp ) has a multivariate normal distribution with unknown mean
vector θ = (θ1 , . . . , θp ) and unknown covariance matrix Σ (X ∼ Np (θ, Σ)). For
this purpose, using the generalized Bayes estimator, we estimated mean vector
under a combination of balanced loss function and asymmetric linear exponential
(LINEX) loss function.
   Balanced loss functions and their role in estimation have captured the interest
of many researchers. The balanced loss function was introduced by Zellner (2009)
to reﬂect two criteria: goodness of ﬁt and precision of estimation. In Zellner’s
framework, the target estimator was least-squares, but such a target can be viewed
more broadly (e.g., Jozani, Marchand & Parsian 2006, 2014). For more details
about the use of this loss, we refer to Jozani et al. (2012), Cao & He (2017),
Zinodiny et al. (2017), Karamikabir & Arashi (2018), Karamikabir et al. (2020),
Marchand & Strawderman (2020), and Karamikabir & Afshari (2020, 2021) to
mention a few.
    Suppose that X is a random vector having a multivariate normal distribution
with mean vector parameter θ. The balanced-type loss function is deﬁned as
follows:

      L∗ω,δ0 (θ, δ) = ωρ (δ0 (X), δ(X)) + (1 − ω)ρ (θ, δ(X)) ,          0 ≤ ω < 1.      (1)

where δ0 is a target estimator of θ obtained for instance using the criterion
of maximum likelihood, least-squares, unbiasedness etc. ρ(·) is an arbitrary
multivariate loss function and δ(X) is an estimator of p-vector parameter θ based
on the random vector X.
   In the balanced-type loss function (1), with the change of multivariate loss
function ρ(·), we can deﬁne diﬀerent types of the balanced-type
                                                               loss functions. If
                                                                                
                                                                  T
we consider multivariate LINEX loss function ρ(l, δ) = b ea (δ−l) − aT (δ − l) − 1
with the scale parameter b and the p-vector shape parameter a = (a1 , . . . , ap )T ,
then we have the balanced-LINEX loss function as follows:
                      T                                      
   Lω,δ0 (θ, δ) = bω ea (δ(X)−δ0 (X)) − aT (δ(X) − δ0 (X)) − 1
                              T                             
                   +b(1 − ω) ea (δ(X)−θ) − aT (δ(X) − θ) − 1 , 0 ≤ ω < 1.(2)

If ω = 0, then the balanced-LINEX loss function becomes the basic case of LINEX
loss function. For more information about this loss function see Jozani et al.
(2012).

                Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 107–123

Wavelet Shrinkage Generalized Bayes Estimation                                         109

    Shrinking and truncating the data directly or the coeﬃcients in their Fourier
series expansions is an old technique in signal and image processing. For non-local
bases, such as trigonometric, shrinking the coeﬃcients can aﬀect the global shape
of the reconstructed function and introduce unwanted artifacts. In the context
of function estimation by wavelets, the shrinkage has an additional feature; it is
connected with smoothing (denoising) because the measures of smoothness of a
function depend on the magnitudes of its wavelet coeﬃcients (Vidakovic, 2009).
Wavelet methods are usually employed as a form of nonparametric regression, and
the techniques take on many names such as wavelet shrinkage, curve estimation,
or wavelet regression. Two diﬀerent kinds of threshold in denoising is the hard
threshold and the soft threshold. Donoho & Johnstone (1994) deﬁne the hard and
soft thresholding functions. Given a wavelet coeﬃcient X and a threshold value
λ > 0, the hard threshold value is given by
                             δ hard (X) = XI(|X| ≥ λ),
and the soft thresholding wavelet shrinkage estimation is given by
                     δ sof t (X) = sign(X)(|X| − λ)I(|X| ≥ λ).                         (3)
where I(·) is an indicator function.
    We try to make a connection between the generalized Bayes estimator and
the wavelet shrinkage estimator. For this purpose, we will ﬁnd a threshold value
for the soft thresholding wavelet shrinkage estimator using the generalized Bayes
estimator.
   In this paper, we also generalized the paper of Karamikabir & Afshari (2019)
by changing loss function and the covariance matrix and Karamikabir & Afshari
(2020) by changing the diagonal covariance matrix σ 2 Ip to the unknown covariance
matrix Σ. Recently, the problem of estimating a mean vector parameter has
received several new developments. For example, we can refer to Pal et al. (2007),
Jiang & Zhang (2009), Tsukuma & Kubokawa (2015), Fourdrinier & Strawderman
(2015), Joly & Oliveira (2017), Karamikabir & Arashi (2018) and Karamikabir &
Afshari (2020).
   Finally, we present a method to select the threshold value in wavelet
regularization. For this purpose, the threshold value is selected using the
generalized Bayes estimator and the method described in Section 3.
    The paper is outlined as follows. In Section 2, we ﬁnd the generalized Bayes
estimator when X ∼ Np (θ, Σ) under a balanced-LINEX loss function. In Section
3, we discuss the shrinkage Wavelet generalized Bayes estimation and threshold
value, and in Section 4 the numerical performance of the proposed estimator using
a simulation study. In Section 5 we investigate a real example, and in Section 6
concludes the paper.


2. Main Result
   In this section, we investigate the point estimation of the mean vector θ
when the covariance matrix Σ is unknown. For this purpose, we suppose that

               Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 107–123

110                                             Hamid Karamikabir & Mahmud Afshari


X ∼ Np (θ, Σ) and we ﬁnd the generalized Bayes estimator for θ with respect
to the improper prior π(θ) = 1 under the balanced-LINEX loss function. Also
suppose that X|θ ∼ Np (θ, Σ) and δ(X) = (δ(X1 ), . . . , δ(Xp ))T be an estimator for
θ. In this regard, we need the following theorem.
Theorem 1 (Rudin 1976, Chapter 7, pp. 148). Suppose limn→∞ fn (x) = f (x)
where x ∈ H. Put Mn = supx∈H |fn (x) − f (x)|. Then fn → f uniformly on H if
and only if Mn → 0 as n → ∞.
Theorem 2 (Rudin 1976, Chapter 7, pp. 167). Suppose fn (x) is Riemann
integrable on [a, b], for n = 1, 2, . . . , and suppose fn (x) → f (x) uniformly on
[a, b]. Then f (x) is Riemann integrable on [a, b], and
                           Z b                  Z b
                               f (x)dx = lim        fn (x)dx.
                           a              n→∞    a

   Now, in the following theorem, we ﬁnd the generalized Bayes estimator for θ
under the balanced-LINEX loss function in (2).
Theorem 3. Suppose that X ∼ Np (θ, Σ), θ ∈ Rp , under the balanced-LINEX loss
function, the generalized Bayes estimator for θ with respect to the improper prior
π(θ) = 1, is the following:
                                                                          
             −a                                                     1 T
  δ ∗ (X) =      log   ω exp −a T
                                  δ 0 (X)   + (1 − ω) exp   −a T
                                                                 X +   a Σa    .
            ∥a∥2                                                     2

Proof . For π(θ) = 1, the posterior distribution is π(θ|X) ∼ Np (X, Σ). It is easy
to check that the posterior loss function of an arbitrary estimator δ(X) is given
by
                            Z
     ∗
    r (π(θ|X), δ(X)) =          Lω,δ0 (θ, δ)π(θ|X)dθ
                                T
                              Θ
                                                                           
                       = E bω ea (δ(X)−δ0 (X)) − aT (δ(X) − δ0 (X)) − 1
                                         T                            
                            +b(1 − ω) ea (δ(X)−θ) − aT (δ(X) − θ) − 1 X .

The generalized Bayes estimator is obtained by the following:
∂r∗ (π(θ|X), δ(X))                T                          T
                                                                              
                    = bE ωaT ea (δ(X)−δ0 (X)) + (1 − ω)aT ea (δ(X)−θ) − aT X
      ∂δ(X)
                               T                          T
                                                                             
                    = b ωaea (δ(X)−δ0 (X)) + (1 − ω)aea δ(X) Mθ|X (−aT ) − a
                      = 0,

then the generalized Bayes estimator as follows:
                                                           
   ∗        −a                                         1 T
  δ (X) =       log ω exp −a δ0 (X) + (1 − ω) exp −a X + a Σa .
                              T                     T
           ∥a∥2                                         2



               Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 107–123

Wavelet Shrinkage Generalized Bayes Estimation                                            111

    In Theorem 3, we obtained the generalized Bayes estimator δ ∗ (X) with respect
to the target estimator δ0 (X). By changing the target estimator δ0 (X) and the
covariance matrix Σ, the generalized Bayes estimator δ ∗ (X) takes on diﬀerent
types. In this regard, we have the following corollaries by Theorem 3:

Corollary 1. Suppose that X ∼ Np (θ, σ 2 Ip ), then the generalized Bayes estimator
is
                                                                             
   ∗          −a                                                    1 2
  δ (X) =         log ω exp −a δ0 (X) + (1 − ω) exp −a X + σ ∥a∥
                               T                               T             2
                                                                                  .
             ∥a∥2                                                    2

Also, if that ω = 0, then the generalized Bayes estimator is

                                                           σ2
                                    δ ∗ (X)      =   X−       a.
                                                           2
Corollary 2. Suppose that X ∼ Np (θ, Σ) and δ0 (X) = X, then the generalized
Bayes estimator is
                                                           
                          a                           1 T
           δ ∗ (X) = X −      log   ω + (1 − ω) exp     a Σa    .        (4)
                         ∥a∥2                         2

Also, if that Σ = σ 2 Ip , then the generalized Bayes estimator is
                                                 2      
              ∗          a                        σ
            δ (X) = X −      log ω + (1 − ω) exp     ∥a∥2
                                                             .                            (5)
                        ∥a∥2                       2

   The following proposition shows that the results of Huang (2002), Torehzadeh
& Arashi (2014) and Karamikabir & Afshari (2019) are only a special case of
Theorem 3.

Proposition 1. The special case of Theorem 3 is the following:

  1. Suppose that X ∼ N (θ, σ 2 Ip ). The generalized Bayes estimator for ω = 0,
     aT = (0, . . . , ai , . . . , 0) and b = n1 is as follows:

                                                               ai σ 2
                                            δ ∗ (Xi ) = Xi −          .
                                                                 2
      See Huang (Huang, 2002).

  2. Suppose that X ∼ SM N (θ, σ 2 , G) (the covariance matrix of multivariate
     normal distribution). The generalized Bayes estimator for ω = 0, aT =
     (0, . . . , ai , . . . , 0) and b = n1 is as follows:

                                                         ln α(a2i , σ 2 )
                                    δ ∗ (Xi ) = Xi −                      ,
                                                              ai
                              R∞    a2
                                     iσ
                                        2

      where α(a2i , σ 2 ) =   0
                                   e 2t dG(t). See Torehzadeh & Arashi (2014).

                  Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 107–123

112                                              Hamid Karamikabir & Mahmud Afshari


  3. Suppose that X ∼ Np (θ, Σ), The generalized Bayes estimator for ω = 0,
     aT = (0, . . . , ai , . . . , 0) and b = n1 is as follows:
                                                       ai σii
                                    δ ∗ (Xi ) = Xi −          .
                                                         2
      See Karamikabir & Afshari (2019).

    Now, we want to ﬁnd minimax and admissible estimator based on general Bayes
estimator. Suppose that X ∼ Np (θ, Σ) where θ and Σ is unknown. Also suppose
that π(θ) be an arbitrary proper prior, under the balanced-LINEX loss function,
the Bayes risk of the estimator δ ∗ (X) is given by
                         Z
    r(π(θ), δ ∗ (X)) =       R(θ, δ ∗ (X))π(θ)dθ
                         Z Θ      Z       T                                        
                     =       π(θ)      bω ea (δ(x)−δ0 (x)) − aT (δ(x) − δ0 (x)) − 1
                           Θ        X                                 
                                         T
                         +b(1 − ω) ea (δ(x)−θ) − aT (δ(x) − θ) − 1 dx dθ.             (6)

The target estimator obtained using the criterion of the maximum likelihood, least-
squares, unbiasedness and etc. We have selected target estimator as the maximum
likelihood δ0 (X) = X for the balanced-LINEX loss function (see Corollary 2).
     In the following theorem, we want to ﬁnd the the Bayes risk of the estimator
δ ∗ (X) by using the equation (6). For this purpose, we ﬁrst obtain the risk
R(θ, δ ∗ (X)) and then integrate it with respect to θ. If the the Bayes risk
r(π(θ), δ ∗ (X)) is constant value, the generalized Bayes estimator δ ∗ (X) estimator
is Minimax.

Theorem 4. Suppose that X ∼ Np (θ, Σ), π(θ) be an arbitrary proper prior and
δ0 (X) = X. Under the balanced-LINEX loss function in (2) the Bayes risk of the
estimator δ ∗ (X) in (4) is given by
                                                            
                       ∗                               1 T
              r(π(θ), δ (X)) = b log ω + (1 − ω) exp     a Σa    .          (7)
                                                       2

Also the generalized Bayes estimator is minimax.

Proof . By using Corollary 2, we have the generalized Bayes estimator δ ∗ (X) in
(4). In this case, the risk function calculated as follows.

R(θ, δ ∗ (X)) =     [Lω,δ0 (θ, δ(X))]
                   E(
                                                                              
                                              a                       1 T
               =   E bω exp aT X −                log   ω +  (1 − ω)e 2 a Σa   − X
                                             ∥a∥2
                                                                         
                                 a                       1 T
                   −aT X −           log   ω + (1 − ω)e  2 a Σa   − X    −  1
                              ∥a∥  
                                   2
                                                                                     
                                                  a                       1 T
                   +b(1 − ω) exp aT X −               log    ω + (1 − ω)e 2 a Σa   − θ
                                                ∥a∥2

                Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 107–123

Wavelet Shrinkage Generalized Bayes Estimation                                               113

                                                                    )
                               a                      1 T
                  −a T
                          X−       log ω + (1 − ω)e 2   a  Σa
                                                                −θ −1
                              ∥a∥2
                                          −1                              
                                     1 T                             1 T
              =   bω ω + (1 − ω)e 2 a Σa        + log ω + (1 − ω)e 2 a Σa − 1
                                                 −1
                                                      e−a θ MX|θ (aT )
                                            1 T            T
                  +b(1 − ω) ω + (1 − ω)e 2 a Σa
                            h                                  1 T
                                                                                 i
                  +b(1 − ω) E −aT X|θ + log ω + (1 − ω)e 2 a Σa + aT θ − 1
                                  1 T
                                          −1                       1 T
                                                                           
              =   bω ω + (1 − ω)e 2 a Σa       + b log ω + (1 − ω)e 2 a Σa − b
                                           1 T
                                                  −1 1 T
                  +b(1 − ω) ω + (1 − ω)e 2 a Σa       e 2 a Σa
                                     1 T
                                            
              =   b log ω + (1 − ω)e 2 a Σa .                                    (8)

By using the risk R(θ, δ ∗ (X)) in (8), we have the following Bayes risk
                                        Z
                 r(π(θ), δ ∗ (X)) =         R(θ, δ ∗ (X))π(θ)dθ
                                          Θ
                                                            1 T
                                                                 
                                   = b log ω + (1 − ω)e 2 a Σa .

Since Bayes risk r(π(θ), δ ∗ (X)) is a constant value, so δ ∗ (X) is a minimax
estimator.

Corollary 3. In Theorem 4, suppose that X ∼ Np (θ, σ 2 Ip ), then the Bayes risk
of the estimator δ ∗ (X) in (5) is given
                                                                                  
                     ∗                                                    σ2
             r(π(θ), δ (X))      =   b log ω + (1 − ω) exp                   ∥a∥2        .
                                                                          2

Also, if ω = 0, then the Bayes risk of the estimator δ ∗ (X) in (4) is given by

                                                   b T
                              r(π(θ), δ ∗ (X)) =     a Σa.
                                                   2
And the Bayes risk of the estimator δ ∗ (X) in (5) is given by

                                                  b σ2
                             r(π(θ), δ ∗ (X)) =        ∥a∥2 .
                                                    2

    In the following Lemma, we ﬁnd the posterior distribution and generalized
Bayes estimators a multivariate normal distribution Np (θ, Σ) with conjugate prior
distribution Np (0, ϱ2k Σ).

Lemma 1. Suppose that X|θ ∼ Np (θ, Σ) and the prior distribution πk (θ) =
Np (0, ϱ2k Σ), then, we have the following results.
                                                                           
                                                           ϱ2k        ϱ2k
  • The posterior distribution π(θ|X) is Np              ϱ2k +1
                                                                  X, ϱ2 +1 Σ .
                                                                      k




               Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 107–123

114                                                Hamid Karamikabir & Mahmud Afshari


   • Under the balanced-LINEX loss function in (2), when δ0 (X) = X, the Bayes
     estimator for θ is:
                                                                          
                       a                           1             ϱ2k
     δ (X) = X −
      πk
                         log ω + (1 − ω) exp            T
                                                       a X+              T
                                                                        a Σa . (9)
                    ∥a∥2                       ϱ2k + 1       2(ϱ2k + 1)

Proof . The posterior distribution π(θ|X) is obtained as follows.

 π(θ|X)      ∝       π(θ)
               f (ω|θ)                                                
                       1     T −1        T −1       T −1    1 T −1
             ∝ exp −       x Σ x + θ Σ θ − 2θ Σ x + 2 θ Σ θ
                     2                                 ϱk
                       1 T      −1     1 −1        T −1
             ∝ exp − θ        Σ + 2Σ           θ+θ Σ x
                     2              ϱk                                  
                       1      −1    1 −1              T −1   −1    1 −1 −1
             = exp −        Σ + 2Σ            θ θ − 2θ Σ (Σ + 2 Σ ) x
                                               T

                    ( 2            ϱk
                                                         T 
                                                                   ϱk
                                                                           −1
                         1 h               1 −1 −1 −1                 1 −1
                                     −1                         −1
             ∝ exp −          θ − (Σ + 2 Σ ) Σ x              Σ + 2Σ
                         2                 ϱk                         ϱk
                                                i )
                                  1
               × θ − (Σ−1 + 2 Σ−1 )−1 Σ−1 x
                                 ϱk
                                                                  
                                 1                       1
               ∼ Np (Σ−1 + 2 Σ−1 )−1 Σ−1 X, (Σ−1 + 2 Σ−1 )−1
                      2         ϱk                     ϱk
                          ϱk         ϱ2k
               = Np            X, 2       Σ .
                       ϱ2k + 1     ϱk + 1
Now similar to Theorem 3, a generalized Bayes estimator is obtained by the
following:
∂r ∗ (π(θ|X), δ(X))
                            T                         T
                                                                      
                       =    bE ωaT ea (δ(X)−δ0 (X)) + (1 − ω)aT ea (δ(X)−θ) − aT X
      ∂δ(X)
                                    T                              T
                                                                                              
                       =    b ωaT ea (δ(X)−δ0 (X)) + (1 − ω)aT ea δ(X) Mθ|X (−aT ) − aT
                       =    0,

then we have the following Bayes estimator with respect to the target estimator
δ0 (X).
                                              2              2         
              −a                                    ϱ             ϱk
                   log ωe−a δ0 (X) + (1 − ω) exp − 2 k aT X +
                           T
δ πk (X) =                                                               aT Σa            .
             ∥a∥ 2                                ϱk + 1      2(ϱ2k + 1)

Finally by replacing δ0 (X) = X in the Bayes estimator δ πk (X), we have the
following Bayes estimator.
                                                                                 
                     a                               1               ϱ2k
 δ πk (X) = X −          log   ω + (1 − ω) exp           a T
                                                             X +            a T
                                                                                Σa    .
                   ∥a∥2                          ϱ2k + 1         2(ϱ2k + 1)



     Now, in the following theorem, we prove that the generalized Bayes estimators
δ ∗ (X) is admissible and minimax under the balanced-LINEX loss function.

                  Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 107–123

Wavelet Shrinkage Generalized Bayes Estimation                                           115

Theorem 5. Suppose that X ∼ Np (θ, Σ) and δ0 (X) = X, then under the balanced-
LINEX loss function, δ ∗ (X) in (4) is admissible and a minimax estimator.


Proof . We know R(θ, δ) is continuous in θ for any δ. Suppose that δ ∗ is not
admissible. Then, there exists an estimator δ such that R(θ, δ) < R(θ, δ ∗ ), with
strict inequality for some θ, say θ0 . Since R(θ, δ) and R(θ, δ ∗ ) are continuous in
θ, there exist strictly positive constants c1 and c2 such that

               R(θ, δ) < R(θ, δ ∗ ) − c1     f or   θ ∈ [θ : |θ − θ0 | < c2 ],

Consider a sequence of priors πk (θ) = Np (0, ϱ2k Σ), with limk→∞ ϱ2k = ∞, and
uniformly, limk→∞ πk (θ) → π(θ), where π(θ) < ∞ is a proper distribution. Using
the technique of minimizing posterior expected loss, under the balanced-LINEX
loss function by using Theorem 1, we have the Bayes estimator δ πk (X) in (9). By
using the balanced-LINEX loss function Lω,δ0 (θ, δ(X)) in (2), the risk R(θ, δ πk (X))
as follows:
            (                                                                    
                                                    1                  ϱ2k
      bE      ω exp − log ω + (1 − ω) exp                 T
                                                         a X+                   T
                                                                              a Σa
                                                ϱ2k + 1           2(ϱ2k + 1)
                                                                          
                                          1                  ϱ2k
            +ω log ω + (1 − ω) exp            a T
                                                  X   +             aT Σa − 1
                           (          ϱ2k + 1            2(ϱ2k + 1)
                                           h                      1
            +(1 − ω) exp aT X − aT log ω + (1 − ω) exp 2                   aT X
                                                                  ϱk + 1
                                      )               "
                  ϱ2k         i
            +            a Σa − a θ − (1 − ω) aT X − aT θ
                          T        T
              2(ϱ2k + 1)
                                                                          #)
                                        1                  ϱ2k
            − log ω + (1 − ω) exp             T
                                             a X+                   T
                                                                   a Σa
                                    ϱ2k + 1            2(ϱ2k + 1)
               "                                                          −1
                                         1                   ϱ2k
       = bE ω ω + (1 − ω) exp                  T
                                              a X+                   T
                                                                    a Σa
                                     ϱ2k + 1            2(ϱ2k + 1)
                                                                              #
                                                              2
                                         1                  ϱ
            + log ω + (1 − ω) exp            aT X +           k
                                                                   aT Σa       −1
                                     ϱ2k + 1            2(ϱ2k + 1)
                                  −a θ  T

             " − ω)MX|θ (a X)e
                            T
           +b(1
                                                                          −1 #
                                      1                    ϱ2k
           E    ω + (1 − ω) exp               T
                                            a X+                     T
                                                                   a Σa
                                  ϱ2k + 1              2(ϱ2k + 1)
                              
              " −ω)E a X|θ + 
           −b(1          T
                                   b(1 − ω)aT θ
                                                                              −1
                                         1                     ϱ2k
      =    bE ω ω + (1 − ω) exp                 a T
                                                    X  +               a T
                                                                           Σa
                                      ϱ2 + 1             2(ϱ2k + 1)
                                  k                                        
                                         1                    ϱ2k
           + log ω + (1 − ω) exp               a T
                                                   X  +               a T
                                                                          Σa     −1
                                     ϱ2k + 1             2(ϱ2k + 1)
                                                                                −1
                                               1                   ϱ2k
           +(1 − ω) ω + (1 − ω) exp                 a T
                                                        X  +               a T
                                                                               Σa
                                           ϱ2k + 1             2(ϱ2k + 1)

                Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 107–123

116                                                 Hamid Karamikabir & Mahmud Afshari

                                #
                        1 T
            × exp         a Σa        .                                                (10)
                        2

By using Theorem 1 for equation (10), we can obtain the values of risk as follows.
                                       Z
                r(πk (θ), δ πk (X)) =      R(θ, δ πk (x))π(θ)dθ
                                       ZΘ Z
                                    =         gk (x)dx dθ.
                                                Θ    χ

where
                                                                        −1
                                       1                   ϱ2k
  gk (x) =     bω ω + (1 − ω) exp    2 +1    a T
                                                 X   +             a T
                                                                       Σa
                                   ϱ
                                     k                 2(ϱ2k + 1)           
                                           1                   ϱ2k
               + log ω + (1 − ω) exp               T
                                                 a X+                   T
                                                                       a Σa    −1
                                       ϱ2k + 1            2(ϱ2k + 1)
                                                                              −1
                                                 1                 ϱ2k
               +(1 − ω) ω + (1 − ω) exp                T
                                                      a X+                  T
                                                                           a Σa
                                           ϱk2 + 1           2(ϱ2k + 1)
                       1 T
               × exp     a Σa .
                       2

As a result
                                                    −1
                                               1 T
      lim gk (x) =       bω ω + (1 − ω) exp     a Σa
      k→∞
                                              2         
                                                   1 T
                         +b log ω + (1 − ω) exp      a Σa    −b
                                                   2
                                                            −1              
                                                       1 T                1 T
                         +b(1 − ω) ω + (1 − ω) exp       a Σa       exp     a Σa
                                                     2                2
                                                 1 T
                    =    b log ω + (1 − ω) exp     a Σa     = g(x).
                                                 2

Again, we put that Mk = supx∈Rp |gk (x) − g(x)|, then we can write
                                                               
                                                          1 T
     lim sup |gk (x) − g(x)| = b log ω + (1 − ω) exp        a Σa
    k→∞ x∈Rp                                              2
                                                                 
                                                            1 T
                                 −b log ω + (1 − ω) exp       a Σa    = 0.
                                                            2

By Theorem 1, since limk→∞ Mk = 0, then limk→∞ gk (x) = g(x) uniformly on
Rp . As a result by Theorem 2, we can write
                                                                  
             lim r(πk (θ), δ πk (X)) = b log ω + (1 − ω)ψ σ 2 aT a .
              k→∞

Now, according to r(π(θ), δ ∗ (X)) in equation (7) we have

                    lim {r(π(θ), δ ∗ (X)) − r(πk (θ), δ πk (X))} = 0.                  (11)
                k→∞


                Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 107–123

Wavelet Shrinkage Generalized Bayes Estimation                                               117
                      R
Let c3 = limk→∞ inf |θ−θ0 |<c2 πk (θ)dθ. Since limk→∞ ϱ2k = ∞, then c3 > 0.
Therefore, for k is large enough

     r(πk , δ ∗ ) − r(πk , δ πk )   ≥   r(πk , δ ∗ ) − r(πk , δ)
                                        Z
                                    =       (R (θ, δ ∗ ) − R (θ, δ)) πk (θ)dθ > c1 c2 > 0.
                                         Rp

This contradicts with equation (11). As a result, δ ∗ (X) is an admissible estimator.
The minimaxity of δ ∗ (X) follows from its admissibility and the constant risk
phenomenon (7).


3. Shrinkage Wavelet Generalized Bayes Estima-
   tion
    In this section, the goal is to ﬁnd a particular type of the soft wavelet estimator
using the generalized Bayes estimator. In the issue, Huang (2002) investigated
the shrinkage wavelet estimation problem in the multivariate normal by diagonal
covariance matrix σ 2 Ip and Torehzadeh & Arashi (2014) extended his result for
a scale mixture of multivariate normal distributions. Finally Karamikabir &
Afshari (2019) investigated the shrinkage wavelet estimation problem in the class
of elliptically distribution, in LINEX loss function.
   Consider the following model:

                                           X = θ + ε,

where X = (X1 , . . . , Xp )T are the p × 1 random vector, ε = (ε1 , . . . , εp )T
are independent identical distribution Np (0, Σ), and θ = (θ1 , . . . , θp )T are the
p-vector mean vectors. Again, suppose that X|θ ∼ Np (θ, Σ) and δ(X) =
(δ(X1 ), . . . , δ(Xp ))T be an estimator for θ.
    For deonising or shrinkage coeﬃcients, one of the most important concepts
in wavelets and deionising is using thresholds. Shrinkage of the empirical wavelet
coeﬃcients works best in problems where the underlying set of the true coeﬃcients
of f is sparse and the remaining few large coeﬃcients explain most of the functional
form in f . By shrinking, the empirical coeﬃcients towards zero, the smaller ones
which contain primarily noise may be reduced to negligible levels, hence denoising
the signal.
   Let Y1 , . . . , Yn are observed data from model,

                                         Y = f (Z) + η,

where the {ηi } is some noise and {Zi } is some points from domain of f . Typically
n is an integer power of 2.
   Note that the observations are sampled from distribution f but with some noise
and we are interested to remove noises. To achieve this aim, observations or noisy
data are converted to wavelet coeﬃcients. Denoised coeﬃcients are returned to
the Y domain by the inverse discrete wavelet transformation.

                  Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 107–123

118                                              Hamid Karamikabir & Mahmud Afshari


   In this section, we suppose that X ∼ Np (θ, σ 2 Ip ). In this regard, we use the
condition of corollary 2, under the balanced-LINEX loss function with δ0 (X) = X.
Consider the generalized Bayes estimator in (4). For aT = (0, . . . , ai , . . . , 0) (ith
element is ai ), the ith element of δ ∗ (X) is
                                                            
                              1                        1 2 2
              δ ∗ (Xi ) = Xi − log ω + (1 − ω) exp       ai σ     .
                              ai                       2

   We again suppose that aT = (0, . . . , ai , . . . , 0) in the balanced-LINEX loss
function. We consider speciﬁcally ai values depending on signs of θi ’s
                        (
                          c   f or, θi > 0,
                   ai =                          i = 1, . . . n.
                          −c f or, θi < 0,
where c > 0 is some constant. Such an error criterion discourages estimators from
over-estimation in magnitude (i.e., in absolute value) and results in shrinkage
estimation towards zero. In other words, we can be considered this issue as a
regularization problem that regularizes or shrinks the wavelet coeﬃcient estimates
towards zero.
     Under such a loss criterion the generalized Bayes estimator in   (4) is given by
δ ∗ (Xi ) = Xi − sign(θi )λi , where λi = 1c log ω + (1 − ω) exp σ2ii c2 .
    The wavelet estimation problem can be treated via the estimation of the
mean vector θ from a elliptical distribution X|θ ∼ Np (θ, Σ). Often the signs of
parameters θi ’s are not known. A natural approach is to use sign(Xi ) to estimate
sign(θi ) and make truncation at zero. In conclusion, we have the empirical version
of δ ∗ in the following Proposition.
Proposition 2. According to δ sof        t
                                        (Xi ) in (3), by choosing the threshold value of
λi = c log ω + (1 − ω) exp 2 c , the soft wavelet shrinkage estimator for θ by
       1                          σii 2

using δ sof t (Xi ) can be obtained as follows.
                    (
                      (Xi − λi ) ∨ 0 f or, Xi ≥ 0,
    δ sof t (Xi ) =                                      = sign(Xi )(|Xi | − λi )+ .
                      (Xi + λi ) ∧ 0 f or, Xi < 0,


4. Simulation
    In this section, we checked theoretical outcomes with the numerical
computation and simulation to investigate the performance of the soft wavelet
shrinkage estimator in Section 3.
    We compare the new threshold method to the three commonly used shrinkage
strategies, i.e, hard and soft thresholding with the universal threshold and false
discovery rate (FDR). To assess the performance, we calculated the average mean
squared error (AMSE) from the m = 1000 simulations. The value of AMSE is
obtained as follows:
                                 Xm X N                     2
                        m−1 N −1          f (xi ) − fˆ(xi,j ) ,
                                   j=1 i=1


                Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 107–123

Wavelet Shrinkage Generalized Bayes Estimation                                             119

where f (xi ) is the true signal and fˆ(xi,j ) is the estimate of the function from
simulation j. In general, lower values of AMSE represent the accuracy of the     
                                                                             2 1
estimate. We suppose that X ∼ N2 (θ, Σ) where θ = (0, 0) and Σ =                    .
                                                                             1 3
Tables 1 and 2 represent the AMSE with respect to c and ω for wavelet estimator
based on the hard and soft universal threshold, FDR threshold and new threshold
for the X ∼ N2 (θ, Σ). As shown in Tables 1 and 2, the AMSE amount obtained
in the new method is lower than that of the methods. Also, by increasing of the
value of ω, the estimation AMSE increases and by increasing the N and c, the
AMSE of the all method decreases.
   Table 1: AMSE for hard and soft universal, FDR and new threshold for c = 25.
     N                 New threshold                Universal    Universal      FDR
           ω = 0.2        ω = 0.5       ω = 0.8        Soft        Hard
    128   0.01615664    0.01616252     0.01617411   0.04245191   0.22163175   0.11313481
    256   0.00931865    0.00932817     0.00934689   0.03439703   0.19397322   0.11066790


   Table 2: AMSE for hard and soft universal, FDR and new threshold for c = 35.
     N                 New threshold                Universal    Universal      FDR
           ω = 0.2        ω = 0.5       ω = 0.8        Soft        Hard
    128   0.01550174    0.01550226     0.01550329   0.04245191   0.22163175   0.11313481
    256   0.00798796    0.00798963     0.00799290   0.03439703   0.19397322   0.11066790


    In general, as the amount of ω increases, the risk increases. The reason for
this is the value of threshold (λ). As the amount of ω increases, the amount of
λ decreases. Also, by increasing the c or σii , the AMSE decreases. Because the
amount of c or σii increases, the amount of λ increases.
    Now, we checked theoretical outcomes with the numerical computation and
simulation to investigate the performance of the soft wavelet shrinkage estimator
and generalized Bayes estimator. All calculations in this section are done using R
software.
   To investigate the risk of estimators, a Monte Carlo simulation study was
performed to compare the risk values estimators for the N8 (θ, Σ)    √where Σ is       
randomly generated using Wishart distribution and θ is selected as      k, 0, . . . , 0
                                                            P
                                                            p
and k = 0, 0.1, 0.2, . . . , 10. In this case, ∥θ∥ = θT θ =   θi 2 = k. These risk
                                                                  i=1
values have been obtained using the 1000 Monte Carlo simulation replications
and plotted in Figure 1 for p = 8, c = 25, diﬀerent values of ω, the soft wavelet
shrinkage estimator and generalized Bayes estimator.
    In Figure 1 the soft wavelet shrinkage estimator risk curve is lower that of the
generalized Bayes estimator, i.e., the soft wavelet shrinkage estimator dominates
the generalized Bayes estimator. As the value of ω increases, the superiority of the
soft wavelet shrinkage estimator over the generalized Bayes estimator is increases.



                Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 107–123

120                                                                                   Hamid Karamikabir & Mahmud Afshari

                                  ω=0.2                                                                                 ω=0.5




                                                                                      35
                      The soft wavelet shrinkage estimator                                                 The soft wavelet shrinkage estimator




         35
                      Generalized Bayes estimator                                                          Generalized Bayes estimator




                                                                                      30
         30




                                                                                      25
         25




                                                                                      20
         20
  Risk




                                                                            Risk

                                                                                      15
         15




                                                                                      10
         10




                                                                                      5
         5




              0   2           4               6       8            10                          0       2            4            6        8       10

                                     ||θ||                                                                               ||θ||
                                                                            ω=0.8
                                         25




                                                              The soft wavelet shrinkage estimator
                                                              Generalized Bayes estimator
                                         20
                                         15
                                  Risk

                                         10
                                         5




                                                  0       2             4                  6       8           10

                                                                              ||θ||

Figure 1: Risk plot for the soft wavelet shrinkage estimator and generalized Bayes
          estimator with p = 8 for selected values of ω.



5. 3D Road Network Data Set
    In this section, we further investigate the average risk value of the soft wavelet
shrinkage estimator for real data set. For this sake, we use the 3D road network
data set from Guo et al. (2012). This dataset was constructed by adding elevation
information to a 2D road network in North Jutland, Denmark (covering a region of
185×135 km2 ). Elevation values where extracted from a publicly available massive
Laser Scan Point Cloud for Denmark. This 3D road network was eventually used
for benchmarking various fuel and CO2 estimation algorithms. This dataset can be
used by any applications that require to know very accurate elevation information
of a road network to perform more accurate routing for eco-routing, cyclist routes
etc. The dataset contains 4 variables and 434873 observations.
    We have implemented a bootstrap analysis to evaluate the risk functions. Table
3 lists the average risk value of the soft wavelet shrinkage estimator for diﬀerent
values of ω. As shown in Table 3, by increasing of the value of ω, the average
risk value decreases. In the case of ω = 0, the balanced-LINEX loss function

                      Revista Colombiana de Estadística - Theoretical Statistics 45 (2022) 107–123

Wavelet Shrinkage Generalized Bayes Estimation                                               121

is the basic case of LINEX loss function and it has the most average risk value.
Therefore, it can be concluded that the risk values can be reduced by using the
balanced-LINEX loss function.

Table 3: Average risk value of the soft wavelet shrinkage estimator for 3D road network
         data set.
                 δ           ω=0       ω = 0.2     ω = 0.4     ω = 0.6      ω = 0.8
           δ sof t (X)     1.610567    1.397633   1.185041    0.9731351    0.7630683




6. Conclusion
    In this paper, we consider the generalized Bayes shrinkage estimator of mean
vector for multivariate normal distribution under balanced-LINEX loss function.
We assume that the random vector X having Np (θ, Σ) distribution with the
unknown covariance matrix Σ. We ﬁnd minimax and admissible estimator of mean
vector based on generalized Bayes estimator. Theoretical ﬁndings of this paper are
further supported by some numerical analyses. In this regard, the performance
evaluation of the proposed class of estimators is checked through a simulation
study and real data set.


Acknowledgements
   The authors would like to thank the research committee of Persian Gulf
University. Also, the authors would like to thank the editors and reviewers for
their valuable comments, which greatly improved the readability of this paper.
                                                                                
                 Received: December 2020 — Accepted: November 2021


References
Cao, M. X. & He, D. (2017), ‘Admissibility of linear estimators of the common mean parameter in general linear models under a balanced loss function’, Journal of Multivariate Analysis 153, 246–254.
Donoho, D. L. & Johnstone, I. M. (1994), ‘Ideal spatial adaptation by wavelet shrinkage’, Biometrika 81, 425–455.
Fourdrinier, D. & Strawderman, W. E. (2015), ‘Robust minimax stein estimation under invariant data-based loss for spherically and elliptically symmetric distributions’, Metrika 78(4), 461–484.
Guo, C., Ma, Y., Yang, B., S., J. C. & Kaul, M. (2012), ‘EcoMark: Evaluating models of vehicular environmental impact’, SIGSPATIAL/GIS pp. 269–278.
Huang, S. Y. (2002), ‘On a Bayesian aspect for soft wavelet shrinkage estimation under an asymmetric linex loss’, Statistics and Probability Letters 56, 171–175.
Jiang, W. & Zhang, C. H. (2009), ‘General maximum likelihood empirical Bayes estimation of normal means’, The Annals of Statistics 37(4), 1647–1684.
Joly, E. Lugosi, G. & Oliveira, R. I. (2017), ‘On the estimation of the mean of a random vector’, Electronic Journal of Statistics 11, 440–451.
Jozani, J. M., Leblanc, A. & Marchand, E. (2014), ‘On continuous distribution functions, minimax and best invariant estimators, and integrated balanced loss functions’, Canadian Journal of Statistics 42, 470–486.
Jozani, M. J., Marchand, É. & Parsian, A. (2006), ‘On estimation with weighted balanced-type loss function’, Statistics and Probability Letters 76, 733–780.
Jozani, M. J., Marchand, É. & Parsian, A. (2012), ‘Bayesian and Robust Bayesian analysis under a general class of balanced loss functions’, Statistical Papers 53, 51–60.
Karamikabir, H. & Afshari, M. (2019), ‘Wavelet Shrinkage Generalized Bayes Estimation for Elliptical Distribution Parameters under LINEX Loss’, International Journal of Wavelets, Multiresolution and Information Processing 14(1), 1950009.
Karamikabir, H. & Afshari, M. (2020), ‘Generalized Bayesian Shrinkage and Wavelet Estimation of Location Parameter for Spherical Distribution under Balance-type Loss: Minimaxity and Admissibility,’, Journal of Multivariate Analysis 177(1), 104583.
Karamikabir, H. & Afshari, M. (2021), ‘New wavelet SURE thresholds of elliptical distributions under the balance loss’, Statistica Sinica 31(4), 1829–1852.
Karamikabir, H. Afshari, M. & Arashi, M. (2018), ‘Shrinkage estimation of non-negative mean vector with unknown covariance under balance loss’, Journal of Inequalities and Applications 2018, 331.
Karamikabir, H., Afshari, M. & Lak, F. (2020), ‘Wavelet threshold based on Stein’s unbiased risk estimators of restricted location parameter in multivariate normal’, Journal of Applied Statistics 48(10), 1712–1729.
Marchand, E. & Strawderman, W. E. (2020), ‘On shrinkage estimation for balanced loss functions’, Journal of Multivariate Analysis 175, 104558.
Pal, N., Sinha, B. K., Chaudhuri, G. & Chang, C. H. (2007), ‘Estimation Of A Multivariate Normal Mean Vector And Local Improvements’, Statistics 26(1), 1–17.
Rencher, A. C. & Christensen, W. F. (2012), Methods of Multivariate Analysis, third edition edn, John Wiley & Sons.
Rudin, W. (1976), Principle of Mathematical Analysis, MacGraw-Hill.
Torehzadeh, S. & Arashi, M. (2014), ‘A note on shrinkage wavelet estimation in bayesian analysis’, Statistics and Probability Letters 84, 231–234.
Tsukuma, H. & Kubokawa, T. (2015), ‘Estimation of the mean vector in a singular multivariate normal distribution’, Journal of Multivariate Analysis 140(4), 245–258.
Vidakovic, B. (2009), Statistical Modelling by Wavelets, John Wiley and Sons.
Zellner, A. (2009), Bayesian and non-bayesian estimation using balanced loss functions, in J. O. Berger & S. S. Gupta, eds, ‘Statistical decision theory and related topics V’, Springer, New York.
Zinodiny, S., Rezaei, S. & Nadarajah, S. (2017), ‘Bayes minimax estimation of the mean matrix of matrix-variate normal distribution under balanced loss function’, Statistics and Probability Letters 125, 110–120.
