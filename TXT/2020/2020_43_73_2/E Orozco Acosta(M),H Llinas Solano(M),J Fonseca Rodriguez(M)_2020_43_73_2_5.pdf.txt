Convergence Theorems in Multinomial Saturated and Logistic Models. Teoremas de convergencias en los modelos saturados y logísticos multinomiales
Universidad Simón Bolívar, Barranquilla, Colombia. Universidad de Norte, Barranquilla, Colombia.  Corporación Politécnico de la Costa Atlántica, Barranquilla, Colombia
Abstract
In this paper, we develop a theoretical study about the logistic and saturated multinomial models when the response variable takes one of R ≥ 2 levels. Several theorems on the existence and calculations of the maximum likelihood (ML) estimates of the parameters of both models are presented and demonstrated. Furthermore, properties are identiﬁed and, based on an asymptotic theory, convergence theorems are tested for score vectors and information matrices of both models. Finally, an application of this theory is presented and assessed using data from the R statistical program.
Key words: Multinomial logit model; Saturated model; Logistic regression; Maximum likelihood estimator; Score vector; Fisher information matrix.
Resumen
En este artículo se desarrolla un estudio teórico de los modelos logísticos y saturados multinomiales cuando la variable de respuesta toma uno de R ≥ 2 niveles. Se presentan y demuestran teoremas sobre la existencia y cálculos de las estimaciones de máxima verosimilitud (ML-estimaciones) de los parámetros de ambos modelos. Se encuentran sus propiedades y, usando teoría asintótica, se prueban teoremas de convergencia para los vectores de puntajes y para las matrices de información. Se presenta y analiza una aplicación de esta teoría con datos tomados de la librería aplore3 del programa R.
Palabras clave: Modelo logístico multinomial; Modelo saturado; Regresión logística; Estimador de máxima verosimilitud; Vector score; Matriz de información de Fisher.



1. Introduction
    For classifying individuals, logistic regression models are generalized linear
models used (Cox 1958), which allow covariates that have been continuously and
categorically escalated to predict any outcome that has been categorically escalated
(Darlington 1990). These models do not generate assumptions on explanatory
variable distributions and generalize multiple regression analysis techniques to
cases having a categorical dependent variable and a categorical or numerical
predictor variable. Hence, regardless of study design, the logistic regression model
is a direct probability model that is able to provide valid estimates (Harrell 2015).
Accordingly, one of the most common model variants that is assessed is when
the response variable is binary, whether nominal or ordinal, as evidenced by
Hosmer & Lemeshow (2000), Agresti (2013) and Monroy, Morales & Dávila (2018).
Furthermore, LLinás (2006) discussed certain theoretical details of these models.
    For example, applied sciences (such as biomedical and social sciences) often
deal with nominal response variables at several levels as well as a vector of
explanatory variables in which certain components may be intervals while others
may be nominal scale measurements. In this case and other cases in which the
answers are not ordinal and the levels are not organized in a sequential order,
a multinomial logistic regression may assist in assessing the relationship between
nominal responses and the set of explanatory variables; moreover, its applicability
is computationally accessible (Chan 2005, Long 1987).
   The multinomial logit model is a generalized linear model, which can be
considered as a direct extension of the binary logit model since its categories
can contract and reduce it to a binary model. As per Begg & Gray (1984), this
reduced model is able to properly estimate both logistic parameters and their
corresponding standard errors. In fact, it is a special case of discrete choice models
(or conditional logit models) introduced by McFadden (1973) who generalized
binary logistic regression by allowing more than two discrete responses.
   Based on our literature review, several studies have been published on
multinomial logistic regression models. This technique has become widespread
and its use has been very critical within social sciences, marketing applications,
and demographic and educational research studies (Chuang 1997, Peng, Lee &
Ingersoll 2002, Pohlman & Leitner 2003). For example, in classical works, such as
Hosmer & Lemeshow (2000), Díaz & Morales (2009), Kleinbaum & Klein (2010)
and Agresti (2013), these models are viewed as alternative solutions to monitor
data analysis-related issues. Among their multiple applications, Anderson,
Verkuilen & Peyton (2010) used a multinomial logistics model to study the

                Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

Multinomial Saturated and Logistic Models                                              213

psychometric validity of a multiple response instrument. Similarly, as evidenced in
Tan, Christiansen, Christensen, Kruse & Bathum (2004); Chen & Kao (2006) and
McNevin, Santos, Gamez-Tato, Álvarez-Dios, Casares, Daniel, Phillips & Lareu
(2013), these models have been very useful in genetics. Subsequently, to assess
the reliability of cancer diagnoses, multinomial logistic regression models have
been used (Lloyd & Frommer 2008). Furthermore, in epidemiology, this model
has been used in an explanatory study of factors aﬀecting malaria treatments
in African pregnant women (Exavery, Mbaruku, Mbuyita, Makemba, Kinyonge
& Kweka 2014). In economics, the model is implemented as part of a study
that assessed occupational patterns and trends in The Netherlands (Dessens,
Jansen, G. B. Jansen, Ganzeboom & Van der Heijden 2003). In demography,
to determine risks associated with variables considered for each of these studies,
Kim (2015) and Schnor, Vanassche & Bavel (2017) used three-level multinomial
logistic regression models. Finally, Monyai, Lesaoana, Darikwa & Nyamugure
(2016) applied multinomial logistic regression to educational factors derived from
the 2009 General Household Survey in South Africa; however, Ekström, Esseen,
Westerlund, Grafström, Jonsson & Ståhl (2018) applied logistic regression models
to data collected from environmental monitoring programs.
    Nevertheless, certain studies, such as Fahrmeir & Kaufmann (1985), McCullagh
& Neider (2018), and Agresti (2013), failed to provide a detailed development
of a general asymptotic theory for maximum likelihood (ML) estimation for
independent but not identically distributed variables.          However, classical
Mathematical Statistics books, such as Zacks (1971) and Rao, Rao, Statistiker,
Rao & Rao (1973), only mention identically distributed independent variables
that are not applicable to generalized linear models. Nevertheless, many studies,
such as Wedderburn (1974), Wedderburn (1976), or McCullagh (1983), discuss the
more generalized concept of quasi-likelihood functions, which are important for
logistical models with repeated measurements. Therefore, because of the critical
role that multinomial logistic models play in several applications, this study seeks
to develop a theory for independent but not identically distributed variables, i.e.,
a theory that is indeed outlined in the literature but not discussed in detail.
    Therefore, for independent but not identically distributed variables, theoretical
details must be generalized to multinomial model applications when the response
variable takes any of R ≥ 2 levels, which is the primary contribution of this work.
In fact, multinomial models in which the response variable may take one of three
levels are addressed in LLinás & Carreño (2012) and LLinás, Arteta & Tilano
(2016).
    This paper is organized as follows. Section 2 presents the multinomial model,
and Section 3, we brieﬂy introduce the satured model. Then, in Section 4, we
discuss the results from the score vector and the information matrix for the
saturated model. Subsequently, in Section 5, we develop the theory corresponding
to the multinomial logistic model. Section 6 provides the results from the score
vector and the information matrix for this logistic model. Then, in Section 7, we
present and demonstrate a theorem on the existence of logistic parameters and
Section 8 addresses an application of the previously introduced theory. The paper
ends with some conclusions in the Section 9.


               Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

214           Erick Orozco-Acosta, Humberto LLinás-Solano & Javier Fonseca-Rodríguez


2. Basic Model
    Assume that the variable of interest Y able to assume R levels 0, 1, . . . , R − 1.
For each r = 0, 1, . . . , R − 1, we use the notation pr := P (Y = r) to represent
the probability of Y taking the value of r. Making independent observations of
Y , we obtain the sample Y = (Y1 , Y2 , . . . , Yn )T with y = (y1 , y2 , . . . , yn )T , where
yi ∈ {0, 1, 2, · · · , R−1}, i = 1, · · · , n, is a possible value of the sampling variable Yi .
Note that the variables Yi are independent of each other. To build the likelihood
function, R independent variables will be created with values in {0, 1}, as follows:
Uri = 1 si Yi = r and Uri = 0, otherwise, where r = 0, 1, . . . , R − 1 y i = 1, . . . , n.
Note that Uri has a Bernoulli distribution with parameter pri = P (Yi = r). In
terms of Uri , the sampling variables will be Ui = (U0i , U1i , . . . , U(R−1)i ), with
                                                       PR−1
values ui = (u0i , u1i , . . . , u(R−1)i ), where         r=0 uri = 1, for a ﬁxed i value.
                                                              QR−1
Therefore, we achieve the following: P (Ui = ui ) = r=0 puriri , i = 1, . . . , n. Fixing
y we get the likelihood function L for parameter p = (p0 , p1 , . . . , p(R−1) )T , with
pr := (pr1 , pr2 , . . . , prn )T , and using this, the logarithm of the likelihood function
is obtained as follows:
                                  "R−2                             !                 !#
                             X n    X                      X
                                                           R−2               X
                                                                             R−2
    L (p) := ln L(p) =                 uri ln pri + 1 −         uri ln 1 −       pri        (1)
                          i=1    r=0                    r=0                 r=0



3. Saturated Model
      The saturated model is characterized by the following assumptions:

   1. The following are the basic assumptions for the model:
        (a) There are K explanatory variables X1 , . . . , XK (numerical or categorical)
            with values x1i , . . . , xKi , for i = 1, . . . , n (ﬁxed or observed according to
            deterministic or random variables).
        (b) Among the n values (x1i , . . . , xKi ) of X = (X1 , . . . , XK ), there are J
            diﬀerent values, thus deﬁning J populations. Therefore, J ≤ n.
        For each population, j = 1, . . . , J, where:
          • nj is the number of Yij observations (or Urij observations in the
            category r) in each j population. Then, n1 + · · · + nJ = n.
                       X
                       nj
          • Zrj :=            Urij , r = 0, 1, 2, . . . , R − 1 ﬁxed, the random variable
                        i=1
             that represents the sum of the nj observations Urij in j.                  Hence,
                   X
                   nj
                                  X
                                  J       X
                                          n
             zrj =    urij , with   zrj =   uri .
                    i=1                j=1      i=1

        For clarity, the jth (x1j , . . . , xKj ) population will be abbreviated by the
        symbol b for each ﬁxed r = 0, 1, 2, . . . , R − 1.

                  Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

Multinomial Saturated and Logistic Models                                                                   215

  2. For each j = 1, . . . , J population and each i = 1, . . . , n observation in j, it
     is assumed that the variables (Uri | b) are independent of each other, with
     a Bernoulli distribution with parameter prj = P (Uri = 1| b) = E(Uri | b).
     Here vrj := V (Uri | b) = prj (1 − prj ). Hereinafter, the b will be pressed.
     This second assumption implies that, for each r = 0, 1, . . . , R − 1 and each
     population j = 1, . . . , J:

     (a) All prij for i = 1, . . . , n within each population j are equal. That is, the
         dimensional vector (R−1)J is taken as a parameter p = (p1 , p2 , . . . , pJ )T ,
         where pj := (p0j , p1j , . . . , p(R−1)j ).
     (b) The Zrj variables are independent among populations with a binomial
         distribution with parameters nj and prj , With mrj := E(Zrj ) = nj prj
         and Vrj := V (Zrj ) = nj vrj .

     In vector form, Z = (Z0 , Z1 , . . . , ZR−1 )T with Zr := (Zr1 , Zr2 , . . . , ZrJ )T .
     Values are gathered in the z = (z0 , z1 , . . . , zR−1 )T vector, where the zr
     components are deﬁned by zr := (zr1 , zr2 , . . . , zrJ )T . From the above:

        • m := E(Z) = (m0 , m1 , . . . , m(R−2) )T , with mr := (mr1 , mr2 , . . . , mrJ )T .
        • V := Cov(Z), which is a matrix of (R − 1)J × (R − 1)J. The elements
          of this matrix can be seen in item (b) in the proof of theorem 4.

   According to (1), the logarithm of the maximum likelihood function will be:
                   "R−2                           !                !#
                XJ   X                     X
                                           R−2             X
                                                           R−2
        L (p) =          zrj ln prj + nj −     zrj ln 1 −      prj    .     (2)
                   j=1       r=0                          r=0                       r=0


    The following theorem evidences the ML estimator for the saturated model
parameters and the possible values that the logarithm of the corresponding
likelihood function can accept when assessed in a point estimate vector of the
parameter vector.

Theorem 1. For each ﬁxed r = 0, 1, . . . , R − 1 and j = 1, 2, . . . , J, the ML
                             Zrj             zrj
estimators of prj are p̃rj =     with p̃rj =     . Further,
                             nj              nj

                            "R−2                                      !                         !#
                 X
                 J           X                           X
                                                         R−2                       X
                                                                                   R−2
      L (p̃) =         nj          p̃rj ln p̃rj +   1−         p̃rj       ln 1 −         p̃rj        < 0.   (3)
                 j=1         r=0                         r=0                       r=0


Proof . This theorem is demonstrated in the Appendix hereto.


4. Score and Information of the Saturated Model
    In this section, we present and demonstrate some asymptotic properties both
for the score vector and for the information matrix in the saturated model,

                  Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

216           Erick Orozco-Acosta, Humberto LLinás-Solano & Javier Fonseca-Rodríguez


highlighting the fact that we are using independent but not identically distributed
variables, thus providing the details that are not yet found in the literature. These
results are important for perfoming comparison tests using the logistic model. In
the following theorem the corresponding properties are presented.

Theorem 2. In the saturated model:

(a) The score vector of the sample is a column vector of size (R − 1)J and is given
    by S(p) := ∂L                                     T
               ∂p = (S0 (p), S1 (p), . . . , SR−2 (p)) , where Sr (p) is a column vector
                                     Zrj −nj prj  n (p̃ −prj )
      of size J, with elements           vrj     = j rjvrj     . In addition, E(S(p)) = 0.

(b) The information matrix of the sample is a square matrix of size (R − 1)J and
    is given by

                                                                                         
                                               A00             A01   ···       A0(R−2)
                                              A10             A11   ···       A1(R−2)    
                                                                                         
            ℑ(p) := Cov(S(p)) =                ..              ..   ..           ..      ,
                                                .               .      .          .      
                                              A(R−2)0    A(R−2)1     ···    A(R−2)(R−2)

      where Arr′ are diagonal matrices of size J and whose diagonal elements are
                  ′      nj prj pr′ j
                                      if r ̸= r′ . For clarity, ℑ̃ := ℑ(p).
      nj
      vrj if r = r and − vrj v ′    r j

            2
               
(c) E − ∂∂pL2      = −ℑ̃∗ , where ℑ̃∗ is a square matrix of size (R − 2)J given by

                                                                            
                                A00            −A11     ···     −A(R−2)(R−2)
                              −A00             A11     ···     −A(R−2)(R−2) 
                                                                            
                       ℑ̃∗ =    ..              ..     ..           ..      ,
                                 .               .        .          .      
                               −A00            −A11     ···      A(R−2)(R−2)

      where Arr is the diagonal matrix described in the previous item.

(d) Then Z ∗ = ℑ̃ 2 (V ∗ )−1 (Z − m), where V ∗ = diag{v0∗ , v1∗ , . . . , v(R−2)
                                                                            ∗
                       1
                                                                                  }, with
      vr∗ = (vr1 , vr2 , . . . , vrJ ) . Then, S(p) = ℑ̃ 2 Z ∗ or Z ∗ = ℑ̃− 2 S(p).
                                T                          1                   1




Proof .

(a) We denote the random score vector of the i observation by Si (p). The results
    are obtained immediately if the following is taken into account:            1
                                                                      assumption
                                         Pn             PJ P
    from section 3, the property S(p) = i=1 Si (p) = j=1                S
                                                                  i en j i (p)  and
      that E(Zrj ) = nj prj .

(b) Taking a ﬁxed value j = 1, . . . , J. For all r = 0, 1, . . . , R − 2 we ﬁnd that
                                                  
                                      Zrj − nj prj       nj
                             V                       =
                                          vrj           vrj

                   Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

Multinomial Saturated and Logistic Models                                                            217

    and for all r′ = 0, 1, . . . , R − 2 with r′ ̸= r:
                                          
           Zrj − nj prj Zr′ j − nj pr′ j            1                            nj prj pr′ j
     Cov                 ,                   =             Cov(Zrj , Zr′ j ) = −                       .
                vrj                vr ′ j        vrj vr′ j                        vrj vr′ j

(c) With j, k = 1, 2, . . . , J and r, r′ = 0, 1, . . . , R − 2.

                                             ∂ 2L                            2
                                                                                     
    (1) If j ̸= k and r ̸= r′ , then                   = 0. Therefore, E ∂p ∂′ ∂p
                                                                                L
                                                                                       = 0.
                                           ∂pr′ k ∂prj                     r k    rj


    (2) If j = k and r = r′ , then:
                                               "                                        !         #
          ∂ 2L     ∂         Zrj − nj prj           Zrj                    nj (1 − 2prj )         nj
                =                             =−        − prj                                   +       .
          ∂p2rj   ∂prj       prj (1 − prj )         nj                            2
                                                                                vrj               vrj

    (3) Now, for r ̸= r′ and j = k,
                                                                              
                                                                     X
                                                                     R−2

                                                     nk −      Zrk 
                           ∂ 2L         ∂L    Zrk         r=0
                                                                    
                                                                    
                                     =             −               
                         ∂pr′ k ∂prk   ∂pr′ k  prk        X
                                                           R−2
                                                                    
                                                       1−       prk
                                                                     r=0
                                                          
                                               Zrk             nk (1 − 2prk )   nk
                                      =            − prk             2        +     .
                                               nk                   vrk         vrk
                            2                                       2
                                                                              
    That is, for r = r′ , E ∂p
                            ∂ L
                               2  = − vnrk
                                         k
                                           , and for, r ̸= r′ , E ∂p ∂′ ∂p
                                                                         L
                                                                           rk
                                                                                = vnrk
                                                                                     k
                                                                                       .
                               rk                                   r k
    The results found in these three cases therefore demonstrate the subsection.

(d) Since

            ∂L                           h                       i
               = (V ∗ )−1 (Z − m) = ℑ̃1/2 ℑ̃−1/2 (V ∗ )−1 (Z − m) = ℑ̃1/2 Z ∗ ,
            ∂p

    the result is immediate


                                                                                                n
Remark 1. In this case, ℑ̃ has diagonalizable main submatrices with vrjj > 0
elements and the determinant is positive, which implies that ℑ̃ positive deﬁnite
and therefore non singular and the ℑ− 2 root exists (see Harville (1997)).
                                      1




   In the following theorem, asymptotic results are presented and demonstrated
both for the score vector and the information matrix in the saturated model.
                                      nj            1
Theorem 3. Suppose             lim             =    2    > 0 for all j = 1, 2, . . . J and all
                              nj →∞ nvrj           σrj
r = 0, 1, . . . , R − 1. Then, for the saturated model (when n → ∞ and J is ﬁxed):

                  Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

218           Erick Orozco-Acosta, Humberto LLinás-Solano & Javier Fonseca-Rodríguez

                a
 (a) n1 S(p) = n1 S ∗ (p), where S ∗ (p) = ℑ̃(p̃ − p), with pe being the ML-estimator of
     p in the saturated model.
         2 
                  a
 (b) n1 − ∂∂pL2 = n1 ℑ̃∗ . That is, for all j = 1, 2, . . . , J and all r, r′ = 0, 1, . . . , R −
     1, we have                                                
                            1 ∂ 2L           1         ∂ 2L       P
                                          − E                     → 0.
                            n ∂pr′ j ∂prj   n       ∂pr′ j ∂prj
         d                    
 (c) Z ∗ → N (R−1)J 0, I(R−1)J .
                                                                   
                    d                                               ℑ̃
 (d) √1n S(p) → N (R−1)J (0, Ξ̃), where Ξ̃ = lim                    n     .
                                                           n→∞

       a                                               P
Here, = means asymptotic equivalence; →, indicates convergence in probability;
 d
→, indicates convergence in distribution; N (R−1)J , is the (R − 1)J-dimensional
normal distribution and I(R−1)J is the (R − 1)J-dimensional identity matrix .

Proof .
                                         h        i
 (a) As per item (a) of theorem 2, S(p) = diag(ℑ̃) (p̃ − p), where diag(ℑ̃) is
      the diagonal matrix whose elements are the same diagonal elements of ℑ̃.
      Then, n1 (S(p) − S ∗ (p)) is a vector column of size J that has the following
      as elements

                               X
                               R−2
                                    nj prj p0j
                                               (p̃rj − prj ),        j = 1, . . . , J
                                r=0
                                    nvrj v0j

      For a ﬁxed j and r,
                                                              
                   nj prj p0j                 1 p0j prj Zrj        P
                    lim       (p̃rj − prj ) = 2             − prj −→ 0.
            nj →∞ nvrj v0j                   σrj v0j     nj
                                                         2 
 (b) As per item (c) of theorem 2, it is known that E − ∂∂pL2 = −ℑ̃. Suppose
      r, r′ and j are ﬁx and
                                                                                 
                                       1          ∂ 2L             1      ∂ 2L
                        Err′ (n, j) :=      −                     − E −               .
                                       n        ∂pr′ j ∂prj        n    ∂pr′ j ∂prj

           • If r = r′ , then
                                                                                      
                                                 1        Zrj             nj (1 − 2prj )
                              Err′ (n, j) = −                 − prj                        .
                                                 n        nj              vrj    vrj

           • If r ̸= r′ , then
                                                                                       
                                           1        Zrj             nj (1 − 2prj + 2vrj )
                           Err′ (n, j) =                − prj                               .
                                           n        nj              vrj       vrj

                    Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

Multinomial Saturated and Logistic Models                                                  219
                                     p
                            Z
     Now, it is known that nrjj − prj −→ 0 if n → ∞, by the weak law of the
     large numbers. Because
                               nj (1 − 2prj ) nj →∞ 1 (1 − 2prj )
                                              −−−−→ 2             >0
                              nvrj    vrj           σrj   vrj
     and
               nj (1 − 2(prj + vrj )) nj →∞ 1 (1 − 2(prj + vrj ))
                                        −−−−→ 2                   > 0,
              nvrj            vrj            σrj     vrj
                          
                    2        a
     then, n1 − ∂p∂ ′ ∂p
                      L
                         r
                             = n1 ℑ̃∗ .
                          r


 (c) As the variables Zrj converge to the normal distribution by the multivariate
                                        d
     central limit theorem, Z −−−−→ N (R−1)J (m, V ). Then, for a ﬁxed value of
                               n→∞
     J:                                                
             Z ∗ −−−−→ N (R−1)J E(Z ∗ ), ℑ̃−1/2 ℑ̃ℑ̃−1/2 = N (R−1)J (0, I).
                   d
                     n→∞

 (d) Considering item (d) of theorem 2, we have:

                                                                           !1/2
                1                 1     h                       i       ℑ̃
               √          S(p) = √ ℑ̃1/2 ℑ̃−1/2 (V ∗ )−1 (Z − m) =                  Z ∗.
                 n                 n                                    n

     Now, because
                                           1 n−→∞
                                             ℑ̃ −−−−→ Ξ̃,                       (4)
                                           n
                  1/2                                 1/2
                     ℑ̃
                               −−→ Ξ̃1/2 . Therefore, ℑ̃
                                c.s                           d
     we have         n                                    n   → Ξ̃1/2 , and with it,
                                                              −
              d
              → N (R−1)J (0, Ξ̃).
      √1 S(p) −
        n




Remark 2. The assumption from theorem 3 can be interpreted as follows: “The
speed” of each nj −→ ∞ must be the same as that of n −→ ∞. For example, in a
balanced design, all the nj are equal. In this case, nj = Jn . Therefore, the amount
1 nj
n · vj = Jvj is ﬁxed; that is, it does not depend on n. Using the notations from
           1

the previous theorem, σj2 = Jvj because, in this case, the expression (4) becomes
an equal value of n1 ℑ̃ = Ξ̃.


5. Multinomial Logistic Model
   Assumptions 1 and 2 of section 3 include that the design matrix:
                                                   
                                1 x11 · · · x1K
                              1 x21 · · · x2K 
                                                   
                        C= .         ..         .. 
                              ..      .          . 
                                        1   xJ1   ···   xJK

                 Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

220            Erick Orozco-Acosta, Humberto LLinás-Solano & Javier Fonseca-Rodríguez


   has full rank with Rg(C) = 1 + K ≤ J. To deﬁne the logistic model, we take
one of the categories of the dependent variable Y as the reference; let us say for
R − 1, with the following additional assumption:

                                  
                           prj
      gr (xj ) = ln                    = δr + βr1 xj1 + . . . + βrK xjK ,   r = 0, 1, . . . , R − 2,
                          pR−1,j

   where xj := (1, xj1 , . . . , xjK )T . With βr := (δr , βr1 , . . . , βrK )T , the parameter
vector α = (β0 , β1 , . . . , βR−2 )T is a column vector of size (R−1)(1+K). Note that
the assumption Rg(C) = 1+K is important for the parameter α to be identiﬁable.
For an xj observation in population j and for each r,

                                                 exp {gr (xj )}
                                          prj = R−1                ,                                   (5)
                                                X
                                                    exp {gs (xj )}
                                                    s=0


    where gR−1 (xj ) = 0. The logarithm of the likelihood function can be written
as a function of α as follows:
                         "R−1                                           !#
                     XJ    X                        X
                                                    R−1
            L (α) =            zrj gr (xj ) − nj ln     exp {gsj (xj )}    .  (6)
                             j=1   r=0                         s=0


The likelihood equations are found by calculating the ﬁrst derivatives of L (α) with
respect to each of the (R − 1)(1 + K) unknown parameters. Therefore, for each
ﬁxed k = 0, 1, . . . , K and each r = 0, 1, . . . , R, the likelihood equations are given
by

                                                            
                                             
    ∂L    XJ
                            exp{grj (xj )}  X   J
        =                  
              xjk Zrj − nj  R−1               =   xjk (Zrj − nj prj ) .                            (7)
                                               
   ∂βrk   j=1              X                  j=1
                                  exp{gs (xj )
                                              s=0


The maximum likelihood estimator is obtained by equating these equations to
zero and solving the logistic parameters. The solution requires the same type of
iterations that were used to obtain the estimates in binary cases and with three
levels, as demonstrated in LLinás (2006) and LLinás et al. (2016), respectively.
For general cases, results are presented in section 7.


6. Logistic Model Information and Score
    The following theorem shows some properties of the score vector and the
information matrix in a logistic model.

                      Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

Multinomial Saturated and Logistic Models                                                   221

Theorem 4. In a logistic model:

(a) The random score of the sample is a vector column of size (R−1)(1+K) given
               ∂L
    by S(α) := ∂α  = IR−1 ⊗ C T (Z − m). Also, E(S(α)) = 0. Here, IR−1 is the
    identity matrix of order R − 1 and the symbol ⊗ means the Kronecker product.
(b) The information matrix of the sample is given by ℑ(α) := Cov(S(α)) =
    (IR−1 ⊗ C T )V (I2 ⊗ C). Here, IR−1 and I2 are the identity matrices of order
    R − 1 and 2, respectively.
          2          2
          2 ) = ∂α2 = −ℑ.
       ∂ L      ∂ L
(c) E( ∂α

Proof .

(a) Considering Equation (7). Taking βr0 = δr , it can be veriﬁed that S(α) =
    (IR−1 ⊗ C T )(Z − m), where IR−1 is the identical matrix of order R − 1. With
    this result, we have that E(S(α)) = (IR−1 ⊗ C T )(E(Z) − nj prj ) = 0.
(b) For part (a),

        ℑ(α) = (IR−1 ⊗ C T )Cov(Z)(IR−1 ⊗ C T )T = (IR−1 ⊗ C T )V (I2 ⊗ C).                  (8)

(c) Considering mrj as deﬁned in assumption 2 from section 3, for r, r′ =
    0, 1, . . . , R − 2, and j = 1, . . . , J, k = 1, . . . , K all ﬁxed values, we have
    ∂mrj            ∂prj
    ∂β ′ = nj ∂β ′ . The following two cases are found:
       r k               r k


       • If r = r′ , then ∂βrk
                            rj ∂p                     rj    ∂m
                               = vrj xjk . That is, ∂βrk = nj vrj xjk .
       • If r ̸= r′ ,
                 ∂prj        exp{grj (xj )} exp{gr′ j (xj )}xjk
                       =                                        2 = −prj pr′ j xjk .
                 ∂βr k
                    ′
                                                X
                        1 + exp{gr′ j (xj )} +    exp{grj (xj )}
                                                   r̸=r ′

                     ∂m
                      rj
              Thus, ∂βrk = −nj prj pr′ j xjk .
    Considering the results obtained in both cases, we have:

             ∂ 2L    ∂                                      ∂m
                2
                  =     (IR−1 ⊗ C T )(Z − m) = −(IR−1 ⊗ C T )    .
             ∂α     ∂α                                        ∂α
                                  
              ∂α = V I(R−1) ⊗ C , we can conclude from Equation (8) that
    Because, ∂m
    ∂2L                        ∂2L
    ∂α2 = −ℑ and with that, E( ∂α2 ) = E(−ℑ) = −ℑ.




Remark 3. For the particular case of non-grouped data, where nj = 1, ∀j and
J = n, Z = U , m = (p0 , p1, . . . , . . . , p(R−2) )T , with pr = (pr1 , . . . , prn )T and C is
the original design matrix of (R − 1) × (1 + K).

                    Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

222          Erick Orozco-Acosta, Humberto LLinás-Solano & Javier Fonseca-Rodríguez


Theorem 5. Considering the assumptions from sections 3 and 5:

(a) The matrix ℑ has a full rank Rg(ℑ) = (R − 1)(1 + K) and is positive deﬁnite.
                            nj    1
(b) Suppose that lim             =2 > 0 does exist, then a Ξ square matrix, positive
                    nj →∞ nvrj   σrj
      deﬁnite and of size (R − 1)(1 + K), also exists such that:

               1       d
              √ S(α) −−−−→ N (R−1)(1+K) (0, Ξ) , f or a f ixed value of J.
                n     n→∞


              d
      Here, −→ means convergence in distribution and N (R−1)(1+K) is the (R −
      1)(1 + K)-dimensional normal distribution.

Proof .

(a) Taking into account
                       that Rg (C) = 1 + K, where ℑ is square of (R − 1)(1 + K),
    that Rg AT A = Rg (A) and that Rg (IR−1 ⊗ C) = Rg(IR−1 )R(C):
                                                   
               Rg (ℑ) = Rg (IR−1 ⊗ C T )V (IR−1 ⊗ C) = (R − 1)(1 + K).

      Here, ⊗ is the Kronecker product and IR−1 , the identity matrix of order R −1.
      This indicates that ℑ has a full rank. Now, we must prove that ℑ is positive
      deﬁnite. That is, ∀ u ̸= 0, uT ℑu > 0. With u ̸= 0 being any column vector of
      (R − 1)(1 + K), we have
                                 h               1      1
                                                                   i
                    uT ℑu = uT (IR−1 ⊗ C T )(V 2 )T V 2 (IR−1 ⊗ C) u
                               1             T  1               
                          = V 2 (IR−1 ⊗ C)u        V 2 (IR−1 ⊗ C)u .

                              
      But V 1/2 I(R−1) ⊗ C u is a column vector of (R − 1)(1 + K). Therefore, for
      all u ̸= 0, it is true that uT ℑu ≥ 0. However, uT ℑu = 0 if and only if it is
                    1
      true that V 2 (IR−1 ⊗ C) u = 0. Now,

        • For r = r′ , the components nj vrj > 0, vrj = prj (1 − prj ).
        • For r ̸= r′ , the components −nj prj pr′ j < 0.

    Therefore, u = 0. Then, ∀ u ̸= 0, uT ℑu > 0. Thus, ℑ is positive deﬁnite.
                                      T
(b) With λ := λ0 , . . . , λ(R−1)(1+K) being any vector of real numbers, we want
    to prove that
                                        
                                             1 X T
                                                 n
                                   1
                             λT √ S(α) = √          λ Si (α)
                                    n         n i=1

      has a asymptotic one-dimensional normal distribution, where Si (α) is the score
      vector of observation i. This is checked below:

                  Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

Multinomial Saturated and Logistic Models                                              223
                                
      • We have that E λT Si (α) = λT E (Si (α)) = λT (0) = 0.
      • The information matrix ℑi (for a Yi observation in population     j)
        corresponding to the logistic model is given by ℑi = Cov (Si )(α) . As
        ℑi is positive deﬁnite ∀i in j,
                                  
                       V λT Si (α) = λT Cov (Si (α)) λ = λT ℑi λ > 0.

                                  e ∗ , where V ∗ is as per theorem 2(d). Now,
         It is clear that V = V ∗ ℑV
                               
                               ∗ ℑ̃    ∗
         n ℑ = (IR−1 ⊗ C )V        n [V (IR−1 ⊗ C)]. Therefore, as per Tilanos
         1                  T

         Theorem 3.3.1b (Tilano & Arteta 2012): n1 ℑ −−−−→ Ξ̃. Then,
                                                               n→∞


                     1                        
                       ℑ −−−−→ (IR−1 ⊗ C T )V ∗ Ξ̃ [V ∗ (IR−1 ⊗ C)] := Ξ
                     n    n→∞


         y Rg (Ξ) = (R − 1) (1 + K).


                                             d
    Then, we must prove that √1n S(α) −−−−→ N (R−1)(1+K) (0, Ξ) for a ﬁxed value
                                            n→∞
                                                                                J→∞
    of J. For the non-grouped case, it is true that Ξ holds J1 ℑ(α) −−−−→ Ξ.
    Considering the above and knowing that Ξ is positive deﬁnite, we have

                      1X
                            n
                                            1
                            V λT Si (α) = λT ℑλ −−−−→ λT Ξλ > 0.
                      n i=1                 n    n→∞




    In addition Lindberg’s condition holds, that is,

                     1X
                        n                    2                      
           ∀ε > 0,         EλT Si (α) λT Si (α) · 1{[λT Si (α)]2 >ε2 n} −→ 0.
                     n i=1                                               n→∞



                                                    
                                       d
    Therefore, we have λT √1n S (α) −→ N1 0, λT Ξλ , when n → ∞. Then,
    when applying the multivariate central limit theorem, it is concluded that
              d
    √1 S (α) −→ N2(1+K) (0, Ξ), when n → ∞ and J is ﬁxed.
      n




Remark 4. For the case of non-grouped data, where J = n, the assumption given
in (b) makes no sense because J is not ﬁxed. Then, it is assumed immediately
                                                           d
that J1 ℑ has a positive deﬁnite limit Ξ and √1J S(α) −→ N (R−1)(1+K) (0, Ξ) when
J −→ ∞.

               Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

224           Erick Orozco-Acosta, Humberto LLinás-Solano & Javier Fonseca-Rodríguez


7. Existence and Calculations of the Logistic
   Parameters
Theorem 6 (Existence theorem). The ML-estimations α̂ of α exist, are unique
and calculated according to the following recursion formula:

          α̂(0) = 0
      α̂(t+1) = α̂(t) + [(IR−1 ⊗ C T )V̂ (t) (IR−1 ⊗ C)]−1 (IR−1 ⊗ C T )(Z − m̂(t) ),

where V̂ and m̂ are the estimated matrix of covariances and the expected vector of
Z, respectively, (hence, V̂ and m̂), deﬁned in section 3 assumption 2. In addition,
asymptotically we have
                                              
      √            a     −1    1 ∂L        1 ∂L
        n(α̂ − α) = Cov       √        . √
                                n ∂α        n ∂α
                     √                             −1
                   = n (IR−1 ⊗ C T )V (IR−1 ⊗ C)        (IR−1 ⊗ C T )(Z − m).
                               2
                            2 = −ℑ is a full rank matriz (R−1)(K +1). Therefore,
                         ∂ L
Proof . By theorem 4 (c) ∂α
only ML-estimations α̂ may be used as solutions of the (R − 1)(K + 1) equations

                                        ∂L (α)
                                        S(α) :== 0.                           (9)
                                          ∂α
                                               
   Alternatively, by theorem 4 (a), IR−1 ⊗ C T (Z − m) = 0. The following must
            (α̂)
be true: ∂L∂α    = 0. Using the Taylor approximation, if α1 is a point between α
and α̂, then
                        ∂L (α)   ∂L (α̂) ∂ 2 L (α)
                               =         +         (α − α̂).
                          ∂α       ∂α        ∂α2
    Considering Equation (9), , this expression can be rewritten by theorem 4 as
                                   −1
α̂ − α = (IR−1 ⊗ C T )V (IR−1 ⊗ C)      (IR−1 ⊗ C T )(Z − m), where V1 = V (α1 ).
As α1 is a point on the line segment that joins α and α̂, α1 = tα̂ + (1 − t)α,
for all t ∈ [0, 1]. Under the assumption that α̂ is strongly consistent for α, that
        c.s                                   a.s             P
is, α̂ −→ α, n −→ α, by components α1 −→ α ⇒ α1 −→ α. This implies
      √                                           −1 1
that n(α̂ − α) = (IR−1 ⊗ C T ) n1 V1 (IR−1 ⊗ C)       √ (IR−1 ⊗ C T )(Z − m). As
                                                        n
      P
α1 −→ α, when n −→ ∞, we have
                            −1
             T 1                  1                     a
    (IR−1 ⊗ C ) V1 (IR−1 ⊗ C)    √ (IR−1 ⊗ C T )(Z − m) =
               n                   n
  |                         {z                        }
                                   √
                                       n(α̂−α)
                                                −1
                                  T 1                 1
                         (IR−1 ⊗ C ) V (IR−1 ⊗ C)    √ (IR−1 ⊗ C T )(Z − m) .
                                    n                  n
                       |                        {z                        }
                                   √
                                        n[(IR−1 ⊗C T )V (IR−1 ⊗C)]−1 (IR−1 ⊗C T )(Z−m)



                  Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

Multinomial Saturated and Logistic Models                                                 225
                                             −1
   We have α̂ ∼
              = α+ (IR−1 ⊗ C T )V (IR−1 ⊗ C)       (IR−1 ⊗C T )(Z −m). Replacing
α on the right side by the t-th approximation α̂(t) of α, we obtain the recursion
formula that provides the (t + 1) − th approximation α̂(t+1) of α̂.


8. Example
    Fontanella, Early & Phillips (2008) present results from a study that examines
the inﬂuence of both clinical and non-clinical factors on level of aftercare
decisions. The corresponding data set (named APS data) were modiﬁed to
protect conﬁdentiality. It can be downloaded from the aplore3 library in the R
program. For the application, we selected only two variables, which are described
in Table 1.
                   Table 1: Code sheet for the variables in the Study.
     Variable      Description               Code/Values                     Name
         1         Placement combined        0 = Residential, 1 = In-      PLACE3 (Y )
                                             termediate residential, 2 =
                                             Day treatment or Outpa-
                                             tient
         2         History of violence       0=No, 1= Yes                   VIOL (X)


To apply the multinomial logistics model, we have used Day treatment or
Outpatient (2) as the reference outcome value. When these data were entered
in the R statistical package and grouped into populations. First, the Table 2
shows the cross-classiﬁcation of PLACE3 versus history of violence (VIOL). We
observe that the dependent variable Y takes one of the possible R = 3 values and
the explanatory variable X is dichotomic.

Table 2: Cross-Classiﬁcation of Placement (PLACE3) by History of Violence (VIOL).
                                               History of violence (X)
          PLACE3 (Y )                          No (0)         Yes (1)       Total
          Intermediate residential (0)           26             104          130
          Residential (1)                        15             104          119
          Day treatment or Outpatient (2)        80             179          259
          Total                                  121            387          508


We found that J = 2, with n1 = 121 (Group for X = 0) and n2 = 387 (Group
for X = 1). In this case, Z = (15, 104, 26, 104)T . By applying theorem 1,
the vectors of estimated parameters were pe = (0.124, 0.269, 0.215, 0.269)T and
                                     d
L (p̃) = −515.7323. The matrix Vb := Cov(Z) is of 4 × 4 and has the form:
                                                                     
                          13.140   0.000           −3.223       0.000
                         0.000   76.052            0.000     −27.948 
                   Vb = 
                         −3.223
                                                                      
                                   0.000           20.413       0.000 
                           0.000 −27.948            0.000       76.052

                  Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

226        Erick Orozco-Acosta, Humberto LLinás-Solano & Javier Fonseca-Rodríguez


By applying theorem 2, we could easily verify that E(S(e   p)) = 0 and that the
information matrix in the saturate model is provided by
                                                                        
                              1114.189       0.000 −175.925        0.000
                                0.000 1969.306         0.000 −723.702 
     b p) := Cov(S(e
     ℑ(e     d      p)) =                                               
                           −175.925         0.000   717.231       0.000 
                                 0.000 −723.702         0.000 1969.306

Since J = 1 + K (J = 2 and K = 1) then C would be a non-singular or invertible
matrix. Therefore, αb = (βb0 , βb1 )T with βbr = C −1 gbr for r = 0, 1. Here
                                                                   T
                                                   pbr1           pbr2
               βbr := (δbr , βbr1 )T , gbr := ln           , ln
                                                   pb21           pb22

That is, there is a one-to-one relationship between the parameters of the saturated
model and those of the logistics model. That is, the models express the same thing,
where p̂rj = p̃rj for each j = 1, 2. Taking into account that
                                                                      
                 −1.674                  −1.124                    1 0
       gb0 =               ,     gb1 =              ,    C −1 =
                 −0.543                  −0.543                   −1 1

we found that αb = (−1.674, 1.131, −1.124, 0.581)T . Applying the theorem 4, the
estimation of the information matrix in logistic model is given by
                                                                        
                                 89.192      76.052 −31.171 −27.948
                              76.052        76.052 −27.948 −27.948 
      ℑ(b       d
         α) := Cov(S(b α)) =                                            
                              −31.171 −27.948         96.465     76.052 
                                   −27.948    −27.948       76.052      76.052

As the estimator of the covariance matrix of the maximum likelihood estimator
is the inverse of the observed information matrix, then the estimated covariance
matrix for the ﬁtted model is
                                                                         
                                 0.079 −0.079  0.013               −0.013
                               −0.079  0.094 −0.013                0.018 
          Vb (b     b α)]−1 = 
              α) = [ℑ(b                                                   
                               0.013 −0.013   0.051               −0.051 
                                −0.013  0.018 −0.051                0.066
When we apply the package nnet in R, we get the same results as above. The
results of ﬁtting the three-category logistic regression model, using the multinom
function, to these data are presented in Table 3. In this table, appear the estimated
coeﬃcients, the estimated standard error of the coeﬃcients, the values of the
                        d and the 95% conﬁdence interval for the odds ratio for
estimated odds ratio (OR)
PLACE3 = 0 versus PLACE3 = 2 and for PLACE3 = 1 versus PLACE3 = 2.
From the Table, we see that statistically the VIOL variable is signiﬁcantly
associated with adolescent placement. Table 4 shows the results obtained when
performing the comparison test of the null model with the logistic model.

               Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

Multinomial Saturated and Logistic Models                                                  227

       Table 3: Results of Fitting the Logistic Regression Model to the Data.
         Logit    Coeﬃcient       Estimation      Std. Error       d
                                                                   OR        95% CI
           0      Constant            -1.674         0.2814
                  VIOL                1.131          0.3072        3.10     (1.70, 5.66)
           1      Constant            -1.124         0.2257
                  VIOL                0.581          0.2572        1.79     (1.08, 2.96)

         Table 4: Comparison test of the null model with the logistic model.
               Model       DF            b
                                    −2L (θ)    ∆DF            b
                                                       ∆(−2L (θ))         P-value
               Null       1014     1048.742
               Residual   1012     1031.465      2       17.2774          0.0002



From this Table, the log-likelihood for the constant only model is L (δb0 ) =
                                                         α) = 515.73225. The
524.37093 and the loglikelihood of the ﬁtted model is L (b
value of the statistic is

                                     α) − L (δb0 )] = 17.2774,
                                2[L (b

which yields a p-value of 0.0002. In conclusion, having a history of violence is a
signiﬁcant factor for being placed in some type of residential facility.


9. Conclusions
    Recent studies, such as Zacks (1971), Rao et al. (1973), Fahrmeir & Kaufmann
(1985), McCullagh & Neider (2018), and Agresti (2013), fail to provide a detailed
development of a general asymptotic theory for ML estimation for independent but
not identically distributed variables of generalized linear models. Furthermore,
other works, such as Wedderburn (1974), Wedderburn (1976), or McCullagh
(1983), only limit themselves to discussing the more general concept of quasi-
likelihood functions, which are important for logistic models with repeated
measurements. Based on these gaps identiﬁed in the literature, for independent
but not identically distributed variables, theoretical details must be generalized to
multinomial model applications when the response variable consider any R ≥ 2
levels. This is the primary contribution of this work. In fact, multinomial models
where the response variable may take one of three levels are addressed in LLinás
& Carreño (2012) and LLinás et al. (2016)). In this study, we extended these last
two works to cover the cases where R > 3.
   We assessed multinomial logistic and saturated models where the response
variable takes one of R ≥ 2 values, emphasizing the fact that we used independent
but not identically distributed variables, thus providing details that are not yet
found in the literature. For this purpose, we demonstrated the properties of
the score vector and the information matrices for these models. Furthermore,
based on an asymptotic theory, we proved the convergence theorems for the score

                 Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

228           Erick Orozco-Acosta, Humberto LLinás-Solano & Javier Fonseca-Rodríguez


vector and information matrices of both models and emphasized on the fact that
these vectors exhibit normal multivariate distributions. Moreover, we validated a
theorem about the existence and calculation of ML estimates for the parameters
of the multinomial saturated model. Similarly, we presented and demonstrated
a theorem for the existence of ML estimates for logistic parameters, thus brieﬂy
explaining the iteration method used for its calculation, i.e., the NewtonRaphson
method.
    Based on the results from this work, we will be able to compare the logistic
model against its corresponding saturated model, which will allow us to reduce
the number of observations and perform faster computer-based assessments. For
future studies, the results yielded by this study may be used to construct test
statistics, as well as their corresponding asymptotic distributions.


Appendix
Proof of theorem 1. We ﬁx r and j as constant values. For 0 < prj < 1, of (2) we
            ∂L
have that ∂p rj
                = 0 if and only if

                                      X
                                      R−2                            X
                                                                     R−2
                          zrj − zrj            prj − nj prj + prj          zrj = 0.
                                         r=0                         r=0
                      z
      That is, prj = nrjj . Now:
                                                                           !
                                                                X
                                                                R−2
                                                        nj −         zrj     
                             ∂ 2L    z                                       
                                      rj                       r=0           
                                  = − 2 +                                 !2  .
                                2
                             ∂prj     prj                     X
                                                               R−2            
                                                                             
                                                         1−          prj
                                                               r=0

                             X
                             R−2
      However, since 0 <            zrj < nj , we have:
                              r=0
                                                                                 
                                         2                                       
                     ∂ 2L                nj                      nj              
                                     = −
                                         zrj +                                  !
                                                                                   < 0.
                     ∂p2rj prj = zrj                             X
                                                                  R−2
                                                                                  
                                    nj
                                                           nj −            zrj
                                                                  r=0

                      z
      That is, p̃rj = nrjj . Now, the following extreme cases must be assessed:

           X
           R−2
                                         ∂ 2L                        nj
  • If           zrj = 0, then                           = −                       < 0. In this case, L
           r=0
                                         ∂p2rj prj =p̃rj             X
                                                                     R−2
                                                               1−          p̃rj
                                                                     r=0
        decreases in p̃rj . That is, L (p̃) assumes a maximum value when p̃rj = 0.

                   Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 211–231

Multinomial Saturated and Logistic Models                                                 229

         X
         R−2
                           ∂ 2L               nj
  • If         zrj = nj then  2            =       > 0. In this case, L increases in p̃rj .
       r=0
                           ∂p rj prj =p̃rj   p̃ rj
     That is, L (p̃) assumes a maximum when p̃rj = 1.



                                                                  
                 Recibido: octubre de 2019 — Aceptado: mayo de 2020


References
Agresti, A. (2013), Categorical Data Analysis, Wiley Series in Probability and Statistics, 3 edn, John Wiley and Sons.
Anderson, C., Verkuilen, J. & Peyton, B. (2010), ‘Modeling polytomous item responses using simultaneously estimated multinomial logistic regression models’, Journal of Educational and Behavioral Statistics 35(4), 422–452.
Begg, C. B. & Gray, R. (1984), ‘Calculation of polychotomous logistic regression parameters using individualized regressions’, Biometrika 71(1), 11–18.
Chan, Y. H. (2005), ‘Biostatistics 305. multinomial logistic regression’, Singapore medical journal 46(6), 259–269.
Chen, Y.-H. & Kao, J.-T. (2006), ‘Multinomial logistic regression approach to haplotype association analysis in population-based case-control studies’, BMC Genetics 7(1), 1–12.
Chuang, H.-L. (1997), ‘High school youths’ dropout and re-enrollment behavior’, Economics of Education Review 16(2), 171–186.
Cox, D. R. (1958), ‘The regression analysis of binary sequences’, Journal of the Royal Statistical Society: Series B (Methodological) 20(2), 215–232.
Darlington, R. B. (1990), Regression and linear models, McGraw-Hill College. Dessens, J., Jansen, W., G. B. Jansen, W., Ganzeboom, B. & Van der Heijden, P. (2003), ‘Patterns and trends in occupational attainment of ﬁrst jobs in the netherlands, 1930-1995: Ordinary least squares regression versus conditional multinomial logistic regression’, Journal of the Royal Statistical Society. Series A (Statistics in Society) 166(1), 63–84.
Díaz, L. & Morales, M. (2009), Análisis estadístico de datos categóricos, Editorial Universidad Nacional de Colombia.
Ekström, M., Esseen, P.-A., Westerlund, B., Grafström, A., Jonsson, B. & Ståhl, G. (2018), ‘Logistic regression for clustered data from environmental monitoring programs’, Ecological Informatics 43, 165 – 173.
Exavery, A., Mbaruku, G., Mbuyita, S., Makemba, A., Kinyonge, I. & Kweka, H. (2014), ‘Factors aﬀecting uptake of optimal doses of sulphadoxine-pyrimethamine for intermittent preventive treatment of malaria in pregnancy in six districts of tanzania’, Malaria Journal 13(1), 2–9.
Fahrmeir, L. & Kaufmann, H. (1985), ‘Consistency and asymptotic normality of the maximum likelihood estimator in generalized linear models’, Annals of Statistic 13(1), 342–368.
Fontanella, C. A., Early, T. J. & Phillips, G. (2008), ‘Need or availability? modeling aftercare decisions for psychiatrically hospitalized adolescents’, Children and Youth Services Review 30(7), 758 – 773.
Harrell, F. (2015), Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis, Springer Series in Statistics, Springer International Publishing.
Harville, D. (1997), Matrix algebra from a statistician’s perspective, Springer-Verlag.
Hosmer, D. & Lemeshow, S. (2000), Applied logistic regression, Wiley Series in Probability and Statistics, second edition edn, John Wiley & Sons Inc.
Kim, J. (2015), ‘School socioeconomic composition and adolescent sexual initiation in malawi’, Studies in Family Planning 46(3), 263–279.
Kleinbaum, D. & Klein, M. (2010), Logistic Regression. A Self-Learning Text, Statistics for Biology and Health, third edition edn, Springer-Verlag New York.
LLinás, H. (2006), ‘Precisiones en la teoría de los modelos logísticos’, Revista Colombiana de Estadística 29(2), 239–265.
LLinás, H., Arteta, M. & Tilano, J. (2016), ‘El modelo de regresión logística para el caso en que la variable de respuesta puede asumir uno de tres niveles: estimaciones, pruebas de hipótesis y selección de modelos’, Revista de Matemática: Teoría y Aplicaciones 23(1), 173–197.
LLinás, H. & Carreño, C. (2012), ‘The multinomial logistic model for the case inwhich the response variable can assume one ofthree levels and related models’, Revista Colombiana de Estadística 35(1), 131–138.
Lloyd, C. J. & Frommer, D. J. (2008), ‘An application of multinomial logistic regression to estimating performance of a multiple-screening test with incomplete veriﬁcation’, Journal of the Royal Statistical Society. Series C (Applied Statistics) 57(1), 89–102.
Long, J. S. (1987), ‘A graphical method for the interpretation of multinomial logit analysis’, Sociological Methods & Research 15(4), 420–446.
McCullagh, P. (1983), ‘Quasi-likelihood functions’, The Annals of Statistics 11(1), 59–67.
McCullagh, P. & Neider, J. (2018), Generalized Linear Models, CRC Press.
McFadden, D. (1973), Conditional Logit Analysis of Qualitative Choice Behavior, BART impact studies ﬁnal report series: Traveler behavior studies, Institute of Urban and Regional Development, University of California.
McNevin, D., Santos, C., Gamez-Tato, A., Álvarez-Dios, J., Casares, M., Daniel, R., Phillips, C. & Lareu, M. (2013), ‘An assessment of bayesian and multinomial logistic regression classiﬁcation systems to analyse admixed individuals’, Forensic Science International: Genetics Supplement Series 4(1), 63 – 64.
Monroy, L., Morales, M. & Dávila, L. (2018), Análisis estadístico de datos categóricos, Universidad Nacional de Colombia.
Monyai, S., Lesaoana, M., Darikwa, T. & Nyamugure, P. (2016), ‘Application of multinomial logistic regression to educational factors of the 2009 general household survey in south africa’, Journal of Applied Statistics 43(1), 128–139.
Peng, C.-Y. J., Lee, K. L. & Ingersoll, G. M. (2002), ‘An introduction to logistic regression analysis and reporting’, The journal of educational research 96(1), 3–14.
Pohlman, J. & Leitner, D. (2003), ‘A comparison of ordinary least squares and logistic regression’, The Ohio journal of science 103, 118–125.
Rao, C. R., Rao, C. R., Statistiker, M., Rao, C. R. & Rao, C. R. (1973), Linear statistical inference and its applications, Vol. 2, Wiley New York.
Schnor, C., Vanassche, S. & Bavel, J. (2017), ‘Stepfather or biological father? education-speciﬁc pathways of postdivorce fatherhood’, Demographic Research 37, 1659–1694.
Tan, Q., Christiansen, L., Christensen, K., Kruse, T. A. & Bathum, L. (2004), ‘Apolipoprotein e genotype frequency patterns in aged danes as revealed by logistic regression models’, European Journal of Epidemiology 19(7), 651–656. *http://www.jstor.org/stable/3582754
Tilano, J. & Arteta, M. (2012), Modelos logísticos multinomiales: estimaciones, pruebas y selección de modelos, Master’s thesis, Universidad del Norte.
Wedderburn, R. W. M. (1974), ‘Quasi-likelihood functions, generalized linear models, and the gauss-newton method’, Biometrika 61(3), 439–447.
Wedderburn, R. W. M. (1976), ‘On the existence and uniqueness of the maximum likelihood estimates for certain generalized linear models’, Biometrika 63(1), 27–32.
Zacks, S. (1971), The Theory of Statistical Inference, Wiley series in probability and mathematical statistics, John Wiley.
