On Predictive Distribution of K-Inﬂated Poisson Models with and Without Additional Information. Acerca de la distribución predicitiva de modelos Poisson K-inﬂados
Instituto Tecnológico Autónomo de México, Ciudad de México, México.  Brock University, Ontario, Canada
Abstract
This paper addresses diﬀerent approaches in ﬁnding the Bayesian predictive distribution of a random variable from a Poisson model that can handle count data with an inﬂated value of K ∈ N, known as the KIP model. We explore how we can use other source of additional information to ﬁnd such an estimator. More speciﬁcally, we ﬁnd a Bayesian estimator of future density of random variable Y1 , based on observable X1 from the K1 IP(p1 , λ1 ) model, with and without assuming that there exists another random variable X2 , from the K2 IP(p2 , λ2 ) model, independent of X1 , provided λ1 ≥ λ2 , and compare their performance using simulation method.
Key words: KIP model; Bayesian statistics; Bayesian  predictive distribution; Simulation.
Resumen
Este artículo presenta diferentes enfoques para buscar la distribución bayesiana predictiva de una variable aleatoria con un valor inﬂado k ∈ N conocido como el modelo KIP. Se explora como usar una fuente de información adicional para encontrar el estimador. Especíﬁcamente, se busca un estimador Bayesiano de la densidad futura de una variable aleatoria Y1 , basada en una variable observable X1 a partir del modelo K1 IP(p1 , λ1 ), con y sin el supuesto de que existe otra variable aleatoria X2 del modelo K2 IP(p2 , λ2 ), independiente de X1 , si λ1 ≥ λ2 , y se compara su desempeño usando un método de simulación.
Palabras clave: Modelo KIP; Estadísticas bayesianas; Distribución predictiva bayesiana; Simulación.

1. Introduction
   The probability mass function (pmf) of a count variable X that follows a
Poisson model, Po(λ), is given by
                                   e−λ λx
                  P (X = x | λ) =         , x = 0, 1, . . . , λ > 0.           (1)
                                     x!
Model (1) is used to represent count data, especially rare events such as highway
accidents, earthquakes, incidents of terrorism, or airplane crashes. Often this
type of dataset exhibits more zero or other observations than the Poisson model
would anticipate. In these situations, a k-inﬂated Poisson (KIP) model is better
applicable. Let us suppose that the probability of a random variable X is inﬂated
at the value k. This model is a two–component mixture model combining a point
mass at x = k with a Poisson model and it has the pmf
                                {               −λ k
                                  p + (1 − p) e k!λ     if x = k
             P (X = x | p, λ) =            −λ x                                (2)
                                  (1 − p) e x!λ         if x ∈ N \ {k}.
We denoted this model by KIP(p, λ). The KIP model reduces to the ZIP model,
the zero inﬂated Poisson, when k = 0, and to the Poisson model when p = 0
(see Mullahy, 1986 and Lambert, 1992). Unhapipat, Tiensuwan & Pal (1986)
studied the Bayesian properties of the ZIP model with application in the public
health. There has been some research which considers multiple inﬂations. Lin &
Tsai (2013) have discussed a model that can be applied to both excessive zeros and
ones known as the zero–and–one–inﬂated Poisson, or ZOIP model, and Melkersson
& Rooth (2000) proposed a zero–and–two–inﬂated Poisson model, which accounts
for a relative excess of both zero and two children in modeling complete female
fertility. Sadeghkhani & Ahmed (1986) studied the Bayesian properties of the
K1 -and-K2 –inﬂated Poisson, or K1 K2 P model, for any Ki ∈ N for i = 1, 2 in the
ice hockey. In this paper, we address the problem of estimating the future density
of random variable Y1 , based on observable X1 from the K1 IP(p1 , λ1 ) model, with
(and without) assuming that there exists another random variable X2 , from the
K2 IP(p2 , λ2 ) model, independent of X1 , provided λ1 ≥ λ2 . We also study diﬀerent
kinds of predictive distributions for Y1 .
    The main objective is how can we use another variable X2 independent of X1
(seemingly irrelevant at ﬁrst glance, but have some linkage through the unknown
parameters) to predict the future distribution of Y1 based on X1 . In here we have
assumed that λ1 ≥ λ2 . For instance, from previous information or historical data
we know, say, the mean number of the accidents in region A is greater than region
B. (Accidents in region A and B are assumed to be independent) How can we
predict the future number of accidents in region A by knowing that additional
information?
    The remainder of this paper is organized as follows: Section 2 presents a review
of the problem, the likelihood function, and Bayesian setups. In Section 3, we study
diﬀerent Bayesian predictive densities of random variables, while in Section 4, we
compare the results using simulation studies. Finally, we make some concluding
remarks in Section 5.

               Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 173–182

On Predictive Distribution of K-Inﬂated Poisson Models...                                                                    175

2. Problem Setup
   Let X1 = (X11 , . . . , X1n1 ) be a random sample of size n1 from K1 IP(p1 , λ1 ),
independent of X2 . Likewise, let X2 = (X21 , . . . , X2n2 ), a random sample of size
n2 from K2 IP(p2 , λ2 ). Let X = (X1 , X2 ) with parameters pi and λi , for i = 1, 2
are unknown. Based on previous study, we will assume λ1 ≥ λ2 .
   If we, suppose n1k1 and n2k2 , are the number of x1i = k1 and x2m = k2 for
i = 1, . . . , n1 and m = 1, . . . , n2 , are quite large, then likelihood function can be
given by

  L(p1 , λ1 , p2 , λ2 | x)
   (                         k1
                                    )n         (                           k2
                                                                                 )n
                                         1k1                                          2k2
                        −λ1 λ1                                        −λ2 λ2                        n −n1k              n2 −n2k
 ∝   p1 + (1 − p1 ) e                              p2 + (1 − p2 ) e                         (1 − p1 ) 1    1 (1 − p   2)        2
                             k1 !                                         k2 !
                                         ∑                                  ∑
                      −(n1 −n1k )λ1        {i:x1i ̸=k1 } x1i −(n2 −n2k )λ2    {m:x2m ̸=k2 } x2m
                  ×e             1    λ1                    e         2    λ2
                                                                        ∑
                     n1k                                                       x1i +k1 (n1k −j)
                      ∑1 (n1k ) j                n −j −λ (n −j) {i:x1i ̸=k1 }
                                                                                           1
                   ∝            1
                                   p1 (1 − p1 ) 1 e 1 1            λ1
                     j=0
                              j
                                                                               ∑
                            n2k                                                         x2m +k2 (n2k −l)
                             ∑2 (n2k ) l                  n −l −λ(n2 −l) {m:x2m ̸=k2 }
                                                                                                    2
                         ×
                                       l
                                         2
                                             p2 (1 − p2 ) 2 e           λ2                               .                   (3)
                             l=0


We also consider the following prior densities:

           p1 , p2 ∼ independent U (0, 1) ,                                                                                   (4)
                      (       )α−β+1
                         1
         (λ1 , λ2 ) ∼               , β > 0, 0 < α < β,                                       and     λ1 ≥ λ2 > 0.            (5)
                        λ1 λ2

    The prior in (5) was introduced by Komaki (2004) as a class of improper
shrinkage prior for the mean of Poisson distribution that the Jeﬀreys prior
corresponds to α = 0 and β = 21 . Lemma 1 gives the joint posterior distribution
under prior densities in (4) and (5).
Lemma 1. Suppose that X1 = (X11 , . . . , X1n1 ) is a random sample of size n1
from K1 IP(p1 , λ1 ) and that X2 = (X21 , . . . , X2n2 ) is a random sample of size n2
from K2 IP(p2 , λ2 ) with prior density in (4) and (5). Then by assuming
                               ∑
                 wj (x1 ) =        xi1 + k1 (n1k1 − j) + β − α , and               (6)
                                     {i:x1i ̸=k1 }
                                               ∑
                       wl (x2 ) =                         xm2 + k2 (n2k2 ) + β − α ,                                          (7)
                                     {m:x2m ̸=k2 }


  1. the joint posterior π(p1 , p2 , λ1 , λ2 | x) density is given by
             ∑n1k1 ∑n2k2 (n1k )(n2k ) j
                                1        2 p (1 − p )n1 −j pl (1 − p )n2 −l
                j=0    l=0    j        l     1       1       2       2

                                 e−λ1 (n1 −j) λ1 j 1 e−λ2 (n2 −l) λ2 l
                                                                        w (x )−1                          w (x2 )−1
             ∑n1k1 ∑n2k2 (n1k )(n2k )                                                                                   ,     (8)
                             1      2 Bet(j + 1, n − j + 1)
              j=0   l=0    j      l                 1
                  Bet(l + 1, n2 − l + 1) Q(n1 − j, wj (x1 ), wl (x2 ))(n2 − l)−wl (x2 )

                      Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 173–182

176                                                          Abdolnasser Sadeghkhani & S. Ejaz Ahmed


          where                                          ∫ ∞
                                          Q(a, b, c) =         e−at tb−1 γ(c; t) dt ,              (9)
                                                         0

          and γ(c; d) is the upper incomplete gamma function and is given as
          ∫ d b−1 −t
           0
             t e dt.

  2. The joint posterior π(p1 , λ1 | x) density is given by
                ∑n1k1 ∑n2k2 (n1k )(n2k ) j
                                       2 p (1 − p )n1 −j e−λ1 (n1 −j) λ j
                                1
                                                                       w (x1 )−1
                 j=0   l=0    j      l    1      1                     1
                            Bet(l + 1, n2 − l + 1)γ(wl (x2 ); λ1 )(n2 − l)−wl (x2 )
               ∑n1k1 ∑n2k2 (n1k )(n2k )                                             .             (10)
                               1      2 Bet(j + 1, n − j + 1)
                j=0   l=0    j      l               1
                  Bet(l + 1, n2 − l + 1) Q(n1 − j, wj (x1 ), wl (x2 ))(n2 − l)−wl (x2 )

          This implies
          a) the marginal posterior π(λ1 | x), given by
              ∑n1k1 ∑n2k2 (n1k )(n2k ) −λ1 (n1 −j) wj (x1 )−1
               j=0   l=0     j
                               1
                                   l
                                     2 e          λ1          Bet(j + 1, n1 − j + 1)
                                   Bet(l + 1, n2 − l + 1)γ(wl (x2 ); λ1 )(n2 − l)−wl (x2 )
                 ∑n1k1 ∑n2k2 (n1k )(n2k )
                                         2 Bet(j + 1, n − j + 1)
                                                                                           .     (11)
                                  1
                  j=0   l=0     j      l                1

                      Bet(l + 1, n2 − l + 1) Q(n1 − j, wj (x1 ), wl (x2 ))(n2 − l)−wl (x2 )

          b) the marginal posterior π(p1 | x), given by


               ∑n1k1 ∑n2k2 (n1k )(n2k ) j
                               1      2 p (1 − p )n1 −j
                j=0   l=0    j      l    1      1
                Bet(l + 1, n2 − l + 1) Q(n1 − j, wj (x1 ), wl (x2 ))(n2 − l)−wl (x2 )
               ∑n1k1 ∑n2k2 (n1k )(n2k )                                               .           (12)
                                1     2 Bet(j + 1, n − j + 1)
                j=0   l=0     j     l                1
                  Bet(l + 1, n2 − l + 1) Q(n1 − j, wj (x1 ), wl (x2 ))(n2 − l)−wl (x2 )

Proof . The numerator of (8) in (1) is the product of likelihood function (3) and
prior densities (4) and (5) and denominator is obtained as follows:

          1k1 (
  ∫ 1∫ ∞ n∑                )
                       n1k1 j
                             p1 (1 − p1 )n1 −j e−λ1 (n1 −j) λ1 j 1
                                                             w (x )−1
                                                                      dλ1 dp1
      0   0     j=0
                        j
                               2k2 (
                      ∫ 1∫ λ1 n∑               )
                                           n2k2 l
                                                p2 (1 − p2 )n2 −l e−λ2 (n2 −l) λ2 l 2
                                                                                w (x )−1
                  ×                                                                      dλ2 dp2 . (13)
                       0   0                l
                                    l=0

The second line in (13) is
                n2k2 (          )
                ∑        n2k2
                                    Bet(l + 1, n2 − l + 1)γ(wl (x2 ); λ1 )(n2 − l)−wl (x2 ) .     (14)
                          l
                l=0



                       Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 173–182

On Predictive Distribution of K-Inﬂated Poisson Models...                                            177

By using (9), (13) can be written as

  n1k1 n2k2 (          )(          )
   ∑∑           n1k1        n2k2
                                       Bet(j + 1, n1 − j + 1)
   j=0 l=0
                 j           l

                       Bet(l + 1, n2 − l + 1) Q(n1 − j, wj (x1 ), wl (x2 ))(n2 − l)−wl (x2 ) ,

which completes the proof. The proof of (2) can be done similarly.

   Note that by using (11) and (10), one can obtain the Bayes parameters of p1
and λ1 , given observed x = (x1 , x2 ), restricted to λ1 ≥ λ2 , as is demonstrated in
Lemma 2.

Lemma 2. The Bayes estimator of parameters p1 and λ1 , under the squared error
loss function, are given by
           ∑n1k1 ∑n2k2 (n1k )(n2k )
                              1   2 Bet(j + 2, n − j + 1)
             j=0     l=0    j   l               1
            Bet(l + 1, n2 − l + 1) Q(n1 − j, wj (x1 ), wl (x2 ))(n2 − l)−wl (x2 )
   p̂1,π = ∑n1k1 ∑n2k2 (n1k )(n2k )                                                                 (15)
                            1     2 Bet(j + 1, n − j + 1)
            j=0   l=0     j     l                1
                Bet(l + 1, n2 − l + 1) Q(n1 − j, wj (x1 ), wl (x2 ))(n2 − l)−wl (x2 )

             ∑n1k1 ∑n2k2 (n1k )(n2k )
                j=0      l=0       j
                                       1
                                                l
                                                    2   Bet(j + 1, n1 − j + 1)
               Bet(l + 1, n2 − l + 1) Q(n1 − j, wj (x1 ) + 1, wl (x2 ))(n2 − l)−wl (x2 )
  λ̂1,π =      ∑n1k1 ∑n2k2 (n1k )(n2k )
                                        2 Bet(j + 1, n − j + 1)
                                                                                         .          (16)
                                 1
                j=0    l=0     j      l                 1

                  Bet(l + 1, n2 − l + 1) Q(n1 − j, wj (x1 ), wl (x2 ))(n2 − l)−wl (x2 )

Moreover, the marginal posterior of parameter of λ1 , given x1 , can be obtained as
follows:
                   ∑n1k1 (n1k ) j
                                1 p (1 − p )n1 −j Γ(w (x ))(n − j)−wj (x1 )
                      j=0     j     1      1           j 1      1
   π(p1 | x1 ) = ∑n1k1 (n1k )                                                      , (17)
                  j=0     j
                            1   Bet(j + 1, n − j + 2) Γ(wj (x1 ))(n1 − j)−wj (x1 )

                       ∑n1k1 (n1k )
                                               Bet(j + 1, n − j + 2) e−λ1 (n1 −j) λ1 j
                                           1
                                                                                   w (x1 )−1
                         j=0           j
  π(λ1 | x1 ) = ∑n1k1 (n1k )                                                                    .   (18)
                       j=0     j
                                   1       Bet(j + 1, n − j + 2) Γ(wj (x1 ))(n1 − j)−wj (x1 )

therefore, one can use (17) to obtain (unrestricted) Bayes estimator of parameter
λ1 ,
           ∑n1k1 (n1k )
                      1 Bet(j + 2, n − j + 2) Γ(w (x ))(n − j)−wj (x1 )
            j=0     j                            j   1     1
     p̂1 = ∑n1k1 (n1k )                                                    , (19)
            j=0    j
                      1 Bet(j + 1, n − j + 2) Γ(wj (x1 ))(n1 − j)−wj (x1 )

           ∑n1k1 (n1k )
                      1 Bet(j + 1, n − j + 2) Γ(w (x ) + 1)(n − j)−wj (x1 )−1
             j=0    j                             j  1            1
   λ̂1 =      ∑n1k1 (n1k )                                             −wj (x1 )
                                                                                 .                  (20)
                j=0     j
                          1 Bet(j + 1, n − j + 2) Γ(wj (x 1 ))(n 1 − j)



                  Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 173–182

178                                                 Abdolnasser Sadeghkhani & S. Ejaz Ahmed


3. Bayesian Predictive Densities

   We consider the problem of constructing Bayes predictive density for the future
observation y, based on observable x. We use the Kullback Leibler (KL) loss
function (divergence)
                                             ∑              q(y | θ)
                 KL(q(y | θ)), q̂π (y; x)) =   q(y | θ) log            ,      (21)
                                             y
                                                            q̂π (y; x)

where q̂π (y; x) is the Bayes predictive distribution in estimating the density
Y ∼ q(· | θ), and θ is a(n) (vector of) unknown parameter(s). The corresponding
risk function given as follows:
                      rq̂ (θ) = EX|θ KL(q, q̂)
                                ∑           ∑               q(y | θ)
                              =    q(x | θ)    q(y | θ) log            .                           (22)
                                 x           y
                                                            q̂π (y; x)

Previous studies (see, Corcuera & Giummolé, 1999), indicates that under the KL,
the Bayes predictive distribution for Y based on observed value x, and posterior
density π(· | x), is given as
                                        ∫
                           q̂π (y; x) =   q(y | θ) π(θ | x) dθ.             (23)
                                              Θ

The following theorem provides the Bayes predictive distribution under the KL
loss function.
Theorem 1. Let X1 = (X11 , . . . , X1n1 ) be a random sample from from
K1 IP(p1 , λ1 ), independent of X2 . Let X2 = (X21 , . . . , X2n2 ) be random sample
from K2 IP(p2 , λ2 ). Then the Bayes predictive distribution of future random
variable Y1 ∼ K1 IP(p1 , λ1 ), by considering the additional information λ1 ≥ λ2 ,
q̂(Y1 = y1 ; x) for y1 ∈ N \ {k1 } is given by
      ∑n1k1∑n2k2 (n1k )(n2k )
        j=0   l=0     j
                          1
                              l
                                  2   Bet(j + 1, n1 − j + 2) Bet(l + 1, n2 − l + 1)
                          Q(n1 − j + 1, wj (x1 ) + y1 , wl (x2 ))(n2 − l)−wl (x2 )
       ∑n1k1 ∑n2k2 (n1k )(n2k )
                              2 Bet(j + 1, n − j + 1) Bet(l + 1, n − l + 1)
                                                                                               ,   (24)
   y1 ! j=0   l=0     j
                        1
                            l               1                         2

                                              Q(n1 − j, wj (x1 ), wl (x2 ))(n2 − l)−wl (x2 )
and for y1 = k1 , q̂(Y1 = k1 ; x) is given by
      ∑n1k1∑n2k2 (n1k )(n2k )
        j=0   l=0     j
                          1
                              l
                                  2   Bet(j + 1, n1 − j + 2) Bet(l + 1, n2 − l + 1)
                          Q(n1 − j + 1, wj (x1 ) + k1 , wl (x2 ))(n2 − l)−wl (x2 )
       ∑n1k1 ∑n2k2 (n1k )(n2k )
                              2 Bet(j + 1, n − j + 1) Bet(l + 1, n − l + 1)
                                                                                               ,   (25)
   k1 ! j=0   l=0     j
                        1
                            l               1                         2

                                              Q(n1 − j, wj (x1 ), wl (x2 ))(n2 − l)−wl (x2 )

      ∑n1k1∑n2k2 (n1k )(n2k )
        j=0   l=0     j
                          1
                              l
                                  2   Bet(j + 2, n1 − j + 1) Bet(l + 1, n2 − l + 1)
                                       Q(n1 − j + 1, wj (x1 ), wl (x2 ))(n2 − l)−wl (x2 )
      ∑n1k1 ∑n2k2 (n1k )(n2k )                                                            .        (26)
        j=0    l=0    j
                          1
                              l
                                  2   Bet(j + 1, n1 − j + 1) Bet(l + 1, n2 − l + 1)
                                            Q(n1 − j, wj (x1 ), wl (x2 ))(n2 − l)−wl (x2 )


                 Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 173–182

On Predictive Distribution of K-Inﬂated Poisson Models...                                                    179

Proof . According to (23), the proof of (24) is given by
             ∫ 1∫ ∞∫ 1∫ λ1
                                       1 − p1 −λ1 y1
                                              e  λ1 π(p1 , p2 , λ1 , λ2 | x) dλ2 dλ1 dp2 dp1 ,              (27)
                  0   0       0   0      y1 !
replacing joint posterior density function (8) in (27). Doing calculations similar
to those in the proof of Lemma (1), gives the proof. Equation (25) can be proven
likewise using the following formula:
     ∫ 1∫ ∞∫ 1∫ λ1 (                     )
                          1 − p1 −λ1 y1
                     p1 +        e    λ1   π(p1 , p2 , λ1 , λ2 | x)dλ2 dλ1 dp2 dp1 .
       0 0   0 0            y1 !



    If we ignore the additional information λ1 ≥ λ2 , we can ﬁnd the q̂mre , the
minimum risk equivariant (mre) distribution, which is the Bayes estimator under
KL loss function and non–informative prior without contemplating the restriction
of the parameters. The following lemma, gives the mre distribution.
Lemma 3. The minimum risk equivariant (mre) distribution for Y1 ∼
K1 IP(p1 , λ1 ), under KL loss function and given X1 = (X11 , . . . , X1n1 ), and prior
densities (1/λ1 )α−β+1 , for β > 0, 0 < α < β, independent of p1 ∼ U (0, 1) is given
by

  q̂(Y1 = k1 ; x1 ) =
    ∑n1k1 +1 (n1k +1)
      j=0
                   1
                   j
                        Bet(n1 − j + 1, j + 1) Γ(wj (x1 ) + k1 + 1)(n − j + 1)−wj (x1)−k1
                 ∑n1k1 (nk )                                                              ,                 (28)
             k1 ! j=0      1 Bet(n1 − j + 2, j + 1) Γ(w (x1 ) + 1)(n1 − j)−wj (x1 )
                         j                              j




  q̂(Y1 = y1 ; x1 ) =
 ∑n1k1 (n1k )
   j=0
                  1   Bet(j + 1, n1 − j + 2) Γ(wj (x1 ) + y)(n1 − j + 1)−wj (x1 )−y1
              j
            ∑n1k1 (n1k )                                                                , y1 ∈ N \ {k1 }.   (29)
     y1 !     j=0         j
                              1   Bet(j + 1, n1 − j + 1) Γ(wj (x1 ))(n1 − j)−wj (x1 )



4. Comparison of Bayes and Plug-In Distributions
   In this section, we simulated a random random variable of 1000 from two
independent variables X1 ∼ K1 IP(0.2, 8) with k1 = 2, and X2 ∼ K2 IP(0.3, 5) with
k2 = 0, respectively. Table 1 and Figure 1 are based on these two random samples.

Table 1:       Simulation of 1000 observations form two independent variables X1 ∼
              K1 IP(0.2, 8) with k1 = 2, and X2 ∼ K2 IP(0.3, 5) with k2 = 0.
                                      Simulation from X1      Simulation from X2
                                             k1 = 2                  k2 = 0
                                          n1k1 = 219              n2k2 = 301
                                           x̄1 = 6.66              x̄2 = 3.50




                          Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 173–182

180                                                                 Abdolnasser Sadeghkhani & S. Ejaz Ahmed


 0.30                                                                  0.35

 0.25                                                                  0.30

                                                                       0.25
 0.20
                                                                       0.20
 0.15
                                                                       0.15
 0.10
                                                                       0.10

 0.05                                                                  0.05

 0.00                                                                  0.00
        0    2      4     6      8         10       12       14               0   2     4      6      8   10       12


                 Figure 1: Pmf’s of the K1 IP and K2 IP corresponding to Table 1.



   The goal of this simulation is to ﬁnd the density of future random variable
X1 ∼ KIP(0.2, 8), by considering λ1 = 8 ≥ λ2 = 5. Table 2 gives the Bayes
predictive distribution (Theorem 1), the mre distribution (Lemma 3), the plug-in
predictive distribution based on mle and the plug-in predictive distribution based
on posterior expectations of the unknown parameters (Lemma 2).

Table 2: The Bayes, mre and plug-in predictive distributions (based on mle (equation
         3) and posterior expectation) for future observation y1 ∼ K1 IP(0.2, 8), with
         k1 = 2.
                        y1 ∈ N \ {2}                                                                      y1 = 2

                        ∑219 (219)
                           j=0       j
                                           Bet(j + 1, 1002 − j)
                           Γ(y − 2(j − 3330.25))(1001 − j)2(j−3330.25)−y1
   q̂(·)                        ∑    (219)                                                                 0.21
                            y1 ! 219
                                  j=0 j    Bet(j + 1, 1001 − j)
                                 Γ(2(3330.25 − j))(1000 − j)2(j−3330.25)

                        ∑219 ∑301 (2019)(301)
                           j=0       l=0        j        l
                                                              Bet(j + 1, 1002 − j) Bet(l + 1, 1001 − l)
                                 Q(1001 − j, y1 − 2(−3330.25 + j), 3500.5)(1000 − l)−3500.5
   q̂π (·)                                                                                                 0.23
                                               1.12635850689 × 1016647

                        1915.07
   q̂plug,mle (·)         y1 !
                                7.9y1                                                                      0.21


                        0.0003
   q̂plug,b (·)           y1 !
                               7.9y1                                                                       0.22


   The p-values for Pearson’s chi-squared tests for the Bayes, the mre, and
two plug-in distributions in Table 2, are 0.86, 0.98 and 0.99 respectively, and
therefore one can conclude that all the proposed distributions, estimate the future
density well.
   Moreover, We can compare the performance of the predictive distributions in
terms of the risk function based on the KL loss function given in equation (22).
Figure 2 illustrates the risk functions of the Bayes predictive distribution (using
the additional information) and the mre predictive distribution.

                        Revista Colombiana de Estadística - Theoretical Statistics 43 (2020) 173–182

On Predictive Distribution of K-Inﬂated Poisson Models...                                    181

                    Risk


                   1.5




                   1.0                                                           mre
        Out[ ]=
                                                                                 Bayes

                   0.5



                                                                          λ1
                           5       10       15      20       25      30   λ2

Figure 2: The risk functions of the Bayes and the mre predictive distributions
          (the predictive distributions with and without considering the additional
          information respectively ) under the KL loss function.


5. Conclusions
    In this paper, we introduced diﬀerent predictive distributions for a future
random variable from the KIP model, with and without considering additional
information, and compared their performance using a simulation.


Acknowledgements
    Abdolnasser Sadeghkhani acknowledges the ITAM and Asociación Mexicana de
Cultura, A.C. for supporting this paper. S. Ejaz Ahmed acknowledges the Natural
Sciences and the Engineering Research Council of Canada, and the Ontario Centre
of Excellence for supporting this research.
   The authors are grateful to the editor and the anonymous reviewers for their
valuable comments and helpful suggestions.
            [                                                      ]
             Recibido: septiembre de 2019 — Aceptado: abril de 2020


References
Corcuera J M, Giummolé F. A generalized bayes rule for prediction.(1999). Scandinavian Journal of Statistics.
Komaki F. Simultaneous prediction of independent poisson observables.(2004). The Annals of Statistics.
Lambert D. Zero-inﬂated Poisson regression with an application to defects in manufacturing.(1992). Technometrics.
Lin T H, Tsai M H. Modeling health survey data with excessive zero and K responses.(2013). Statistics in Medicine.
Melkersson M, Rooth D O. Modeling female fertility using inﬂated count data models.(2000). Journal of Population Economics.
Mullahy J. Speciﬁcation and testing of some modiﬁed count data models.(1986). Journal of Econometrics.
Sadeghkhani A, Ahmed S E. The application of predictive distribution estimation in multiple–inﬂated poisson models to ice hockey data.(1986).Journal of Model assisted statistics and application.
Unhapipat S, Tiensuwan M, Pal, N. Bayesian predictive inference for zero–inﬂated poisson (zip) distribution with applications.(1986). American Journal of Mathematical and Management Sciences.