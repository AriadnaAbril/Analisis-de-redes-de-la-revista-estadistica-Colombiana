On a New Procedure for Identifying a Dynamic Common Factor Model. Sobre un nuevo procedimiento para identiﬁcar un modelo de factores comunes dinámicos
Pontificia Universidad Javeriana, Bogotá, Colombia.  Universidad Nacional de Colombia, Bogotá, Colombia. Universidad Carlos III de Madrid, Madrid, España
Abstract
In the context of the exact dynamic common factor model, canonical correlations in a multivariate time series are used to identify the number of latent common factors. In this paper, we establish a relationship between canonical correlations and the autocovariance function of the factor process, in order to modify a pre-established statistical test to detect the number of common factors. In particular, the test power is increased. Additionally, we propose a procedure to identify a vector ARMA model for the factor process, which is based on the so-called simple and partial canonical autocorrelation functions. We illustrate the proposed methodology by means of some simulated examples and a real data application.
Key words: Canonical correlations; Dynamic common factors; Multivariate time series.
Resumen
En el contexto del modelo exacto de factores comunes dinámicos, las correlaciones canónicas en series de tiempo multivariadas son usadas para identiﬁcar el número de factores latentes. En este artículo, establecemos la relación entre correlación canónica y la función de autocovarianza del proceso de los factores, con el ﬁn de modiﬁcar una prueba estadística diseñada para identiﬁcar el número de factores comunes. En particular, se incrementa la potencia de la prueba. Adicionalmente, proponemos un procedimiento para identiﬁcar el modelo VARMA para el proceso de los factores, el cual está basado en lo que denominamos las funciones de autocorrelación simple y parcial. Ilustramos la metodología propuesta por medio de ejemplos simulados y una aplicación con datos reales.
Palabras clave: Correlación canónica; Factores comunes dinámicos; Series de tiempo multivariadas.


1. Introduction
   The dynamic common factor model is of interest when an observable
multivariate stochastic process {yt }, of dimension m, is generated by an
unobservable stochastic process {ft }, of dimension r, with r < m, via the equation

                                     yt = Pft + εt ,                                    (1)

where {εt } is a multivariate stationary noise process of dimension m, with mean
0 and variance Σε , and P is an m × r matrix, known as the loading matrix or the
weight matrix. The components of the ft vector are the common factors of the yt
data vector, while εt contains their speciﬁc or idiosyncratic components. A crucial
aspect in the analysis of this type of models is the identiﬁcation of r, the number
of common factors. Several studies have been carried out in this sense, mainly in
the high-dimension scenario, in which it is assumed that m goes to inﬁnity. For
complete details, see the papers, among others, of Stock & Watson (2011), Lam
& Yao (2012), Ahn & Horenstein (2013), and their associated references.
    This paper is based on the low-dimension approach (ﬁxed m) proposed by Peña
& Box (1987) (from this point forward Peña-Box model), when the {ft } process is
stationary and {εt } is white noise, which is known as the Exact Dynamic Factor
Model (EDFM) because its speciﬁc components are orthogonal processes. Peña
& Poncela (2006) extend this paper to the case where {ft } is non-stationary and
design a statistical test to specify r. This statistical test is a function of the
canonical correlations between yt and yt−k , for some lag k, k = 1, 2, . . . Under
the null hypothesis of r factors, the authors show that this statistic is distributed
asymptotically as a χ2(m−r)2 , for each lag k. Nevertheless, as is shown in Section
2, at each lag k the test detect the rank of the covariance matrix of ft and ft−k ,
but this matrix may not have full rank. Then, this alternative may detects less
than r common factors, which imply that the power of the test is reduced. Indeed,
let us consider this example: let ft = (f1t , f2t )⊤ , where f1t = a1t + θ1 a1,t−1 and
                                                                          (1)

              (2)
f2t = a2t + θ3 a2,t−3 , with {ait }, i = 1, 2, white noise processes. The number of
factors is 2 but the rank of matrix Cov(ft , ft−k ) is 1 if k = 1 or k = 3 and zero
otherwise. This fact makes that the Peña & Poncela’s (2006) test does not detect
the correct number of factors at any lag k = 1, 2, . . .
    In this paper, we develop a modiﬁcation of the Peña & Poncela’s (2006)
statistical test, which avoids the low-power problem quoted above. Essentially,
the idea is to use in the calculation of the test statistic, the canonical correlations
between yt and a linear combination of some lagged vectors yt−i , for i ∈ {1, 2, . . .}.

                   Revista Colombiana de Estadística - Theoretical Statistics 44 (2021) 1–21

On a New Procedure for Identifying a Dynamic Common Factor Model                                            3

Also, we propose a methodology to identify ARMA models for the factor processes,
using the canonical and partial canonical correlation to identify the dependence
orders of the factor models. We only consider the case in which {ft } is stationary.
   It is important to highlight that in the Peña-Box model the observed data
vector yt depends on ft contemporaneously. Then, the model is known as the
DFM in static form. In the DFM in dynamic form, initially proposed by Geweke
(1977) (cited by Stock & Watson (2016); Doz & Fuleky (2020)), the vector of
observed time series can also depend on diﬀerent lags of the common factors.
Therefore, our contributions are based on the DFM in the static form.
    The paper is organized as follows. Section 2 presents the basic results that
relate canonical correlations to the marginal autocovariance functions of the factors
processes and their use in the modiﬁcation of Peña & Poncela’s (2006) statistical
test. In section 3, we present a procedure to identify the ARMA factor models.
Section 4 includes some simulated examples and an empirical application to
precipitation data in Colombia. Section 5 concludes.


2. An Extension of the Statistical Test
     For each t, let ft = (f1t , . . . , frt )⊤ , where ⊤ denotes the matrix transpose
operation, and let γi (k) be the i-th component on the diagonal of the
autocovariance matrix of {ft } at lag k. Following Nieto et al.’s (2016) assumptions,
in particular that the marginal processes {fit } and {fjt } are orthogonal for all
i, j = 1, 2, . . . , r, with i ̸= j, the components outside of the diagonal are zero.
                                    ⊤
In addition, the restriction P Σ−1     ε P = Ir is imposed to solve the identiﬁcation
problem (Peña & Poncela 2006), where In is the n × n identity matrix. The
remaining notation will be deﬁned as new deﬁnitions are introduced.
   From Peña & Poncela’s (2006) paper, we consider the random matrix
                             −1                                           −1
               X
               T                     X
                                     T                     X
                                                           T                        X
                                                                                    T
 c(k) = 
 M                    (yt yt⊤ )                 ⊤
                                            (yt yt−k )                  ⊤
                                                                  (yt−k yt−k )            (yt−k yt⊤ ),   (2)
              t=k+1                 t=k+1                 t=k+1                    t=k+1

where T denotes the sample size of an observed multivariate time series of the
process {yt }, and the statistic

                                                      X
                                                      m−r
                           Sm−r (k) = −(T − k)                          bj (k)),
                                                                log(1 − λ                                 (3)
                                                          j=1


where λ b1 (k) ≤ λ                 bm (k) are the ordered eigenvalues of the matrix
                  b2 (k) ≤ · · · ≤ λ
c
M (k), for a given lag k = 1, 2, . . . The limit distributions of these statistics are
obtained by Peña & Poncela (2006). In particular, they ﬁnd that, when the process
{ft } is stationary, the limit matrix of the sequence Mc(k) is

                         −1                                   −1
                                   ⊤
M (k) = P E[ft ft⊤ ]P ⊤+Σε P E[ft ft−k                ⊤
                                       ]P ⊤ P E[ft−k ft−k ]P ⊤+Σε P E[ft−k ft⊤ ]P ⊤ .


                        Revista Colombiana de Estadística - Theoretical Statistics 44 (2021) 1–21

4                                      Stevenson Bolívar, Fabio H. Nieto & Daniel Peña


    Before establishing our main results, it is worth noticing the following fact: in
Peña & Poncela’s (2006) Theorem 3, it is claimed that the limit matrix of the
sequence {M c(k)}, indexed by T , has rank r for all lags k. However, we have found
that for some models and some lags k the rank is less than r. As mentioned before,
let us consider this example: let ft = (f1t , f2t )⊤ , where f1t = a1t + θ1 a1,t−1 and
                                                                          (1)

              (2)
f2t = a2t + θ3 a2,t−3 , with {ait }, i = 1, 2, white noise processes. The number of
                                             ⊤
factors is 2, but the rank of matrix E[ft ft−k   ] is 1 when k = 1 or k = 3 and zero
otherwise. This fact makes that the rank of the limit matrix in Peña & Poncela’s
(2006) Theorem 3 depends on k. From now on, we denote the rank of that matrix
as r(k), k = 1, 2, . . .
    To avoid this drawback of the methodology, we propose the following results.

Proposition 1. Let {ft } be a stationary stochastic process, then, given the lag k,
the r(k) ≤ r non-zero eigenvalues of M (k) belong to the set
                                                                         
                                           |γj (k)|
            Λ(k) := µj (k) ∈ R : µj (k) =
                       2
                                                     , j = 1, 2, . . . , r ,    (4)
                                          γj (0) + 1

where µj (k) is a canonical correlation between yt and yt−k , with respective
canonical variables a⊤         ⊤
                     j yt and bj yt−k where

                    −1
                    Σ ε Pj
        bj =                   ∈ Rm           and          aj = sign{γj (k)}bj .        (5)
               (γj (0) + 1)1/2

Here R and Rm denote, respectively, the set of real numbers and the m-
dimensional real Euclidean space and Pj is the j-th column of the load
matrix P .

Proof . See Appendix A.1.

     Note that the largest value of the set Λ(k) is the ﬁrst canonical correlation, the
second largest value of Λ(k) is the second canonical correlation and so on up to the
r(k)-th canonical correlation (Anderson 1984). Additionally, note that r(k) = r if
|γj (k)| > 0 for all j = 1, . . . , r and the m − r remaining canonical correlations are
equal to zero.
    An important implication of Proposition 1 is the following: if for a given k ̸= 0,
γi (k) = 0 for some i = 1, . . . , r, then, at this lag, there is a maximum of r − 1 non-
zero canonical correlations. This fact is in line with the above comment about
the loss of power of the test. In order to improve the performance of Peña &
Poncela’s
       X (2006) test, we propose to use the canonical correlations between yt and
yt† =     yt−k , where K is a set of lags (not necessarily consecutive). To obtain K,
     k∈K
we propose the following two-step procedure: ﬁrst, we run Peña and Poncela’s test
for lags k = 1, 2, . . . , k0 , for some k0 . Then, a lag k ∈ K if the Peña & Poncela’s
(2006) test detects at least one common factor at it. In the examples below, we
will give more suggestions to obtain K in practice.

                   Revista Colombiana de Estadística - Theoretical Statistics 44 (2021) 1–21

On a New Procedure for Identifying a Dynamic Common Factor Model                                                        5

                                                                    ⊤       −1
Proposition 2. Given K and the restriction P Σε P = Ir , the r non-zero square
canonical correlations between yt and yt† belong to the set
                                                                                                             
                                                             |γj (K)|                                        
    †                             †
  Λ (K) :=           j (K) ∈ R : µj (K) =
                    µ2†                                                               , j = 1, 2, . . . , r       ,   (6)
                                               (γj (0) + 1)1/2 (γ† + k† )1/2                                 
                                                                        j


                                                               †⊤ †
and its respective canonical variables are given by b⊤
                                                     j yt and aj yt , with

                        −1                                                                      −1
                     Σ ε Pj                                                                  Σ ε Pj
         bj =                                 and             a†j =sign{γj (K)}                           ,           (7)
                (γj (0) + 1)1/2                                                           (γj† + k † )1/2
                                                              X                              X X
where k † is the cardinality of K, γj (K) =                         γj (k) and γ† =                           γj (|k2 −
                                                                                      j
                                                              k∈K                           k1 ∈K k2 ∈K
k1 |), j = 1, 2, . . . , r.

Proof . See Appendix A.2.

    Now, we deﬁne
                                     −1                                            −1
                      X
                      T                      X
                                             T
                                                          ⊤         X
                                                                    T
                                                                                  ⊤          X
                                                                                             T
   M † (K) =                 (yt yt⊤ )             (yt yt† )             (yt† yt† )              (yt† yt⊤ ),      (8)
                     t=k̄+1                 t=k̄+1                 t=k̄+1                   t=k̄+1


where k̄ = max K, and obtain the following result.
                                            X
                                  ⊤
Proposition 3. Let Γf (k) = E[ft ft−k ]. If   Γf (k) has rank r, then, as T goes
                                                              k∈K
to inﬁnity, the matrix sequence {M † (K)} converges in distribution to a constant
matrix that has rank r.

Proof . Using Q = Σ−1                                    ⊤               ⊤
                       ε [P P⊥ ], where P⊥ is such that P P⊥ = 0 and P⊥ P⊥ =
Im−r , the proof follows the basic ideas in Peña & Poncela’s (2006) paper (pages
1954-1955).

   Additionally, bearing in mind that the eigenvalues of M † (K) are the square
canonical correlations between yt and yt† , we obtain the following proposition.

Proposition 4. Let µb2†
                     1 (K) ≤ µb2†              b2†
                               2 (K) ≤ · · · ≤ µ m (K) be the ordered eigenvalues
                †
of the matrix M (K). Then, under the null hypothesis that the limit matrix in
Proposition 3 has m − r eigenvalues equal to zero, the asymptotic distribution of
the statistic
                                                           X
                                                           m−r
                              Sm−r (K) = −(T − k̄)                         b2†
                                                                   log(1 − µ j (K))                                   (9)
                                                           j=1


is χ2(m−r)2 .

                         Revista Colombiana de Estadística - Theoretical Statistics 44 (2021) 1–21

6                                      Stevenson Bolívar, Fabio H. Nieto & Daniel Peña


Proof . According to Proposition 2, there are m − r canonical correlations equal
to zero between yt and yt† . This means that there are m − r linear combinations of
yt that are not correlated to m − r linear combinations of yt† ; consequently, there
                                                  ⊤ †
are m − r regressions of the form a⊤     j yt = Wj yt + ut where Wj = 0m×1 for each
j = 1, . . . , m − r, where the aj ’s are the canonical vectors associated to the m − r
canonical correlations equal to zero. The rest of the proof follows the lines of the
corresponding result in Peña & Poncela’s (2006) paper. We note that T − k̄ is the
number of observations that is used to compute the canonical correlations.

    Remark: To compute γj (K), we sum over all the lags in K the autocovariances
of the same jth factor (in the order established at the beginning of this section).
It may happen that we found autocovariances of the same absolute value but
diﬀerent sign and thus, γj (K) might be equal to zero, even if for some k ∈ K,
γj (k) ̸= 0. Therefore, instead of using only the sum over all the lags in K, an
alternative is to use diﬀerent linear combinations to calculate yt† . Indeed, to deﬁne
a linear combination, a lag in K is kept constant (1 is the coeﬃcient) and the
coeﬃcients corresponding to the other lags alternate between +1 and −1. In
              †
this way, 2(k −1) possible linear combinations can be analyzed. The main idea
behind the use of diﬀerent linear combinations yt† is to avoid the cases, in which
the number of factors is underestimated; therefore, we propose to choose r as the
maximum number of factors detected with the diﬀerent linear combinations. The
examples below illustrate this point.
    In summary, the procedure for specifying r is the following:

STEP 1 Set the maximum number of lags k0 and run Peña and Poncela’s test for
       lags k = 1, 2, . . . , k0 .
STEP 2 Set K such as a lag k ∈ K if the Peña and Poncela’s test detects at least
       one common factor at it, k = 1, 2, . . . , k0 .

STEP 3 Deﬁne the 2(k −1) possible linear combinations yt† , keeping a lag constant
                        †


       (1 is the coeﬃcient) and the coeﬃcients corresponding to the other lags
       alternate between +1 and −1.
STEP 4 Run the test (4) with each linear combination yt† and choose r as
       the maximum number of factors detected with the diﬀerent linear
       combinations.

    A simulated example
    To illustrate the issues remarked above about the statistical test performance,
we conduct a Monte Carlo experiment. The design of the simulation is the
following: we set m = 6 and r = 2 and consider the factor model




                   Revista Colombiana de Estadística - Theoretical Statistics 44 (2021) 1–21

On a New Procedure for Identifying a Dynamic Common Factor Model                          7
                                                    
                                   1.00         0.00
                                  1.00         1.00
                                                    
                                                    
                                  0.00         1.00
                            yt =                     f t + εt ,                      (10)
                                  1.00         0.00
                                                    
                                 −1.00         1.00
                                   0.00        −1.00

where ft = (f1t , f2t )⊤ , {εt } ∼ W N (0, I6 ), f1t = a1t + 0.8a1,t−1 and f2t =
a2t − 0.7a2,t−3 , with {at = (a1t , a2t )⊤ } ∼ W N (0, I2 ). We put T = 1000 and
simulate 1000 time series from the process {yt }. For each generated time series
and ﬁxing a signiﬁcance level of 5%, Peña and Poncela’s test was run sequentially
as a test on the maximum number of factors and we stop and identify r factors as
soon as the hypothesis is rejected, which will be denoted r′ from now on.
    Table 1 presents the percentage of times (in the cells) that the test identiﬁes r′
factors (columns) at each lag k (rows), k = 1, 2, . . . , 15. Clearly, we can see that
one common factor is identiﬁed at lags k = 1, 3 and zero factors at the other lags.

Table 1: Percentage of times that the test identiﬁes r′ factors in the simulated model.
                                 r′ = 0     r′ = 1     r′ = 2     r′ = 3
                        k=1         0.0      96.0         3.8        0.2
                        k=2       93.0         6.8        0.2        0.0
                        k=3         0.0      93.8         5.6        0.6
                        k=4       93.4         6.2        0.4        0.0
                        k=5       92.4         7.3        0.3        0.0
                        k=6       93.6         6.2        0.2        0.0
                        k=7       92.2         7.4        0.4        0.0
                        k=8       93.2         5.9        0.9        0.0
                        k=9       94.0         5.8        0.2        0.0
                        k = 10    93.2         6.7        0.1        0.0
                        k = 11    94.0         5.7        0.3        0.0
                        k = 12    94.4         5.6        0.0        0.0
                        k = 13    94.2         5.7        0.1        0.0
                        k = 14    93.7         6.2        0.1        0.0
                        k = 15    92.8         6.9        0.3        0.0


    For this model, we get K = {1, 3} with k † = 2; therefore, one have
22−1 = 2 possible linear combinations. These are yt† = yt−1 + yt−3 (1) and
yt† = yt−1 − yt−3 (2). In Table 2 we present the results obtained using the two
linear combinations yt† . We observe that r = 2 is clearly identiﬁed for any yt† .

Table 2: Percentage of times that the proposed test identiﬁes r′ factors in the simulated
         model.

                  Linear combination      r′ = 0     r′ = 1     r′ = 2     r′ = 3
                          (1)                0.0        0.0      96.6         3.4
                          (2)                0.0        0.0      95.3         4.7



                   Revista Colombiana de Estadística - Theoretical Statistics 44 (2021) 1–21

8                                      Stevenson Bolívar, Fabio H. Nieto & Daniel Peña


    In order to illustrate that the test proposed in this section stays valid even when
the covariance Γf (k) is a full rank matrix for some lag k, we modiﬁed the factor
models in this example as follow: f1t = 0.8f1,t−1 + a1t and f2t = −0.7f2,t−3 + a2t .
Table 3 presents the percentage of times that the test identiﬁes r′ factors at each
lag k with this simulated AR model, k = 1, 2, . . . , 15.

Table 3: Percentage of times that the test identiﬁes r′ factors in the simulated model.
                                 r′ = 0     r′ = 1     r′ = 2     r′ = 3
                        k=1         0.0       76.0       23.5        0.5
                        k=2         0.0       78.8       20.7        0.5
                        k=3         0.0        0.0      95.5         4.3
                        k=4         0.0       78.4       21.4        0.2
                        k=5         0.0       79.8       19.4        0.8
                        k=6         0.0        1.4      94.5         3.9
                        k=7         4.8       79.7       15.4        0.1
                        k=8        13.3       75.6       10.9        0.2
                        k=9         0.0       30.0       68.2        1.8
                        k = 10     27.4       63.3        9.2        0.1
                        k = 11     37.4       56.0        6.5        0.1
                        k = 12      0.9       57.0       41.2        0.9
                        k = 13     45.0       50.4        4.6        0.0
                        k = 14     48.2       48.1        3.7        0.0
                        k = 15     10.3       66.4       23.3        0.0


   We can see that about 95% of times two common factor is identiﬁed at lags
k = 3, 6 and between one a two common factors at lags k = 1, 2, 4, 5.

Table 4: Percentage of times that the proposed test identiﬁes r′ factors in the simulated
         model.

                  Linear combination      r′ = 0     r′ = 1     r′ = 2     r′ = 3
                          (1)                0.0        0.0       96.1        3.8
                          (2)                0.0        0.0       95.9        4.1


    For this model, we use K = {1, 3} with k † = 2 with the two linear combinations
(1) yt† = yt−1 + yt−3 (1) and (2) yt† = yt−1 − yt−3 (2). In Table 4 we present the
results obtained using the two linear combinations yt† . We observe that r = 2 is
clearly identiﬁed for any yt† .


3. A Procedure for Identifying the Common-
   Factors Model
    Usually in practice, the factor models are identiﬁed using preliminary estimates
of the factor processes (see, among others, Peña & Poncela’s (2006) and Nieto
et al.’s (2016) papers). However, if there is much uncertainty in these estimates, the
model identiﬁcation process might lead to wrong models. To avoid this problem,
we propose another alternative to the factors model identiﬁcation, which consists

                   Revista Colombiana de Estadística - Theoretical Statistics 44 (2021) 1–21

On a New Procedure for Identifying a Dynamic Common Factor Model                             9

in using the relationship between canonical correlations and the factor-processes
autocovariance function established in Proposition 1.
    The main idea is to plot the canonical correlations that are related to the
same factor against lag values. Because of Proposition 1, we plot at each lag the
absolute value of the autocovariance of factor fjt divided by the constant γj (0)+1,
for each j = 1, . . . , r. Loosely speaking, we plot a function that is proportional
to the absolute value of the autocorrelation function of the jth factor. For each
j = 1, . . . , r, we call the function that corresponds to the jth factor as the jth
canonical autocorrelation function (CACF).
    It is worth noticing that, when the canonical correlations are ordered in
descending way, given that each canonical correlation is proportional to the
autocovariance of a particular factor (see Proposition 1), a speciﬁc order is deﬁned
for the components of ft at each lag, but this ordering on the components of ft is
not necessarily the same at all lags. Then, to avoid this drawback, there are two
main goals to achieve. The ﬁrst one is to deﬁne a unique order for the components
of ft and the second, ordering at each lag the canonical correlations based on this
particular components order.
    From now on, we denote µ2ji:k ∈ Λ(k) the ith squared canonical correlation
for a given lag k, in descending order of magnitude, and bji:k denote its related
eigenvector, where i = 1, 2, . . . , r(k) and ji:k ∈ {1, 2, . . . , r}.
   Our proposal to deﬁne the same order for all lags is based on the following
lemmas.
Lemma 1. For any lag k, the canonical vector related to the factor fjt is given
by bj := (γj (0) + 1)−1/2 Σ−1
                           ε Pj , j = 1, 2, . . . , r.

Proof . Let B = [b1 , b2 , . . . , br ] = Σ−1
                                           ε P (Γf (0) + Ir )
                                                              −1/2
                                                                   , then
                  B ⊤ yt = B ⊤ (P ft + εt ) = (Γf (0) + Ir )−1/2 (ft + νt ),
where νt = P ⊤ Σ−1
                ε εt . Notice that {νt } is a white-noise process with mean 0(r×1)
and variance matrix Ir ; therefore, the canonical variables B ⊤ yt have covariance
matrix Ir and their associated canonical correlations are the absolute value of the
diagonal components of the matrix
                         ⊤
             E[B ⊤ yt , yt−k B] = (Γf (0) + Ir )−1/2 Γf (k)(Γf (0) + Ir )−1/2 ,
as was shown in Proposition 1.
Lemma 2. Given the set of lags K, if P                              for all k ∈ K
                                      bj is an eigenvector of M (k) P
then bj is an eigenvector of M S(K)1 = k∈K M (k), with eigenvalue k∈K µ2j (k),.

Proof . Multiplying M S(K) by bj , we obtain
                       "          #     "          #
                         X                X
                                              2
                           M (k) bj =        µj (k) bj .                                 (11)
                              k∈K                 k∈K


  1 A similar idea is applied in Lam & Yao’s (2012) paper to detect the number of factors.




                     Revista Colombiana de Estadística - Theoretical Statistics 44 (2021) 1–21

10                                                Stevenson Bolívar, Fabio H. Nieto & Daniel Peña


    Based on Lemma 2, we propose to use as speciﬁc order of the components
of ft , the one speciﬁed by the eigenvalues of M S(K) ordered in descending
way. We denote as bj1:K , bj2:K , . . . , bjr:K their related eigenvectors, which are the
column vectors b1 , b2 , . . . , br deﬁned in Proposition 1, Lemma 1 and Lemma 2,
but rearranged according to the deﬁned order. Then, at each lag, we order the
canonical correlations by matching each one to the eigenvector of M S(K) that
is collinear to its canonical vector. That association is possible by means of
the cosine similarity between the eigenvectors of M S(K) with the eigenvectors
of M (k), which are the normalized canonical vectors. Hence, by Lemma 1, all
the canonical correlations that match with a particular eigenvector of M S(K) are
related to the same factor.
    This proposal, as stated in Proposition 5, is based on the fact that the sequence
of squared cosines of the angle between an eigenvector of M c(k) and an eigenvector
of MdS(K), indexed by T , converges in probability to 1 if they are related to the
same factor, otherwise it converges to 0.
Proposition 5. For any lag k ∈ K, let bbji:K , bji:K , bbji:k and bji:k be the eigenvectors
associated to the ith non-zero largest eigenvalue of M      d S(K), M S(K), M    c(k) and
M (k), respectively. If M (k) has r(k) ≤ r eigenvalues diﬀerent of zero, then, as
T → ∞, for i = 1, . . . , r and i′ = 1, . . . , r(k),
                                         !2                                 !2
                  bb⊤ bbj                               ⊤
                                                      bji′ :k bji:K                                 
                    ji′ :k i:K
                                              →
                                              p
                                                                                 = cos2 θji′ :k ji:K ,   (12)
                b
              ||bji′ :k || ||bbji:K ||            ||bji′ :k || ||bji:K ||

where θji′ :k ji:K is the angle between bji′ :k and bji:K .

Proof . It follows using the consistency property of the estimators bbji′ :k and bbji:K ,
and the continuous mapping theorem.
                               
    Note that cos2 θji′ :k ji:K takes its maximum value if bji′ :k and bji:K are collinear
to the same column vector of Σ−1   ε P ; hence, by Lemma 1, they are related to the
same particular factor. In this case, ji′ :k = ji:K .
   In our proposed methodology, we also deﬁne the partial canonical
autocorrelation function (PCACF), which, jointly with the CACF, let us identify
ARMA models for the factors, in a similar way to the Box-Jenkins methodology.
The main idea is to plot the partial canonical correlations that are related to the
same factor against lag values, based on the same order deﬁned on the CACF,
given that we face the same issues. We call the function that corresponds to the
jth factor as the jth partial canonical autocorrelation function (PCACF).
    To establish such deﬁnition, in Proposition 6 we show the relation between
partial canonical correlation and a modiﬁed partial autocorrelation of the latent
factors, in which we use Reinsel’s (1997) concept of partial canonical correlations
between two random vectors. To ﬁx ideas, the partial canonical correlations
between yt and yt−k are the canonical correlations between yt and yt−k given yt−1 ,
yt−2 , · · · , yt−k+1 and can be calculated as the non trivial canonical correlations
between [yt⊤ , yt−1⊤            ⊤
                     , . . . , yt−k+1          ⊤
                                      ]⊤ and [yt−1    ⊤
                                                   , yt−2            ⊤ ⊤
                                                          , . . . , yt−k ] .

                        Revista Colombiana de Estadística - Theoretical Statistics 44 (2021) 1–21

On a New Procedure for Identifying a Dynamic Common Factor Model                                  11

                                                  ⊤
    Based on these ideas and being Yt;k = [yt⊤ , yt−1            ⊤
                                                      , . . . , yt−k+1 ]⊤ , we obtain the
following result.

Proposition 6. Given a lag k, the rp (k) ≤ r partial canonical correlations
diﬀerent from zero between yt and yt−k are equal to the absolute values of the
                                  ∗       ∗
partial autocorrelation between fj,t and fj,t−k , j = 1, . . . , rp (k), with ft∗ = ft + νt
and {νt } ∼ W N (0, Ir ). Furthermore, the respective canonical variables are

          gj (k)⊤ Yt;k           and            hj (k)⊤ Yt−1;k , j = 1, 2, . . . , rp (k),

with hj (k) = βj (k) ⊗ Σ−1
                         ε Pj ∈ R
                                       km
                                               and gj (k) = αj (k) ⊗ Σ−1
                                                                       ε Pj ∈ R
                                                                                       km
                                                                                           , where
αj (k) and βj (k) are the canonical vectors associated to the non trivial canonical
                          ∗    ∗                ∗
correlations between [fj,t  , fj,t−1 , . . . , fj,t−k+1            ∗
                                                        ]⊤ and [fj,t−1    ∗
                                                                       , fj,t−2            ∗
                                                                                , . . . , fj,t−k ]⊤ .
Here, ⊗ denotes the Kronecker product.

Proof . See Appendix A.3.

    Because Proposition 6, on the PCACF we plot the absolute value of the partial
                                   ∗
autocorrelations of the processes fjt , for each j = 1, . . . , r. Loosely speaking, we
plot a function that shows the M A process behavior of the factors plus a noise
process in absolute value; therefore, if the variance of the added noise process
is negligible with respect to the variance of the factor we get the PACF of the
M A(qj ) process, otherwise an M A(pj + qj ) process is observed (Peña 2010).
    It is worth noticing that, when the partial canonical correlations are ordered
in descending way, as in the CACF, a speciﬁc order is deﬁned for the components
of ft at each lag, but this ordering on the components of ft is not necessarily the
same at all lags. Then, using the same order deﬁne for the CACF, we order at
each lag the partial canonical correlations based on this particular components
order, as presented below.
    In what follows, let ηji:k (k) be the ith partial canonical correlation for a given
lag k, in descending order of magnitude, and let hji:k be its related eigenvector,
where i = 1, 2, . . . , rp (k) and ji:k ∈ {1, 2, . . . , r}.
   Now, from Proposition 6, we note that for a given i, i = 1, 2, . . . , rp (k), the
coeﬃcients of yt in the respective canonical variable form the vector hji:k [1 : m],
the m ﬁrst elements of the canonical vectors hji:k , which is collinear to one of
the columns of the matrix Σ−1    ε P . Therefore, a similar result to Proposition 5 is
obtained with the sequence of squared cosines of the angles between b    hji′ :k [1 : m],
a consistent estimator of hji′ :k [1 : m], and the previously deﬁned vector bbji:K ,
i = 1, 2, . . . , r. Hence, as in a similar way that in the CACF, we match all the
partial canonical correlations related to the same factor.
   Additionally, to test the number of rp (k) active factors on the partial canonical
correlations at lag k, we use the statistic

                                                       X
                                                       m
                Cm−rp (k) (k) = −(T − k − 1)                     log(1 − ηbj2i:k (k)),         (13)
                                                    i=rp (k)+1



                      Revista Colombiana de Estadística - Theoretical Statistics 44 (2021) 1–21

12                                         Stevenson Bolívar, Fabio H. Nieto & Daniel Peña


which is asymptotically a χ2(m−rp (k))2 , where the square partial canonical
correlations, ηbj21:k (k) ≥ ηbj22:k (k) ≥ · · · ≥ ηbj2m:k (k), are the m smallest eigenvalues of
the matrix
          " T                 #−1 T                     " T                   #−1 T
            X                       X                     X                      X
Mcp (k) =                 ⊤
                 (Yt;k Yt;k )                   ⊤
                                        (Yt;k Yt−1;k  )                ⊤
                                                             (Yt−1;k Yt−1;k )                 ⊤
                                                                                    (Yt−1;k Yt;k ).
             k+1                   k+1               k+1                       k+1

For more details of the asymptotic distribution of this statistic see Reinsel
(1997), on the identiﬁcation stage in the scalar component models (SCM) of
Tiao & Tsay (1989).
    To use these ideas in practice, we propose the following methodology, which is
illustrated in the ﬁrst example of Section 4.

STEP 1 First, deﬁne the set K as the set of all lags k ≤ k0 , for a ﬁxed k0 ≥ 1,
       such that the Peña-Poncela’s testP identify       at least one factor and get
                             d
       the r eigenvectors of M S(K) := k∈K M     c(k), related to the r maximum
       eigenvalues. Notice that the descending order of these eigenvalues deﬁne
       an speciﬁc order for the eigenvectors bbj1:K , bbj2:K , . . . , bbjr:K and by Lemma
       1 for the factors, which we propose to use as the unique order for all
       lags.
STEP 2 At each lag k, associate each canonical correlation to a particular
        factor via the association to a particular eigenvector of matrix M                d  S(K),
        according to Lemma 2.               For this purpose, start associating the
        estimated largest canonical correlation µ            bj1:k (k) to one of the eigenvectors
       bbj , bbj , . . . , bbj , by selecting the one having the maximum cosine
          1:K   2:K           r:K

        similarity with its eigenvector bbj1:k , that is, the largest value of the
        squared cosine of the angle between both vectors (maximum correlation
        between two random canonical vectors). Similarly, to the next estimated
        canonical correlations µ   bj2:k (k) ≥ µ     bj3:k (k) ≥ · · · ≥ µ bjr(k):k (k) assign one
        of the eigenvectors bbj1:K , bbj2:K , . . . , bbjr:K , but excluding eigenvectors that
        were already assigned to higher canonical correlations.
STEP 3 At each lag k, associate each partial canonical correlation to a particular
       factor via the association to a particular eigenvector of matrix M          d S(K).
       Follow the same ideas of the STEP 2, using the partial canonical
       correlations ηbj1:k (k) ≥ ηbj2:k (k) ≥ · · · ≥ ηbjrp (k):k (k) and theirs respective
       eigenvectors bhj [1 : m], b
                             1:k
                                    hj [1 : m], . . . , b
                                            2:k
                                                        hj         [1 : m].
                                                               rp (k):k



STEP 4 For each i = 1, 2, . . . , r, plot the ith sample CACF deﬁned by ϑbi (k, K) =
       bji:K (k), 0 < k < T , where µ
       µ                                  b2ji:K (k) is the ith eigenvalue of the matrix
       c(k), ordering on STEP 2.
       M
STEP 5 For each i = 1, 2, . . . , r, the ith sample PCACF deﬁned by φ         bi (k, K) :=
       ηbji:K (k), 0 < k < T , where ηbj2i:K (k) is the ith of the m smallest eigenvalues
       of the matrix M   cp (k), ordering on STEP 3.


                      Revista Colombiana de Estadística - Theoretical Statistics 44 (2021) 1–21

On a New Procedure for Identifying a Dynamic Common Factor Model                          13

STEP 6 Based on the CACF and the PCACF plots, identify ARMA models for
       the factor processes, in a similar way to the Box-Jenkins methodology,
       bearing in mind that the ith CACF is proportional to the ACF of the ith
       factor process, see Proposition 1 and the ith PCACF shows the PACF
       of fit∗ , see Proposition 6.

    For the last step, it is important to highlight that, in terms of absolute
values, the CACFs show the AR behavior of each factor process because the
CACFs are proportional to theirs ACFs (see Proposition 1). In the same way,
the PCACFs show the MA behavior of each factor process plus a noise process
(see Proposition 6). In summary, in absolute values, the ith canonical and partial
canonical correlations show the ARM A(pj , qj ) behavior of the associated factor if
the variance of the added noise process is negligible with respect to the variance
of the factor, otherwise an ARM A(pj , pj + qj ) behavior is observed (Peña 2010),
where pj and qj are, respectively, the autoregressive and moving average order
of the factor related with the jth partial canonical autocorrelation correlation
function.


4. Some Examples
4.1. A Simulated Model
   We simulate again model (11), using as sample size T = 1000. As was found
previously, we obtain K = {1, 3} after setting k0 = 13. We recall that using the
two possible linear combinations we identify 2 common factors. With the simulated
data, we get µb2j1:K (K) = 0.22 and µb2j2:K (K) = 0.18, the ordered eigenvalues of the
matrix M d S(K), with eigenvectors bbj1:K = (−0.56, −0.58, −0.31, −0.38, 0.27, 0.20)⊤
and bbj2:K = (−0.24, 0.18, 0.40, −0.09, 0.63, −0.59)⊤ , respectively.
     The next step is to use this speciﬁc order at all lags. Then, for each eigenvector
bbj and bbj , we match a canonical correlation and a partial canonical correlation
   1:K        2:K
 at each lag k = 1, 2, . . . , k0 , based on the methodology mentioned before. As
 an illustration of our proposed ordering methodology, we calculate the cosine
 similarity to order the canonical correlations at lags k = 1 and k = 3. For lag k = 1,
 bj1:1 (1) = 0.46 with eigenvector bbj1:1 = (0.61, 0.48, 0.17, 0.37, −0.47, 0.04)⊤ and
 µ
 bj2:1 (1) = 0.10 with eigenvector bbj2:1 = (0.62, −0.47, −0.52, −0.12, −0.03, 0.31)⊤ .
 µ
 The cosine similarity of bbj1:1 with the vectors bbj1:K and bbj2:K are 0.87 and 0.12,
 respectively. Then, we relate µ                            bj2:1 (1) to bbj2:K . In the same
                                    bj1:1 (1) to bbj1:K and µ
 way, at lag k = 3, we got 0.16 and 0.85 as the cosine similarity of the vector
bb1:3 with bbj and bbj , respectively; hence, we relate µ      bj1:3 (3) = 0.42 to bbj2:K and
              1:K        2:K

 bj2:3 (3) = 0.07 to bbj1:K .
 µ
     In other words, ϑb1 (1, K) = 0.46, ϑb2 (1, K) = 0.07, ϑb1 (3, K) = 0.10 and
 b
 ϑ2 (3, K) = 0.42 as is shown in the Figure 1, where we plot the CACF and the
 PCACF according to the order speciﬁed by bbj1:K and bbj2:K . We use gray bars
 in both graphics to indicate the canonical correlations and the partial canonical

                    Revista Colombiana de Estadística - Theoretical Statistics 44 (2021) 1–21

14                                       Stevenson Bolívar, Fabio H. Nieto & Daniel Peña


correlations that are statistically diﬀerent from zero, according to the tests (3) and
(13) mentioned before.

      0.4                cacf                                             pcacf




                                                        0.4
ϑ1(k, K )




                                                  ϕ1(k, K )
 0.2




                                                   0.2
      0.0




                                                        0.0
            1   3   5      7    9   11   13                   1   3   5     7     9   11   13
                         (a) CACF and PCACF of the ﬁrst factor process
      0.4




                                                        0.4
ϑ2(k, K )




                                                  ϕ2(k, K )
 0.2




                                                   0.2
      0.0




                                                        0.0


            1   3   5      7    9   11   13                   1   3   5     7     9   11   13
                        (b) CACF and PCACF of the second factor process
Figure 1: Plot of CACF and PCACF of the simulated data. Gray bars indicate
          the canonical correlations and the partial canonical correlations that are
          statistically diﬀerent from zero, according to the tests (3) and (13) mentioned
          before.

    Notice that the ﬁrst CACF and PCACF show an M A(1) behavior; hence they
are related to the factor f1t and the second ones, an M A(3), as it is factor f2t . In
this example, the CACF and PCACF show exactly the same expected behavior
of the ACF and PACF proposed by Box & Jenkins (1970), because an M A(q)
process plus a white noise process still being an M A(q) process (Peña 2010).


4.2. A Real Data Application
    This example is taken from Nieto et al.’s (2016) paper, where the total monthly
rainfall time series were used. The rainfalls were measured in meteorological
stations located at the airports of six cities in Colombia: Bucaramanga(y1 ),
Cúcuta(y2 ), Ibagué(y3 ), Medellín(y4 ), Manizales(y5 ) and Bogotá(y6 ). Figure 2
presents the deseasonalized time series and, with the Peña & Poncela’s (2006)
test at the lags 1, 2, . . . , 13, we get that the lags 1, 4 and 6 present at leat one
factor. With K = {1, 4, 6} we detect two common factors using the test proposed
in Section 2.




                    Revista Colombiana de Estadística - Theoretical Statistics 44 (2021) 1–21

On a New Procedure for Identifying a Dynamic Common Factor Model                                  15

         Bucaramanga (y1)                                       Cúcuta (y2)
   200
                                                          200
   100                                                    100

     0                                                      0

                                                         −100
  −100
                                                         −200
                1980         1990      2000       2010                 1980     1990   2000      2010


         Ibagué (y3)                                            Medellín (y4)
                                                          200
   200
                                                          100
   100

     0                                                      0

  −100
                                                         −100

                1980         1990      2000       2010                 1980     1990   2000      2010


         Manizales (y5)                                         Bogotá (y6)
   200                                                    150

                                                          100
   100
                                                           50

     0                                                      0

                                                         −50
  −100
                                                         −100
                1980         1990      2000       2010                 1980     1990   2000      2010



                                     Figure 2: Rainfall time series.


    Figure 3 presents the CACF and the PCACF of the deseasonalized time series
following our proposed methodology. To the ﬁrst factor, Figure 3(a), in the CACF
non-zero correlations are observed at lags 1, 4 and 6 (according to Peña and
Poncela’s test) and in the PCACF at lags 1 and 6 (according to the test 13).
Also, a possible decreasing behavior in the CACF from the ﬁrst correlation, that
suggests an M A(1) process. For the second factor, Figure 3(b), in the CACF and
the PCACF a non-zero correlations are observed at lag 1 (according to the test
(3) and (13)) and a possible decreasing behavior is observed in the CACF, that
suggests an AR(1) process.
    To the estimation of the parameters, we maximize the likelihood function
using the EM algorithm. On step E, we use the Kalman ﬁlter and the
smoothing algorithm. On step M , we use the space-state representation proposed
by Metaxoglou & Smith (2007) for VARMA models, intending to simplify the
maximization process from step M , as the authors mention in their paper. Also,
to solve the identiﬁcation problem of the model, restriction P ⊤ Σ−1
                                                                  ε P = Ir was
imposed, and using Jungbacker & Koopman (2015) ideas we transform the data
yt∗ = AL yt in step E, with AL = P ⊤ Σε−1 , obtaining the transformed model
yt∗ = AL yt = ft + AL εt , where AL P = Ir and var[AL εt ] = Ir .




                            Revista Colombiana de Estadística - Theoretical Statistics 44 (2021) 1–21

16                                             Stevenson Bolívar, Fabio H. Nieto & Daniel Peña

                               cacf                                                pcacf



         0.20




                                                                0.20
 ϑ1(k, K )




                                                        ϕ1(k, K )
0.10




                                                       0.10
        0.00




                                                               0.00
                1   3    5      7     9   11   13                      1   3   5     7     9   11   13

                              (a) CACF and PCACF of the ﬁrst factor process
         0.20




                                                                0.20
 ϑ2(k, K )




                                                        ϕ2(k, K )
0.10




                                                       0.10
         0.00




                                                                0.00
                1   3    5      7     9   11   13                      1   3   5     7     9   11   13
                             (b) CACF and PCACF of the second factor process
                    Figure 3: Plot of CACF and PCACF of Colombian rainfalls

    Based on the above speciﬁcation, the following model with two common factors
is estimated:
                                               
                                 7.52 −18.19
                              11.14 −32.96
                                               
                                               
                              19.73        8.14
                         Yt =                   f t + εt ,
                              22.60      14.33
                                               
                              16.52 −6.73
                                12.46 −3.03

where f1t = 0.12f1,t−6 + 0.117a1,t−1 + 0.122a1,t−4 + a1t , f2t = 0.36f2,t−1 + a2t ,
Σa = diag(4.01, 0.69) and Σε = diag(2226.9, 1853.7, 3463.2, 1279.2, 1566.2, 712.7).
    The structure of the factors can be seen by columns of the P matrix. The ﬁrst
one has a positive eﬀect on the time series, with a minor scale on the Bucaramanga
rainfall (y1t ). The second separates Ibagué (y3t ) and Medellin (y4t ) from the others
cities precipitations.


5. Conclusions
    In this paper, (1) we establish the relationship between canonical correlations
and the autocovariance function of the factor process. Based on this relation, we
modify Peña & Poncela’s (2006) test to detect the number of common factors,
increasing the test power. Additionally, (2) we establish the relationship between

                         Revista Colombiana de Estadística - Theoretical Statistics 44 (2021) 1–21

On a New Procedure for Identifying a Dynamic Common Factor Model                          17

partial canonical correlations and the partial autocorrelation function of the factor
process. Finally, (3) we propose to use the canonical vectors to link the canonical
and partial canonical correlations at each lag to a speciﬁc factor process. These
three ﬁndings allow us to propose a procedure to identify a vector ARMA model
for the factor process, which is based on the so-called simple and partial canonical
autocorrelation functions.
                                                                          
                  Received: March 2019 — Accepted: September 2020


References
Ahn S C, Horenstein A R. Eigenvalue Ratio Test for the Number of Factors.(2013). Econometrica.
Anderson T. An Introduction to Multivariate Statistical Analysis- Wiley Series in Probability and Statistics - Applied Probability and Statistics Section Series.(1984). Wiley.
Box G E P, Jenkins G M. Time series analysis: forecasting and control.(1970).  Holden-Day.
Doz C, Fuleky P. Dynamic factor models in Macroeconomic Forecasting in the Era of Big Data.(2020). Springer.
Geweke J. The dynamic factor analysis of economic timeseries models.(1977). Latent Variables in Socio-Economic Models.
Jungbacker B, Koopman S J. Likelihood-Based Dynamic Factor Analysis for Measurement and Forecasting.(2015). The Econometrics Journal.
Lam C, Yao Q. Factor modeling for high-dimensional time series: Inference for the number of factors1.(2012). Annals of Statistics.
Metaxoglou K, Smith A. Maximun likelihood estimation of VARMA models using a state-space EM algorithm.(2007). Journal of Time Series Analysis.
Nieto F H, Peña D, Saboyá D. Common seasonality in multivariate time series.(2016). Statistica Sinica.
Peña D. Análisis de series temporales El Libro Universitario - Manuales.(2010). Alianza Editorial.
Peña D, Box G E P. Identifying a Simplifying Structure in Time Series.(1987). Journal of the American Statistical Association.
Peña D, Poncela P. Nonstationary dynamic factor analysis.(2006). Journal of Statistical Planning and Inference.
Reinsel G C. Elements of Multivariate Time Series Analysis 2 edn.(1997). Springer-Verlag.
Stock J H, Watson M. Dynamic factor models in P C Michael and D F Hendry eds Oxford Handbook on Economic Forecasting.(2011). Oxford University Press.
Stock J H, Watson M W. Dynamic factor models factor-augmented vector autoregressions and structural vector autoregressions in macroeconomics in J B Taylor and H Uhlig eds Handbook of macroeconomics.(2016). Handbook of Macroeconomics.
Tiao G C, Tsay R S. Model speciﬁcation in multivariate time series.(1989). Journal of the Royal Statistical Society.