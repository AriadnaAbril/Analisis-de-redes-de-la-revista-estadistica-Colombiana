Objective Prior Distributions to Estimate the Parameters of the Poisson-Exponential Distribution. Distribuciones previas objetivas para estimar los parÃ¡metros de la distribuciÃ³n Poisson-Exponencial
Universidade Estadual Paulista, SÃ£o Paulo, Brasil
Abstract
In this paper, a set of important objective priors are examined for the Bayesian estimation of the parameters present in the Poisson-Exponential distribution P E . We derived the multivariate Jereys prior and the Maximal Data Information Prior. Reference prior and others priors proposed in the literature are also analyzed. We show that the posterior densities resulting from these approaches are proper although the respective priors are improper. Monte Carlo simulations are used to compare the eciencies and to assess the sensitivity of the choice of the priors, mainly for small sample sizes. This simulation study shows that the mean square error, meanbias and coverage probability of credible intervals under Gamma, Jefreys' rule and Box & Tiao priors presented equal results, whereas Jereys and Reference priors showed the best results. The MDIP prior had a worse performance in all analyzed situations showing not to be indicated for Bayesian analysis of the P E distribution. A real data set is analyzed for illustrative purpose of the Bayesian approaches.
Key words : Bayesian; Poisson-Exponential; Jefreys; MDIP; Objective; Prior.
Resumen
En este artÃ­culo, se examina un conjunto de importantes priori objetivas para la estimaciÃ³n bayesiana de los parÃ¡metros de la distribuciÃ³n Poisson -Exponencial (P E ). Derivamos la priori Jereys multivariada y la Maximal Data Information Prior. TambiÃ©n se analizan la priori de Referencia y otras prioris propuestas en la literatura. Mostramos que las distribuciones posterioris resultantes de estos enfoques son adecuadas, aunque las respectivas prioris son impropias. Las simulaciones de Monte Carlo se utilizan para comparar las eciencias, para evaluar la sensibilidad de la elecciÃ³n de las prioris, principalmente para tamaÃ±os de muestra pequeÃ±os. Este estudio de simulaciÃ³n muestra que los errores cuadrÃ¡ticos medios, el sesgo medio y la probabilidad de cobertura de los intervalos creÃ­bles bajo la Gamma, regla de Jereys y Box & Tiao mostraron resultados iguales, mientras que los prioris de Jereys y Reference mostraron los mejores resultados. El priori MDIP tuvo un peor desempeÃ±o en todas las situaciones analizadas mostrando no estar indicado para el anÃ¡lisis bayesiano de la distribuciÃ³n P E . Se analiza un conjunto de datos reales con nes ilustrativos de los enfoques bayesianos.
Palabras clave : Bayesiano; Jefreys; MDIP; Objetiva; Poisson-Exponencial; Priori.



1. Introduction
     The Poisson-Exponential (P E ) is a lifetime distribution with increasing failure
rate introduced by Cancho et al. (2011). This model is derived in a complementary
risks scenario where the lifetime associated with a particular risk is not observa-
ble, rather we observe only the maximum lifetime value among all risks.                This
distribution can be used to engineering problems where after increasing the failure
rate the function may become stabilized.

     As is well known, the choice of a prior distribution is the fundamental part
of any Bayesian analysis.     Objective priors, generally also called noninformative
priors, refer to the case where relatively little information is available a priori, that
is, information about model parameters is not considered substantial compared to
information from a data set or in situations where a researcher is not able to
express his/her prior opinion into a prior distribution.

     Because there is not a precise denition about the concept of noninformative
prior, there are in the Bayesian literature several forms of formulating noninforma-
tive priors, for instance, Jereys (1967), MDIP (Zellner, 1977, 1984), Tibshirani
(1987), reference (Bernardo, 1979) and many others. Therefore, a study to derive
the priors for a distribution tted for the experimental data and check if these
proposed priors lead to the same posterior inference results is of great practical
interest.   Furthermore, it is desirable to compare the dierent priors to check if
any of them are preferable, especially for small samples.

     Rodrigues et al. (2018) studied dierent non-Bayesian methods of estimation
for the parameters of P E distribution and Louzada-Neto et al. (2011) provide a
Bayesian inference by using a Jereys's rule prior representing weak information for
the parameters of P E distribution. Tomazella et al. (2013) also present a Bayesian
analysis for the parameters of P E distribution but using the Reference prior dis-
tribution proposed by Bernardo (1979). Singh et al. (2014) proposed the Bayesian
approach under symmetric and asymmetric loss functions comparing them with the
maximum likelihood procedure to estimate the parameters of Poisson-Exponential



                     Revista Colombiana de EstadÃ­stica - Applied Statistics 46 (2023) 93-110

Objective Priors to Estimate the Poisson-Exponential Parameters                         95



distribution for complete sample. Belaghi et al. (2019) obtain the estimates un-
der the maximum likelihood approach, and Bayesian estimates using dierent loss
functions for estimation and prediction problems when the lifetime data following
the Poisson-Exponential distribution are observed under type-II censoring.

   In this paper a more complete set of important objective priors representing
a situation of absense or weak information of the parameters of the Poisson-
Exponential distribution are examined. We derived the multivariate Jereys prior
proposed by Jereys (1967) and the Maximal Data Information Prior (MDIP) pro-
posed by Zellner (1977). Others types of objective priors proposed in the literature
are also analyzed. We are also interested in selecting an objective prior that best
represents a state of little knowledge a priori about the parameters.

   In Bayesian analysis with objective priors, it should be justied that the poste-
rior densities are proper. In this paper, all the posterior densities of the parameters
of P E distribution will result proper posterior distributions although the priors are
improper. In addition, a simulation study is performed using dierent sample sizes
and we examine the bias, mean square error and frequentist coverage probabilities
in order to compare the performance of the proposed priors.

   One purpose of Bayesian inference is to obtain the marginal posterior densities
because they provide complete information about parameters of interest such as
Bayes estimator, mode and credible intervals. For this, we need to integrate the
joint posterior density with respect to each parameter. However, since the marginal
posterior densities cannot be obtained in a closed form through the integration, the
Markov Chain Monte Carlo (MCMC) techniques, in special Metropolis.Hastings
algorithm, to generate samples of values of Î¸ and Î» from the joint posterior distri-
butions is also carried out.

   The outline of the remaining sections is organized as follows. In Section 2, the
Poisson-Exponential, its properties and the expected Fisher information matrix
derived by Cancho et al. (2011) was reviewed; in Sections 3 and 5, we derive
the Jereys and MDIP priors, respectively. Section 4 reviews the Reference prior
developed by Tomazella et al. (2013) and Section 6 presents other proposed priors.
Section 7 illustrates and discusses the results from the simulation performance
and in Section 8 we introduce an applied example provided by Lawless (2003) to
illustrate the Bayesian approach proposed. Finally in section 9, we have presented
the conclusions.




2. The Poisson-Exponential Distribution
   Let be T representing the lifetime of a component under the Poisson-Exponential
distribution, denoted by P E , then the density is given by


                                     Î¸Î» exp{âˆ’Î»t âˆ’ Î¸eâˆ’Î»t }
                           f (t) =                        ,                            (1)
                                           1 âˆ’ eâˆ’Î¸
for all t > 0 and depending on the shape and scale parameters Î¸ > 0 and Î» > 0,
respectively.



                    Revista Colombiana de EstadÃ­stica - Applied Statistics 46 (2023) 93-110

96                                                          Fernando A. Moala & Gustavo Moraes

     The survival and hazard functions associated to (1) are given, respectively, by,


                                          1 âˆ’ exp{âˆ’Î¸eâˆ’Î»t }
                                S(t) =                     ,           t>0
                                               1 âˆ’ eâˆ’Î¸
and
                                      Î¸Î» exp{âˆ’Î»t âˆ’ Î¸eâˆ’Î»t }
                             h(t) =                        ,           t > 0.
                                        1 âˆ’ exp{âˆ’Î¸eâˆ’Î»t }

     The pth quantile of the P E is given by


                        1            h                    i
                 tp =     log Î¸ âˆ’ log âˆ’ log p âˆ’ eâˆ’Î¸ (p âˆ’ 1) ,                0 < p < 1.
                        Î»
     According to Cancho et al. (2011), the raw moments of T are given by,


                                  Î¸Î“(k + 1)
             Âµk = E(T k ) =                    Fk+1,k+1 ([1, . . . , 1], [2, . . . , 2], âˆ’Î¸),   (2)
                                 Î»k (1 âˆ’ eâˆ’Î¸ )
where the generalized hypergeometric function, denoted by Fpq (a, b, Î¸), is dened
as
                                      âˆž        Qp
                                      X     Î¸j i=1 Î“(ai + j) Î“(ai )âˆ’1 )
                   Fpq (a, b, Î¸) =                Qp                        ,                   (3)
                                      j=0
                                          Î“(j + 1) i=1 Î“(bi + j) Î“(bi )âˆ’1 )

with a = (a1 , a2 , . . . , ap ) and b = (b1 , b2 , . . . , bq ).

     From (2), we have the mean and variance given, respectively, by


                                           Î¸
                            E(T ) =                F22 ([1, 1], [2, 2], âˆ’Î¸)                     (4)
                                       Î»(1 âˆ’ eâˆ’Î¸ )
and

                     Î¸       h                                     Î¸                            i
  var(T ) =                    F33 ([1, 1, 1], [2, 2, 2], âˆ’Î¸) âˆ’         F22 ([1, 1].[2, 2], âˆ’Î¸)  .
               Î»2 (1 âˆ’ eâˆ’Î¸ )                                    1 âˆ’ eâˆ’Î¸

   Suppose we have independent identically distributed lifetimes t1 , t2 , . . . , tn from
P E . The likelihood function for the parameters Î¸ and Î», based on the random
sample t = (t1 , t2 , . . . , tn ), is given by

                                         (    n          n
                                                                   )
                                Î¸Î» n
                                            X          X
                                                              âˆ’Î»ti
                 L(Î¸, Î»| t) âˆ         exp âˆ’Î»     ti âˆ’ Î¸     e        .
                              1 âˆ’ eâˆ’Î¸        i=1        i=1

     Cancho et al. (2011) provide the conditions which are needed in order to obtain
the existence and uniqueness of the MLE when the other parameter is known. They
also provide the Fisher information matrix given by

                                                                -
                                                 I           I12
                                       I(Î¸, Î») = 11                ,                            (5)
                                                 I12         I22

                         Revista Colombiana de EstadÃ­stica - Applied Statistics 46 (2023) 93-110

Objective Priors to Estimate the Poisson-Exponential Parameters                                                                    97



with
                                                        âˆ‚2                            neÎ¸
                                                                    
                                                                               n
                                      I11 = âˆ’E              ln L          =        âˆ’ Î¸       ,
                                                        âˆ‚Î¸2                    Î¸ 2  (e âˆ’ 1)2

                                        âˆ‚2                nÎ¸
                     I12 = âˆ’E(              ln L) = âˆ’              F22 ([2, 2], [3, 3], âˆ’Î¸)
                                       âˆ‚Î¸âˆ‚Î»           4Î»(1 âˆ’ eâˆ’Î¸ )
and

                               âˆ‚2          n         Î¸2                                     
        I22 = âˆ’E(                  ln L) =    1 +             F 33 ([2, 2, 2], [3, 3, 3], âˆ’Î¸)  .                                  (6)
                               âˆ‚Î»2         Î»2     4(1 âˆ’ eâˆ’Î¸ )


3. Jereys Prior
     A well known weak prior to represent a situation with little information avail-
able a priori about the parameters was proposed by Jereys (1967). Since then
Jereys prior has played an important role in Bayesian inference.                                                    His prior is
derived from Fisher Information matrix I (Î¸ , Î») as

                                                                  p
                                                  Ï€(Î¸, Î») âˆ        det I (Î¸, Î») .                                                 (7)


     Box & Tiao (1973) give an explaining of the derivation of the noninformative
Jereys priors in terms of data translated likelihood.

     Jereys prior is widely used due to its invariance property under one-to-one
transformations of parameters.

Theorem 1.                Jereys prior for (Î¸, Î») parameters of P E is given by:

                s
            1        1          eÎ¸                 Î¸2                                             Î¸                                 2
Ï€(Î¸, Î») âˆ                 âˆ’                 1+                  F33 ([2, 2,2], [3, 3,3], âˆ’Î¸) âˆ’                   F22 ([2, 2], [3, 3], âˆ’Î¸)
            Î»        Î¸2       (eÎ¸ âˆ’ 1)2           4(1 âˆ’ eâˆ’Î¸ )                                      4(1 âˆ’ eâˆ’Î¸ )
                                                                                                                                  (8)

Proof . Immediate from (5) and (7).
     As this prior is an improper prior then it should be justied that the posterior
density is proper.

Corollary 1. The joint posterior density p(Î¸ ,Î» | t) for parameters (Î¸ , Î») under
Jereys prior (8) is proper.
Proof . Since eâˆ’Î»t â‰¤ 1 for all ti > 0, i = 1, . . . , n, and Î» > 0, it follows that
Z âˆžZ âˆž                                        Z âˆž Z âˆž                       n
                                                              Î¸Î» n     n   X      o
                p(Î¸, Î» | t)dÎ¸dÎ» â‰¤                                   exp  âˆ’Î»     ti  eâˆ’nÎ¸ Ï€(Î¸, Î»)dÎ¸dÎ» =
 0     0                                      0       0     1 âˆ’ eâˆ’Î¸         i=1
                              Z âˆž hZ âˆž                      Pn                 i Î¸eâˆ’Î¸ n
                                              Î» nâˆ’1 eâˆ’Î»          i=1 ti   dÎ»              Ï†(Î¸)dÎ¸
                                0         0                                      1 âˆ’ eâˆ’Î¸

                                    Revista Colombiana de EstadÃ­stica - Applied Statistics 46 (2023) 93-110

98                                                                           Fernando A. Moala & Gustavo Moraes

where
         s
                           eÎ¸                     Î¸2
                                                                     ,2      ,3
              1                                                                               Î¸                                2
Ï†(Î¸) =             âˆ’                    1+                 F33 ([2, 2 ], [3, 3 ], âˆ’Î¸) âˆ’                     F22 ([2, 2], [3, 3], âˆ’Î¸)   .
              Î¸2       (eÎ¸ âˆ’ 1)2             4(1 âˆ’ eâˆ’Î¸ )                                     4(1 âˆ’ eâˆ’Î¸ )
                                                                                                                                  (9)
                                                                                              Râˆž           nâˆ’1 âˆ’Î»
                                                                                                                     Pn
     Taking the integration with respect to Î», we have                                                 Î»      e         i=1 ti   dÎ» =
                                                                                               0
     Î“(n) 
           n and hence,
    Pn
     i=1 ti


               Z âˆžZ âˆž                                                             Z âˆž
                                                      Î“(n)                                    Î¸eâˆ’Î¸ n
                                 p(Î¸, Î» | t)dÎ¸dÎ» â‰¤  P         n                                     Ï†(Î¸)dÎ¸.
                   0       0                           n
                                                           t                           0    1 âˆ’ eâˆ’Î¸
                                                       i=1   i

                                                                  n
                            Î¸eâˆ’Î¸                            Î¸eâˆ’Î¸             Î¸e   âˆ’Î¸
     Now, since
                           1âˆ’eâˆ’Î¸
                                 â‰¤ 1, then                 1âˆ’eâˆ’Î¸
                                                                          â‰¤ 1âˆ’e âˆ’Î¸ and


                       Z âˆž                                      Z âˆž
                                  Î¸eâˆ’Î¸ n                                   Î¸eâˆ’Î¸
                                          Ï†(Î¸)dÎ¸ â‰¤                                Ï†(Î¸)dÎ¸ = 0.311677.                             (10)
                       0        1 âˆ’ eâˆ’Î¸                            0      1 âˆ’ eâˆ’Î¸

     The last integral in (10) is evaluated numerically by using Mathematica.
Therefore

                                Z âˆžZ âˆž
                                                                       Î“(n)
                                                  p(Î¸, Î» | t)dÎ¸dÎ» â‰¤  P        n < âˆž.
                                                                       n
                                   0         0
                                                                       i=1 t i




4. Reference Prior
     Another well-known class of noninformative priors is the reference prior pro-
posed by Bernardo (1979) and further improved by Berger & Bernardo (1992).

     The idea is to derive a prior Ï€ (Ï•) that maximizes the expected posterior infor-
mation about the parameters provided by independent replications of an experi-
ment relative to the information in the prior. A natural measure of the expected
information about Ï• provided by data x is given by



                                                 I(Ï•) = Ex [K(p(Ï• | x), Ï€(Ï•))]
where


                                                                                           p(Ï• | x)
                                                                 Z
                                K(p(Ï• | x), Ï€(Ï•)) =                    p(Ï• | x) log                 dÏ•                           (11)
                                                                   Î¦                         Ï€ (Ï•)
is the Kullback-Leibler distance. Thus, the reference prior is dened as the prior
Ï€(Ï•) that maximizes the expected Kullback-Leibler distance between the posterior
density p(Ï•|x) and the prior density Ï€(Ï•), taken over the experimental data.



                                 Revista Colombiana de EstadÃ­stica - Applied Statistics 46 (2023) 93-110

Objective Priors to Estimate the Poisson-Exponential Parameters                              99



   The prior density Ï€ (Ï•) which maximizes the functional (11) is found through
calculus of variation and, the solution is not explicit. However, when the posterior
p(Ï• | x) is asymptotically normal, this approach leads to Jereys prior for a single
parameter situation. If on the other hand, we are interested in one of the para-
meters, being the remaining parameters nuisances, the situation is quite dierent,
and the appropriated reference prior is not a multivariate Jereys prior. Bernardo
argues that when nuisance parameters are present, the reference prior should de-
pend on which parameters are considered to be of primary interest. The reference
prior in this case is derived as follows. We will present here the two-parameters
case in details. For the multiparameter case, Berger & Bernardo (1992).

   Let Î¸ = (Î¸1 , Î¸2 ) be the whole parameter, Î¸1 being the parameter of interest
and Î¸2 the nuisance parameter. The algorithm is given as follows:

Step 1: Determine Ï€2 (Î¸2 | Î¸1 ), the conditional reference prior for Î¸2 assuming that
Î¸1 is given,
                                                     p
                                  Ï€2 (Î¸2 | Î¸1 ) =        I22 (Î¸1 , Î¸2 ),

where I22 (Î¸1 , Î¸2 ) is the (2,2)-entry of the Fisher Information Matrix.

Step 2: Normalize Ï€2 (Î¸2 | Î¸1 ).
   Case Ï€2 (Î¸2 | Î¸1 ) is improper, choose a sequence of subsets â„¦1 âŠ† â„¦2 âŠ† Â· Â· Â· â†’ â„¦
on which Ï€2 (Î¸2 | Î¸1 ) is proper. Dene


                                                      1
                                 cm (Î¸1 ) = R
                                                   Ï€ (Î¸ | Î¸ )dÎ¸2
                                                 â„¦m 2 2 1

and
                           pm (Î¸2 | Î¸1 ) = cm (Î¸1 )Ï€2 (Î¸2 | Î¸1 )1â„¦m (Î¸2 ).

Step 3: Find the marginal reference prior for Î¸1 , i.e., the reference prior for the
experiment formed by marginalizing out with respect to pm (Î¸2 | Î¸1 ). We obtain

                                n 1 Z                              det I (Î¸1 , Î¸2 )    o
               Ï€m (Î¸1 ) âˆ exp               pm (Î¸2 | Î¸1 )log                        dÎ¸2 .
                                  2    â„¦m                           I22 (Î¸1 , Î¸2 )

Step 4: Compute the reference prior for (Î¸1 , Î¸2 ) when Î¸2 is a nuisance parameter:

                                                                     
                                                 cm (Î¸1 )Ï€m (Î¸1 )
                    Ï€(Î¸1 , Î¸2 ) = lim                                      Ï€(Î¸2 | Î¸1 ),
                                   mâ†’âˆž          cm (Î¸1âˆ— ) Ï€m (Î¸1âˆ— )
        âˆ—
where Î¸1 is any xed point with positive density for all Ï€m .

   Tomazella et al. (2013) derive reference prior and prove that the corresponding
posteriori density is proper.

Theorem 2.       Reference prior for (Î¸, Î») parameters of P E is given by:
                           1        Î¸2                                  
               Ï€(Î¸, Î») âˆ      1+        âˆ’Î¸
                                           F33 ([2, 2, 2], [3, 3, 3], âˆ’Î¸) Ï†(Î¸),             (12)
                           Î»     4(1 âˆ’ e )

                      Revista Colombiana de EstadÃ­stica - Applied Statistics 46 (2023) 93-110

100                                                       Fernando A. Moala & Gustavo Moraes

where Ï†(Î¸) is given by (9).
Proof . See Tomazella et al. (2013).
                                                                                       
      Note that Reference prior is the product of Jereys prior and the term               1+
      2
                                     
   Î¸
     âˆ’Î¸ F 33 ([2, 2,2], [3, 3,3], âˆ’Î¸) .
4(1âˆ’e )

Corollary 2. The joint posterior density p(Î¸ ,Î» | t) for parameters (Î¸ , Î») under
Reference prior given in (12) is proper.
Proof . See Tomazella et al. (2013).

5. Maximal Data Information Prior (MDIP)

      It is of interesting that the data gives more information about the parameter
than the information on the prior density, otherwise, there would not be justica-
tion for the realization of the experiment. Thus, we wish a prior density Ï€ (Ï•) that
provides the gain in the information supplied by the data the largest as possible
relative to the prior information of the parameter, that is, maximizes the infor-
mation on the data. With this idea, Zellner (1977), Zellner (1984) and Zellner &
Min (1992) derived a prior which maximize the average information in the data
density f (x | Ï•) relative to that one in the prior. Let

                                          Z b
                             H(Ï•) =             f (x | Ï•) ln f (x | Ï•)dx
                                           a
be the negative entropy, the measure of the information in f (x | Ï•).              Thus, the
following functional criterion is employed in the MDIP approach:

                               Z b                         Z b
                   G[Ï€(Ï•)] =         H(Ï•)Ï€(Ï•)dÏ• âˆ’                Ï€(Ï•) ln Ï€(Ï•)dÏ•,
                                 a                          a
which is the prior average information in the data density minus the informa-
tion in the prior density.     G[Ï€ (Ï•)] is maximized by selection of Ï€ (Ï•) subject to
Rb
 a Ï€(Ï•)dÏ• = 1.
      The solution is then a proper prior density given by

                                n     o
                    Ï€(Ï•) = k exp H(Ï•) ,      a â‰¤ Ï• â‰¤ b,                                 (13)


        âˆ’1
            Rb    n     o
where k    = a exp H (Ï•) dÏ• is the normalizing constant.
      Therefore, the MDIP is a prior that leads to an emphasis on the information
in the data density or likelihood function, that is, its information is weak in com-
parison with data information. More details of the construction of this prior can
be found in Zellner (1984).



                      Revista Colombiana de EstadÃ­stica - Applied Statistics 46 (2023) 93-110

Objective Priors to Estimate the Poisson-Exponential Parameters                                101



    Zellner (1990) and Zellner (1996) show several interesting properties of MDIP
and additional conditions that can also be imposed to the approach reeting given
initial information. However, the MDIP has restrictive invariance properties.

     We suppose that we do not have much prior information available about Î¸ and
Î». Therefore, under this condition, the prior distribution MDIP for the parameters
(Î¸ , Î») of P E density (1) is also appropriated for our inference problems.


Theorem 3.            MDIP prior for (Î¸, Î») parameters of P E is given by
                          Î»Î¸        n      Î¸      
                                                                                âˆ’Î¸
                                                                                   o
      Ï€Z (Î¸, Î») âˆ               exp  âˆ’              F22 ([1, 1], [2, 2], âˆ’Î¸) âˆ’ e     .        (14)
                        1 âˆ’ eâˆ’Î¸        (1 âˆ’ eâˆ’Î¸ )

Proof . Firstly, we have to evaluate the negative entropy of the distribution
f (t | Î¸, Î»),
                                       Z âˆž         Î¸Î» exp{âˆ’Î»t âˆ’ Î¸eâˆ’Î»t } 
                        H(Î¸, Î») =            ln                              f (t)dt,
                                        0                 1 âˆ’ eâˆ’Î¸
and after some algebras,

                                              Î¸Î» 
                           H(Î¸, Î») = ln              âˆ’ Î»E(T ) âˆ’ Î¸E(eâˆ’Î»T ).
                                             1 âˆ’ eâˆ’Î¸
    From mean of the P E in (4) we have

                                Î¸Î»         Î¸
        H(Î¸, Î») = ln                âˆ’Î¸
                                       âˆ’            F22 ([1, 1], [2, 2], âˆ’Î¸) âˆ’ Î¸E(eâˆ’Î»T ),
                                1âˆ’e      (1 âˆ’ eâˆ’Î¸ )

with F22 ([1, 1], [2, 2], âˆ’Î¸) dened in (3).
                                                  Râˆž                  âˆ’Î»t
                                                                          }
    Now, to evaluate the expectance E(e      ) = 0 eâˆ’Î»t Î¸Î» exp{âˆ’Î»tâˆ’Î¸e
                                                         âˆ’Î»T
                                                                1âˆ’eâˆ’Î¸
                                                                            dt con-
                               âˆ’Î»t          âˆ’Î»T     1     1
sider the transformation u = e     then E(e     ) = Î¸ + 1âˆ’eâˆ’Î¸ .
    Hence, the entropy of P E is obtained as

                            Î¸Î»         Î¸                                    Î¸eâˆ’Î¸
    H(Î¸, Î») = ln                   âˆ’            F 22 ([1, 1], [2, 2], âˆ’Î¸) +         âˆ’ 1.      (15)
                           1 âˆ’ eâˆ’Î¸   (1 âˆ’ eâˆ’Î¸ )                             1 âˆ’ eâˆ’Î¸

    From (13) and (15), the MDIP prior for parameters of P E is obtained.


Corollary 3.          The joint posterior density for parameters (Î¸, Î») under MDIP prior
(14) is proper.
Proof . The joint posterior density for parameters (Î¸, Î») is given by
                                      n           n
                    Î¸Î» n+1     n   X           X        o   n      Î¸             o
p(Î¸, Î» | t) âˆ           âˆ’Î¸
                             exp  âˆ’Î»     t i âˆ’ Î¸     eâˆ’Î»ti exp âˆ’       âˆ’Î¸
                                                                           F22 âˆ’ eâˆ’Î¸ ,
                    1âˆ’e              i=1         i=1
                                                                 (1 âˆ’ e )

where F22 = F22 ([1, 1], [2, 2], âˆ’Î¸).
                âˆ’Î»t
    Since e           â‰¤ 1 for all ti > 0, i = 1, . . . , n, and Î» > 0, it follows that

                            Revista Colombiana de EstadÃ­stica - Applied Statistics 46 (2023) 93-110

102                                                                             Fernando A. Moala & Gustavo Moraes



  Z âˆžZ âˆž
               p(Î¸, Î» | t)dÎ¸dÎ» â‰¤
      0    0
          Z âˆž hZ âˆž             Pn       i                    Î¸    n+1         n      Î¸               o
                     Î» n+1 eâˆ’Î» i=1 ti dÎ»                                eâˆ’nÎ¸ exp âˆ’             F22 âˆ’ eâˆ’Î¸ dÎ¸.
              0           0                                1 âˆ’ eâˆ’Î¸                 (1 âˆ’ eâˆ’Î¸ )

                   Râˆž
                              Î» n+1 eâˆ’Î»
                                          Pn
                                              i=1 ti
                                                                 Î“(n     +2)
      From
                      0
                                                       dÎ» = 
                                                                Pn
                                                                                +2 we have
                                                                                n

                                                                    i=1 ti



                                       Î“(n+2)                            Î¸eâˆ’Î¸ n+1
Z âˆžZ âˆž                                                      Z âˆž                      n      Î¸             o
                  p(Î¸, Î» | t)dÎ¸dÎ» â‰¤  P        n+2                         âˆ’Î¸
                                                                                   exp âˆ’       âˆ’Î¸
                                                                                                   F22 âˆ’ eâˆ’Î¸ dÎ¸.
  0       0                             n                       0       1âˆ’e              (1 âˆ’ e )
                                        i=1 ti



                        Î¸eâˆ’Î¸
      Due to
                       1âˆ’eâˆ’Î¸
                             â‰¤ 1 then

                                        Î“(n+2)
 Z âˆžZ âˆž                                                         Z âˆž
                                                                           Î¸eâˆ’Î¸     n      Î¸              o
                   p(Î¸, Î» | t)dÎ¸dÎ» â‰¤  P       n+2                           âˆ’Î¸
                                                                                  exp âˆ’       âˆ’Î¸
                                                                                                   F22 âˆ’ eâˆ’Î¸ dÎ¸.
  0           0                         n                           0     1âˆ’e           (1 âˆ’ e )
                                        i=1 ti


      From numerical integration we have

                      Z âˆž
                                   Î¸eâˆ’Î¸      n      Î¸      
                                                                       âˆ’Î¸
                                                                          o
                                          exp  âˆ’              F 22 âˆ’ e      dÎ¸ = 0.835182,
                          0      1 âˆ’ eâˆ’Î¸         (1 âˆ’ eâˆ’Î¸ )
and therefore

                                  Z âˆžZ âˆž
                                                                     Î“(n+2)
                                                p(Î¸, Î» | t)dÎ¸dÎ» â‰¤  P         n+2 < âˆž.
                                   0      0                           n
                                                                      i=1 t i




6. Other proposed priors
      A possible simplication of Jereys prior is to consider a noninformative prior
from Ï€ (Î¸, Î»)= Ï€ (Î»| Î¸ )Ï€ (Î¸ ). Using the Jereys' rule, we have

                                                            r
                                                                     âˆ‚2        
                                              Ï€(Î¸, Î») âˆ         E           ln L  Ï€(Î¸),                       (16)
                                                                        âˆ‚Î»2
                                 
                       âˆ‚2
where E
                      âˆ‚Î»2 ln L         is given by (6) and Ï€(Î¸) is a noniformative proper prior.

      Louzada-Neto et al. (2011) consider Ï€ (Î¸ ) given by the Gamma distribution
with parameters a and  b.

Theorem 4.                     The Jereys' rule prior is given by
                                                                        1 aâˆ’1 âˆ’bÎ¸
                                                       Ï€(Î¸, Î») âˆ          Î¸  e .                              (17)
                                                                        Î»

                                    Revista Colombiana de EstadÃ­stica - Applied Statistics 46 (2023) 93-110

Objective Priors to Estimate the Poisson-Exponential Parameters                        103



Proof . In fact, from element I22 given in (6) and prior (16) with Ï€(Î¸) given by
a gamma distribution with hyperparameters a and  b the considered prior is
obtained.


   Let us denote the prior (17) as Jereys' rule.


Corollary 4.   The joint posterior density p(Î¸, Î» | t) for parameters (Î¸, Î») under
Jereys' rule prior given in (17) is proper.
Proof . See Louzada-Neto et al. (2011).
Theorem 5.    Another noninformative prior distribution is assumed considering
independence between Î¸ and Î», given by
                                                 1
                                    Ï€(Î¸, Î») âˆ      .                                  (18)
                                                Î¸Î»

   Let us denote the prior (18) as Box & Tiao prior.
Corollary 5. The joint posterior density p(Î¸,Î» | t) for parameters (Î¸, Î») under
Box & Tiao prior given in (18) is proper.
Proof . The proof is similar to that considered by Louzada-Neto et al. (2011).
   Other prior specications also could be used, as independent informative Gamma
distributions, that is,
                              Ï€Î¸ (Î¸) âˆ¼ Gamma(aÎ± , bÎ± )                                (19)

and
                              Ï€Î» (Î») âˆ¼ Gamma(aÎ» , bÎ» ),                               (20)

where aÎ¸ , bÎ¸ , aÎ» and bÎ» are known hyperparameters and Gamma(a, b) denotes a
                                                          2
gamma distribution with mean a/b and variance a/b .

   Since shape Î¸ and scale Î» parameters of P E assumes values greater than zero
then we can pre-establish as usual a Gamma prior distribution with shape and
scale hyperparametersa > 0 and b > 0, respectively. Values of a â†’ 0 and
b â†’ 0 suggest a vague (absence of information) prior distribution. Thus, the
hyperparameters in (19) and (20) could be choosen such as 0.01 or 0.001 to provide
no prior information.

   Table 1 displays the joint prior distributions obtained for the dierent ap-
proaches shown in this paper.




7. Simulated Data
   This section presents a simulation in order to compare and choose a prior
distribution proposed in this paper which better represent a situation of weak
information about the parameters.



                    Revista Colombiana de EstadÃ­stica - Applied Statistics 46 (2023) 93-110

104                                                         Fernando A. Moala & Gustavo Moraes

                               Table 1: Joint prior distributions
                 Prior                                     Ï€ (Î¸ , Î»)
                Jereys                                  Î»
                                                          1
                                                            Ï†(Î¸)
                                                    Î¸2
                                                                                       
               Reference           1
                                   Î»
                                     Ï† (Î¸ )  1 + 4(1âˆ’e âˆ’Î¸ ) 33 ([2, 2,2], [3, 3,3], âˆ’Î¸)
                                                            F
                                            n                                            o
                 MDIP             Î»Î¸
                                1âˆ’eâˆ’Î¸
                                      exp âˆ’ (1âˆ’eÎ¸âˆ’Î¸ ) F22 ([1, 1], [2, 2], âˆ’Î¸) âˆ’ eâˆ’Î¸
                Gamma                               a
                                                  Î» âˆ’1 eâˆ’bÎ» Î¸câˆ’1 eâˆ’dÎ¸
              Jereys' rule                              1  a
                                                           Î¸ âˆ’1 eâˆ’bÎ¸
                                                         Î»
              Box & Tiao                                      1
                                                             Î»Î¸




   The simulated data are generate from P E distribution with parameter values
Î¸ = 5 and Î» = 2 for dierent sample sizes, as n = 10 (small), 30, 50 (moderate),
100 and 200 (large).

      We report the average estimators, the mean square error (MSE) and the mean
bias, over 1000 generated samples for each sample size.                          The eciency for the
estimators was compared according to these measures. The results are reported
in Tables 2 and 3.

      We also need to appeal to numerical procedures to extract characteristics of
marginal posterior distributions such as Bayesian estimators and credible intervals.
We can use the MCMC algorithm to obtain a sample of Î¸ and Î» from the joint
posterior. The chain is run for 105 000 iterations with a burn-in period of 5000
and jumping 100.


       Table 2: Simulation results for dierent sample sizes and parameters Î¸ = 5.
  n      Estimate     Gamma        Jereys'rule     Jereys       Reference         MDIP       Box&Tiao
           Bias        4.7966         4.8092         3.5161         3.0793        20.4176       6.6193
  10       MSE        82.9010        84.8641        27.8572        21.3021        738.9883     440.7152
         Estimator     7.0448        7.0711          7.5547         6.8291         24.9537      8.9153

           Bias       1.6384          1.6420            1.5211         1.4486       2.3645      1.6602
  30       MSE        5.1780          5.1551            4.7750         4.2047      21.0929      5.3872
         Estimator    5.2461          5.2460            5.7086         5.4990      6.7998       5.2830

           Bias       1.1173          1.1218            1.0748         1.0493      1.2731       1.1279
  50       MSE        2.1494          2.1604            2.1012         1.9480      3.1424       2.2008
         Estimator    5.1577          5.1594            5.3817         5.2644      5.7675       5.1773

           Bias       0.7402          0.7412            0.7428         0.7266      0.8212       0.7418
 100       MSE        0.9019          0.9023            0.9198         0.8755      1.1420       0.9042
         Estimator    5.1775          5.1800            5.2746         5.2168      5.4502       5.1855

           Bias       0.4984          0.4996            0.4979         0.4944      0.5221       0.4896
 200       MSE        0.4066          0.4091            0.4108         0.4012      0.4605       0.3866
         Estimator    5.0781          5.0772            5.1240         5.0976      5.2050       5.0734

      Some of the points are quite clear from the results.                      As expected, when the
sample size increases, it is observed that the performances of all estimators become
better and closer, the MSE and biases decrease, although slowly.



                      Revista Colombiana de EstadÃ­stica - Applied Statistics 46 (2023) 93-110

Objective Priors to Estimate the Poisson-Exponential Parameters                           105


        Table 3: Simulation results for dierent sample sizes and parameters Î» = 2.
  n       Estimate    Gamma     Jereys'rule    Jereys   Reference   MDIP     Box & Tiao
            Bias      0.5627      0.5616         0.4813    0.4490     1.3930     0.5900
  10        MSE       0.5161      0.5198         0.4129    0.3538     3.1809     0.6065
          Estimator   1.9667      1.9708         2.2300    2.1430     3.3163     2.0055

            Bias       0.2877      0.2876        0.2643     0.2589    0.3414     0.2893
  30        MSE        0.1417      0.1421        0.1225     0.1168    0.2234     0.1438
          Estimator    1.9848      1.9855        2.0687     2.0383    2.2195     1.9899

            Bias       0.2120      0.2111        0.2020     0.1999    0.2251     0.2119
  50        MSE        0.0729      0.0724        0.0679     0.0658    0.0874     0.0732
          Estimator    1.9987      1.9992        2.0369     2.0196    2.1164     2.0013

            Bias       0.1460      0.1459        0.1455     0.1437    0.1567     0.1460
  100       MSE        0.0337      0.0339        0.0340     0.0330    0.0399     0.0338
          Estimator    2.0226      2.0232        2.0383     2.0298    2.0771     2.0241

            Bias       0.0973      0.0979        0.0973     0.0967    0.1007     0.0963
  200       MSE        0.0150      0.0151        0.0150     0.0148    0.0161     0.0146
          Estimator    2.0049      2.0044        2.0120     2.0080    2.0307     2.0042



   The results from Tables 2 and 3 show that the P E distribution is not indicated
for samples with size n < 30 due to the poor estimates obtained whatever prior
used. In addition, the Reference prior shown better performance than the others.

   The results of the estimation of parameter Î» given in Table 3 show practically
insignicant dierences even for sample sizes n < 30 and the values of Bias and
MSE decrease very slowly when n increases.

   Therefore, the conclusion of this analysis is that the shape parameter Î¸ must
be considered for the choice of the best prior to be used.

   Other criterion for comparison of the prior densities consists on checking the
frequentist coverage probabilities of the posterior intervals. Tables 4 and 5 illus-
trate the coverage probabilities.


         Table 4: Frequentist coverage probability of the 95% intervals for Î¸ = 5.
           n     Gammas    Jereys'rule     Jereys   Reference   MDIP    Box&Tiao
           10     0.90        0.89            0.94      0.95       0.65     0.88
           30     0.93        0.93            0.96      0.95       0.91     0.93
           50     0.94        0.94            0.95      0.96       0.93     0.95
           100    0.94        0.94            0.94      0.95       0.93     0.94
           200    0.95        0.95            0.94      0.94       0.93     0.95


   In terms of coverage probability, the simulation study indicates that the Re-
ference prior performs better than the other priors, mainly for the parameters Î¸ .
Tables 4 and 5 illustrate that the coverage probabilities are often smaller than
the nominal level for sample size n < 30, although the Reference prior performs
slightly better than the others for this case. We can also see that the MDIP prior
performs most poorly.



                      Revista Colombiana de EstadÃ­stica - Applied Statistics 46 (2023) 93-110

106                                                 Fernando A. Moala & Gustavo Moraes

         Table 5: Frequentist coverage probability of the 95% intervals for Î» = 2.
           n    Gammas     Jereys'rule   Jereys   Reference   MDIP      Box&Tiao
          10     0.93         0.94          0.94      0.97       0.64       0.93
          30     0.93         0.93          0.94      0.95       0.91       0.93
          50     0.95         0.96          0.95      0.95       0.91       0.94
          100    0.96         0.95          0.94      0.95       0.92       0.95
          200    0.95         0.96          0.94      0.96       0.94       0.96


      Some of the points are quite clear from the numerical results.

      As expected, a moderate large (n â‰¥ 50) sample size is needed to achieve the
desirable accuracy, and in this case the choice of the priors become irrelevant.

   The same result cannot be completely achieved when n is a small value (n <
30). In this case, we conclude that Jereys and Reference priors provide better
estimation than any other priors presented in the study and, consequently, both
can represent a situation of weak information a priori. Furthermore, a comparison
between Jereys and Reference priors shows that Reference prior slightly domi-
nates Jereys when n is very small. Note that MDIP prior does not provide good
estimates among all the considered class of priors.

      Therefore, based on this simulated study we recommend the Bayesian approach
with Reference prior as the best inference to estimate the parameters and these
results are of great interest in applications of the Poisson-Exponential distribution.




8. An Example With Literature Data
      The data set below was obtained from Lawless (2003). The data given arose
in tests on endurance of deep groove ball bearings. The data are the number of
million revolutions before failure for each of the 23 ball bearings in the life test
and they are 17.88, 28.92, 33.00, 41.52, 42.12, 45.60, 48.80, 51.84, 51.96, 54.12,
55.56, 67.80, 68.44, 68.64, 68.88, 84.12, 93.12, 98.64, 105.12, 105.84, 127.92, 128.04,
173.40.

      In Figure 1, we have the plots of the histogram with tted density, and empirical
with tted cumulative functions modeled by P E .           From Figure 1, we observe a
good t of the P E distribution for the data (Lawless data). Thus, based on the
plots, we can assume that P E distribution is appropriated to analyze this dataset.

      Table 6 presents the posterior mean, standard deviation and credible interval
considering each prior distribution for the parameters Î¸ and Î». From this Table
we see that the Reference prior is more appropriate for both Î¸ and Î» although
the dierences with the other priors are not so signicant as observed also in the
results obtained by the simulation given in Tables 3, 4 and 5. The exception again
is the MDIP prior, whose results are also far from.

      The graphical representations of the marginal posterior densities for the param-
eters Î¸ and Î» are shown in Figure 2. Comparing the marginal posterior densities we
can see the posteriors for both parameter are quite similar. The plots of posteriors
p(Î¸ | t) are so close such that one choice is almost impossible.


                      Revista Colombiana de EstadÃ­stica - Applied Statistics 46 (2023) 93-110

Objective Priors to Estimate the Poisson-Exponential Parameters                                                                                                    107




          0.020
          0.015
          0.010
          0.005
          0.000




                      0                                            50                                         100                                                   150
          1.0




                                                                                                                                                                     â—
                                                                                                                                                                     â—
                                                                                                                    â—
                                                                                                                    â—
                                                                                                                    â—
                                                                                                   â—
                                                                                                  â—â—
          0.8




                                                                                            â—
                                                                                            â—
                                                                                    â—
                                                                                    â—
                                                                               â—
                                                                               â—
                                                                    â—
          0.6




                                                                   â—
                                                                   â—
                                                                  â—â—
                                                                  â—â—â—
                                                          â—
                                                        â—
          0.4




                                                      â—
                                                      â— â—â—
                                                 â—    â—
                                             â—   â—
                                             â—
          0.2




                                         â—
                                        â—â—
                                        â—
                                    â—
                                â—   â—
                                â—
                      â—
          0.0




                      â—



                                                 50                                         100                                            150



Figure 1: Plots of histogram and empirical cumulative function with respective tted
                               functions.
             0.15




                                                                                                                                Jeffreys
                                                                                                                                Reference
                                                                                                                                Jefreys' rule
             0.10
p(Î¸| x)




                                                                                                                                Gammas
                                                                                                                                Boxâˆ’Tiao
             0.05
             0.00




                           0                                  5                    10                               15                                       20

                                                                                        Î¸
             60




                                                                                                                                                 Jeffreys
                                                                                                                                                 Reference
p(Î»| x)




                                                                                                                                                 Jeffreys'rule
             40




                                                                                                                                                 Gammas
                                                                                                                                                 Boxâˆ’Tiao
             20
             0




                    0.01                      0.02                      0.03                           0.04              0.05                                     0.06

                                                                                        Î»

Figure 2: Marginal posterior densities of parameters Î¸ and Î» for the data (Lawless
                               data).




                                             Revista Colombiana de EstadÃ­stica - Applied Statistics 46 (2023) 93-110

108                                                Fernando A. Moala & Gustavo Moraes

   Table 6: Estimators and 95% condence intervals of Î¸ and Î» for dierent prior.
                              Î¸                                        Î»
 Prior         Estimator   sd          CI                Estimator     sd            CI
 Gamma          6.9254   2.6523 (2.6606, 13.3033)         0.0344     0.0064   (0.0226, 0.0474)
 Jereys' rule  7.0384   2.6962 (2.7252, 12.9593)         0.0346     0.0063   (0.0221, 0.0481)
 Jereys        7.5568   2.8554 (3.2070, 14.3470)         0.0357     0.0065   (0.0242, 0.0501)
 Reference      7.0820   2.6152 (2.9928, 13.3740)         0.0348     0.0060   (0.0226, 0.0465)
 MDIP           13.7054 13.2475 (3.9285, 50.5502)         0.0428     0.0120   (0.0269, 0.0746)
 Box&Tiao       7.0629   2.6331 (2.8454, 12.9287)         0.0346     0.0063   (0.0222, 0.0463)



9. Conclusions
      In this paper, the objective priors are derived for the parameters of P E dis-
tribution.   A study to check if these priors lead to the same posterior inference
for small and moderate sample sizes is of great practical interest. This way, a si-
mulation study was performed and it indicated that the Reference prior performs
better than the other priors for both parameters Î¸ and Î».

      In all the evaluation criteria of the considered priors, Gamma, Jereys' rule and
Box&Tiao presented equal results whereas Jereys and Reference priors showed
close and better results. The MDIP prior had a worse performance in all analyzed
situations showing not to be indicated for Bayesian analysis of the P E distribution.

      The results from Tables 2 and 3 show that the Poisson-Exponential distribution
should not be indicated for dataset with size n < 30 due to the poor estimates
obtained whatever prior used.

      In addition to the results obtained from the simulation study of objective priors
treated in this paper for the specic case of the Poisson-Exponential distribution,
it is also worth highlighting the advantages and disadvantages of using these priors
in the general case, as we will see below.

      Jereys prior is quite universal and invariant in the sense of yielding proper-
ly tranformed priors under reparametrization, however, Jereys himself noticed
diculties with the method when the parameter is multi-dimensional.

      Reference prior provides one of the most successful general methods to de-
rive noninformative prior distributions. In practice, however, reference priors are
typically dicult to use. Undesirable properties include lack of invariance to repa-
rameterization and nonuniqueness of prior due to the choice of the parameter of
interest.

      The MDIP prior provides a fresh view and operational results for the problem
of selecting diuse prior. Side conditions reecting initial information that may
be available can readily be used to derive it. Therefore, MDIP prior can be quite
useful for problems where moments are known. As for the invariance, the MDIP
prior is invariant for a class of transformations much smaller than the Jereys
prior. A serious problem with the MDIP prior is that it can lead to an improper
posterior density more often than other objective priors for reliability distributions,
for instance with Gamma and Generalized Exponential distributions.



                      Revista Colombiana de EstadÃ­stica - Applied Statistics 46 (2023) 93-110

Objective Priors to Estimate the Poisson-Exponential Parameters                         109

                 
                  Received: May 2022  Accepted: November 2022
References
Belaghi R, Asl M, Alma O, Singh S, Vas, M. Estimation and prediction for the poisson-exponential distribution based on type-ii censored data.(2019).  American Journal of Mathematical and Management Sciences.
Berger J O, Bernardo J M. On the development of the reference prior method.(1992).  Bayesian Statistics.
Bernardo J M. Reference posterior distributions for bayesian inference.(1979). Journal Royal Statistical Society.
Box G E P, Tiao G C. Bayesian inference in statistical analysis.(1973)..
Cancho V G, Louzada-Neto F, Barriga G D C. The poisson-exponential lifetime distribution.(2011). Computational Statistics and Data Analysis.
Jefreys S H. Theory of probability.(1967). Oxford U Press.
Lawless J F. Statistical Models and Methods for Lifetime Data.(2003). Wiley.
Louzada-Neto F, Cancho V G, Barriga G D C. The poisson-exponential distribution: a bayesian approach.(2011). Journal of Applied Statistics.
Rodrigues G, Louzada F, Ramos P. Poisson-exponential distribution: diferent methods of estimation.(2018). Journal of Applied Statistics.
Singh S, Singh U, Kumar M. Estimation for the parameter of poisson- exponential distribution under bayesian paradigm.(2014).Journal of Data Science.
Tibshirani R. Noninformative priors for one parameters of many.(1987). Biometrika.
Tomazella V L D, Cancho V G, Louzada-Neto F. Bayesian reference analysis for the poisson-exponential lifetime distribution.(2013). Chilean Journal of Statistics.
Zellner A. Maximal data information prior distributions.(1977). New methods in the applications of bayesian methods.
Zellner A. Maximal data information prior distributions.(1984). Basic Issues in Econometrics.
Zellner A. Bayesian methods and entropy in economics and econometrics-Maximum Entropy and Bayesian Methods.(1990). Dordrecht, Netherlands: Kluwer Academic Publishers.
Zellner A. Models, prior information and bayesian analysis.(1996). Journal of Econometrics.
Zellner A, Min C-K. Bayesian analysis model selection and prediction.(1992). University of Chicago.