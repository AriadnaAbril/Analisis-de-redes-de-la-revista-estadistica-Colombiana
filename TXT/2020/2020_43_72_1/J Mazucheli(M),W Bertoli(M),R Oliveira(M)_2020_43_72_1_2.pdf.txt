Two Useful Discrete Distributions to Model Overdispersed Count Data.  Dos distribuciones discretas útiles para modelar datos de recuento sobredispersos
State University of Maringá, Maringá, Brazil. Federal University of Technology - Paraná, Curitiba, Brazil. University of São Paulo, Ribeirão Preto, Brazil
Abstract
The methods to obtain discrete analogs of continuous distributions have been widely considered in recent years. In general, the discretization process provides probability mass functions that can be competitive with the traditional model used in the analysis of count data, the Poisson distribution. The discretization procedure also avoids the use of continuous distribution in the analysis of strictly discrete data. In this paper, we seek to introduce two discrete analogs for the Shanker distribution using the method of the infinite series and the method based on the survival function as alternatives to model overdispersed datasets. Despite the diﬀerence between discretization methods, the resulting distributions are interchangeable. However, the distribution generated by the method of the infinite series method has simpler mathematical expressions for the shape, the generating functions, and the central moments. The maximum likelihood theory is considered for estimation and asymptotic inference concerns. A simulation study is carried out in order to evaluate some frequentist properties of the developed methodology. The usefulness of the proposed models is evaluated using real datasets provided by the literature.
Key words: Maximum likelihood estimation; Discrete distributions; Monte Carlo simulation; Overdispersion; Shanker distribution.
Resumen
Los métodos para obtener análogos discretos de distribuciones continuas han sido ampliamente considerados en los últimos años. En general, el proceso de discretización proporciona funciones de probabilidad en masa que pueden ser competitivas con el modelo tradicional utilizado en el análisis de datos de conteo, la distribución de Poisson. El procedimiento de discretización también evita el uso de la distribución continua en el análisis de datos estrictamente discretos. En este artículo, intentamos introducir dos análogos discretos para la distribución de Shanker utilizando el método de la serie infinita y el método basado en la función de supervivencia como alternativas para modelar conjuntos de datos sobre dispersados. A pesar de la diferencia entre los métodos de discretización, las distribuciones resultantes son intercambiables. Sin embargo, la distribución generada por el método de series infinitas tiene expresiones matemáticas más simples para la forma, las funciones de generación y los momentos centrales. La teoría de máxima verosimilitud se considera para la estimación y las preocupaciones de inferencia asintótica. Se lleva a cabo un estudio de simulación para evaluar algunas propiedades frecuentistas de la metodología desarrollada. La utilidad de los modelos propuestos se evalúa utilizando conjuntos de datos reales proporcionados por la literatura.
Palabras clave: Estimación de máxima verosimilitud; Distribuciones discretas; Distribución de Shanker; Simulación del Monte Carlo; Sobredispersión.



1. Introduction
In recent decades, the building of a probabilistic distribution by discretization of a
continuous random variable has been widely addressed in the literature. The main
purpose of the discretization is to generate distributions that can be used for the
analysis of strictly discrete data. For example, in survival analysis is common to
use continuous distributions to model discrete data, so the discretization acts as
a subterfuge to avoid this process. Several applications where continuous distri-
butions were used to model discrete data can be found in Klein & Moeschberger
(1997), Meeker & Escobar (1998), Kalbfleisch & Prentice (2002), Lee & Wang
(2003), Lawless (2003), Collett (2003), Hamada, Wilson, Reese & Martz (2008),
among others. A complete survey regarding all discretization methods and some
discretized distributions can be found in Chakraborty (2015a).
    One of the first proposed discretization methods is based on the definition of
a probability mass function (pmf) that depends on an infinite series. The first
traces of this method were presented by Good (1953), who proposed the discrete
Good distribution to model population frequencies of species. Such an approach
was considered by other authors to define discrete analogs, and we will point out a
few. Haight (1957) proposed the discrete Pearson III distribution to model queues
with baking and Siromoney (1964) introduced the Dirichlet’s Series distribution
as an alternative model to describe the frequency of wet days (rain-spells). After a
long break, this method was revived by Kemp (1997) that formally introduced the
discrete Normal distribution and derived its main statistical properties. The dis-

                                         Revista Colombiana de Estadística 43 (2020) 21–48

Two Useful Discrete Distributions to Model Overdispersed Count Data                  23

crete Exponential distribution was proposed by Sato, Ikota, Sugimoto & Masuda
(1999) to describe the defect count frequencies on wafers or chips. Bi, Faloutsos
& Korn (2001) introduced the discrete Lognormal distribution and with applica-
tion to internet clickstream data, among others. Inusah & Kozubowski (2006)
presented the discrete Laplace distribution discussing that, relative to the dis-
crete Normal, the proposed model has closed forms for the pmf, for the generating
functions and the central moments. The skewed version of the discrete Laplace
distribution was proposed by Kozubowski & Inusah (2006). Further, Kemp (2008)
introduced the discrete Half-Normal distribution studying its relation with other
distributions and Nekoukhou, Alamatsaz & Bidram (2012) proposed the discrete
Generalized Exponential distribution as an attempt to model ranking frequencies
of graphemes in the Slovene language.
    Another popular method to obtain discrete analogs of continuous random vari-
ables is the one based on the survival function (sf) of the original distribution.
This method was proposed by Nakagawa & Osaki (1975) and has the interest-
ing feature of preserving the original sf of its integer part for the generated pmf
(Kemp 2004, Chakraborty 2015a). Several authors also considered the discretiza-
tion method based on the sf, and we will point out a few. Nakagawa & Osaki
(1975) proposed the discrete Weibull distribution and discussed its main prop-
erties. The Geometric-Weibull distribution considering a discrete analog for the
Weibull component was introduced by Bracquemond & Gaudoin (2003). Roy
(2004) proposed the discrete Rayleigh distribution and presented its usefulness
in the stress-strength analysis. The discrete Burr and Pareto distributions were
introduced by Krishna & Pundir (2009) for application in reliability estimation in
series systems. Jazi, Lai & Alamatsaz (2010) proposed the discrete Inverse Weibull
distribution and discussed diﬀerent estimation methods for the model parameters.
Gómez-Déniz & Calderín-Ojeda (2011) introduced the discrete Lindley distribu-
tion and illustrated its application using an automobile claim frequency data.
The discrete Gamma distribution was proposed by Chakraborty & Chakravarty
(2012), which derived several statistical properties of this model. Besides, Nek-
oukhou, Alamatsaz & Bidram (2013) presented the discrete Type II Generalized
Exponential distribution, and Hussain & Ahmad (2014) introduced the discrete
Inverse Rayleigh distribution as alternatives to model overdispersed count data.
    The main aim of this paper is to use the methods of infinite series and of
the sf to propose discrete analogs for the Shanker distribution, which is a one-
parameter lifetime model proposed by Shanker (2015). We expect the proposed
models to be suitable alternatives to model overdispersed count datasets. The
Shanker distribution can be seen as a modification of the one-parameter Lindley
distribution (Ghitany, Atieh & Nadarajah 2008).
   A continuous random variable X is said to have Shanker distribution if its
probability density function (pdf) can be written as
                                   θ2
                   fX (x; θ) =          (θ + x) e−θx ,   x ∈ R+ ,                   (1)
                                 θ2 + 1
where θ ∈ R+ is the shape parameter. The author has shown that this model is
a two component mixture of an Exponential distribution with scale parameter θ

                                       Revista Colombiana de Estadística 43 (2020) 21–48

24                                Josmar Mazucheli, Wesley Bertoli & Ricardo Oliveira


and a Gamma distribution having shape parameter 2 and scale parameter θ, with
mixing proportions given, respectively, by θ2 ν −1 and ν −1 , where ν = θ2 + 1. For
the one-parameter Lindley distribution the mixing proportions are, respectively,
θν −1 and ν −1 , where ν = θ + 1.
    A comprehensive discussion about the statistical properties of the Shanker
distribution, such as moments, hazard function, stochastic orderings, parameter
estimation, among others is also presented on the mentioned paper. The corre-
sponding sf is given by
                               [          ]
                                     θx
                    SX (x; θ) = 1 + 2       e−θx ,  x ∈ R+ ,                (2)
                                    θ +1

for θ ∈ R+ .
    This paper is organized as follows. In Section 2, we briefly present the methods
of infinite series and of the sf to define discrete analogs of continuous distributions.
In Section 3, we introduce two types for the discrete Shanker distribution and
derive the main statistical properties of each model. In Section 4, the problem
of estimating the parameter of the proposed models is addressed, and inference
procedures are discussed. In Section 5, a simulation study is conducted in order to
evaluate the performance of the presented methodology. In Section 6, applications
of the proposed models to real datasets are considered to illustrate its usefulness.
Concluding remarks are addressed in Section 7.


2. Discretization Methods
In this section, we present two discretization methods that will be considered to
obtain discrete analogs for the Shanker distribution. It is important to point out
that the paper of Chakraborty (2015a) is possibly the only paper with exhaustive
discussion on various methods of discretization.


2.1. Discretization by Infinite Series
The method of discretization by infinite series was firstly considered by Good
(1953), which has proposed the discrete Good distribution to model population
frequencies of species. A random variable Y is said to have a discrete Good
distribution if its pmf can be written as

                                         δy yα
                     P (Y = y; α, δ) = ∑∞ j α ,           y ∈ Z+ ,
                                        j=1 δ j

for α ∈ R and δ ∈ (0, 1). The method of infinite series is characterized by the
following definition.
   Let X be a continuous random variable. If X has probability density function
(pdf) fX (x; θ) with support on R, then the corresponding discrete random variable
Y has pmf given by

                                        Revista Colombiana de Estadística 43 (2020) 21–48

Two Useful Discrete Distributions to Model Overdispersed Count Data                    25

                                    f (y; θ)
                   P (Y = y; θ) = ∑∞ X            ,         y ∈ Z,
                                   j=−∞ fX (j; θ)

where θ is the vector of parameters indexing the distribution of X.
    This method was studied by several authors, including Kulasekera & Tonkyn
(1992), Doray & Luong (1997), Kemp (1997) and Sato et al. (1999), which pro-
posed a version of the method when the continuous random variable of interest is
defined on R+ . Thus, if the random variable X is defined on R+ , the pmf of Y
becomes
                                     f (y; θ)
                    P (Y = y; θ) = ∑∞X            ,  y ∈ Z+ .               (3)
                                    j=0 fX (j; θ)


    One of the most recent examples of the use of this method is that provided
is the discrete analogue of the generalized Exponential distribution introduced by
Nekoukhou et al. (2012) having pmf
                                           [∞ (                ]−1
                                            ∑ α − 1 ) (−1)j λj
                                   x α−1
   P (Y = y; α, λ) = λ x−1
                             (1 − λ )                                  ,    y ∈ Z+ ,
                                            i=1
                                                   j     1 − λ1+j

for α ∈ R+ and λ ∈ (0, 1).
   A possible drawback of such method is the fact that, in some instances, the
generated pmf may have no closed form, which is the case of the generalized
Exponential model. However, it will be shown that this is not the case when
obtaining the discrete analog for the Shanker distribution by this method.


2.2. Discretization by Survival Function
The method of discretization by sf was proposed by Nakagawa & Osaki (1975).
This method allows us to discretize a continuous random variable from its sf.
Several properties of the survival and of the risk functions were studied by Brac-
quemond & Gaudoin (2003), Roy (2003), Kemp (2004), Chakraborty (2015a),
among others. The most important feature of this method is that it preserves
the original sf on its integer part for the generated pmf (Chakraborty 2015a).
Some other contributions in this area are given by Chakraborty & Chakravarty
(2016), Chakraborty (2015b), Chakraborty & Gupta (2015) and Chakraborty &
Chakravarty (2012). According to Roy (2003), we can define a discrete random
variable from a continuous one as follows.
   Let X be a continuous random variable. If X has sf SX (x; θ), then the discrete
random variable Y = ⌊X⌋ has pmf given by

               P (Y = y; θ) = SX (y; θ) − SX (y + 1; θ) ,       y ∈ Z+ ,               (4)

where ⌊·⌋ denotes the floor function, which returns the highest integer value smaller
or equal to its argument.

                                        Revista Colombiana de Estadística 43 (2020) 21–48

26                                Josmar Mazucheli, Wesley Bertoli & Ricardo Oliveira


   It is noteworthy to mention that if the original sf has closed form, then the
generated pmf will also have. For example, the Weibull distribution with pdf
                                                (    )θ
                                           x
                                   θ θ−1 − µ
                    fX (x; µ, θ) = θ x e                  ,   x ∈ R+ ,
                                  µ

and sf
                        SX (x; µ, θ) = e−t( µ ) ,
                                               x θ
                                                          x ∈ R+ ,

where θ, µ ∈ R+ are, respectively, the shape and the scale parameters, was one
of the first discretized distributions by this method. Nakagawa & Osaki (1975)
proposed the discrete Weibull distribution which pmf for the random variable
Y = ⌊X⌋ is given by
                                         y θ         y+1 θ
                 P (Y = y; µ, θ) = e−( µ ) − e−( µ ) ,           y ∈ Z+ ,

for (θ, µ) ∈ R+
              2
                . It is straightforward to prove that the above equation correspond
to a proper pmf since it involves simple exponential terms.



3. The Discrete Shanker Distribution
In this section, we will consider both methods previously presented to define dis-
crete analogs for the Shanker distribution. For ease of notation, each probabilistic
model provided by these methods will be denoted by T1DS (Type I Discrete
Shanker) and T2DS (Type II Discrete Shanker) distributions, respectively. For
each version of this model, the main statistical properties as the shape, the gener-
ating functions and the central moments will be discussed.


3.1. Type I. Discrete Shanker Distribution

By considering equation (3), one can define the one-parameter T1DS distribution.
We have the following definition.
    Let X be a continuous random variable having Shanker distribution (1) with
parameter θ ∈ R+ . Let h (z) = ez − 1, z ∈ R. The pmf of Y having T1DS
distribution is given by

                                  h2 (θ)
               P (Y = y; θ) =              (θ + y) e−θ(y+1) ,        y ∈ Z+ ,        (5)
                                θh (θ) + 1

for θ ∈ R+ .

Proposition 1. The equation (5) is a proper pmf.

                                        Revista Colombiana de Estadística 43 (2020) 21–48

Two Useful Discrete Distributions to Model Overdispersed Count Data                       27
                                           ∑∞
Proof . Here we have to prove that            y=0 P (Y   = y; θ) = 1 for θ ∈ R+ . Then,
               ∞
               ∑                      ∞
                                      ∑     h2 (θ)
                     P (Y = y; θ) =                  (θ + y) e−θ(y+1)
               y=0                    y=0
                                          θh (θ) + 1
                                                 { ∞               ∞
                                                                             }
                                      h2 (θ) e−θ     ∑            ∑
                                                           −θy           −θy
                                    =              θ     e     +      ye
                                      θh (θ) + 1     y=0          y=0
                                                 { θ               }
                                      h2 (θ) e−θ    θe         eθ
                                    =                    + 2
                                      θh (θ) + 1 h (θ) h (θ)
                                    = 1,

which concludes the proof.

    For a random variable Y behaving accordingly a T1DS distribution, we will
adopt the notation Y ∼ T1DS (θ). The pmf (5) does not involves complicated
expressions and therefore, the probabilities can be straightforwardly computed, as
for example
                                            θh2 (θ) e−θ
                            P (Y = 0; θ) =              ,
                                             θh (θ) + 1
for θ ∈ R+ . Figure 1 depicts the behavior of the pmf (5) for selected values of θ.
   We have derived some theoretical properties of the T1DS distribution. These
properties are stated in the following propositions.

Proposition 2. Let Y ∼ T1DS (θ). The sf of Y is given by

                                  e−θy [1 − (θ + y) h (−θ)]
                     S (y; θ) =                             ,    y ∈ Z+ ,
                                          θh (θ) + 1

for θ ∈ R+ .

Proof . By definition, S (k; θ) = P (Y > k; θ) = 1 − P (Y 6 k; θ). Then,

                         ∑
                         k
                          h2 (θ)
       S (k; θ) = 1 −              (θ + y) e−θ(y+1)
                    y=0
                        θh (θ) + 1
                               { k                       }
                    h2 (θ) e−θ     ∑          ∑ k
                                        −θy          −θy
                =1−              θ     e    +     ye
                    θh (θ) + 1     y=0        y=0
                               { (            )               (         )}
                    h2 (θ) e−θ θ eθ − e−θk        ke−θk     eθ 1 − e−θk
                =1−                             −         +
                    θh (θ) + 1        h (θ)        h (θ)        h2 (θ)
                     e−θy [1 − (θ + y) h (−θ)]
                =                              ,
                             θh (θ) + 1

which concludes the proof.



                                            Revista Colombiana de Estadística 43 (2020) 21–48

28                                      Josmar Mazucheli, Wesley Bertoli & Ricardo Oliveira

     PMF                                                 PMF
 0.18       ●   ●                                    0.40   ●




                    ●


 0.13                                                0.30       ●

                        ●



        ●
 0.09                       ●                        0.20
                                                                    ●
                                ●


 0.04                               ●                0.10
                                                                        ●
                                        ●
                                            ●
                                                                            ●
                                                 ●
                                                                                ●
                                                                                    ●   ●
 0.00                                                0.00                                   ●   ●   ●



        0   1   2   3   4   5   6   7   8   9   10          0   1   2   3   4   5   6   7   8   9   10
                            x                                                   x
     PMF                                                 PMF
 0.65   ●                                            0.80   ●




 0.49                                                0.60




 0.33                                                0.40

            ●



 0.16                                                0.20
                                                                ●

                ●

                    ●                                               ●
                        ●                                               ●
 0.00                       ●   ●   ●   ●   ●    ●   0.00                   ●   ●   ●   ●   ●   ●   ●



        0   1   2   3   4   5   6   7   8   9   10          0   1   2   3   4   5   6   7   8   9   10
                            x                                                   x
Figure 1: Behavior of the pmf of the T1DS distribution (upper-left-panel: θ = 0.5,
          upper-right-panel: θ = 1.0, lower-left-panel: θ = 1.5 and lower-right-panel:
          θ = 2.0).



Proposition 3. Let Y ∼ T1DS (θ). The probability generating function (pgf) of
Y is given by                       [ (        )    ]
                              h2 (θ) θ eθ − s + s
                      G (s) =                      2 ,
                              [θh (θ) + 1] (eθ − s)
for s ̸= eθ .




                                                Revista Colombiana de Estadística 43 (2020) 21–48

Two Useful Discrete Distributions to Model Overdispersed Count Data                 29
                                ( )
Proof . By definition, G (s) = E sY . For s ̸= eθ , we have that
                          ∞
                          ∑      h2 (θ)
                G (s) =       sy           (θ + y) e−θ(y+1)
                        y=0
                               θh (θ) +  1
                                    { ∞                 ∞
                                                                    }
                        h2 (θ) e−θ      ∑              ∑
                                              y −θy          y −θy
                      =               θ      s e    +      ys e
                        θh (θ) + 1      y=0            y=0
                                    {                             }
                        h2 (θ) e−θ         θ            se−θ
                      =                           +
                        θh (θ) + 1 1 − se−θ         (1 − se−θ )
                                                                2
                               [  (       )     ]
                        h2 (θ) θ eθ − s + s
                      =                        2 ,
                         [θh (θ) + 1] (eθ − s)

which concludes the proof.

Proposition 4. Let Y ∼ T1DS (θ). The moment generating function (mgf) of Y
is given by                         [ (        )      ]
                             h2 (θ) θ eθ − et + et
                     M (t) =                         2 ,                (6)
                              [θh (θ) + 1] (eθ − et )
for t ̸= θ.
                                ( )
Proof . By definition, M (t) = E etY . For t ̸= θ, we have that
                         ∞
                         ∑       h2 (θ)
               M (t) =       ety           (θ + y) e−θ(y+1)
                       y=0
                               θh (θ) + 1
                                   { ∞                  ∞
                                                                      }
                       h2 (θ) e−θ      ∑               ∑
                     =               θ      e−y(θ−t) +     ye−y(θ−t)
                       θh (θ) + 1      y=0             y=0
                                   {                                  }
                                −θ
                         2
                       h (θ) e              θ            e−(θ−t)
                     =                             +(              )2
                       θh (θ) + 1 1 − e−(θ−t)          1 − e−(θ−t)
                              [ (         )     ]
                       h2 (θ) θ eθ − et + et
                     =                         2 ,
                        [θh (θ) + 1] (eθ − et )

which concludes the proof.

Proposition 5. Let Y ∼ T1DS (θ). The cumulant generating function (cgf) of Y
is given by
                              [ (       )    ]                          (       )
   C (t) = 2 log [h (θ)] + log θ eθ − et + et − log [θh (θ) + 1] − 2 log eθ − et ,

for t ̸= θ.

Proof . Straightforward. Since C (t) = log [M (t)], the result follows.

                                      Revista Colombiana de Estadística 43 (2020) 21–48

30                                    Josmar Mazucheli, Wesley Bertoli & Ricardo Oliveira


Proposition 6. Let Y ∼ T1DS (θ). The characteristic function (cf) of Y is given
by                                  [ (         )       ]
                              h2 (θ) θ eθ − eit + eit
                      ϕ (t) =                          2 ,
                               [θh (θ) + 1] (eθ − eit )
                 √
for t ∈ R and i = −1 is the imaginary number.

Proof . Straightforward. The result is obtained by noticing that ϕ (t) = M (it).


   It can be easily noticed that equation (6) is infinitely diﬀerentiable on t, since it
involves exponential terms of its argument. Thus, from Proposition 4, the ordinary
moments of Y can be derived by
                          { r                                 }
         ′      h2 (θ)       d       1       dr+1        1
       µr =                θ r θ          +                        ,    r > 1,
             [θh (θ) + 1]    dt (e − et ) dtr+1 (eθ − et ) t=0

for θ ∈ R+ . Hence, the mean (µ) and the variance (σ 2 ) of Y are given, respectively,
by                                            (       )
                                 ′   θh (θ) + eθ + 1
                           µ = µ1 =                     ,
                                     [θh (θ) + 1] h (θ)
and                                       [ 2 2     (      )          ]
                                           θ h (θ) + eθ + 3 θh (θ) + 2 eθ
                       = µ′2 − (µ′1 ) =
                   2                 2
               σ                                             2               .
                                                 [θh (θ) + 1] h2 (θ)
    A normalized measure of dispersion can be obtained by using the variance-to-
mean relationship. This measure is the well-known index of dispersion (ID) which,
in this case, is given by
                               [ 2 2        (      )            ]
                         σ2     θ h (θ) + eθ + 3 θh (θ) + 2 eθ
                    ID =    =                                        .        (7)
                          µ   [θh (θ) + 1] [θh (θ) + (eθ + 1)] h (θ)

     Analogously, the coeﬃcient of variation (CV) of Y has the form
                                 √
                        σ    eθ/2 [θ2 h2 (θ) + (eθ + 3) θh (θ) + 2]
                  CV = =                                            .
                        µ              θh (θ) + (eθ + 1)

     Another useful measure is the zero-modification (ZM) index

                               ZM = 1 + µ−1 log [P (Y = 0)] ,

which is defined based on the Poisson distribution. This index can be easily
interpreted since ZM > 0 indicates zero-inflation, ZM < 0 indicates zero-deflation
and ZM = 0 indicates no zero-modification. For the T1DS distribution, we have
that the ZM index is given by

                 [log (θ) + 2 log [h (θ)] − log [θh (θ) + 1] − θ] [θh (θ) + 1] h (θ)
      ZM = 1 +                                                                       .    (8)
                                          θh (θ) + (eθ + 1)

                                             Revista Colombiana de Estadística 43 (2020) 21–48

Two Useful Discrete Distributions to Model Overdispersed Count Data                      31

   The asymmetry degree and the flatness of a probabilistic model are usually
measured by its coeﬃcients of skewness and kurtosis, respectively. The first one
can be computed by the third central moment, normalized by the variance raised
to the power 3/2 and the latter is given by the fourth central moment divided
by the square of the variance. These coeﬃcients are essential to characterize the
shape of any distribution but, for the T1DS model, extensive and very complicated
expressions were obtained for such measures. For this reason, the expressions of
these coeﬃcients are omitted here. However, Table 1 summarizes, for selected val-
ues of θ, the nature and the behavior of these coeﬃcients along with the measures
previously stated in the propositions.
         Table 1: Theoretical descriptive statistics under T1DS distribution.
                                             Measures
         θ
              Mean     Variance    ID         CV         ZM      Skewness   Kurtosis
        0.5   3.4604    8.0707    2.3322     0.8208     0.3239    1.3979    5.9460
        1.0   1.1639    1.8413    1.5820     1.1658     0.2119    1.5946    6.5432
        1.5   0.4938    0.6527    1.3209     1.6351     0.1344    2.0171    8.2771
        2.0   0.2405    0.2842    1.1819     2.2170     0.0819    2.5495    10.9373
        2.5   0.1271    0.1404    1.1048     2.9487     0.0492    3.2249    15.0018
        3.0   0.0703    0.0748    1.0616     3.8808     0.0295    4.1028    21.5466


    When assessing equation (8) more deeply, we have obtained that ZM → 0 as
θ → ∞ and ZM → 1 as θ → 0. This implies that, besides the usual case (ZM = 0),
the T1DS distribution is suitable to deal with zero-inflation, but is not indicated
to modeling zero-deflated datasets. Further, the coeﬃcient of variation, the coeﬃ-
cient of skewness, and the coeﬃcient of kurtosis are increasing as θ increases. On
the other hand, the larger values for the mean, variance and index of dispersion
are obtained for small values of θ. The T1DS distribution may be considered an
interesting alternative to model overdispersed datasets (σ 2 > µ). This can be seen
by noticing that the index of dispersion is a decreasing function on θ and also
ID → 1 as θ → ∞ and ID → ∞ as θ → 0+ . In fact, limθ→0 ID is undefined since
the lateral limits are not equal, but the results presented in Table 1 allow us to
conclude that ID increases as θ approaches zero. Therefore, as ID is always greater
than 1, we conclude that σ 2 > µ for all θ ∈ R+ .
Proposition 7. Let Y ∼ T1DS (θ). The mode (y0 ) of Y is given by
                           {
                             0,        if r (θ) < 0
                      y0 =                                                              (9)
                             ⌈r (θ)⌉ , if r (θ) > 0,

where r (θ) = h−1 (θ) − θ is a real-valued threshold and ⌈·⌉ is the ceiling function,
which returns the lowest integer value greater or equal to its argument. If r (θ) = k,
k ∈ Z+ , then the T1DS distribution is bimodal with modes k and k + 1.

Proof . The ratio of consecutive probabilities is given by
                                   [             ]
               P (Y = y + 1; θ)             1
                                = 1+               e−θ ,   y ∈ Z+ ,                    (10)
                 P (Y = y; θ)           (θ + y)

                                           Revista Colombiana de Estadística 43 (2020) 21–48

32                                 Josmar Mazucheli, Wesley Bertoli & Ricardo Oliveira


for θ ∈ R+ . From (10), it is clear that P (Y = y + 1; θ) = P (Y = y; θ) if y =
r (θ) = h−1 (θ) − θ. More generally, we have the following relations

     i) P (Y = y + 1; θ) < P (Y = y; θ) if y > r (θ);

     ii) P (Y = y + 1; θ) = P (Y = y; θ) if y = r (θ);

 iii) P (Y = y + 1; θ) > P (Y = y; θ) if y < r (θ).

    Now, let k ∈ Z+ . By (i), if r (θ) < 0 then y0 = 0 since y ∈ Z+ . If r (θ) > 0 and
r (θ) ̸= k then P (Y = r (θ) ; θ) = 0 and therefore, by (i) and (iii), y0 = ⌈r (θ)⌉,
that is, P (Y = ⌈r (θ)⌉ ; θ) > P (Y = k; θ) for all k ̸= ⌈r (θ)⌉. Finally, if r (θ) > 0
and r (θ) = k then

                        (iii)             (ii)                 (i)
       P (Y = k − 1; θ) < P (Y = k; θ) = P (Y = k + 1; θ) > P (Y = k + 2; θ) ,

and therefore, both k and k + 1 are modes, implying bimodality. This concludes
the proof.

Proposition 8. The T1DS distribution has an increasing hazard rate.

Proof . Since (10) is a decreasing function on y, P (Y = k; θ) is log-concave and
therefore, the T1DS distribution has an increasing hazard rate. Hence, the proof.


    For instance, if θ = 0.5 then r(θ) ≈ 1.04 and hence, y0 = 2, as can be seen in the
upper-left-panel of the Figure 1. In addition, it can also be proved that equation
(5) satisfies P2 (Y = y; θ) > P (Y = y − 1; θ) P (Y = y + 1; θ) for r (θ) ̸= k, which
implies unimodality (see Theorem 3 by Keilson & Gerber (1971)). The relation-
ship between log-concavity, unimodality and increasing hazard rate of discrete
distributions has been discussed by Grandell (1997).

Proposition 9. The T1DS distribution has heavy tails as θ approaches zero.

Proof . The heavy-tail (HT) index is defined by the ratio (10) when y → ∞. A
discrete distribution is said to have heavy tails if HT → 1 when y → ∞. Hence,
                             {          [             ]}
                                                1
              lim HT = lim e−θ lim 1 +                   = lim e−θ = 1,
             θ→0         θ→0       y→∞       (θ + y)       θ→0


which concludes the proof.


3.2. Type II. Discrete Shanker Distribution
By considering equations (2) and (4), one can define the one-parameter T2DS
distribution. We have the following definition.

                                         Revista Colombiana de Estadística 43 (2020) 21–48

Two Useful Discrete Distributions to Model Overdispersed Count Data                      33

   Let X be a continuous random variable having Shanker distribution (1) with
parameter θ ∈ R+ . Let h (z1 , z2 ) = 1 + z1 (z1 + z2 ), z1 ∈ R+ and z2 ∈ Z+ . The
pmf of Y = ⌊X⌋ having T2DS distribution is given by

                                e−θy [                           ]
              P (Y = y; θ) =    2
                                      h (θ, y) − h (θ, y + 1) e−θ ,       y ∈ Z+ ,     (11)
                               θ +1
for θ ∈ R+ .

Proposition 10. The equation (11) is a proper pmf.

Proof . The result comes analogous to the proof of Proposition 1.

   For a random variable Y distributed accordingly a T2DS distribution, we will
adopt the notation X ∼ T2DS (θ). For this version, the probabilities can be easily
computed as noticed for the T1DS distribution. Then,
                                          1 [                        ]
                     P (Y = 0; θ) =           h (θ, 0) − h (θ, 1) e−θ ,
                                       θ2 + 1
for θ ∈ R+ . Figure 2 depicts the behavior of the pmf (11) of Y , using selected
values for θ.
   We have also derived some theoretical properties of the T2DS distribution.
These properties are stated in the following propositions.

Proposition 11. Let Y ∼ T2DS (θ). The sf of Y is given by

                                   h (θ, y + 1) e−θ(y+1)
                      S (y; θ) =                         ,    y ∈ Z+ ,
                                           θ2 + 1
for θ ∈ R+ .

Proof . The result comes analogous to the proof of Proposition 2.

Proposition 12. Let Y ∼ T2DS (θ). The pgf of Y is given by
                 ( 2   )(      ) [(         )               ]
                  θ + 1 e2θ + s − θ2 + 1 (s + 1) − θ (s − 1) eθ
         G (s) =                                  2             ,
                                (θ2 + 1) (s − eθ )

for s ̸= eθ .

Proof . The result comes analogous to the proof of Proposition 3.

Proposition 13. Let Y ∼ T2DS (θ). The mgf of Y is given by
                    ( 2   ) [( θ         )         ]
                     θ +1     e − et − 1 eθ + et + θeθ (et − 1)
           M (t) =                                   2          ,                      (12)
                                 (θ2 + 1) (et − eθ )

for t ̸= θ.

                                           Revista Colombiana de Estadística 43 (2020) 21–48

34                                        Josmar Mazucheli, Wesley Bertoli & Ricardo Oliveira

     PMF                                                    PMF
 0.19         ●                                         0.45   ●

                  ●


        ●

 0.14                 ●                                 0.34

                                                                   ●

                          ●

 0.09                                                   0.22
                              ●

                                                                       ●
                                  ●
 0.05                                                   0.11
                                      ●
                                                                           ●
                                          ●
                                              ●
                                                   ●                           ●
                                                                                   ●
                                                                                       ●   ●
 0.00                                                   0.00                                   ●   ●   ●



        0     1   2   3   4   5   6   7   8   9   10           0   1   2   3   4   5   6   7   8   9   10
                              x                                                    x
     PMF                                                    PMF
 0.67   ●                                               0.81   ●




 0.51                                                   0.61




 0.34                                                   0.41


              ●


 0.17                                                   0.20
                                                                   ●


                  ●

                      ●                                                ●
 0.00                     ●   ●   ●   ●   ●   ●    ●    0.00               ●   ●   ●   ●   ●   ●   ●   ●



        0     1   2   3   4   5   6   7   8   9   10           0   1   2   3   4   5   6   7   8   9   10
                              x                                                    x
Figure 2: Behavior of the pmf of the T2DS distribution (upper-left-panel: θ = 0.5,
          upper-right-panel: θ = 1.0, lower-left-panel: θ = 1.5 and lower-right-panel:
          θ = 2.0).


Proof . The result comes analogous to the proof of Proposition 4.
Proposition 14. Let Y ∼ T2DS (θ). The cgf of Y is given by
                    {(       ) [( θ        )       ]     (      )}
         C (t) = log θ2 + 1      e − et − 1 eθ + et + θeθ et − 1 −
                    (      )        (        )
                 log θ2 + 1 − 2 log et − eθ ,

for t ̸= θ.

Proof . The result comes analogous to the proof of Proposition 5.
Proposition 15. Let Y ∼ T2DS (θ). The cf of Y is given by
                   ( 2   ) [( θ         )          ]   (        )
                    θ +1     e − eit − 1 eθ + eit + θeθ eit − 1
           ϕ (t) =                                  2             ,
                                (θ2 + 1) (eit − eθ )

                                                  Revista Colombiana de Estadística 43 (2020) 21–48

Two Useful Discrete Distributions to Model Overdispersed Count Data                   35
                    √
for t ∈ R and i =       −1 is the imaginary number.

Proof . The result comes analogous to the proof of Proposition 6.

    For this version, it is also clear that equation (12) is infinitely diﬀerentiable on
t. Therefore, from Proposition 13, the ordinary moments of Y can be derived by
                    { [         (           )                          ]
                ′       θ   dr eθ − et − 1         θ   dr (et − 1)
              µr = e                          + 2                        +
                            dtr (et − eθ )2      θ + 1 dtr (et − eθ )2
                                     }
                     dr       et
                                          ,    r > 1,
                    dtr (et − eθ )2
                                     t=0

for θ ∈ R+ . Hence, the mean and the variance of Y are given, respectively, by
                              ( 2    )(        )
                               θ + 1 eθ − 1 + θeθ
                          µ=                      2  ,
                                 (θ2 + 1) (eθ − 1)

and
                    ( θ    )      (        )       [ (   )  ]
                     e − 1 θ4 + e2θ − 1 θ3 + 2 e2θ + 1 − 5eθ θ2
             σ2 =                                               +
                                     (θ2 + 1)2 (eθ − 1)4
                    ( 2θ    )    (        )
                     e − 1 θ + eθ − 2 eθ + 1
                                                   .
                         (θ2 + 1)2 (eθ − 1)4

   Now, the ID of Y is given by
                ( θ      )      (       )       [ (        )        ]
                  e − 1 θ4 + e2θ − 1 θ3 + 2 e2θ + 1 − 5eθ θ2
           ID =                                                   2   +
                     [(θ2 + 1) (eθ − 1) + θeθ ] (θ2 + 1) (eθ − 1)
                        ( 2θ     )    (       )
                         e − 1 θ + eθ − 2 eθ + 1
                                                             2,
                [(θ2 + 1) (eθ − 1) + θeθ ] (θ2 + 1) (eθ − 1)

and the CV has the form
                                            √
                                                g (θ)
                             CV =                             ,
                                    (θ2 + 1) (eθ − 1) + θeθ

where
                   (      )    (       )    [ (       )     ]
            g (θ) = eθ − 1 θ4 + e2θ − 1 θ3 + 2 e2θ + 1 − 5eθ θ2 +
                   ( 2θ    )   (      )
                     e − 1 θ + eθ − 2 eθ + 1.

    As for the T1DS distribution, the coeﬃcients of skewness and kurtosis of the
T2DS distribution are represented by an extensive and very complicated expres-
sion. These expressions will also be omitted, but Table 2 summarizes, for selected
values of θ, the nature and the behavior of these coeﬃcients along with the mea-
sures previously presented in this subsection.

                                        Revista Colombiana de Estadística 43 (2020) 21–48

36                                     Josmar Mazucheli, Wesley Bertoli & Ricardo Oliveira

           Table 2: Theoretical descriptive statistics under T2DS distribution.
                                         Measures
          θ
                Mean     Variance       ID          CV         ZM       Skewness   Kurtosis
          0.5   3.1088    7.8607      2.5292     0.9009       0.3915     1.4522    6.1036
          1.0   1.0423    1.7050      1.6356     1.2528       0.2300     1.7173    7.0468
          1.5   0.4578    0.6090      1.3307     1.7047       0.1379     2.1104    8.7843
          2.0   0.2290    0.2709      1.1830     2.2732       0.0824     2.6168    11.3721
          2.5   0.1231    0.1361      1.1046     2.9967       0.0492     3.2770    15.3967
          3.0   0.0688    0.0727      1.0609     3.9229       0.0294     4.1470    21.9449



     For this version, the ZM index is given by
               [ [                         ]      (       )] ( 2   )(     )2
                log h (θ, 0) − h (θ, 1) e−θ − log θ2 + 1      θ + 1 eθ − 1
     ZM = 1 +                                                                .                (13)
                                    (θ2 + 1) (eθ − 1) + θeθ

    The limit properties of (13) are equal to those obtained for (8), that is, ZM → 0
as θ → ∞ and ZM → 1 as θ → 0. This implies that the T2DS distribution is also
suitable to deal with zero-inflation, but is not indicated to modeling zero-deflated
datasets. On the other hand, since θ ∈ R+ , the central moments of the T2DS
distribution present the same behavior concerning those derived from the T1DS
distribution. Moreover, since equation (13) presents the same limit properties of
equation (7), we conclude that the T2DS distribution may also be considered as
an alternative to model overdispersed datasets.
Proposition 16. Let Y ∼ T2DS (θ). The mode (y0 ) of Y is given by (9), where

                                                2            θ2 + 1
                                   r (θ) =               −          ,
                                             (eθ − 1)           θ
is a real-valued threshold. If r (θ) = k, k ∈ Z+ , then the T2DS distribution is
bimodal with modes k and k + 1.

Proof . The ratio of consecutive probabilities is given by

                   P (Y = y + 1; θ)   [h (θ, y + 1) − h (θ, y + 2)] e−θ
                                    =                                   ,                     (14)
                     P (Y = y; θ)          [h (θ, y) − h (θ, y + 1)]
for θ ∈ R+ . Thus, the result comes analogous to the proof of Proposition 7.
Proposition 17. The T2DS distribution has an increasing hazard rate.

Proof . One can notice that equation (14) is also a decreasing function on y.
In this case, if follows that P (Y = k; θ) is log-concave and therefore, the T2DS
distribution has an increasing hazard rate. Hence, the proof.

    For the T2DS distribution, it can also be proved that equation (11) satisfies
P2 (Y = y; θ) > P (Y = y − 1; θ) P (Y = y + 1; θ) for r (θ) ̸= k, which implies uni-
modality. In addition, one can notice that the form of the mode of the T2DS

                                               Revista Colombiana de Estadística 43 (2020) 21–48

Two Useful Discrete Distributions to Model Overdispersed Count Data                       37

distribution is exactly equal to the T1DS model but, in this case, if θ = 0.5 then
r(θ) ≈ 0.58 and hence, the mode is 1, as can be seen in the upper-left-panel of the
Figure 2.
Proposition 18. The T2DS distribution has heavy tails as θ approaches zero.

Proof . By considering the HT index previously defined, we have that
                    {                                        }
                       −θ      [h (θ, y + 1) − h (θ, y + 2)]
      lim HT = lim e       lim                                 = lim e−θ = 1,
     θ→0        θ→0       y→∞    [h (θ, y) − h (θ, y + 1)]       θ→0

which concludes the proof.


4. Maximum Likelihood Estimation
In this section, we will address the issue of estimating the parameter θ of both
versions of the discrete Shanker distribution. We have adopted the frequentist ap-
proach, and here we will derive the maximum likelihood function for the T1DS and
T2DS models. Using these functions, one can obtain point estimates for param-
eter θ in each case. Moreover, suitable estimates for the confidence intervals can
be obtained using large-sample approximations, that is based on the asymptotic
properties of the maximum likelihood estimators.


4.1. Inference Under T1DS Distribution
Let Y = (Y1 , . . . , Yn ) a random sample of size n from the T1DS distribution and
y = (y1 , . . . , yn ) its observed values. The log-likelihood function for parameter θ
can be expressed as
                                                                   ∑
                                                                   n
 ℓn (θ; y) = −n {θ (y + 1) − 2 log [h (θ)] + log [θh (θ) + 1]} +         log (θ + yi ) , (15)
                                                                   i=1

where y is the sample mean. The maximum likelihood estimator (MLE) of θ can
be obtained by direct maximization of the log-likelihood function. Thus, the first
order derivative of (15) respect to θ (score function) is given by
                                                      [           ]
                d                           2neθ    n θeθ + h (θ)     ∑n
                                                                            1
   Un (θ; y) =    ℓn (θ; y) = −n (y + 1) +        −                 +            .
               dθ                           h (θ)      θh (θ) + 1     i=1
                                                                          θ + yi

    There is no closed-form solution for the MLE of θ and therefore, standard opti-
mization algorithms such Newton-Raphson based methods may be used to obtain
numerical estimates. By the maximum likelihood theory, a consistent estimator
for the variance of θb is obtained by the inverse of the Fisher information, that is,
              [                ]
                  d
    In (θ) =E − Un (θ; Y)
                  dθ
              [                  [             (    )]                          ]
                 2eθ      1 + eθ θ (θ − 1) + eθ − 4      h2 (θ) e−θ ( −θ      )
           =n 2        −                        2      +            ζ e , 1, θ ,
                h (θ)              [θh (θ) + 1]          θh (θ) + 1


                                       Revista Colombiana de Estadística 43 (2020) 21–48

38                                   Josmar Mazucheli, Wesley Bertoli & Ricardo Oliveira


where ζ is the Lerch-Phi function (Bateman & Erdélyi 1953) defined as ζ (z, a, v) =
∑ ∞             −a
       j
  j=0 z (j + v)    for |z| < 1.
   Finally, in order to obtain intervallic estimates for θ, one can use large-sample
approximations for the 100 × (1 − α) % two-sided confidence interval (CI), that is,
                                           √
                                θb ± z1−α/2 In−1 (θ̂),

where z1−α/2 is the upper (α/2)th percentile of the standard Normal distribution.


4.2. Inference Under T2DS Distribution
Let Y = (Y1 , . . . , Yn ) a random sample of size n from the T2DS distribution and
y = (y1 , . . . , yn ) its observed values. The log-likelihood function for parameter θ
can be expressed as
                   [         (      )] ∑
                                       n
                                            [                              ]
     ℓn (θ; y) = −n θ y + log θ2 + 1 +   log h (θ, yi ) − h (θ, yi + 1) e−θ ,               (16)
                                              i=1

where y is the sample mean. The MLE of θ can be obtained by direct maximization
of the log-likelihood function. The first order derivative of (16) respect to θ is given
by
                         [             ]
                                 2θ
         Un (θ; y) = − n y + 2           +
                               θ +1
                     ∑ n
                         (2θ + yi ) + [θ (θ + yi + 1) − (2θ + yi + 1) + 1] e−θ
                                                                               .
                     i=1
                              [(θ + yi ) θ + 1] − [(θ + yi + 1) θ + 1] e−θ

    Now, in order to estimate the variance of θ, b one have to obtain the Fisher
information of θ. In this case, this quantity has the form
              [ (            )    ]2    (                                                     )
           nθ2 θ θ2 + θ + 3 + 1 6 F5 1, a1 , a1 , a2 , a2 , b1 ; a3 , a3 , b2 , a4 , a4 ; e−θ
  In (θ) =                            3                                                         ,
                             (θ2 + 1) [(θ2 + 1) (eθ − 1) − θ] eθ
where p Fq is the generalized hypergeometric function (Slater 1966), whose argu-
ments are given by
                                            [ (          )   ] √
                  [θ (θ + 2θ + 3) + 2] eθ − θ θ2 + 4θ + 3 + 4 − c1
            a1 =                                                     ,
                                    2 (θ2 + 1) (eθ − 1)
                                            [ (          )   ] √
                  [θ (θ + 2θ + 3) + 2] eθ − θ θ2 + 4θ + 3 + 4 + c1
            a2 =                                                     ,
                                    2 (θ2 + 1) (eθ − 1)
                           (      ) [ (               )  ] √
                        θeθ θ2 + 3 − θ θ2 + 2θ + 3 + 2 − c1
                 a3 =                                           ,
                                    2 (θ2 + 1) (eθ − 1)
and                        (      ) [ (              ) ] √
                        θeθ θ2 + 3 − θ θ2 + 2θ + 3 + 2 + c1
                   a4 =                                     ,
                                   2 (θ2 + 1) (eθ − 1)

                                            Revista Colombiana de Estadística 43 (2020) 21–48

Two Useful Discrete Distributions to Model Overdispersed Count Data                 39
             (       )2 ( 2θ  ) [ (               )   ]
where c1 = θ2 θ2 + 3     e + 1 − θ2 2θ4 − 8θ2 − 10 + 4 eθ ,
          ( 2   )(       )                      [( 2   )     ]           2
           θ + 1 eθ − 1 − θ                       θ + 1 + θ eθ − (θ + 1)
     b1 =                          and     b2 =                            .
              θ (eθ − 1)                                θ (eθ − 1)

   Again, there is no closed-form solution for the MLE of θ. In this case, we can
adopt the same procedure presented in the previous subsection to obtain point
and intervallic estimates for parameter θ.


5. Simulation Study
In this section, we have estimated, using B = 10, 000 Monte Carlo simulation,
the biases, the mean squared error, the coverage probabilities and the coverage
lengths of the MLE θb of both versions of the discrete Shanker distribution. To
run the simulation, we have considered θ = 0.3, 0.6, 0.8, 1.0, 1.5, 1.8 and 2.0 and
sample sizes ranging from 20 to 200 by 30. The inverse-transform method for
discrete distributions (Rubinstein & Kroese 2008) was implemented to generate
the pseudo-random samples. The simulation process was performed using Ox
Console (Doornik 2007). The quantities of interest were estimated by the following
expressions.

   • BIAS(θ)b = 1 ∑B (θbi − θ).
                  B i=1

   • M SE(θ)b = 1 ∑B (θbi − θ)2 .
                 B i=1
                3.92 ∑B
   • CLθ (n) =         i=1 sθbi .
                 B
                 1 ∑B
   • CPθ (n) =                       sθbi < θ < θbi + 1.96b
                         I{θbi − 1.96b                    sθbi }, where I{·} denotes
                B i=1
     the indicator function.

    For both versions, the behavior of the average bias and average mean squared
error are shown in Figures 3 and 4. The results for the coverage probabilities and
the coverage lengths are reported in Tables 3 and 4.
    From Figures 3 and 4, in each scenario and for T1DS and T2DS distributions,
we have that the bias of θb is positive and tends to zero when the sample size
increases. Also, the mean squared error of θb tends to zero in each case. For the
coverage probabilities, we have CPθ (n) around 0.94 and 0.96 for both discretiza-
tions, and the coverage length tends to zero when the sample size increases. Table
5 reports the percentage of times, out of 10, 000 Monte Carlo simulations, that
the Voung’s test (Vuong 1989) judges that the generated data is coming from the
same distribution.




                                      Revista Colombiana de Estadística 43 (2020) 21–48

40                                                                 Josmar Mazucheli, Wesley Bertoli & Ricardo Oliveira




                        0.112




                                                                                                       0.112
                        0.085




                                                                                                       0.084
Estimated bias for θ^




                                                                               Estimated bias for θ^
                        0.058




                                                                                                       0.056
                                ●
                        0.031




                                                                                                       0.028
                                     ●

                                          ●
                                                    ●                                                          ●
                                                             ●
                                                                    ●     ●
                        0.004




                                                                                                       0.001
                                                                                                                    ●
                                                                                                                         ●         ●        ●     ●     ●




                                20   50   80       110       140   170   200                                   20   50   80       110       140   170   200
                                               Sample size                                                                    Sample size
Figure 3: (left-panel) Estimated bias for θb – by infinite series. (right-panel) Estimated
          bias for θb – by survival function ( : θ = 0.3, ⃝ : θ = 0.6, △ : θ = 0.9,
          + : θ = 1.0, × : θ = 1.5, 3 : θ = 1.8 and ▽ : 2.0).
                        0.229




                                                                                                       0.215
                        0.172




                                                                                                       0.161
Estimated mse for θ^




                                                                               Estimated mse for θ^
                        0.114




                                                                                                       0.107
                        0.057




                                                                                                       0.054
                        0.000




                                                                                                       0.000




                                ●                                                                              ●
                                     ●    ●         ●                                                               ●    ●         ●
                                                             ●      ●     ●                                                                 ●     ●     ●




                                20   50   80       110       140   170   200                                   20   50   80       110       140   170   200
                                               Sample size                                                                    Sample size
Figure 4: (left-panel) Estimated MSE for θb – by infinite series. (right-panel) Estimated
          MSE for θb – by survival function ( : θ = 0.3, ⃝ : θ = 0.6, △ : θ = 0.9,
          + : θ = 1.0, × : θ = 1.5, 3 : θ = 1.8 and ▽ : 2.0).



    Using the same simulation scenarios as previously described, we have esti-
mated the probability of correct selection (PCS) using the diﬀerence between the
maximized log-likelihood functions as the discrimination criterion. Let ℓk be the
log-likelihood function of the TkDS distribution. We choose T1DS or T2DS as
the preferred model if the statistic Tn = ℓ1 (θ̂; y) − ℓ2 (θ̂; y) is greater than or less
than zero, respectively. Estimates of the PCS’s are shown in the Figure 5.




                                                                         Revista Colombiana de Estadística 43 (2020) 21–48

Two Useful Discrete Distributions to Model Overdispersed Count Data                         41

Table 3: Estimated coverage probability and length of coverage probability for θb (T1DS
         distribution).
                                                    Values for θ
        Quantity   n
                           0.3      0.6      0.9        1.0        1.5      1.8      2.0
                   20    0.9528   0.9510   0.9522     0.9552     0.9628   0.9590   0.9599
                   50    0.9476   0.9503   0.9496     0.9537     0.9533   0.9588   0.9571
                   80    0.9516   0.9503   0.9515     0.9517     0.9517   0.9492   0.9588
        CPθ (n)    110   0.9502   0.9490   0.9518     0.9528     0.9506   0.9533   0.9586
                   140   0.9494   0.9501   0.9500     0.9522     0.9490   0.9513   0.9547
                   170   0.9508   0.9486   0.9503     0.9504     0.9505   0.9504   0.9559
                   200   0.9500   0.9512   0.9497     0.9486     0.9507   0.9515   0.9502
                   20    0.1790   0.3264   0.4897     0.7087     0.9965   1.3622   1.7805
                   50    0.1121   0.2041   0.3041     0.4333     0.6001   0.8011   1.1449
                   80    0.0884   0.1610   0.2395     0.3403     0.4690   0.6233   0.8103
        CLθ (n)    110   0.0753   0.1371   0.2038     0.2894     0.3979   0.5279   0.6840
                   140   0.0667   0.1215   0.1805     0.2560     0.3517   0.4662   0.6033
                   170   0.0605   0.1102   0.1636     0.2320     0.3186   0.4222   0.5456
                   200   0.0558   0.1016   0.1508     0.2138     0.2934   0.3887   0.5021

Table 4: Estimated coverage probability and length of coverage probability for θb (T2DS
         distribution).
                                                    Values for θ
        Quantity   n
                           0.3      0.6      0.9        1.0        1.5      1.8      2.0
                   20    0.9685   0.9658   0.9615     0.9574     0.9661   0.9687   0.9667
                   50    0.9614   0.9498   0.9483     0.9531     0.9568   0.9534   0.9558
                   80    0.9584   0.9554   0.9531     0.9503     0.9536   0.9490   0.9529
        CPθ (n)    110   0.9541   0.9680   0.9572     0.9496     0.9494   0.9513   0.9563
                   140   0.9573   0.9608   0.9631     0.9516     0.9497   0.9506   0.9578
                   170   0.9567   0.9548   0.9587     0.9504     0.9506   0.9500   0.9556
                   200   0.9542   0.9456   0.9480     0.9482     0.9517   0.9514   0.9542
                   20    0.2035   0.3714   0.5576     1.2078     1.0897   1.4578   1.8686
                   50    0.1226   0.2272   0.3420     0.4805     0.6547   0.8588   1.1078
                   80    0.0953   0.1779   0.2683     0.3770     0.5114   0.6678   0.8545
        CLθ (n)    110   0.0806   0.1510   0.2279     0.3205     0.4336   0.5654   0.7214
                   140   0.0710   0.1335   0.2017     0.2835     0.3832   0.4993   0.6362
                   170   0.0642   0.1209   0.1827     0.2570     0.3471   0.4522   0.5755
                   200   0.0590   0.1113   0.1682     0.2367     0.3198   0.4163   0.5295

 Table 5: Percentage of times out of 10,000 that the null hypothesis is not rejected.
               Data generated from T1DS                     Data generated from T2DS
   θ                  Sample size                                   Sample size
           20     50      100      200   500           20      50      100       200   500
  0.3     75.93 74.32    68.55 60.03    53.95         92.87   92.59   88.54     80.74 71.12
  0.6     88.66 88.02    87.93    86.14 75.83         94.54   94.45   93.04     91.52 84.05
  0.9     94.01 93.45    93.08    91.97 89.05         96.42   95.89   95.13     94.70 92.87
  1.0     95.65 94.33    94.22    93.28 91.06         96.70   96.17   95.79     95.69 93.78
  1.5     99.49 98.75    96.75    95.38 94.79         99.59   99.17   97.52     96.30 95.78
  1.8     99.86 99.75    98.29    95.98 94.71         99.88   99.82   98.82     96.55 95.62
  2.0     99.95 99.93    99.30    96.97 95.28         99.97   99.92   99.36     97.17 95.70




                                           Revista Colombiana de Estadística 43 (2020) 21–48

42                                                                                 Josmar Mazucheli, Wesley Bertoli & Ricardo Oliveira




                                             0.942




                                                                                                                                            0.947
Estimated probability of correct selection




                                                                                               Estimated probability of correct selection
                                                                                                                                                                                  ●
                                                                                         ●




                                             0.794




                                                                                                                                            0.847
                                                                    ●
                                                                                                                                                                   ●
                                             0.645




                                                                                                                                            0.746
                                                              ●

                                                                                                                                                             ●


                                                          ●
                                                                                                                                                         ●


                                                                                                                                                    ●
                                             0.496




                                                                                                                                            0.645
                                                     ●
                                             0.348




                                                                                                                                            0.545
                                                     20       100   200                  500                                                        20       100   200            500
                                                                     Sample size                                                                                    Sample size
Figure 5: Left-panel: Estimated PCS when data are generated from T1DS distribution.
          Right-panel: Estimated PCS when data are generated from T2DS ( : θ =
          0.3, ⃝ : θ = 0.6, △ : θ = 0.9, + : θ = 1.0, × : θ = 1.5, 3 : θ = 1.8, and
          ▽ : θ = 2.0).



6. Application to real data

In this section, both versions of the discrete Shanker distribution are considered to
model two real datasets from diﬀerent areas. The goodness-of-fit of the proposed
models is compared with those accessed by the Poisson (P) and Negative Binomial
(NB) distributions. The parameterizations considered to fit the NB model is the
same implemented in the R software. For the first application, we have considered
the total number of borers per hill in each plot for a control group and three
treatment groups. This dataset was firstly analyzed by Bliss & Fisher (1953). In a
field experiment of insect pests on the corn borer, four treatments were arranged
in 15 randomized blocks. At the end of the season, eight hills of corn were selected
randomly in each plot, and the borers were recorded. Here, we are considering the
data from the second treatment (Saha (2008), Table 9). The second one relates
to the number of contract strikes in US manufacturing beginning each month
between January 1968 and December 1976 (Kennan 1985). All computations to
obtain the results presented in this section were performed using the R environment
(R Development Core Team 2017). The executable scripts are available from the
authors upon justified request.
    Table 6 presents some descriptive statistics for each dataset. The raw data
used in this section can be found in Appendix A. The initial analysis highlights
the presence of overdispersion (see the index of dispersion), justifying the choice
of the discrete Shanker distribution to describe such data. Moreover, the sample
mode of the second dataset is greater than 0 and so, if one of the versions of our
model fit these data, then we expect a value smaller than 0.50 for the MLE of θ
in this case.

                                                                                        Revista Colombiana de Estadística 43 (2020) 21–48

Two Useful Discrete Distributions to Model Overdispersed Count Data                             43

              Table 6: Variables and descriptive statistics for each dataset.
    Dataset           Variable             n      Mean    Median     Var.    ID (%)    CV (%)
                  Number of borers
       1                                   120     1.48     1.00     3.19    215.26    120.46
                       per hill
                     Number of
       2                                   108     5.24     5.00    14.07    268.52     71.58
                   contract strikes



    In Table 7 we present the frequency distribution of each sample. The expected
frequencies were obtained through the estimated probabilities, that were computed
using the MLEs. Frequencies in bold relate to those one closer to the observed
ones. The results show that the proposed models provide reliable fit in both cases.

           Table 7: Observed and expected frequencies from the fitted models.
                                                            Expected
                    Counts        Observed
                                                   P       NB    T1DS       T2DS
                                                 Dataset 1
                      0               43         27.23   44.28    39.17     41.26
                      1               35         40.38   31.08   34.51      32.67
                      2               17         29.95   19.10    21.84     20.77
                      3               11         14.81   11.17    12.15     11.95
                      4                5          5.49    6.38    6.32      6.50
                      >5               9          2.13    7.27     5.72     6.45
                                                 Dataset 2
                      0                5          0.57    5.42     4.29      7.26
                      1               12          3.00   10.11    11.43     11.72
                      2               14          7.86   12.69   13.90      13.10
                      3               11         13.72   13.94    13.86     12.77
                      4                9         17.98   12.66    12.60     11.59
                      5               14         18.84   11.25    10.84     10.07
                      6                9         16.46    9.53    9.01       8.48
                      >7              34         28.98   28.64    27.34     27.55



        Table 8: Parameter estimates and gof measures for the fitted models.
                                                      95% CI
           Model     Par.         MLE (SE)                           χ2 (p-value)     d.f.
                                                  Lower Upper
                                                 Dataset 1
              P        µ     1.483 (0.111)        1.265    1.701   38.59 (< 0.001)     4
                       µ     1.483 (0.162)        1.167    1.800
            NB                                                       1.47 (0.689)      3
                       ϕ     1.333 (0.644)        0.601    2.065
           T1DS        θ     0.884 (0.048)        0.789    0.978     3.71 (0.446)      4
           T2DS        θ     0.820 (0.050)        0.723    0.918     2.36 (0.671)      4
                                                 Dataset 2
              P        µ     5.241 (0.220)        4.809    5.672   76.64 (< 0.001)     6
                       µ     5.241 (0.369)        4.517    5.964
            NB                                                       3.69 (0.594)      5
                       ϕ     2.897 (0.644)        1.634    4.159
           T1DS        θ     0.357 (0.022)        0.312    0.401     4.31 (0.635)      6
           T2DS        θ     0.330 (0.022)        0.288    0.373     4.67 (0.586)      6



                                                  Revista Colombiana de Estadística 43 (2020) 21–48

44                                  Josmar Mazucheli, Wesley Bertoli & Ricardo Oliveira


    The MLEs, SEs, and 95% asymptotic CIs for the parameters of each fitted
model are presented in Table 8. The goodness-of-fit was assessed using the χ2
statistic. For Dataset 1, the chi-squared value for the T2DS distributions is
χ2 = 2.36, with a corresponding p-value ≈ 0.68, highlighting the adherence of
the T2DS distribution. Also, for Dataset 2, we have obtained χ2 = 4.67 (p-value
≈ 0.59) for the T1DS model. The goodness-of-fit accessed by T1DS distribution
was found to be quite similar to T2DS model. Model selection was performed
using the Akaike information criterion with correction for finite samples (AICc),
the Bayesian information criterion (BIC), and the Hannan-Quinn information cri-
terion (HQC). These measures are presented in Table 9. One can notice that the
smaller values of the given criteria are provided by one of the discrete Shanker ver-
sions. Therefore, we may conclude that exists evidence that the proposed models
adhere well to the considered datasets and hence, they can be regarded as excellent
alternatives for the modeling of count data in the presence of overdispersion.

                   Table 9: Comparison criteria for the fitted models.
                               Dataset 1                      Dataset 2
           Model
                      AICc       BIC       HQC       AICc       BIC       HQC
             P        440.41    443.16     441.51    641.64    644.28     642.69
            NB        404.71    410.18     406.87    570.69    575.94     572.75
           T1DS       405.14    407.90     406.24   569.42     572.06     570.47
           T2DS      403.65     406.40     404.75    570.55    573.19     571.60




7. Concluding Remarks
In this paper, two versions of the discrete Shanker distribution were introduced as
alternatives to model overdispersed count datasets. To derive the proposed mod-
els, we have considered the methods of infinite series and survival function. Some
statistical properties as the mean, variance, coeﬃcients of variation, skewness, and
kurtosis for each version were discussed. Also, it was shown that both versions
of the discrete Shanker distribution are suitable options to deal with zero-inflated
datasets. Moreover, we have derived the log-likelihood, the score function, and we
have considered asymptotic intervallic estimation for parameter θ of both versions.
Also, we have performed a Monte Carlo simulation study where the bias, the mean
squared error, and the coverage lengths of the MLEs as well the coverage probabil-
ity of the asymptotic CIs were computed. These measures indicate the suitability
of the considered methodology. The usefulness of the proposed models was eval-
uated by fitting each one to two datasets with characteristics of overdispersion.
The model selection was performed using the AICc, BIC, and HQC criteria. The
goodness-of-fit was assessed by the χ2 statistic. The obtained results demonstrate
that the T1DS and T2DS distributions can be competitive with standard discrete
models provided by literature.




                                           Revista Colombiana de Estadística 43 (2020) 21–48

Two Useful Discrete Distributions to Model Overdispersed Count Data                   45

Acknowledgements

The authors would like to thank the anonymous referees for its insightful sug-
gestions that certainly contributed to improving this work. Josmar Mazucheli
gratefully acknowledges the partial financial support from the Paraná Research
Foundation (FA) - Grant 64/2019. The research of Wesley Bertoli is supported
by the Federal University of Technology - Paraná and by the Paraná Research
Foundation (FA) - Doctoral Grant: CP 18/2015.



Appendix A. Raw Datasets

   The two real datasets used in the paper to illustrate the usefulness of the
proposed models are provided in Table A1.

                    Table A1: Real datasets used in Section 6.
                                        Dataset 1
                      0   0    0    0      0      0   0    0    0
                      0   0    0    0      0      0   0    0    0
                      0   0    0    0      0      0   0    0    0
                      0   0    0    0      0      0   0    0    0
                      0   0    0    0      0      0   0    1    1
                      1   1    1    1      1      1   1    1    1
                      1   1    1    1      1      1   1    1    1
                      1   1    1    1      1      1   1    1    1
                      1   1    1    1      1      1   2    2    2
                      2   2    2    2      2      2   2    2    2
                      2   2    2    2      2      3   3    3    3
                      3   3    3    3      3      3   3    4    4
                      4   4    4    5      5      5   5    6    7
                      7   8    8
                                       Dataset 2
                     5    4     6   16    5      8    8    9    10
                     10   7     1    6     5     6    5    13    6
                     10   13    4   8      5     0    2    2     2
                     8    4    11   4     8      9    9    4    0
                     9    8     5   5     10     3    5    4    6
                     6    5     1   2      2     2    2    4     3
                     2    3     1   2      0     1    1    1     1
                     5    7     2   9      3     6    9    3     3
                     5    9    10   9     15    18    13   10   9
                     7    7     0   3      3     4    2    1     2
                     2    3     0   5      5     1    1    1     1
                     8    5     9   6      3     4    6    2     3




              [                                                         ]
               Received: December 2018 — Accepted: August 2019

                                        Revista Colombiana de Estadística 43 (2020) 21–48

46                              Josmar Mazucheli, Wesley Bertoli & Ricardo Oliveira


References
Bateman H, Erdélyi A. Higher transcendental functions.(1953). McGraw-Hill.
Bi Z, Faloutsos C, Korn F. The DGX distribution for mining massive skewed data.(2001). ACM.
Bliss C I, Fisher R A. Fitting the negative binomial distribution to biological data.(1953). Biometrics.
Bracquemond C, Gaudoin O. A survey on discrete lifetime distributions.(2003). International Journal of Reliability.
Chakraborty S. Generating discrete analogues of continuous probability distributions - A survey of methods and constructions.(2015). Journal of Statistical Distributions and Applications.
Chakraborty S. A new discrete distribution related to generalized Gamma distribution and its properties.(2015). Communications in Statistics - Theory and Methods.
Chakraborty S, Chakravarty D. Discrete Gamma distributions: Properties and parameter estimation.(2012). Communications in Statistics - Theory and Methods.
Chakraborty S, Chakravarty D. A new discrete probability distribution with integer support on (−∞, +∞).(2016). Communications in Statistics - Theory and Methods.
Chakraborty S, Gupta R D. Exponentiated Geometric distribution: Another generalization of Geometric distribution.(2015). Communications in Statistics - Theory and Methods.
Collett D. Modelling survival data in medical research.(2003). Chapman and Hall.
Doornik J A. Object-oriented matrix programming using Ox.(2007). Timberlake Consultants Press and Oxford.
Doray L G, Luong A. Efficient estimators for the Good family.(1997). Communications in Statistics - Simulation and Computation.
Ghitany M E, Atieh B, Nadarajah S. Lindley distribution and its application.(2008). Mathematics and Computers in Simulation.
Gómez-Déniz E, Calderín-Ojeda E. The discrete Lindley distribution: Properties and applications.(2011). Journal of Statistical Computation and Simulation.
Good I J. The population frequencies of species and the estimation of population parameters.(1953). Biometrika.
Grandell J. Mixed Poisson processes.(1997). Chapman and Hall/CRC.
Haight F A. Queueing with balking.(1957). Biometrika.
Hamada M S, Wilson A G, Reese C S, Martz H F. Bayesian reliability.(2008). Springer.
Hussain T, Ahmad M. Discrete inverse Rayleigh distribution.(2014). Pakistan Journal of Statistics.
Inusah S, Kozubowski T J. A discrete analogue of the Laplace distribution.(2006). Journal of Statistical Planning and Inference.
Jazi M A, Lai C D, Alamatsaz M H. A discrete inverse Weibull distribution and estimation of its parameters.(2010). Statistical Methodology.
Kalbfleisch J D, Prentice R L. The statistical analysis of failure time data.(2002). Wiley.
Keilson J, Gerber H. Some results for discrete unimodality.(1971). Journal of the American Statistical Association.
Kemp A W. Characterizations of a discrete Normal distribution.(1997). Journal of Statistical Planning and Inference.
Kemp A W. Classes of discrete lifetime distributions.(2004). Communications in Statistics - Theory and Methods.
Kemp A W. The discrete Half-Normal distribution Birkhäuser Boston.(2008). Advances in Mathematical and Statistical Modeling.
Kennan J. The duration of contract strikes in US manufacturing.(1985). Journal of Econometrics.
Klein J P, Moeschberger M L. Survival analysis: Techniques for censored and truncated data.(1997). Springer-Verlag.
Kozubowski T J, Inusah S. A skew Laplace distribution on integers.(2006). Annals of the Institute of Statistical Mathematics.
Krishna H, Pundir P S. Discrete Burr and discrete Pareto distributions.(2009). Statistical Methodology.
Kulasekera K B, Tonkyn D W. A new discrete distribution with applications to survival dispersal and dispersion.(1992). Communications in Statistics - Simulation and Computation.
Lawless J F. Statistical models and methods for lifetime data.(2003). John Wiley and Sons.
Lee E T, Wang J W. Statistical methods for survival data analysis.(2003). John Wiley and Sons.
Meeker W Q, Escobar L A. Statistical methods for reliability data.(1998). John Wiley and Sons.
Nakagawa T, Osaki S. The discrete Weibull distribution.(1975). IEEE Transactions on Reliability.
Nekoukhou V, Alamatsaz M H, Bidram H. A discrete analog of the Generalized Exponential distribution.(2012). Communication in Statistics - Theory and Methods.
Nekoukhou V, Alamatsaz M H, Bidram H. Discrete generalized Exponential distribution of a second type.(2013). Statistics - A Journal of Theoretical and Applied Statistics.
R Development Core Team.R: A language and environment for statistical computing.(2017). R Foundation for Statistical Computing.
Roy D. The discrete Normal distribution.(2003). Communication in Statistics Theory and Methods.
Roy D. Discrete Rayleigh distribution.(2004). IEEE Transactions on Reliability.
Rubinstein R Y, Kroese D P. Simulation and the Monte Carlo method.(2008). Wiley Series in Probability and Statistics.(2008). John Wiley Sons.
Saha K K. Analysis of one-way layout of count data in the presence of over or under dispersion.(2008). Journal of Statistical Planning and Inference.
Sato H, Ikota M, Sugimoto A, Masuda H. A new defect distribution metrology with a consistent discrete exponential formula and its applications.(1999). IEEE Transactions on Semiconductor Manufacturing.
Shanker R. Shanker distribution and its applications.(2015). International Journal of Statistics and Applications.
Siromoney G. The general Dirichlets Series distribution.(1964). Journal of the Indian Statistical Association.
Slater L J. Generalized hypergeometric functions.(1966). Cambridge University Press.
Vuong Q H. Likelihood ratio tests for model selection and non-nested hypotheses.(1989). Econometrica.