EL MÉTODO DE ORTOGONALIZACIÓN DE GRAM-SCHMIDTH Y LA REGRESIÓN MÚLTIPLE
Universidad Nacional de Colombia
Abstract
A geometric approach to the multiple linear regression allows the use of the Gram-Schmidth orthogonalization process to find the regression equation and sums of squares without inverting a matrix.
Introducción.
A menudo los estudiantes tienen dificultades cuando se encuentran frente a un problema de regresión lineal múltiple, simplemente porque para su solución deben calcular la inversa de una matriz de taroafio relativamente grande; este cálculo es muy complejo y sin la ayuda de una calculadora de capacidad apreciable, el problema queda prácticamente fuera de sus manos.
Un enfoque geométrico sencillo del problema permite resolverlo sin necesidad de calcular inversas de matrices y una calculadora con algunas funciones estadísticas es suficiente para la solución numérica.
Problema.
Se tiene una variable Y a explicar con la ayuda de k+1 variables explicativas, mediante el modelo lineal donde b. es el coeficiente de regresión asociado a la variable X, y e es el error cometido al ajustar la observación de la variable Y al modelo; X. es generalmente igual a uno, lo cual hace que b. sea una constante independiente. La inclusión de permite decir que en el caso de error "e" igual a cero, Y es una combinación lineal de las véu^iables X ; en este caso particular,
y sé puede decir que Y es una variable explicada completamente por las variables  X.
Si el error es muy grande, el modelo dado no explica bien el comportamiento de la variable Y; así, cuando se tienen observaciones (independientes) de las diferentes variables y se quiere describir de la mejor manera posible el comportamiento de la variable con la ayuda del modelo lo que se trata de hacer es encontrar los coeficientes  que minimicen el error en forma global para todas las observaciones; de esta manera se encontrará el mejor modelo lineal para explicar el comportamiento de la variable con la ayuda de las variables.
Notación.
        El modelo (1), para cada una de las         n   observacio-
nes, se escribe así :


             b-X^rt+ b., X^ . + ... + b, X^, + e-
              O 10    1 11           k Ik    1
      Y„ = V20 * V2I *
                                   * V2k* «2                   (3)

             b^X - + b.X . + ... + b, X . + e
      ^n =    O nO    1 ni          k nk    n


        Si se llama. X.       al vector de observaciones de     la
Variable      X., es decir :

                        ^ij
                        ^2j
                X. =
                 1     X .
                        n]

donde   X^     es un vector cuyas componentes son todas iguales

 44

a uno. Si además X                 es la matriz cuyas columnas son        X-,
X ,..., X,       y    Y     el vector de observaciones de la varia-^
ble    Y, se puede escribir (3) de la siguiente manera :

        Para minimizar             e = Y-Xb   existen varios criterios;
nosotros nos ocuparemos del más conocido: el de mínimos
cuadrados; este criterio consiste en minimizar con respec-
to a   b   la expresión


                          II 112
                          lleil « nE e^2
                                    isl   ^

       En cualquier libro que trate la regresión múltiple
se encuentra que la solución para               b       está dada por


                     b = (X'X)"^ X»Y


y aquí es clara la necesidad de invertir la matriz (X'X)
de dimensión         (k+1, k+1).


Interpretación geOTiétrloa del problema. Lo dicho en la
Introducción p^ivite observar que lo que se pretende es en-

                                                                    45

centrar la ccanbinación lineal de         X , X^,..., X.     más pró-
xima de Y; esto equivale a buscar el vector            Y generado por
X ,X ,...» X , es decir, Y =b X           + b.X. + ...+ b.X, más    pró-
ximo de       Y.    Esto se obtiene cuando    Y   es la proyección
ortogonal de         Y    sobre el subespacio generado por    X ,X ,
 ,..., X,     y así, e = Y - Y       es ortogonal a ese subespacio.
El piHDblema se convierte entonces en buscar           Y tal que e=
Y - Y      sea ortoconal al subespacio generado por X-,X.,,..,X, .
Esta idea es la que permite utilizar el proceso de ortogo-
nalización de Gram-Schmidth para calcular los coeficientes
de regresión lineal.



                                              ^k+1




'Método.

        Sea    L      el subespacio generado por los vectores
X-, X^,...,X          y   L, . el subespacio generado por los
vettores      XQ,X.,...,X     ,Y. Sean   U ,U.,...,U     los vec-
tores de la base de          L, (r <_ k+1) obtenidos por el pro-
ceso de Gram-Schmidth, se puede completar una.base del
subespacio         L. .,; el proceso dice que el vector faltante

46

para la nueva base está dado por

                             Y'U            Y'U                Y'U
      "r+1 = ^ - ÜÜJ7"o - ü n r " i - ••• -üñT^r                                 í^>
                              0 0               11              r r

y     U       .    es ortogonal a todos los vectores de la base de
           Además,
^k-

       A          Y'UQ             Y'U^                Y'U
                                                                                 (6)
      ^ = U¿UQ "O* U ' U ^ - " I * '••• • ^ U ' U ^ ' ^ r
                                             r r

Y     es la proyección de                   Y     sobre el subespacio       L,   gene-
rado por los vectores                      X ,X^,...,X, .    De manera que el
error del modelo es precisamente                        U     es decir,


              e = Y-Y              = U^,
                                     r+1

y el cuadrado de su norma es lo que en regresión se llama
la suma residual de cuadrados.


          U • es igual a cero, entonces L, y L, .
              Si
           r+1      *^                    k      k+1
tienen la misma base y Y es explicada completamente por
las variables                X.     La expresión (6) da una descomposición
                         A
del vector               Y   en términos de una base ortogonal de                 L, .
                                                                       A           '^
Sin embargo, lo que mas interesa es expresar                           Y     en téirmi-
nos de los vectores observados                        XQ,X.,...,X. .       Peu?a ello
se hace una descomposición de la base ortogonal en térmi-
nos de los vectores                   X.

           Sea       ^A/B           ^^ componente de la proyección del vec-
tor       A       «óbife el vectof          B; es decir.

                                                           47




      Además, si el vector A se expresa como una combina-
ción lineal de los vectores B ,B,,...,B , entonces C ,
representa el coeficiente de B. en el desarrollo dado para
A.

      Con estas notaciones el proceso de ortogonalización de
Gram-Schmidth se puede escribir de la siguiente manera :
  se ha Identificado Y con X^* U. •• «1 error (o voe-
  t<Mr de residuos) y

  Cate «a el aodelo de regresión lineal Imscado.                                       ^

<^      La n o m a de U. elevada al cuadrado da la atasa real-
 \dual de cuadrados. La suma total se obtiene elevando al
   cuad^i^dó la norna de Y-P„.„ U.. Y la suma de cuadrados
   de la regresión'ae c^tlene por sustracción. De esta nanera
   se imede f o r á ^ Ijé^ailla de análisis de varianza y calcular
   el valor de R . '^^%
Bibliografía
Draper N,Smith H.Applied Regression Analysis.(1981).John Wiley and Sons.New York. 
Scheffé H.The Analysis of Variance.(1959).John Wiley and Sons.New York.