CLASIFICACIÓN CON VARIABLES DISCRETAS OBSERVADAS CON ERROR Y CON VARIABLES CONTINUAS
Universidad Nacional de Colombia
Abstract.
This article is concerned with a problem of classification with discretes variables observable with error and with continuous variables. A classification rule is given by the Bayesian method and the parameters are estimated by the method of Máximum likelihood described by R.D. Day (1969) for mixture of normal distributions.
Resumen.
Este articulo considera un problema de clasificación con variables discretas observables con error y con variables continuas. Se da una regla de clasificación por el método Bayesiano y se estiman los parámetros por el método de máxima verosimilitud descrito por N.E. Day (1969) para mezcla de distribuciones normales.
INTRODUCCIÓN.
El problema de discriminación entre dos grupos cuando la información disponible consta de variables, discretas (binarias) y continuas fue considerado por Krzanowski (1975).
En dicho trabajo,se considera un vector de variables binarias y uno de variables continuas. Cada valor de las componentes de X define una celda, resultando asi celdas. Se supone que la distribución de en la población es tal que, condicionalmente, condujo a la celda. Además, la probabilidad de obtener una observación de la celda en la población. Bajo estas hipótesis y suponiendo probabilidades a priori iguales y costos de mala clasificación iguales, se obtiene la siguiente regla de clasificación Bayesiana :
Cuando los parámetros en ambas poblaciones son desconocidos, como es el caso más usual en problemas prácticos.
se consideran dos muestras de tamaño, respectivamente. Estas muestras son utilizadas para construir una regla de clasificación estimada reemplazando los paramétros por las estimaciones de los parámetros en la regla dada anteriormente.
En el problema descrito hasta aquí, podría pensarse que la información proporcionada en las variables discretas es factible de contener error, lo cual, nos lleva, en este trabajo, a considerar el problema teniendo en cuenta dos tipos de variables discretas, las observables con error y las no observables, además de las variables continuas. En esta situación, el propósito de este trabajo es determinar una regla de clasificación usando el método Bayediano, similar al analizado por Krzanowski, efectuando las estimaciones de los parámetros por el método de máxima verosimilitud descrito por N.E. Day (1969) para mezcla de distribuciones normales. Debe notarse que la situación en que las variables discretas se observan con error, se ha estudiado en problemas de análisis de datos categóricos, ver por ejemplo J. Hochberg (1977), quien presenta dos metodologías de estimación usando esquemas de doble muestreo.
DISCRIMINACIÓN CON VARIABLES DISCRETAS OBSERVABLES CON ERROR Y CON VARIABLES CONTINUAS, ESTIMACIÓN POR MÁXIMA VEROSIMILITUD.


     Sea Z=( Zj5-*'«»z )' un vector de variables binarias
observables con error y sea     X = (x,,...,x )' el vector
                         •^           1         q

36


no obseiwable que contiene los valores verdaderos de            las
variables    Z.   (i=l,2,...,q).


      Supongamos qu# si se observa un vector Z       de variables
binarias pertenecientes a la celda m-ésima, la probabili-
dad de que el verdfpdero valor de dichas variables sea dado
por   X perteneciente a la celda j-ésima, viene dada       por


P(X|Z) = P(X = j |Z = m) = p^^,


y que la distribución conjunta de      X y Z   es dada por

P( X,Z)     = q.^ ,


de modo que p . ^ . q . J U q . J

      Además, supongamos que la distribución condicional
de    Y dado   Z,X , es   lHvj v»^ )


Para Z fijo S9 tiene.entonces que


J(y|z) = I jal», xyxlz)



es decir, que la dlstrttucíSn condicidnal de Y, á a ^           Z,
eá una mexela de norhiale» «ultIvaS'iantcA.

      Ahora é i p. (li) •« i t deiíis^dad de proi>abllidad de

                                                                                      37


W =(Z,Y) en            n, (k-1,2), podt.. -s tomar como criterio de
clasificación (suponiendo probabilidades a priori iguales
y costos de mala clasificación iguales) la de asignar                                W
a   ir     si p.(W)/p2(W) >_ 1                  ya         -n     de otra manera. Pero
Pj^(W) = p^(Z,Y) = Pj^(Z)^^(Y|Z) (k=l,2) V por tanto asig-
namos la observación                  W= (Z,Y) a la población               TT si


                    P^(W)       ^^(Y|Z) p^(Z)
                                                                > 1
                    P¡^'        Ío(Y|Z)p.(Z)

y a la población rr _                 de otra manera.                 •


         Se tiene entonces que :


í,(YlZ)p^(Z) [Jexp(-|(Y->fflE-^(Y-py).V.^}jp^(Z)
^(YÍñ¡¡(Z)"fE^^                                                                          "'
es decir                                                                            (2.2)


P,(Z) ^ j,exp(-|<Y-uy)-I-'(Y-..yH t„ p^„)


              i=i

entonces, transformando la expresión (2.3) se obtiene :



        P2<Z)  M                           .
        ----< E                            i               -_                    (2.U)
        Pl(Z)-j.l J                      {Y' A        +b    }
                            1=1      ^           13    iJ

j   j     ,          _-l    ,   (m)        (m) V
donde     A.. = E           (p^. - p..            ),
           13                   2i         13          '

 38

             ,           . (m)' „-l (ra) (m)' -i (m)^                  Pjn
             ^ij- <^lj             ^ \ ^ -^2i ^ 4 i ) ^ ^ n - 7 ^
                                         •^                            *^im


De nuevo, en la práctica los parámetros poblacionales £,e-
neralmente               son desconocidos de modo que la regla de cla-
sificación dada no puede usarse directamente y es necesa-
rio estimar tales ^rámetros.                        Con este fin podemos consi-
derar muestras de tamaños, n.                        y   n^    de las poblaciones
TT^      y TI , respectivamente, y para cada una seguir el mé-
todo de máxima verosimilitud descrito por N. E. Day (1969)
para estimar los parámetros de una mezcla de distribucio-
nes normales. En este caso, para una muestra de tamaño n,
se obtienen las siguientes ecuaciones de máxima verosimi-
militud :

A partir de estas ecuaciones y mediante un proceso de iteración se obtienen las estimaciones de los parámetros en consideración.
Esta estimación podría realizarse por otros tales como el de mínimos cuadrados descrito por el mismo Day (1969), o por el método de la función generatriz de momentos descrito por Quandt y Ramsey (1978). En todos estos casos es factible usar el método iterado (algoritmo EM) descrito por Dempster, Laird y Rubin (1977) para obtener las estimaciones.
BIBLIOGRAFÍA
Day N E.Estimating the components of a mixture of Normal Distributions.(1969).Biometrika.
Hochberg Y.On the use of Double Sampling Schemes in Analyzing Categorical Data with Missclassification Errors.(1977).Journal of the American Statistical Association.
Krzanowski W J.Discrimination and Classification Using Both Binary and Continuous Variables.(1975).Journal of the American Statistical Association.
Dempster A P,Laird N M,Rubin DB.Máximum likelihood from Incomplete Data Via the EM Algorithm.().Journal of the Royal Statistical Society.
Quandt R,Ramsey B R.Estimating Mixtures of Normal Distributions and Switching Regressions with Comments.(1978).Journal of the American Statistical Association.
