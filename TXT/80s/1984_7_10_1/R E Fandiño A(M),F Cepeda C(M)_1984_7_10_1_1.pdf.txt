PALINDROMIA SEUDO-PALINDROMIA Y CADENAS DE NARKOV
Universidad Nacional de Colombia
Resumen. En e s t e a r t í c u l o se ve cómo l a t e o r í a
de r e f l e x i ó n de m a t r i c e s y p a l i n d r o m í a a p o r t a
n u e v a s l u c e s s o b r e l a forma como c o n v e r g e l a
s u c e s i ó n de m a t r i c e s que i n t e r v i e n e en una ca-
dena de Markov a p e r i ó d i c a e i r r e d u c i b l e .

* Las palabras oso, reconocer, e t c . y los números 1 1 ,
  101, e t c . son palindrSmicos (capicúos). Porque l e í -
  dos de derecha (izquierda) a izquierda (derecha) .son
  l a s mismas palabras o números.
** Resumen de t e s i s para optar a l grado de Magister
    Scientiae en la especialidad de Estadística en la
    Universidad Nacional. Autor:Ramón Iandino. Director
    de Tesis: F.J.Cepeda.
    Expuesto también en el Encuentro Regional de Mede-
    l l í n , Mayo 2-S de 198i|.

Introducción.

       La transposición de matrices es un tema
obligado en cualquier curso de álgebra lineal
y alrededor de él motivaremos una serie de in-
quietudes que permitan orientar la compren-
sión del presente artículo.   De otra parte, con
ella están relacionados temas tales como: matri-
ces simétricas, traza, polinomio característico,
descomposición ortogonal, etc. sobre los cuales
giran muchísimas pruebas y criterios útiles en
estadística.

       Sin embargo, pese a ser un tema bastante
trabajado, en nuesta opinión, parece haberse to^
cado muy unilateralmente al reducirlo al solo
caso de la transposición. En el artículo se mue¿
tra un ejemplo de cadenas de Markov, en el cual
aparecen matrices simétricas distintas a la tra
dicional   (palíndromas).



Aplicaciones de los giros de matrices.

       Para apreciar la importancia   concedida
por los investigadores al movimiento de matri-
ces conocido como transposición y sus temas afi
nes, enunciaremos una modesta gama de sus apli-
cac iones :

a) Matrices simétrii-as (A                          = A, donde A CH un.I
matriz cuadrada). S<>ii matriceH síraétri4.-as df l.i
varianza y covarianza, la matriz de correlation
la matriz asociada a una forma cuadrática, ele.

          Consideremos las variables aleatorias X,
y, Z y sus momentos con respecto al origen de
longitud k + t + m ,        donde k ,          I,    y m son naturales,
que notaremos simplemente como:

     **fe £ m " MiX^y^Z"') = EíX^y^Z*")

En particular los momentos de segundo orden,
expresados en su correspondiente matriz como:


   **2,0,0, *^1,1,0 *'l,0,l
                                                    aX    y^xjya Xa {/ y'x2o Xaz

   **1,1,0       **0,2,0 *^0,1,1               Y    0 0               y^yza yo z
                                               ^ xy X y
   **1,0,1       ^^0.1,1 *^0,0,2               y a a y o o                 o
                                               'xz X í/ yz y z                 z

o sea, la matriz de correlación R

                        1    Y        Y
                              ' Xí/   ' XZ

                       xy                 yz
                      ^xz    ^yz          1

b) En cuanto a la traza í t r i A )                       =    ^    ^¿j»
                                                              i=l
A = ia> ¡ ) „ ^ ^ ) , s e   conoce un sin número de pruebas
en análisis multivariante que utilizan dicho

i'Miii «.'|>l o ,    ri>mit     SUD :      los       criterios          de    Wilks,     de
Kii V ,    ile lio t *.• I I 1 ii>;-li.iw l e y ,            «I f .

c)    l.os    polinomios             r.i r a í - t e r í s I i e o s     tienen       Ire-
iiieiites       ;i|i I i c u - i u n e s    en      el       manejo     de     ciertos
ll i t e r i o s     Otiles        en      la       reducción          de    formas     cua
«Iríítir.íK         ,1 s u    forma        canónica.                Recordemos     que
una       forma      cuadrática             puede            expresarse         como:
                                                n        n
      Hx , x „ , . . . , x ) =                  y        y      a     -x x    = X AX


donde         X, X           = (Xj , X» , . . . , X ) , y A es su corres^
pondiente matriz asociada.

             Dichas aplicaciones parecen utilizar el
concepto de trasposición de matrices y algunos
temas afines o relacionados con ella, por eso
nos parece que pudiese ampliarse si sugerimos
otros movimientos distintos a éste, por lo cual
en el presente trabajo lo consideraremos como
pionero de los otros movimientos en cuanto a no
taciones, propiedades y usos.


Grupo diedro u óctico.

             C o n s i d e r e m o s el         coniunto              de las ocho       re-
flexiones            y giros de             im c u a d r a d o .         Este     conjunto
dotado de             la     ley de rompos i i-i ón                    interna     "compo-
sición        entre          fun< i o n e s " c o n s t i t u y e            un grupo    que

s e c o n o c e como d i e d r o u ó c t i c o en l a           literatu-
ra a l g e b r a i c a .   Veamos a l g u n a s e s p e c i ^ i<-idode» :

               Yj •   Yj »      Yv»   Y,i     (cuatro s i m e t r í a s del cua
                "l     "2        *     »       drado)
           1     Pfl = l-ttlj                  (cuatro rotaciones alrede
                          '                     dor de su centro geomé-
                  l = 1,2,3,4                   trico).

que p a r a      facilidad        notaremos        como


 ^       = í^dl =^ ^ 1 ' ^¿2 ' ®2'          ^x = ^ 3 '   ^y = ^ • • • " 2 ^ ^8^

Utilizando una notación semejante a la de la
transposición observaremos que

                      e^: P^ X M •> M
                           (6 . , m ) -»- m®-*^ - ^^í*"^ »


donde M es el conjunto de las matrices cuadra-
das ny.n.

           En este artículo profundizaremos en el es^
tudio de solamente dos movimientos distintos al
usual (transposición), son ellos: ^o " Y^ = Rl'
que llamaremos simplemente "Reflejo vertical" y
que en lo sucesivo notaremos como Rl/.

     e    «Y      = RH, es el "Reflejo horizontal" (RH).

Movimientos que satisfacen propiedade.s bastante

siniil.ires     .i    l.is    de   I ;i    t r . i s p o s i c i ón    on       .ilj^-.iinos
c.'isos ,    como :

involuliv.i:           (A»")»"                 A.    íA^Vy'^V          ^    ^


Linealidad:            (A+8)«" = AR«+6'^«,                      ( A + B ) ' ^ ' ^ = A'^I'+S'^''   ^
                       , .,RI<             .RH         , , A ^ ^ ^       A ^ ^
                       ( CA) =            CA  ,        ÍC A )          = cA                       ^
                       VA,BGM,             VÍ^ e     R.


En tanto que en otras difieren notoriamente, ta
                         RV          RV
les como, (AB)                 = A         «B.       Igualmente se pueden
obtener resultados análogos con otros productos
útiles en estadística                          como:
a) Producto de Hadamard
b) Producto de Kronecker
c) Producto de Khatri-Rao
d) Inversa común e inversa generalizada, etc.
            Sin embargo no ahondaremos en detalles.



Matrices palíndromas.

            Consideremos el conjunto de matrices

 P,(e .) = {m, w^-^ = m} , donde                                6.c P                 m e M
   5    /t                                                        A.        H
que no es otro cosa                       que ocho conjuntos de ma-
trices simétricas desde puntos de vista diferen
tes,    o sea matrices que permanecen                                  inalteradas

según se haga cualquiera de los movimientos o
giros 8-, anteriormente considerados, especies
de "puntos fijos".

       Fueron objetos del presente trabajo los
conj untos:

   P i i ^ 2 ^ = PARV) = {m,m^^ - m } ,

   P,(e^) = P^ifH) = {m,m^^ = m),

que llamaremos conjunto de las matrices palín-
dromas verticales al primero, en tanto que el
segundo el de las palíndromas horizontales. Hje
mos escogido el nombre de palíndroma puesto que
describe perfectamente ciertas propiedades que
queremos caracterizar aquí. Recordemos también
que a los números 11, 101 y a otros se les cono^
ce como primos palíndrc^micos o capicúos, que
leídos de izquierda a derecha o de derecha a iz^
quierda son los mismos.



Aplicaciones.

      Estas nuevas simetrías amplían también el
número de aplicaciones, por eso creemos conve-
niente ilustrar con un ejemplo en cadenas de
Markov de su utilización.
      Recordemos algunas definiciones básicas.

      Un Proceso E s t o c á s t i c o ea una familia
{Xj^l.^y de variables aleatorias.

      El conjunto T es el conjunto de índices o
conjunto de tiempo. El conjunto S es el conjun-
to de todos los posibles valores que pueden to-
mar las variables aleatorias X^; se llama el es^
pació de estados del proceso.                                   >*


             Un Proceso de Markov es un Proceso E s t o -
c á s t i c o en el cual si, dado t €. T y la variable
aleatoria Xj. se tiene que los valores de la va-
riable aleatoria X con S > t no dependen de los
                         s
valores de la variable aleatoria X^^ con u < t .
      Una Cadena de Markov es un Proceso de Mar
kov en el cual S y T son discretas.

      Una Cadena de Markov I r r e d u c i b l e es una
Cadena de- Markov en la cual todos los estados
se comunican.

         Una Cadena de Markov I r r e d u c i b l e y aperlo^
d l c a es una Cadena de Markov I r r e d u c i b l e en la
cual se pasa de un estado a otro en cualquier
número de etapas.

                         Proceso        Estocástico
                           Proceso de Markov
                         Cadena de Markov

                   Cadena de Markov Irreducible

                         Cadena de Markov
                          irreducible y
                             aperiódica




Teorema 1. Toda cadena de Markov i r r e d u c i b l e y
aperiódica t i e n e sus estados r e c u r r e n t e s p o s i -
tivos.
      Un Estado Aperiódico y R e c u r r e n t e                   Posltl
vo se llama Estado E r g ó d l c o .

Teorema 2. Sea P l a m a t r i z de t r a n s i c i ó n en una
e t a p a de una cadena de Markov i r r e d u c i b l e y ape_
r i ó d i a a ; e n t o n c e s e x i s t e un N e n t e r o p o s i t i v o
t a l que p a r a todo n "i. N se t i e n e que Pn                       .(n)
no t i e n e elementos n u l o s .

Teorema 3. Sea P una m a t r i z de Markov a p e r i S d i -

10


aa <; i r r a d u a i b l e aon m n o t a d o s . (Estoí} m c i s i a -
dotj non d,- hcaho fc c u r r a n t e ts poa i t i v o o ) . ?,'nLon
aes :



     tlm f^ = n =                     ; donde v = [%»^i» • • • . ^ ^ J

                           TT'
                                  m^m
       m-1
y       J ir.=        l      y     O-^ir.^l              2/11 e s   uniaa;
       1=0 "^                          ^
y     np = pn = n            y     n p"n = n ;

y     n = [TI^ n , TTj 1 , . . . , T r ^ _ j a ] .

      La matriz II se llama matriz estacionaria
o matriz límite y el vector ir se llama distri-
bución estacionaria de la cadena de Markov con
matriz de probabilidad de transición P.

Nota. Obsérvese que II es palíndroma vertical y
por consiguiente II = EH.

Teorema 4. s i            P es e s t o c á s t i c a ,    entonces P         tam
b i e n lo e s .


Palindromía y Matrices Estacionarias.

Teorema 5. l ) s i               P e s una m a t r i z de Markov ape-
r i ó d i c a e i r r e d u c i b l e aon m e s t a d o s y Jí e s su

                                                                             11


c o r r e s p o n d i e n t e m a t r i z l i m i t e , e n t o n c e s II ee pa^
lindroma v e r t i c a l . En e f e c t o , p o r e l Teorema 3,
n es de la forma:
                 n - [ir^ 1 , TTj n , . . . ,7r
                                                     m- 1-J •

2) S i P es una m a t r i z de Markov a p e r i ó d i c a e
i r r e d u c i b l e aon m e s t a d o s y es además p a l i n d r o -
ma v e r t i c a l y H es su c o r r e s p o n d i e n t e m a t r i z li_
m i t e , entonces, n es palíndroma v e r t i c a l . Por
(1) es i n m e d i a t o .

3) S i P es una m a t r i z de Markov a p e r i d d i o a e
i r r e d u c i b l e con m e s t a d o s y es ademas p a l i n d r o -
ma h o r i z o n t a l y Jl es su c o r r e s p o n d i e n t e m a t r i z
l i m i t e , e n t o n c e s II e s palindroma d o b l e . Por (1)
n es palíndroma v e r t i c a l ^ veamos que además II
es palíndroma h o r i z o n t a l .
          Tenemos IIP = ü^ l u e g o            (nP)E = (II)E, o s e a
II(PE) = HE. Como P es palindroma h o r i z o n t a l ,
np « nE. P e r o IIP = n^ l u e g o IlE = 11^ l u e g o n e.g
palíndroma h o r i z o n t a l . Como Jl es palíndroma ver
t i c a l y h o r i z o n t a l , es palindroma d o b l e .

4) Si P es una m a t r i z de Markov a p e r i ó d i c a e
i r r e d u c i b l e con m e s t a d o s y es además p a l í n d r o -
ma d o b l e , y Jl es su c o r r e s p o n d i e n t e m a t r i z lími_
t e , e n t o n c e s n es palíndroma d o b l e .
Eate resultado es inmediato en virtud de (3).

12

 S) S i P es una m a t r i z de Markov a p e r i ó d i c a e
 i r r e d u c i b l e con m e s t a d o s y e s ademds doblemen^
 t e e s t o c á s t i c a , y Jl es su c o r r e s p o n d i e n t e matriz
 l í m i t e , e n t o n c e s n es t e t r a s i m é t r i c a y de la
forma: II = — 1 1 . En e f e c t o : Como P es d o b l e -
mente e s t o c á s t i c a , e n t o n c e s P e s e s t o c á s t i c a y
ademas a p e r i ó d i c a i r r e d u c i b l e , luego e l
   l m (P ) " = n,,
tn-H»                     1'
                              donde JJ-,L es ¡^palíndroma v e r t i c a l
 de l a forma H, = |a 1,a,l,...,a _,1|. Pero
t l m P*^ = n M (P^)*^ = iP"-)^ M n M n, son ú n i c a s ,             lúe
    -VOO
go         n , = n . Pero como por (1) Jí es palíndroma
ve r t i o a l ,  e n t o n c e s H o s e a 11. es palíndroma hq_
r i z o n t a l . Ademas, H. es una m a t r i z e s t a c i o n a r i a ,
de donde H^ es palíndroma d o b l e . Luego tenemos:
n = [Tr^l,-njU,...,TT^_jl], 2/ nj = [a^ll,ajl,..,a^jll]
                   T
y          n^ = n'                                  TTQI

                                                    TT,U'
Luego              [a^l,ail.---.a^_ll] =

                                                    ""m-^

De d o n d e : a o = a 1. = . . =a             ir      ^1=         •m-1
                                   m-1
O sea:                 n = a 1 II
                                     T

Pero oomo n. es e s t o c á s t i c a (doblemente estocas_
t i c a ) , e n t o n c e s : a = —, donde m es e l orden de
P. Luego:                         m
                               n       m

                                                                        13


6) S i P es una m a t r i z de Markov a p e r i ó d i c a e
i r r e d u c i b l e con m e s t a d o s y es además doblemen^
t e e s t o c á s t i c a y es p a l í n d r o m a , o palíndroma
h o r i z o n t a l , o palindroma d o b l e , o t e t r a s i m S t r i ^
ca y Jl es su c o r r e s p o n d i e n t e m a t r i z l í m i t e , e«
t o n c e s n es t e t r a s i m é t r i c a y de l a forma
n = — 1 1 . Este r e s u l t a d o es inmediato en v i r
        m                                                             —
tud de ( 5 ) .
         De este teorema podemos concluir:
(1) Toda matriz estacionaria (límite) o es pa-
líndroma vertical o es palíndroma doble, o es
tetrasimétrica; ó

(2) Si P es una matriz de Markov aperiódica e
irreducible con m estados, entonces la sucesión
de matrices {P )„_, 9     converge a una matriz
palíndroma vertical o palíndroma doble, o tetra^
simétrica, ó

(3) A toda cadena de Markov aperiódica e irredu^
cible con m estados le corresponde una única ma^
triz estacionaria palíndroma vertical, o palín
droma doble, o tetrasimétrica.

Teorema 6. Si II es una m a t r i z l í m i t e c o r r e s p o n -
d i e n t e a una cadena de Markov a p e r i ó d i c a e irre_
d u o i b l e con m e s t a d o s , e n t o n c e s n es idempoten-
t e . En e f e c t o :

14



                                                            TI TT ,    ."n:m - 1
                                                              O   1

                        [^o"- \ ^              Vl^]          O    1       m-1
                                                            TT i r ,
            ir                                                O 1         m-1

luego n          = n.

Teorema 7. s i n es una m a t r i z c o r r e s p o n d i e n t e a
una cadena de Markov a p e r i ó d i c a e i r r e d u c i b l e
con m e s t a d o s , e n t o n c e s e l r e f l e j o h o r i z o n t a l
                    RH
de J[, o sea n          es i n v e r s a g e n e r a l i z a d a de IT.
En e f e c t o : Como J\. es i d e m p o t e n t e , entonces
                       n = n^       y   n = n^ = n n n.
Luego:       H = H n EE n.
                    RH
Pero nE = n             y EJ\. = J[, ya que n es p a l í n d r o -
ma v e r t i c a l . Luego
                           n = nn'^^ n.
                  RH
De donde n              es una i n v e r s a g e n e r a l i z a d a de n.

Seudo Palindromía.

      Para ilustrar los conceptos de esta sec-
ción utilizaremos las matrices de probabilida-
des de transición y sus correspondientes suce-
siones de potencias de un proceso Markoviano
relacionado con el sistema de descuentos y so-
breprimas para el seguro de responsabilidad ci^
vil en automóviles (Granados, 1983) .

                                            15


      La matriz A (Anexo 1) representa las pro-
babilidades de transición entre los diferentes
estados de descuentos del proceso markoviano co^
rrespondiente al sistema de descuentos vigen-
te, y la matriz B (Anexo 2) representa las pro^
habilidades de transición entre los diferentes
estados de descuentos del proceso markoviano
correspondiente a un sistema de descuentos pro-
puesto (Granados, 1983) .

      Dada una cadena de Markov aperiódica e
irreducible, es decir, dada su matriz de proba-
bilidad de transición en una etapa y la corres-
pondiente sucesión de potencias de la matriz
mencionada se observa que a medida que n crece,
la sucesión tiende a estabilizarse y tiende a
un límite (la matriz estacionaria H ) . Pero re-
cordemos que esta matriz es palíndroma vertical
y por consiguiente las sucesivas potencias de
la matriz original se van tornando en forma pro^
gresiva en palíndromas verticales, es decir, a
partir de cierta potencia (20 y 40) (Anexos 3,
4) las matrices de la sucesión son casi palín-
dromas. Este hecho nos motiva a definir un tipo
especial de matrices a saber; las matrices seu-
dopalíndromas. Como se verá más adelante, toda
sucesión de matrices aperiódicas e irreducibles
converge a su matriz estacionaria o límite a

16


través de una "nube" de matrices seudopalóndro-
mas.
              En la definición vamos a utilizar la nor-
ma de operadores como sigue

    lAj = -cn^ {fe/|Ax| ^ fe|x| para todo   X CK"}

y hacemos las siguientes consideraciones:

Como t l m P** = n,        entonces   t l m (p" - H) = O
     n-H»                             n-x»
y    t l m |p'^-n| - 0.
     n-»-oo
              Luego necesariamente existe N entero posi^
tivo tal que para todo n ^ N+1 se tiene               que
|ppRHp_n| > |P'^-n| y por consiguiente N e s             el
mayor entero para el cual se cumple que
|ppRHp_jj| ^ |p^-n||.

Definición. Sea P la matriz de probabilidad de
transición en una etapa de una cadena de Markov
aperiódica e irreducible y sea n su matriz esta
clonaría.

              Se dice que P es seudopalíndroma       ergódica
de grado N si N es el mayor entero para el cual
se tiene que:

                    iPpRW p.n| ;< |P^-n|.


Consecuencias de la definición.
1) Si P es seudopalíndroma e r g ó d i c a de grado N,

                                                   17


es claro entoncea que | PP'^^P-n|| > || P*'"'"^-n|| y
por consiguiente, pp^*^p « P^ y Pp^^^p está más
cerca de H que P    de n.

2) Toda matriz estacionaria es seudopalíndroma
ergódica de grado 1, pues ||nn'^^n-n|| « |n^-nl|.

3) Toda matriz p de probabilidad de transición
en una etapa correspondiente a una cadena de
Markov aperiódica e irreducible es seudopalín-
droma ergódica de grado N para algún N entero
positivo. Esto resulta de las consideraciones
anteriores a la definición de seudopalindromía
y de la misma definición de seudopalindromía.

4) De la consecuencia (1) se desprende inmedia_
tamente que la sucesión {P P P } es mucho más
rápida que la sucesión {P } y desde luego con-
verge a la misma matriz estacionaria n. En efec^
to :

    n - n^ = nn = nn = nEn = u i m p'"'*'^)EítXm f^)

  = itlm f PE) itCm f ) = a m   if^íPE)}^)

  ^timfñ>^f      - n.
      Con base en las anteriores consecuencias
de la definición de seudopalindromía nos permi-
timos recomendar lo siguiente: Cada vez que se

18


quiera calcular la matriz eatacionaria n corres^
pondiente a una cadena de Markov aperiódica e
irreducible en lugar de calcular la sucesión
{P } se calculará en su lugar la sucesión
{P P P }. Como ésta es máa rápida se ganará
tiempo de computador lo que redunda en un bene-
ficio económico.

        En efecto, en los ejemplos, la sucesión
{A } se estabiliza en 26 pasos mientras que la
          r«wi«
     .... IA  A RnAi mi      ^,.,. en 21_,pasos.
sucesión             | se estabiliza

        Asi mismo, la sucesión {B } se estabiliza
en 70 pasos mientras que la sucesión {B B                   B }
se estabiliza en 66 pasos (Fandiño, 1984).

5) Podemos decir finalmente que si P es una ma-
triz de Markov aperiódica e irreducible y
       n
t l m P = n; la convergencia de la sucesión
n^^
 {P'^}„^i, hacia n se hace a través de una "nube"
      ncN
de matrices seudopalíndromas hasta llegar (en el
límite) a la matriz n. Teniendo en cuenta que
cuando la sucesión {P }„^ju penetra en la "nube"
 (a partir de N) la convergencia hacia n es rapi^
dísima. Lo cual a su vez implica que las matri-
ces con las cuales se trabaja en la práctica aon
precisamente matrices s e u d o p a t l n d r o m a s e r g ó d l -
caS (ver Figura 1) .

                                                      19


ESTOCASTICAS APERIÓDICAS IRREDUCIBLES




  EAI: Estocástica aperiódica i r r e d u c i b l e
  PV: Palíndroma v e r t i c a l

20


         PH:    Palíndroma horizontal
         PVi    Palíndroma doble
        5 PE: Seudo palíndroma ergódica
         EST: Estacionaria
         EV: Eatocástica doble
        TS:     Tetrasimétrica.


Conclusiones.

1) El grupo diedro ha permitido ampliar la opi-
nión que se tiene del concepto de trasposición
de matrices mediante ocho movimientos análogos
y por ende la ampliación de conceptos afines o
relacionados con el de la trasposición y el de
sus aplicaciones.

2) El teorema de descomposición ortogonal que
en nuestro caso corresponde a la descomposición
palíndroma de vectores, sugiere nuevas alterna-
tivas en la explicación e interpretación de los
modelos lineales por cuanto permite apreciar
ciertos efectos especiales en las variables ex-
plicativas de dichos modelos, no especificada
en este resumen.

3) En cuanto a la convergencia ergódica, la pa-
lindromía caracteriza cómo se efectúa realmente
este fenómeno además provee la construcción de
sucesiones que convergen rápidamente a la matriz
estacionaria.

                                                                        21


4) La palindromía tiene grandes posibilidades
de aplicación en el estudio de formas cuadrá-
ticas, estudio de arreglos factoriales, cris-
talografía, etc. lo cual prevee la posibilidad
de realizar investigaciones futuras.




BIBLIOGRAFÍA
Fandiño A. Ramón E. Reflexión de Matrices y Palindromía. Rev. Col. de Estadística No.5, Universidad Nal. de Col., Bogotá, 1982.
Fandiño A. Ramón E. y Cepeda C. Francisco. La Transposición y otras simetrías de matrices cuadradas. Rev. Col. de Estadística No. 7, Universidad Nal. de Col., Bogotá, 1983.
Fandiño A. Ramón E. y Cepeda C. Francisco. Simetrías y giros de matrices cuadradas. Matemática, enseñanza universitaria No.30, Edit. Yu Takeuchl, Bogotá, 1984.
Fandiño A. Ramón. Reflexión de Matrices y Palindromía con resultados estadísticos. Tesis de Magister- Universidad Nal. de Col., Bogotá, 1984.
Granados D. Manuel A. ProceSOS MarkovianOS del sistema de descuentos y sobreprimas para el seguro de responsabilidad civil en automóviles . Trabajo de grado. Carrera de Estadística, Universidad Nal. de Col., Bogotá, 1983.

