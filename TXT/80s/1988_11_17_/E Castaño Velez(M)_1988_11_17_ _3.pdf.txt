OBSERVACIONES INFLUENCIALES
Universidad de Antioquia
Resumen. El análisis de regresión lineal múltiple por mínimos cuadrados ordinarios es quizás la técnica estadística más aplicada por muchas ciencias para modelar la relación entre varias variables. En las últimas décadas ha habido un gran desarrollo en el estudio de los factores que determinan el ajuste de la ecuación de regresión, como son las observaciones, las variables y las hipótesis del modelo. En este artículo presentaremos algunos de los procedimientos empleados para medir el efecto que tiene sobre los resultados del ajuste y sobre las conclusiones obtenidas, la eliminación (o la inclusión) de una o de un grupo de observaciones. También se discutirá el papel que juegan algunas observaciones en la generación o en el enmascaramiento de relaciones de colinealidad en la matriz de datos del modelo de regresión lineal.
Introducción. Varios elementos determinan el ajuste de una ecuación de regresión: las variables, las observaciones y las hipótesis del modelo. En este artículo nos concentraremos fundamentalmente en el papel que juegan las observaciones, individualmente o en grupo, en la estimación y en las conclusiones obtenidas de la ecuación de regresión ajustada, y en algunas de las metodologías más comúnmente empleadas para describir su comportamiento.
Con el fin de comprender mejor el papel realizado por los datos en el ajuste de la ecuación de regresión veamos algunos ejemplos sencillos que nos permitan diferenciar las diferentes formas como una observación puede actuar sobre el ajuste de la ecuación. Con el fin de simplificar la exposición consideremos el modelo lineal simple. (Los siguientes ejemplos se encuentran en Chatterjee y Hadi (1988)).



Ejemplo 1.

     Consideremos los datos del gráfico         1. Suponga que a los
datos marcados con un + queremos agregar, uno a la vez, los
datos marcados con las letras A, 6 y C.

    La inclxisión del punto A en la regresión generaría un pér-
queño residual debido a que A está en la dirección de la recta
que pasa por todos los demás puntos.' Esto implica que la obser-
vación A no tiene una gran influencia sobre la ecuación ajusta-
da, es decir no cambia los coeficientes estimados. Por tanto,
a pesar de que A es un punto extremo en X (por esto es denomi-
nado punto de a l t o poder) y en y, A no es I n i l u e n c l a l para la
estimación de los coeficientes de la ecuación, aunque puede
serlo para el error estándar de los coeficientes de regresión.

    Por otro lado, si B es incluido generará un gran residual
(por esto es llamado un punto O u t l i e r ) y aunque puede no cam-
biar el estimador de la pendiente si alterará el intercepto de
la recta. Su inclusión también puede cambiar la varianza esti-
mada del error y, en consecuencia, las varianzas estimadas de
los coeficientes. Por tanto una observación como B es un o u t -
l i e r (no es de a l t o poder puesto que no es un extremo de X) y

                     =1.^.»^^—^"




40

es un punto I n i t u e n c l a l ,

     Por último, si agregamos .-el punto C (el cual es un punto de
alto poder puesto que es extremo de X) a los datos marcados e<m
+, se genera un gran residual (C será ^Jtonces un *outlier*) y
se alterarán sustancialmente las características de la regre-
sión ajustada (C es un punto influencial). Por tanto C es un
punto outtlier., .Iniluenclal y de alto poder. Este ejemplo ilus-
tra diferentes formas como las observaéiones actúan sobre el
ajuste de la ecuación: en forma de outliers, puntos de alto po-
der y puntos influencíales; se observa también que estos con-
ceptos no son excluyentes y se revelan algtmas relaciones entre
ellos.

     El siguiente es un ejemplo para ilustrar que los 'outliers'
no necesariamente son observaciones influencíales y que las ob-
servaciones influencíales no necesariamente son 'outliers'.

                   Distinción entre 'outliers', puntos de alto
                      poder y observaciones influenciales.

                                                                                      41


Ejemplo 2.
     Considere el gráfico 2. Si ajustamos una linea recta a los
datos del gráfico, claramente A será tui 'outlier' (dejará un
residual grande). Sin embargo su exclusión díficilmente cambia-
rá los resultados del ajuste. Este es el caso de un 'outlier'
que tiene poca influencia sobre 3* Ahora bien, la observación
6 tendrá un residual pequeño pero sin embargo, cuando la ex-
cluímos, los coeficientes de regresión estimados varían sustan-
cialmente. Este es el punto influencial que no es un 'outlier'.

     Estos ejemplos muestran que el solo examen de los residua-
les puede no servir para detectar observaciones aberrantes o
inusuales y que los métodos gráficos basados en residuales so-
lamente, pueden fracasar en reconocer dichos puntos. Por tanto
se necesiten! medidas estadísticas que persdtan detectar el p o -
d e r y la I n f l u e n c i a de las observaciones.


                Los ' o u t l i e r s ' no son necesariamente puntos
                 i n f l u e n c í a l e s n i los puntos i n f l u e n c í a l e s
                            son necesariamente ' o u t l i e r s ' .

42

     Existe una vasta literatura estadística sobre construcción
de medidas para detectar observaciones influencíales y medir sus
efectos sobre varios aspectos del análisis: Besley et al (1980X
Cook y Weisberg (1982), Atkinson (1985), Chatterjee y                        Hadi
(1986, 1988), etc. En este artículo solamente desarrollaremos
algunas medidas basadas en la eliminación de una observación O
de un grupo de observaciones.

     El esquema de este artículo es el siguiente: en la sección
1 se recordarán algunos de los resultados básicos del ajuste
del modelo lineal general por mínimos cuadrados; la sección 2
presenta algunas propiedades e interpretaciones de los elemen-
tos de la matriz proyección P = X(X'X) X'. Además se presentan
definiciones de 'outlier', punto de alto poder y punto influen-
cial así como las relaciones existentes entre estos conceptos.
En la sección 3 se discuten algunas medidas para detectar la
influencia de una sola observación y de un grupo de observacio-
nes sobre la ecuación de regresión; una breve discusión de la
influencia de múltiples observaciones se encuentra en la sec-
ción 4 y la sección 5 introduce el problema de la colinealidad
y su relación con los puntos influencíales.



1. Supuestos y Resultados Básicos de la Estimación
   por Mininos Cuadrados.
1.1. El Modelo.
     Suponemos que la relación existente entre una variable Y y
las variables X, ,X2,« • • ,XÍL, es de la forma

                                   y = X3,+ e
donde
y es el vector r e s p u e s t a de nxl o variable d e p e n d i e n t e .

                                                                 43


X es ima matriz de nxfe de predictores, o regresores, o varia-
bles explicativas,
3 es un vector defexlde coeficientes desconocidos (parámetros)
que van a ser estimados y,
c es un vector de nxl de perturbaciones o errores aleatorios.



1.2. Los Supuestos.

    El procedimiento de estimación por Mínimos Cuadrados Ordi-
narios está basado en los siguientes supuestos:

(a) Hipótesis de Linealidad:
    Este supuesto indica que la ^-ésíma respuesta observada
    puede ser escrita como una función lineal de la >¿-ésíma
    fila x'. de la matriz X, es decir

              y . = x'-B+e,-,    para 1 = 1,2     n.
               >C   -v     'V


(b) Hipótesis Computacional:
   Para que exista un único estimador de 3 es necesario que
    (X'X)   exista o equivalentemente,

              rango (X) = fe


(c) Hipótesis Distribucional:
      i) X está medida sin error,
     ii) e- no depende de x'.,    1=   l,...,n.
   iii) e 'V W (0,a T ) , es decir, la distribución conjunta de
         las n perturbaciones aleatorias és multinormal con
                                                     2
         vector de medias O y matriz de covarianzas o I.

(d) Hipótesis Implícita:
   Todas las observaciones son igtialmente confiables y deben
   jugar un igual papel en la determinación de los resultados

44


     y las conclusiones.



1.3. Resultados Básicos de la Estinación por Mlninros
     Cuadrados.

     El estimador de 3 por el procedimiento de los Mínimos Cua-
drados (E.M.C.) se obtiene minimizando la expresión
(y-XB)'(y-X3) con respecto a 3* Tomando la derivada con res-
pecto al vector 3 obtenemos las ecuaciones normales o de esti-
mación

                             (X'X)B = x'y

Este sistema tiene solución única si y sólo si (X'X)         existe
y en este caso la solución es

                            6 = (X'X)"^X'y

     Si los supuestos de la sección 1.2 se cumplen la teoría de
los E.M.C. proporciona los siguientes resultados:
          (Graybill (1976), Draper y Smith (1981)).

(a) El vector 3 tiene las siguientes propiedades:
         (i) E(3) = 3i es decir B es un estimador insesgado para 3.
      (ií) 3 as el mejor estimador lineal insesgado para 3, y su
                                        • ^ 2 — 1
           matriz de covarianzas es Cov(3) = O (X'X)
     (iii) 3-v N¡^iB,a^ÍX'X)~b-
(b) y - xS - X(X'X)~^X'y = PY ea e l vector de los valores ajus-
     tados de y. Entonces
         ii) Eíh = XB
      (ii) Cov(P) = a^p
     (iii) 9 ' ^ N Í X B , a ^ P ) , donde P - X(X'X)'^X'.
                  n
(c) El vector de nxl de residuales (ordinarios)

                                                                        45


                      e = y-^ = y-py = =ii-p)y

    tiene las siguientes propiedades;
       (i) Eíe) = O
      (ii) Cov(e) = a^(I-P)
     (iii) e 'V. W^(0,a^(I-P))
      (iv) e'e/a2 -v x^ (n-fe)
       (v) e = (I-P)e
                                           2
      (vi) Un estimador insesgado de a         está dado por

                 a^ = e'e/(n-fe) = y(I-P)y/(n-fe)



Observación.

    Las matrices P e I-P son singulares y por tanto las dístrj.
buciones multinórmales de P y de e son singulares              (para
una definición veáse, por ejemplo, Mardia et al., (1978). En
la próxima sección discutiremos las relaciones de los elemen-
tos de las matrices P e I-P con los resultados de la estima-
ción, y sus propiedades e interpretación.



2. Propiedades e Interpretación de los Elementos de
   la Matriz P.

    En la sección 1.3 vimos que la matriz P determina muchos
de los resultados de la estimación por M.C. Esta matriz recibe
el nondire de matriz p r e d i c c i ó n debido a que al ser aplicada a
y produce los valores predichos; también recibe el nombre de
matriz h a t (sombrero, en inglés) puesto que al aplicarla a Y
le coloca el sombrero; otro nombre dado a P es el de matriz
p r o y e c c i ó n , puesto que proyecta ortogonalmente a y sobre el

46


espaciofe-dimensionalgenerado por las columna de X. Análoga -
mente 7-P es llamada matriz r e s i d u a l puesto que al ser aplicada
a y produce los residuales (ordinarios).



2.1. Propiedades de los Elementos de P..

    Denotemos por p . - e l I j - é s i m o elemento de P, l , j ' = l,2,...,n;
                   ' ^ J -1
entonces p.. = x'.(X'X) x • y las siguientes propiedades son cíer-
              A,j      A.     j   .
tas
(a) ZZpJy = k = Z p ^
(b) O < p^j 4 1 para todo i = 1,2               n.
(c) -.5 < p^j < .5 s± I ^ j .
(d) Si X contiene una columna constante, p ^ ^ 1/n, 1 = l , 2 , . . . , n .
ie) Si p ^ = 1 6 PXX = O entonces p ¿ : = 0.


(«> P i i P j j - P i j > o
(h) p^+ej/e'e< 1
( i ) Si X matriz wxfe es de rango fe, entonces para fe f i j o p ^        es
      no decreciente en n, para I = l , . . . , w .



2.2.    Relaciones E n t r e l o s Resultados del A j u s t e y l a
        M a t r i z P.
                                            A        2
    De la sección 1.3 vimos que Cov(y) = a P y que Cov(e) =
 2
a (I-P). De estos resultados obtenemos que los elementos de la
diagonal de la matriz P determinan las varianzas de los valo-
res ajustados y de los residuales:

                    var(5^) = a ^ p ^ ^=1,2,...,n, y

                    var(e^) = a'^(l-p^) ^ = 1,2,...,n.

                                                                     47

        ^                                         2
    Además, puesto que la cov(e.,e.) = -0 p.., el coeficiente
de correlación entre e • y e • está completamente determinado por
                          A,       j
los elementos de P,

            -orrie.,e.)=-p../[il-p^)íl-p..)V^.

    De 1.3 e = (I-P)e lo que indica que la relación entre e y
e sólo depende de P.

    Además, también de 1.3, Y = PY, de donde el ^-ésimo valor
ajustado puede escribirse como


     h -= ¿^ ^ij^j = ^a^i^ L %-^j'                    ^ ^ ^'^   ^•

2.3. Interpretación de los Elementos de p.

    En la sección anterior obtuvimos

                               n
            h'^iíyi'' L^j^j'                  ^-i,2,....«.
    Derivando esta expresión con respecto a y . obtenemos,


            ^ ^ l ^ ^ ^ l ' ^W         '^ " ^>2   n.

y los p.. pueden ser interpretados como la cantidad en que ca^
da valor y . determina a y . (Hoaglin y Welsch (1978)). Similar-
mente, p . - puede ser considerado como la cantidad en que cada
valor y • determina a y . ,
       i              "^
    El recíproco de p - - puede interpretarse como el número
efectivo (o equivalente) de observaciones que determinan a y .
(Huber (1981)). Es decir, si p-. = 1, entonces y . esta deter-

48


minado por y . solamente (una observación). Por otro lado, si
              •A.-


P j ' = O, £/. no tiene influencia sobre y,, mientras que si
P-. = .5, y . está determinado por el equivalente de dos obser-
vaciones.

     Los elementos de P también tienen interpretación geométri-
ca. Primero, cuando X contiene una coluooia constante o cuando
las columnas de X están centradas, la forma cuadrática
v'(X'X)     V = c , donde v es un vector fexl y c es una constante,
define los contomos elípticosfe-dimensionalescentrados en x,
el vector que contiene los promedios de      las columnas de X. El
más pequeño conjunto convexo que contiene la dispersión de los
n puntos de X está contenido en las elipsoides que satisfacen
que c < max(p. . ) . Por tanto p.. está determinado por la locali-
zación de X. en el espacio X; un gran (pequeño) valor de p . .
indica que x, cae lejos (cerca) de la masa de los otros puntos.

     En segundo lugar, el volumen de la elipsoide de (l-a)% de
confianza para B incrementa monótonamente con los p . .. A mayor
p.. mayor es el incremento del volumen de la elipsoide de con-
fianza para 3 cuando la -t-ésima observación es omitida (lo ve-
remos más adelante).

     Para otra interpretación véase, por ejemplo, a Chatterjee y
Hadi (1988).



2.4. La Distribución de los p . . .
                                     ^Á.A.

     La matriz P depende de la matriz de datos X y por los su-
puestos dados, ella es fija. Sin embargo, en algunos casos, X
está medida con error y en ocasiones parece razonable asumir
que las filas de X tienen una distribución multinormal con vec-
tor de medías y y matriz de covarianza Z. En este caso Besley,

                                                                                   49


Kuh y Welsch (1980) prueban que si las filas de X = (I-n)~^ll')X,
donde 1 es un vector de n unos, son i.i.d. , de una población
Multinormal (fe-1) dimensional entonces,


(a)           (n-fe)(p^-n"S/{(fe-1)(1-p^)} -v F(fe-l,Ki-fe)


      Es importante observar que s i I ^ j , no podemos asumir las
filas X . , X . sean independientes, pues

         A,      j


              cov(x.,x.) = E(x..x.)-{E(Xy)}{E(x-)}'

                            = w'-2íw'+n~h) + w'+n~h = n~h.
la cual converge a cero cuando n -»• <»•

      Por tanto la dependencia desaparece a medida que n crece,
y el resultado anterior es cierto sólo aproximadamente.

(b) Si el modelo no contiene constante y las x-, I = 1,2,...,n,
son i.i.d., de una W.(y,Z), entonces


              (n-fe-1)B../{(fe-1) (l-p..)} '^ F í k - l , n - k )


donde p . . = x'.(X'X)~ x-        y    p . . = n~ 4p , .            (Chaterjee y
Hadi , 1988).



2.5. Observaciones 'outliers', de Alto Poder y Ob-
     servaciones Influencíales.

      Antes de presentar algunos de los procedimientos frecuen-
temente empleados para estudiar el comportamiento de los datos
sobre el ajuste de la ecuación de regresión, es conveniente

50


afianzar un poco más los conceptos de 'outliers', puntos de al-
to poder y observaciones influencíales. A continuación daremos
definiciones para ellos y veremos como interactúan entre si.

     O u t l i e r s . En el contexto de la regresión lineal, definimos
un 'outliers' como una observación para la cual su residual es-
tandarizado es grande en magnitud (la apropiada desviación es-
tándar para los residuales será definida en la sección 3) con
respecto a las otras observaciones en el conjunto de datos. Las
observaciones son señaladas como 'outliers' sobre la base de
que la regresión ajustada no es capaz de acomodarlas.

     Observaciones de a l t o poder. Son aquellas para las cuales
el vector X. correspondiente cae lejos, en algún sentido, del
resto de datos. Equivalentemente, un punto con alto poder es
una observación con un P., grande en comparación con las otras
observaciones del conjunto. Las obseirvaciones en el espacio X
que se encuentran aisladas tienen un alto poder. Los puntos
con alto poder pueden ser considerados como 'outliers' en el
espacio X.

           Observaciones I n i t a e n c l a l e s . Son aquellas que indivi-
dualmente o colectivamente tienen una influencia excesiva sobre
la regresión ajustada comparadas con las otras observaciones
del conjunto. Esta definición es subjetiva pero implica que po-
demos ordenar las observaciones de manera razonable de acuerdo
a alguna medida de su influencia.

     Sin embargo, una observación puede no tener la misma in-
fluencia sobre todos los resultados de la regresión. Por ejem-
plo, una observación puede tener influencia sobre 0, los valo-
res ajustados, y/o los estadísticos de ajuste. El primer obje-
tivo es entonces determinar cuál influencia debemos considerar.

                                                                51

Por ejemplo, si la estimación de B es el interés primordial,
entonces la medición de la influencia de las observaciones so-
bre 3 es la indicada, mientras que sí el objetivo es la predic-
ción, entonces una medida de la influencia sobre los valores
predichos es más apropiada que la medición de la influencia so-
bre B.

    También debemos tener en cuenta que:

(a) Los puntos 'outliers' no son necesariamente observaciones
influencíales.

(b) Las observaciones influencíales no son necesariamente 'out-
liers '.

(c) Mientras que las observaciones con grandes residuales no
son deseables, un residual pequeño no implica que la observa-
ción correspondiente sea típica. Esto se debe a que los mínimos
cuadrados evita grandes residuales y puede acomodar un punto
que no es típico a expensas de otros puntos en el conjunto de
datos. En efecto, la tendencia general de puntos con alto poder
es la de tener residuales pequeños e influenciar el ajuste en
forma desproporcionada.

(d) En forma análoga a los residuales, los puntos de altp poder
no son necesariamente influencíales y las observaciones influem
cíales no son necesariamente puntos de alto poder. Sin embargo,
los puntos de alto poder son probablemente influenciales.

    Los ejemplos 1 y 2 ilustran claramente las situaciones an-
teriores.

52


3. Efectos de una Observación Sobre la Ecuación de
   Regresión.

     La hipótesis implícita en el ajuste de M.C. dice que todos
los datos deberían jugar un papel igual en la determinación de
los resultados de la estimación y en las conclusiones que se
desprenden del análisis del modelo

                                    y = XB+e
     Generalmente, sin embargo, no todas las observaciones tie-
nen la misma influencia, y puede ocurrir que una o varias obser^
vaciones tengan una influencia excesiva sobre el ajuste y las
conclusiones derivadas del análisis. En estos casos es importsm
te poder identificarlas y confirmar sus efectos sobre los dife-
rentes aspectos de la regresión. En la literatura estadística
existe una gran cantidad de medidas propuestas para diagnosti-
car observaciones influencíales. Dos formas diferentes de apro-
ximarse a la medición de la influencia son:

(a) El método de la e l i m i n a c i ó n , y

(b) El método de la d l i e r e n c i a c l ó n .

     En (a) se examina cómo los resultados y las conclusiones
del análisis de regresión cambian cuando una (o varias) obser-
vacíón(es) se omite(n).

     En (b) se examinan las tasas de cambio (las derivadas) de
varios resultados de la regresión con respecto a ciertos pará^
metros del modelo.

     En lo que sigue estudiaremos el método de la eliminación
para el caso de una o b s e r v a c i ó n I n i l u e n c l a l . La siguiente sec-
ción desarrollará el caso más general de un grupo de observa-

                                                             53


ciones influencíales.

     Para el método de la diferenciación véase por ejemplo Bes-
ley et al (1980), o Oíatterjee y Hadi (1988).



El Método de la Eliminación.

     Existe un grupo de métodos interrelacionados para detectar
una observación influencial y medir sus efectos sobre varios
aspectos del análisis. Estos métodos se pueden dividir en sie-
te grupos dependiendo del aspecto del ajuste en que nos inte-
resemos; de acuerdo a esto los dividiremos en métodos basados
en

(a) residuales

(b) la lejanía de los puntos en el espacio X-Y,

(c) la curva de influencia (centro de las elipsoides de con-
     fianza) ,

(d) el volumen de las elipsoides de confianza,
(e) la función de-verosimilitud»                  >
(f) subconjuntos de coeficientes de regresión, y,
(g) la estructura de valores y vectores propios de X'X o en la
     descomposición en valores singulares de X.
En lo que sigue daremos algunas de las medidas más empleadas
surgidas de cada uno de estos métodos.

(a) Medidas Basadas en Residuales.
     De la Sección   1.3(e) vimos que e = (I-P>€. Esta identidad
indica que para que e sea un substituto razonable de c,   los
elementos fuera de la diagonal de P deben ser suficientemente
pequeños. Además , si los elementos de c son independientes y
tienen la misma varianza, dicho resultado muestra que los re-

                                 - MJ • -• "


54


siduales no son independientes (a menos que P sea diagonal) y
que no tienen la misma varianza (a menos que los elementos de
la diagonal de P sean iguales). En consecuencia, los residuales
e • pueden ser considerados como un sustituto razonable de los
e • si las filas de X son homogéneas (y por tanto los elementos
de la diagonal de P son aproximadsuaente iguales) y los elemen-
tos fuera de la diagonal de P son suficientemente pequeños.

    Por estas razones, es preferible usar una versión transfor-
mada de los resisduales ordinarios para propósitos de diagnós-
ticos.

      En lugar de los e • podemos usar

                                iic^,o^) = eja^
donde O • e s la desviación estándar del >c-ésimo residual.

      Hay cuatro posibilidades para la transformación anterior:
a A,    . / { e ' e } Va
  . = e A,                 el residual normalizado,
                                           y
b . = e . / o , donde a = i e ' e / i n - k ) } el residual estandarizado,
r . = e,'/{o( l-p ••)} el residual internamente studentizado y,
r'^ = ^//í^f/N(1~P//)} el residual externamente studentizado,
donde
a ^ í l ) = ^i'(^)(I-P)(^))y(^)}/(n-fe-l), 1 = 1,2,....n, es el es-
timador del error cuadrático medio cuando se omite la .¿-ésima
observación y

           ^il) ' ^íD^^iD^ÍD^                  ^il)*   -¿^ l.2,...,n,

es la matriz de predicción para X^ .^.

      Cuál de las formas anteriores es la más adecuada para diag-
nosticar? Las cuatro versiones están muy relacionadas entre sí;
las dos primeras son muy simples y no reflejan la varianza de

                                                                      55


e . ; para diagnosticar son básicamente equivalentes.

    Behnken y Draper (1972) sugieren, que si los elementos de
la diagonal de P (y por lo tanto las varianzas de e.., I = 1,2,
...,n), varían sustancialmente debería preferirse usar r . .

    Muchos autores (por ejemplo Besley et al. (1980); Atkinscm,
(1981, 1982,1985); y Velleman y Welsch, (1981), prefieren ^^
sobre r - por varias razones:
    (i) r . puede interpretarse como el estadístico para con-
trastar la significancia del -c-ésimo vector unidad u. en el mo-
delo de ' o u t l i e r ' de cambio de media definido como

                            EÍY) = XB+u.Q
                                          •A-



donde 6 es el coeficiente de regresión de u . ,              (Chatterjee
                                                  'V-

y Hadi (1988)).
   (ii) ^í '^ t í n - k ~ l ) para la cual existen tablas fácilmente
disponibles.
  (iii) r*. e s una transformación monótona de r . pues

                     r*. = r.{ín-k-l/in-k-r])}^^^

y puesto que r * -*• «> cuando r . -*• n - k , r * refleja más dramática-
mente las grandes desviaciones de lo que lo hace r . .
   (iv) El estimador o , •>. es robusto a errores burdos en la
.¿-ésima observasión.

    Ahora bien, el patrón que siguen los residuales es más in-
formativo que sus magnitudes y por tanto los métodos gráficos
son más útiles que los procedimientos formales de contrastes.

    Behnken y Draper (1972), sugieren el empleo de los r . en
la construcción de los gráficos, mientras Atkinson (1981) pre-

56


fiere los r ^ , Puesto que r*. es una transformación monótona de
los r . las conclusiones obtenidas de los dos gráficos son gene-
raímente las mismas; sin embargo es más fácil la identificación
de 'outliers' en los gráficos basados en los r*-.

     Algunos de los gráficos de residuales más comunes son:
(i) Distribución de frecuencia de los residuales! histogramas,
     gráficos de puntos, gráficos 'stem and leaf y de cajas es-
     quemáticas.
(ii) Gráfico de los residuales en el tiempo.
(iii) Gráfico de probabilidad normal o seminormal.
(iv) Gráfico de residuales contra los valores ajustados.
(v) Gráfico de residuales contra X-, j = 1,2,...,fe.
(vi) Gráficos aditivos.
(vii) Gráficos de componentes-más-residuales.
(viii) Gráficos de residuales parciales aumentados.
Para una descripción de estos métodos gráficos véase, por ejem-
plo, Seber (1977), Daniel y Wood (1980), Atkinson (1985), Chat-
terjee y Hadi (1988).



(b) Medidas Basadas en la Lejanía de los Puntos en el
    Espacio X-y.

     En esta sección discutiremos algunas cantidades para medir
el poder de un punto y también combinaremos los valores de po-
der y los residuales en un gráfico llamado L-R. Este gráfico
permite distinguir entre puntos de alto poder y 'outliers'.



(i) Elementos de la diagonal de P.

     Como vimos anteriormente los elementos de la matriz P y en
particular los elementos de su diagonal, los P - - , juegan un pa-
                                                •A>C

                                                                 57

peí importante en la determinación de los valores ajustados,
los residuales y su estructura de varanza y covarianza. Por
esta razón Hoaglin y Welsch (1978) sugieren el examen de r*. y
p - • y señalan que estos dos Eispectos de   la búsqueda en los
datos son coiiq>lementarios y que ninguno es suficiente por sí
mismo.

    Una pregunta natural es, qué tan grande debe ser un p,.
grande? A continuación se sugieren dos cotas usadas comunmente:
(1) Huber (1981) sugiere que puntos con p.. > 0.2 sean clasifi-
cados como observaciones de alto poder. Esta regla recomienda
que se preste atención especial a las observaciones cuyos valo-
res ajustados estén determinados por un equivalente de cinco o
menos observaciones.

(2) Hoaglin y Welsch (1978) sugieren que puntos con p . . > 2 /n
                                                       >CA,
sean clasificados como puntos de alto poder.

    Las cotas anteriores no deberían ser usadas mecánicamente;
ellas deberían servir como medidas aproximadas de guía general
para detectar problemas en los datos. Se recomienda una compa-
ración de los elementos de la diagonal de P através de gráfi-
cos tales como gráficos de los p ., contra el número de la ob-
servación, diagramas 'stem-leaf y/o cajsis esquemáticas.



(ii) La distancia de Mahalanobis.

   El poder de una observasión puede ser medido por medio de
la distancia de Mahalanobis,


             M. = n(n-2)(p..-n"b/[(n-l)(l-Py.)J


Esta distancia es equivalente a p...

                                              •.— ' "   I




58


( i i i ) La distancia cuadrática estandarizada ponderada

         Suponga que e l modelo t i e n e un término constante definamos


           '^Ij ' ^ j ^ \ j ~ ^ j ^ *   ^ " l»2....,n,      j = 1,2,....fe,


donde x • es e l promedio de l a j-ésima colimma de X. La cantidad
c . . puede ser considerada como el efecto de la j-ésima variable
     j                                                                            _
sobre el\¿-ésimo valor ajustado y . . Se puede mostrar que y .-V =
Z C ; , donde y es él promedio de Y. Daniel y Wood (1971) sugie-
j ^j
ren usar la distancia cuadrática estandarizada ponderada

                                    WSSD^ = Z c \ . / S \

       2       ~2
donde S = Ziy.-Y) /(n-1), para medir la influencia de la >t-ési^
            ^     I ,              f.   -
ma observación sobre y.-~Y. WSSD • es una medida de la suma de
cuadrados de las distancias de X. • con respecto a la media de
la j'-ésima variable X., ponderada por la importancia relativa
de la j'-ésima variable (la magnitud del coeficiente de regre-
sión estimado). Por tanto, WSSd- será grande si la -¿-ésima ob-
                                               A-

servación es extrema en al menos una variable cuyo coeficiente
de regresión estimado es grande en magnitud.
         Un gráfico de dispersión que es efectivo para distinguir
entre un punto de alto poder y un 'outler' es llamado el gráfi-
co L-R. Este se define como el gráfico de dispersión de los va-
                                                                              2
lores P J J contra los residuales normalizados al cuadrado a . de-
finidos antes. El diagrama de dispersión debe caer dentro del
triángulo definido por las siguientes tres condiciones:
(i) 0 < p ^ 4 1
(ii) O 4 a l 4 1
(iii) P ^ + a . ^ 4 1

                                                                          59


For tanto los puntos que caen en la esquina inferior derecha
del diagrama son outliers y los que caen en la esquina superior
izquierda son puntos de alto poder.



(c) La Curva de Influencia.

    Una clase importante de medidas de la influencia de la
.¿-ésima observación sobre los resultados de la regresión está
basada en la idea de la curva de I n i l u e n c i a O iunción de I n -
i l u e n c i a introducida por Hampel (1974). Para una discusión so-
bre este concepto véase Chatterjee y Hadi (1988). Algunas de
las medidas de influencia derivadas de este concepto son:



( i ) La distancia de Cook.

    Bajo normalidadí, l a región de (l-a)% de confianza conjunta
para 3 se obtiene de

                  (B-3) (X'X) (3-3)/fe0^ ^ F(ot;fe,w-fe)

donde F(a;fe,n-fe) es el percentil a-Superior de una distribu-
ción F confey n-fe grados de libertad. Esta desigualdad define
                                        A,

tma región elipsoidal centrada en 3- La influencia de una ob-
servsución puede ser medida por el cambio en el centro de esta
región cuando la .¿-ésima obsetrvación es omitida. Cook (1977)-
definió la medida

         C^ = i t - í ^ ) (X'X) (0-0(^)) /feo^    .¿«1,2,... ,tt,

             = p^/(fe(l-p¿.))


para medir la influencia de la .¿-ésima observación sobre el

60


centro de la elipsoide O, equivalente, sobre los coeficientes
estimados. Esta medida combina información sobre el alto poder
de la observación y r . que da información                sobre 'outliers'.
                            A^

Grandes valores de C• indican que la observación es influencial.
Cook (1977) sugiere que cada C. sea comparada con el percentil
de una F con K y n-fe grados de libertad. Para una discusión so-
bre la distribución de los C , (Cook, (1977)).
                                     A-



(ii) La distancia de Welsch-Kuh.

     La influencia de la .¿-ésima observación sobre el valor pre-
dicho y . puede medirse como el cambio (con relación al error
         A-
e s t á n d a r de y . ) en l a p r e d i c c i ó n en x - cuando l a >¿-ésima obser-
                 A.                               A-
vasión es omitida, es decir;




Besley et al. (1980) sugieren usar o, .^ en lugar de o . Haciendo
                                                 ('t)
algunos reemplazos obtenemos,


                       WK.= I^|(P^/(1-P^))'/^

Besley et al. la llamaron DFFITS . debido a que es la diferencia
escalada entre y . y y . , . ^ . Grandes valores de WK . indican que
la observación es influencial. La distribución de WK. aunque no
                                                                      A*

es exactamente una t e s similar a ella. Debido a esto Velleman
y Welsch (1981) sugieren que valores mayores que 1 o 2 parecen
ser razonables para indicar puntos que merezcan atención espe-
cial. Para otros posibles puntos de calibración para WK.,
(Chatterjee y Hadi (1988)).

62

sión de una observación con un gran residual producirá una gran
reducción en la sima de cuadrados residual SSE. La influencia
de una observación puede ser medida combinando estas dos ideas
y calculando entonces el cambio tanto en e'e como en det(X'X).
Andrews y Pregibon (1978) sugieren en cociente:

     AP^ = SSE^^xdet(X' X^^.)/SSEdet(X'X),       I = l,2,...,n.

Puede mostrarse que la expresión anterior se reduce a
(Chatterjee y Hadi (1988)):

               AP. = p-.+ e^/e'e,      I = l,2,...,n,
                  A*   A^f   'V


y entonces AP. no distingue entre puntos de alto poder y pun-
tos outliers en el espacio Z formado por XxY. Valores pequeños
de AP merecen atención especial. Para otros COTentarios sobre
AP ,      (Draper y John (1981)).



(ii) El cociente de varianzas.

      Una medida alternativa a AP se basa en medir la influencia
de xl-ésima observación comparando cov(3) y cov(3y-x). Si el
rango(X^.O " fc> estas matrices son definidas positivas y exis
ten varias formas de compararlas; el cociente de sus trazas o
el cociente de sus determinantes. Besley et al. (1980) Mugie-
ren usar el cociente de sus determinantes,


  VRj^ = det(a2^j (XJ^jX^^P"Vdet(a2(X'X)~S . -t « 1,... ,n.


Besley et al. (1980), la den(mtinan COVRATIO y muestran que:


              yR^ - {(n-fe-^^) / (n-fe-1) }'^( 1/ (1-p^)).

        ^^Kws.^-^r'


                                                                        61


(iii) La distancia de Cook modificada.

    Atkinson (1981) ha sugerido usar una versión modificada de
la distancia de Cook para detectar .observaciones influencíales.
                              9         9
Esta modificación cambia a        por O . . . . , toma la raíz cuadrada de
C. y ajusta C . por el tamaño muestral. Con estas modificacio-
nes obtenemos
     Cj = KJ I ( P ^ i n - k ) / ((l-p^)fe)) ^^^ = WK^Íin-k) /fe) ^^.

Atkinson (1981) resalta que esta modificación mejora a C. en
tres formas:
(a) C* da más énfasis a puntos extremos.
(b) Cj e s más adecuado para procedimientos gráficos tales como
gráficos de probabilidad normal, y
(c) para el caso perfectamente balanceado donde p- . =fe/n,pa
ra todo I , el gráfico de los Cj es idéntico al de \ r ^ \ .



(d) Medidas Basadas en el Volumen de Elipsoides.

    Una clase alternativa de medidas de la influencia de la
^-ésima obsevación está basado en el cambio del volumen de la
elipsoide de confianza cuando la .¿-ésima observación es omiti-
da. Veamos algunas de ellas.



(i) El estadístico de Andrews-Pregibon.

    El volumen de la elipsoide de confianza para 3 es inversa-
mente proporcional a la raíz cuadrada del det(X'X). Una medida
de la influencia de la .¿-ésima observación sobre el volumen de
la elipsoide de confianza para 3 puede ser obtenida de la com-
paración del det(X'X) y det(Xl .xX^ ..) . Por otro lado, la omi-

                                                                       63

        ^                             2
VRj será mayor que uno cuando ^. es pequeño y p.. es grande, y
será menor que uno cuando r . es grande y p . . es pequeño. Pero
            n                   A..              «VA.

cuando A., y p.. son ambos grandes (o ambos pequeños), VR. tien
de a uno. Estos factores reducen la habilidad de VTi. para de-
tectar observaciones influencíales. Sin emlmrgo, del análisis
de varios conjuntos de datos, se ha observado que VR. señala
correctamente las observaciones influencíales; esto quizás se
deba al hecho de que las observaciones con p • • grande tienden
a jalar la ecuación ajustada hacia ellas y en consecuencia tie-
nen un pequeño residual. Idealmente, cuando todas las observa-
ciones tienen la misma influencia sobre la matriz de covarian-
zas, VRj es aproximadamente uno. La desviación con respecto a
la unidad indica que la X-ésima observación es potencialmente
influencial. Besley et al. (1980) proporcionan puntos dé cali-
bración para VR., Encuentran que la X-ésima observación es,po-
siblaaente influencial si ji'R^-ll > 3fe/n.


(e) Medidas Basadas en l a Función de Verosimilitud.

    Asumiendo que V A, W ^ ( X B , 0 ^ Í ) , e l logaritmo de la función
                           2                                      . •
de verosimilitud de 3 y o e s ,
       1(3,0^) « -nln(2TT)/2-nln(a^)/2-(y-Xé)'(i^-XB)/2a^,

                                                         2     .r
     una región de (l-a)% de confianza para 3 y <? está dada por
(Cox y Hinkley (1974)):

                {(3.a^):2[I(3,5^)-I(3.a2)] <: X(oi.fe+i)>.


donde 3 y <7 son los estimadores máximo verosímiles para 3 y
o   dados por 3 = P y 5     = o ín-kO/n.

64


    La influencia de la /¿-ésima observación puede ser medida
                         ~ ~2       "         ~2
por la distancia entre 1(3,0 ) y ^ ^ ^ ( / \ > ^ ( / 0 ' ^ ° ^ Y Weisberg
(1982) definen la distancia de verosimilitud como:


     LD^(3,a^) = 2[l(3,a^)-I(B(^y,a^^j)].                 ^ = l,2,...,n.

      En Chatterjee y Hadi (1988) se prueba que:

         LD^(3,a^) =

       * n l n {n(n-fe-A5)/(n-1)(n-fe)} + { ( n - l ) / t ^ / ( l - p . .)(n-fe-/t^)}-l
                          A-                              X->         Á , ^    A-


y los autores sugieren que sea comparada con una distribución
 2                                        2 ^
Xfh.-\\' ^s importante observar que LD.(3,CJ ) se basa sobre el
modelo de probabilidad usado mientras que las otras medidas de
influencia discutidas son            estrictamente nunáricas. Una venta-
ja de la distancia de verosimilitud es que puede ser extendida
a otros modelos fuera del modelo lineal normal.


(f) Medidas Basadas en Subconjuntos de Coeficientes
    de Regresión.

      Besley et al. (1980) sugieren medir la influencia de la
-¿-ésima observación sobre el j'-ésimo coeficiente de regresión
estimado como:

                    DFBETAS.. = (3;-3;(^))/var(3.)

                                  = {.>^^./(W)Wy)^=^}(l/(l-p^)^^)

donde W. = (I-Pr.-|)X- siendo Pr-i la matriz predicción de la
matriz X sin la j'-ésima columna X-, y donde W. - es el .¿-ésimo
                                              j                 A,j

                                                                             65


elemento de W.. Valores de [DFBETAS] que exceden 2/n merecen
especial atención.

     Chatterjee y Hadi (1988) describen una medida para la in-
fluencia de la >t-ésima observación sobre q combinaciones line¿
les independientes de los coeficientes de regresión.



4. Efecto de Múltiples Observaciones Sobre la Ecua-
   ción de Regresión.

     Hasta aquí hemos presentado algunos métodos para la detec-
ción de observaciones que I n d i v i d u a l m e n t e pueden ser considera^
das 'outliers', de alto poder o influencíales. En esta sección
discutiremos la necesidad de extender esas medidas para el ca-
so donde varias observaciones actúan conjuntamente sobre los
resultados y las conclusiones de la ecuación ajustada. El pro-
blema de imíltiples observaciones es importante tanto desde el
punto de vista teórico como practico. Desde el punto de vista
teórico, pueden existir situaciones en las que las observacio-
nes pueden ser conjuntamente, pero no i n d i v l d u a l j m . n t e , influen-
cíales, el gráfico 3 ilustra esta situación. Los puntos 1 y 2
no son individualmente influencíales, pero conjuntamente tienen
una gran influencia sobre el ajuste. Esta situación es conocida
como enmascaramiento, debido a que la influencia de una obser-
vación es enmascarada por la presencia de otra (s). Atkinson
(1986) y Leroy y Rousseeuw (1984) sugieren procedimientos para
tratar estas situaciones. Por otro lado, las observaciones 3 y
4 son individualmente influencíales pero no conjuntamente, es
decir, cuando las dos son simultáneamente omitidas.

     Desde el punto de vista práctico, cuando la influencia múl-
tiple existe es mucho más severa que en el caso individual y

                                           •nbw

66


frecuentemente es- pasada por alto en el proceso de ajuste de la
ecuación debido a que es más difícil de detectar. En muchas si-
tuaciones las observaciones con influencia conjunta pueden de-
tectarse empleando los diagnósticos para el caso de la influaa
cia individual.


            Influencia conjunta pero no individual (pun-
              tos 1 y 2) e influencia individual pero
                   no conjunta (puntos 3 y ^ ) •


     Hay tres problemas inherentes al caso de observaciones múl-
tiples. El primero es cómo determinar el tamaño del subconjuto
de observaciones conjuntamente influencíales. El segundo pro-
blema es computacional. Suponga que conocemos el tamaño adecua-
do m para las observaciones conjuntamente influencíales. Ha-
brán entonces n!/ml(n-m)¡ posibles subconjuntos para los cua-
les^ débanos calcular las medidas de influencia. Aún hoy día,
esto puede ser prohibitivo si m y n son grandes. El tercer pro
blema es la dificultad de representar gráficamente las obser-
vaciones. Las observaciones con influencia múltiple frecuente-

                                                                    67

mente no permiten ser examinadas por los métodos antes mencio-
nados , como cajas esquemáticas, gráficos de las observaciones
contra su índice, gráficos 'stem-and l e a f , etc.

    Los procedimientos dados anteriormente para el caso de una
observación pueden ser generalizados en forma directa, en su
mayoría, al caso de múltiples observaciones influencíales. El
lector interesado puede referirse a Besley et al. (1980) o,
Chatterjee y Hadi (1988). Estos procedimientos tienen el incon
veniente de la determinación del subconjunto m. Un procedimien
to introducido por Atkinson (1986) proporciona un método, basa-
do en la regresión de la mínima mediana al cuadrado (Rousseuw
(1984)) que permite determinar el conjunto de observaciones in-
fluencíales a las cuales se les aplicarán las medidas de in-
fluencia generalizadas.

    Otro procedimiento que no necesita la especificación del
tamaño m del número de observaciones conjuntamente influencía-
les se basa en la técnica del análisis 'Cluster' (Gray y Ling
(1984)).



5. Puntos Influencíales y Colinealidad.

    Cuando existen relaciones lineales cercanas entre las co-
luiraias de la matriz X, se presenta un problema llamado c o l i n e a
t i d a d . Esta puede causar graves daños sobre la estimación por
mínimos cuadrados de la ecuación de regresión. Por ejemplo, la
colinealidad puede inflar las varianzas de los coeficientes de
regresión estimados, alterar los signos esperados de ellos y
producir resultados inestables numéricamente.

    Para medir el grado de colinealidad, Besley et al. (1980)

68


emplean el número de condición de la matriz X, el cual se defi-
ne como:

                   K = d ^ / d ^ = (Xj/X^)'/2


donde d. > d > . . . > dy son los valores singulares de X, es de-
cir son los valores de la diagonal de la matriz V, de dimensión
fexfe, obtenida de la descomposición X = UDV', donde U'U = V'V'« I
y donde       X. > X,, >...> Xi son los valores propios de X'X.
Un Valor grande para K e s indicativo de la existencia de al me-
nos una relación de dependencia cercana entre al menos dos co-
lumnas de la matriz X, El mínimo de K es uno.

     Ahora bien, existen dos problemas con K:
(i) K no es invariante bajo cambio de escala de las columnas
     de X.
(ii) K puede ser fuertemente influenciado por uno o algunos po-
      cos puntos extemos en el espacio X.

     El primer problema se resuelve estandarizando cada columna
de X de forma que tenga media cero y varianza uno (véase Bes-
ley (1984) para una discusión sobre las ventajas y desventajas
de centrar y normalizar).

     El segundo problema es más complejo. Los puntos de alto po^
der tienden a influenciar la estructura de valores y vectores
propios y por tanto el número de condición de X. Por ejemplo,
en el gráfico 4 vemos que xm. punto puede crear una colineali-
dad o encubrir una como lo muestra el gráfico 5.

     Por tanto un número de condición pequeño no necesariamente
significa que X no esté mal condicionada. Además, uno o dos
puntos pueden ser los culpables de que exista un número de con
dición grande. Los ptmtos que esconden o crean colinealidad los


                                                         X2
                    C o l i n e a l i d a d creada por un p u n t o .


                 Colinealidad enmascarada por un punto.


llamáronos puntos de c o l i n e a l i d a d I n i l u e n c i a l . Estos puntos son
generalmente, aunque no necesariamente, observaciones con alto
poder. Sin embargo, no todas las observaciones de alto poder
son puntos de colinealidad influencial y no todos los puntos de
colinealidad influencial son observaciones de alto poder.

                                                        'WW.^P—íl—^T"^"



70


     El siguiente ejemplo aparece en Chatterjee y Hadi (1988).



Ejemplo 3

     La tabla 1 muestra los datos para tres variables explica-
tivas X,, X„, X_, p. , y el cambio relativo en número de condi-
ción definido como \K, ...-K\ /K, donde K^ ... es el número de con-
dición de X
              (-t)



     En este ejemplo la observación 1 es de alto poder pero no
es de colinealidad influencial y la observación 2 es de coli-
nealidad influencial pero no es un punto de alto poder.

                                                               71


Diagnósticos de Colinealidad Influencial.

Influencia sobre el número de condición. Una de las medidas
propuestas para cuantificar la influencia sobre el número de
condición de la matriz X es análoga al cambio relativo
\ ^ ( j ) ^ \ / ^ y se define como

                             | H J . \K^.yK\/K
donde K^ .» es una aproximación al número de condición de Xy .^
dada en Chatterjee y Hadi (1988), página 166. H. puede inter-
pretarse como el cambio relativo en el número de condición de
X que resulta al omitirse la .¿-ésima observación. Si H . es
grande y positiva, entonces la eliminación de x- incrementa el
número de condición, y sití. es grande y negativa, la omisión
                                     A*
de X. disminuirá el número de condición

    Otras medidas para determinar la influencia sobre el nume-
ro de condición y los índices de condición, así como métodos
gráficos para detectar puntos cólineales influencíales sé en-
cuentran en Chatterjee y Hadi (1988); estos autores también
presentan un procedimiento para el caso de múltiples puntos
influencíales debido a Kempthorne (1986), el cual no requiere
de la especificación del tamaño del subconjunto.



Conclusiones.

    Hemos discnitido varias de las medidas ]^s empleadas para él
estudio de 'outliers', puntos de alto poder y observaciones in-
fluencíales, tanto en el caso de una sola observación como en
el de múltiples observaciones.

    Las medidas presentadas detectan la influencia sobre dife-

72


rentes aspectos de la ecuación ajustada: coeficientes de regre
sión estimados, valores ajustados, matriz de covarianzeis d é l o s
coeficientes estimados, elipsoides de confianza para 3. elip-
                              2
soides de confianza para B y O .

     En el caso de la colinealidad se ilustró la relación entre
puntos de alto poder y el número de condición de la matriz X;
pueden existir observaciones que crean colinealidad y otras
que la enmascaran y de ahí la importancia de detectar la exis-
tencia de ellas.

     Por último, para la aplicación de las medidas de influen-
cia en el procedimiento de ajuste de la ecuación de regresión,
existen paquetes estadísticos que traen programadas algunas de
ellas. Hadi (1988) ha construido un paquete interactivo y di-
rigido por menú, llamado SAILR, en el cual ha implementado to-
dos los procedimientos estadísticos y gráficos presentados en
su libro S e n s l t l v l t y Analysis l n Linear Regression, escrito con
Chatterjee. Este paquete de fácil aplicación, permite chequear
eficientemente y de forma muy completa el comportamiento de las
observaciones en cada aspecto del análisis.

BIBLIOGRAFÍA
Andrews, D.F., y Pregibon, D., (1978). Finding Outliers That Matter. Journal of the Royal Statistical Society, (B), 40, 85-93.
Atkinson, A . C , (1982). Two Graphical Displays for Outlying and Influential Observations in Regression. Biometrika, 68, 13-20.
Atkinson, A . C , (1982). Regression Diagnostics, Transformations, and Constructed Variables (With discussion). Journal of the Royal Statistical Society (B), 44, 1-36.
Atkinson, A . C , (1985). Plots, Transformations and Regression. Oxford University Press.
Atkinson, A.C., (1986). Masking unmasked. Biometika, 73, 3, pp. 533-41.
Behnken, D.W. y Draper, N.R., (1972). Residuals and their Variance. Technometrics, 11, N- 1, 101-111.
Besley, D.A., (1984). Demeaning Conditioning Diagnostics through Centering (With comments). The American Statistician, 38, N- 2, 73-93.
Besley, D.A., Kuh, E., y Welsch, R.E., (1980). Regression Diagnostics. New York: Wiley.
Chatterjee, S., y Hadi, A.S., (1986). Influential Observations, High Leverage, and Outliers in Linear Regression. Statistical Science, Vol. 1, N- 3, 379-416.
Chatterjee, S., y Hadi, A.S., (1988). Sensitivíty Analysis in Linear Regression. New York: John Wiley & Sons.
Cook, R.D., (1977). Detection of Influential Observations in Linear Regression. Technometrics, 19, 15-18.
Cook, R.D., y Weisberg, S., (1982). Residuals and Influence in Regression. New York and London: Chapman and Hall.
Cox, D.R., y Hinkley, D.V., (1974). Theoretical Statisticis. London: Chapman and Hall.
Daniel, C , y Wood, F.S., (1980). Fitting Ecuations to Data: Computer Analysis of Multifactor Data. 2a Ed., New York: John Wiley & Sons.
Draper, N.R., y John, J.A. (1981). Influential Observations and Outliers in Regression. Technometrics, 23, 21-26.
Draper, R.R. y Smith, H., (1981) Applied Regression Analysis. 2a. Ed., New York: John Wiley & Sons
Gray, J.B., y Ling, R.F., (1984). K-clustering as a Detection Tool for Influential Subsets in Regression (With Discussion). Technometrics, 26, 305-330.
Graybill, F.A., (1976). Theory and Application of the Linear Model. MA: Duxbury Press, North Scituate.
Hadi, A.S., (1988). SAILR: User's Guide. New York: Cornell University.
Hampel, F.R., (1974). The Influence Curve and Its Role in Robust Estimation. JASA, 62, 1179-1186.
Hoaglin, D.C y Welsch, R.E., (1978). The Hat Matrix in Regresion and ANOVA. The American Statistician, 32, 17-22.
Huber, P., (1981). Robust Statistics. New York: Wiley. 
Kempthorne, P.J., (1986). Identifying Rank-Influencial Groups of Observations in Linear Regression Modelling. Memorándum NS-539, Department of Statistics, Harvard University.
Lerdy, A, y Rousseeuw, P.J., (1984). A multiple regression technique for detecting outliers. Report 84-33, Delft University of Technology.
Mardia, K.V., Kent, J.T. y Bibby, J.M., (1979). Multivariate Analysis. Academic Press, London.
Mosteller, F. y Tukey, J.W., (1977). Data Analysis and Regression. Addison-Wesley. Reading, Mass.
Rousseeuw, P.J., (1984). Least median of squares regression. JASA, 79, 871-880.
Seber, G.A.F., (1977). Linear regression Analysis. New York: John Wiley y Sons.
Velleman, P.F., y Welsch, R.E., (1981). Efficient Computing of Regression Diagnostics. The American Statistician, 35, 234-242.
