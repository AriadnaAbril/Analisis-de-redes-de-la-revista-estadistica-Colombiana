MODELOS DE REGRESIÓN CON PARÁMETROS ESTOCÁSTICOS
Universidad de los Andes
Introducción. Supóngase que tenemos un modelo de regresión que llamaremos aquí tradicional:
Normalmente este modelo de regresión múltiple se analiza
bajo un conjunto de suposiciones estándar. A saber:
(i) Las variables independientes X- •, o son valores fijos, o
                                          A.J
     son variables aleatorias independientes de los términos de
     error e •.
(ii) E(ey) = O V-y
(iii) Viej)= O     Vy y en lo general es desconocida.
(iv) COV(e.,£;) = 0 V l , j     i ?í y.
(v) Para un tiempo t , las variables X , j . , ' . ' , X y . son linealmen-
     te independientes.
     Ahora bien, bajo estos supuestos se conoce por el teorema
de Gauss-Markov que los estimadores de Mínimo Cuadrado (MC) de
3 son

                              b = ixh)~h'^y.                             (3)
                                                                     2
Estos son BLU(Best Linear Unbíased Estimates). A su vez a                se
estima por medio de

                       S^ = iy-Xb)"^ íy-Xb)/T-k      ,                   (4)

     Usualmente se comprueban las suposiciones descritas, sobre^
todo con respecto a la homoscedastx:ídad y a la no correlación
(Prueba de Durbín-Watson). Si falla alguna de ellas, los esti-
madores de MC pueden llevar a conclusiones graves y dejan de
tener propiedades deseables. Las predicciones pueden ser bas-
tante imprecisas y por todo ello se recurre a otros métodos.

     El modelo que queremos estudiar se basa fundamentalmente en
la monografía de Newbold y Bos (1985) y difiere adicionalmente
del tradicional en un aspecto más: en (1) los coeficientes 3/
son fijos en el tiempo. Vamos a suponer sin embargo que estos
parámetros son una Serie de Tiempo y por lo tanto nuestro mode-
lo tiene ahora la forma

                                                                           91


                                    K
                            '^t' l^^u^u^H-                               (5)
     Como una posible justificación de lo anterior podríamos,
por ejemplo, tomar la relación

                            C^ = a+By^+e^                                 (6)

que denota la dependencia del consumo del                  ingreso. Al asumir
en (6) 3 fijo, se está diciendo que d C J d y . = 3« es decir que
la propensión a consumir no varía con el tiempo. Es sin embar-
go plausible que dicha propensión cambie a través de los años
por la composición del consumo, por un cambio en los gustos
etc. y aún más, que esté relacionado el de hoy con el de ayer
y así con los anteriores.

     Antes de proceder a desarrollar el modelo (5) propuesto,
necesitamos recapitular algunos conceptos básicos sobre Series
de Tiempo.



2. Serles de Tiempo.

Definición 1.

a) Una Serie de Tiempo B* se llama e s t a c i o n a r l a si cumple
    i) E(B^)=y V;t
    ii) y(B^) = ó^ = y(0) H t
    iii) C0V(3^,3y^ ) = Y(T) , es decir depende de lá distancia
          en el tiempo T.
b) La iuncUSn de a u t o c o r r e l a c i j ó n se define como
    P(T) = Y(T)/Y(0).
    La autorrelaclón muestral a su vez se define como
    ^(T) = C(T)/C(0)

w^

     92


     donde los C's son la covarianza y varianza muestral respectiva^
     mente.

          La gráfica de r i x ) con respecto a T se llama un c o r r e t o -
     grama.



     Definición 2 .

         Una Serie de Tiempo e^. se llama de r u i d o blanco sí, tiene
     media O, varianza a2 y no esta correlacionada, es decir Y(p)=0
      •Vp ^ 0.

          Utilizando las definiciones anteriores y pensando en los
     modelos que deaseamos estudiar, queremos que los parámetros
     B'j. de (5) sean autocorrelacionados. Un modelo que nos refleja
     esta situación es el llamado          Autonegresivo.



     Definición 3.

          La Serie y^. se llama un modelo autorregresivo de orden p ,
     ARÍp), si es de la forma


           yt-^'^ 0'i(y^-ri^> + *2<í^í-2-v>+---Vyí-p-^> + '^í             ^^^
     donde a^. es de ruido blanco y los a ' s son parámetros fijos.

          Es importante anotar que imponiendo ciertas condiciones,
     estas series son estacionarias. No viene al caso deducir aquí
     estas propiedades que se pueden encontrar en cualquier buen
     libro de Series de Tiempo (Fuller [l976]). Sin embargo anota-
     mos que dado que la complejidad va en aumento con el orden p,
     muchos ejemplos y aplicaciones se trabajan con series AR(1) o
     AR(2). En especial para la predicción se tiene que si estamos

                                                                      93

situados en el tiempo T, futuros valores t/_^ dependen sólo de
í/y si estamos en presencia de un modelo AR(1), y, sólo de t/y y
í/y_, si es un modelo AR(2) .

      Hasta este punto hemos introducido modelos que muestran la
evolución a través del tiempo de una sola serie.

      Supongamos ahora que tenemos observaciones en el tiempo so-
bre un conjunto de K series. Para ello definimos y^. = ({/!*»•••»
y¡Af) como el vector observado y asumimos que se trata de un ca-
so estacionario. Esto significa que tenemos un vector fijo de
medias y     = í\i, , . . . ,\iJ) , ima matriz fija de covarianza ^ y que
para todo l , j la covarianza entre y . j . y y • ^ . depende sólo de
1. Una generalización natural de (7) en el caso de una serie
AR(1) puede entonces escribirse en la forma matricial


                         f/^ -u»^^-y^-i-^^•*-'^t                     ^^^

donde $ es una matriz ííxf( de coeficientes fijos y a. es un v«í-
tor K dimensional de ruido blanco que cumple


        Eia^) = 0 , E i a ^ ^ )   = fi^ y   E(a^J   .)-0   Vj ^ 0.   (10)

     Dos conclusiones son de importancia para lo que sigue: La
forma (9) y las propiedades de a^. nos llevan fácilmente a con-
cluir que

1)                                I=*I*^+fia                         (11)

2) Si $ es diagonal, (9) nos dice que cada serie individiial es
     del tipo AR(1). Por lo tanto para la predicción sólo se de-
     pende de su propio pasado. Si $ no lo es entonces, la pre-
     dicción y . y., puede depender de todas los í/• j .

94


     Con las herramientas introducidas hasta e s t e punto sobre
Series de Tiempo vatAós ahora a pasar a formular e l modelo s o -
b r e e l cual queremos t r a b a j a r .



3. Estimación y Predicción, para un Modelo Estocástica

     Suponemos un proceso como en (5), que se puede escribir en
forma matricial como


                              yt = i h - ^ h                 Í12)
     Se asume adicionalmente que el vector BJ. es un proceso del
tipo autoregresivo de primer orden, es decir tiene la forma

                          6^-y = 0(B^_j-y) + a^                (13)

$ es ima matriz K^K, a . y \i vectores K^l, 3^. = (^i/» • • • »^ííy-) '
X^ = (x, •.,... ,X|^). Se dispone de T observaciones sobre las v¿
riables y      y X y se supone que tanto e . como Oj. son de ruido
blanco (CJ. como en la Def 2 y a^ como en (10)). Además no son
correlacionadas entre ellas. Finalmente se tiene para BfiEiB+)
= y y matriz de covariante ^.

     La ecuación (12) puede escribirse en la forma


             ^t " ^t^t"^ H " ^^^ H^^t^^ '^^t^ i^'*' "-t       ^^^^
donde u^ = ^A^t~^^ ^ ^ t '

     La última ecuación muestra que .nuestro modelo puede inter-
pretarse como un caso de parámetro fijo B» pero con un término
de error Uj. que no posee las condiciones clásicas puesto que
exhibe tanto hetereocedasticidad como autocorrelación, es decir
no cumple ni (iii) ni (iv) del modelo tradicional. En efecto.

                                                                       95


dado que las Bf son autocorrelacionadas, también lo serán las
Uj^. Para la varianza de tu se tiene


                            Viuj.) = xllxj.+ a^
                                     tt-^t

    Dado las propiedades que exhibe u ^ , el método de estimación
que usualmente se aplica es el de Mínimos CuadradosGeneralizados.
Sin embargo aquí se presenta el problema que para aplicarlo,
se necesita ¿, que no conocemos.

    La solución que se propone es utilizar lo que se conoce en
la literatura como modelos de estado-espacio y aplicar la téc-
nica de Filtros de Kalman. Esta permite establecer bajo el su-
puesto adicional de normalidad de los términos de error, la
                        i

función de verosimilitud a partir de la cual se estiman los par-
                      2
rámetros fijos \ Í , o , ^ y ' ^ a "



3.1. El Filtro de Kalman.

    Suponemos un modelo dado por la especificación (12)-(13).
El propósito de la técnica que vamos a describir es establecer
un sistema computacional que permite generar la media y y ma-
triz de covarianza ^ de la serie BJ.» dada información en el
tiempo t .   Introducimos la siguiente notación: sean

3(^|n) := EiB^\y^,...yy^ y Pít\n) := VÍB^\y.^              {/„) media y
varianza de 3* condicionadas a la información en el tiempo n.

    En forma similar definimos, y i t \ t - l )   = E(í/+|{/i »-• • »{/+_i)
y h^ := Viy^\y^,...,y^_^).

    Ahora bien, dado la suposición de normalidad y tomando ex-
pectativas condicionadas en nuestro modelo (12) y (13) obtene-

III9 .<..




   96


   mos las ecuaciones que caracterizan el Filtro de Kalman: para


      B i l \ t - 1 ) = y + í)|B(^-l|í-l)-y| = $BU-l|^-l) + [I-*]y    (14.1)

                       P í t \ t - 1 ) = $P(^-l|í-l)$^+fí^           (14.2)

                          yit\t-l) = x^Bit\t-l)                      (14.3)

                          h^ = x^Pit\t-l)x^+a^                       (14.4)


       Igualmente e utilizando propiedades de la distribución nor-
   mal multivariada, (Wilks [1962]) completamos el conjunto de
   ecuaciones anteriores con

            Bit\t) = Bít\t-l) + Pit\t-l)x^'^^ y^-x^^Bit\t-l)]        (14.5)


            Pít\t) = Pít\t-1) - Pit\t-l)x^h2^x^^Pít\t-l)             (14.6)

        El proceso o filtro se inicia dándole valores a 3(0|0) y
    P(0|0) y calculando los restantes valores esperados y varian-
    zas a partir de las ecuaciones (14) en forma recursiva. Como
    los Bf son por la especificación (13) un proceso AR(1),

                            B(0|0) = y

   y P(0|0) se obtiene por (11) como solución del sistema lineal

                            P(0|0) = $P(0|0)/+íí^.                   (15)

       El paso siguiente es establecer la función de verosimili-
   tud basándose en la información obtenida recursivamente por el
   Filtro de Kalman. Para ello utilizamos la siguiente propiedad:


        iíy^,'--,yj) = iíyi)iiy2\yi)iiy2\y2*yi^' "^^yr^yj-v' • '^1^

                                                                  97


      Por (14.3) y (14.4) se conoce además que y*\y*_i      y-, és
N í x ^ i t \ t - 1 ) , h ^ y por lo tanto


 log L = log (í(í/j,...,ír¿)
                                                                (16)
       = -T/2 log27T-Vii I log h^-^/2 I \^y^-^^BÍt\t-l)^h~^ .
                        A,'" X          A»** X


Maximizar log L o lo que es lo misn» minimizar


        G = V2 I log h^+ Yz I [y^-xjB(t|^-l)]\^^            (17)
               A*^X              A,—i

                     2
  con respecto a y, a , $ y S2 nos arroja estimadores para estos
  parámetros.

      Como se observa G involucra para t ~ 1,...,T tanto BC-tl-í'l)
  como también P(;t|-t-l) (por medio de h j ) y estos a su vez son
  funciones de los parámetros a estimar. Resolver analíticamente
  este problema de minimización es prácticamente imposible y co-
  mo veremos en el ejemplo posterior necesitamos recurrir a mé-
  todos numéricos. Sin embargo antes de exponer éstos queremos
  exponer brevemente como se presenta el problema de la predic-
  ción y la estimación de 3**



  3.2. Predicción y Estimación de 3^.

      El propósito de la predicción es dar o indicar futuros va-
  lores, basados en la información conocida en un momento deter-
  minado del tiempo. En nuestro caso queremos predecir tanto los
  futuros valores de los parámetros estocásticos Bf así como tam-
  bién de la variable dependiente t/y..

      Si se conocen los parámetros fijos que intervienen en el

                         •^^-fST:




98

modelo (12)-(13), entonces lo que deseamos conocer es

 B(T+l|T) = $B(T+l-l|T)+Il-$Iy;     P(T+l|T) = $P(T+l-llT)/-H2
                                                             (18)
 í/(T+l|T) = x!f^^B(T+llT); h j ^ ^ - xJ^+^PiT+l\T)Xj^^+a^   (19)

que se obtienen directamente de las ecuaciones (14). Natural-
mente no se conocen los valores de los parámetros y debemos
utilizar los estimadores de máxima verosimilitud anteriormente
calculados.

     Finalmente queremos estimar los parámetros BJ. sobre el pe-
ríodo t = 1,...,T. El estimador debe basarse en toda la infor-
mación que se tiene y por lo tanto el estimador óptimo de B*
resulta ser 3(-t|T) . De igual manera para conocer los errores
estándar se requiere de P(í|T).

    El algoritmo empleado anteriormente no da estos valores
puesto que solo arroja media y varianza de By- dada información
hasta el tiempo t . Solo conocemos por lo tanto BdJT) y P(T|T)
por las ecuaciones (14). La solución nos la ofrece Ansley y
Kohn [1982], donde de nuevo utilizando propiedades de la dis-
tribución multivariada normal se obtiene


 Bit\T) = Bit\t)+Pít\t)(^^Pit+l\t)'^[Bit+l\T)-Bít+l\t)]      (20)

      Pít\T) = Pít\t)-Pit\t)<i''^Pít+l\t)~^*
                                                             (21)
               -\Pit+l\t)-Pit+l\T)]PU+l\t)~hPít\t)

     El cálculo se inicia poniendo t = T - l y utilizando las ecua
ciones (14), se obtienen recursivamente con t = T-2, T-3,...
estimadores para B(-t|T) y P(-t|T) . Nótese que de nuevo es nece-

                                                                     99


s a r i o emplear para los valores de los parámetros f i j o s los e s -
timadores de máxima verosimilitud.



4. Ejemplo de un Nodelo de Inversión.
    El modelo que queremos tratar tiene la siguiente especifi-
cación


                    h = ^t^ ^th"" ^ t h - 1 ^ h                    ^22)
                  2                        ^            -
donde e^. es W(0,a ) , I^ denota la inversión (o Formación Bruta
de Capital), Y. e l ingreso (o PIB) y R^. una tasa de interés.
                           T                  T
Definimos B* s= ibj.,Cj.,dj.) X. := i^»^f*^-(-^i)    • Suponemos adi-
cionalmente que Bj. sigue un proceso AR(1), es decir que
                    3^-y - *(e^_i-y) + «^                          <23)

donde a^. es W(0,íí ) , no hay autocorrelación y tampoco está co-
rrelacionada con el término de error de (22)

    La especificación dada por (22) y (23) sigue los lineamÍCT
mientos trazados por el sistema (12)-(13) y desde el pimto de
vista de su interpretación económica podría erguirse que por
ejemplo por cambios tecnológicos los parámetros no son fijos.

    Sin embargo y antes de embarcarse en un modelo y especifi-
carlo con parámetros estocásticos es deseable correr algunas
pruebas de hipótesis que nos verifique el supuesto dé estar
efectivamente en presencia de im modelo no tradicional. Varias
pruebas han sido propuestas para este efecto, cuya descripción
trasciende el alcance de este trabajo. Dejamos como referencia
en especial, la sinopsis en Chow [l983j donde se describen sie-
te posibles alternativas con sus respectivas notas bibliográfi-

100

cas y los tests descritos en el artículo base de Newbold y Bos
[1985]. De igual manera este trabajo sólo trató el caso en el
cual los parámetros estocásticos siguen un proceso autorregre^
sivo. Es posible extender la metodología empleada a procesos
más complejos como los llamados modelos ARMA (autoregresivos y
de promedio móvil).

           Ahora bien para simplificar un tanto las cosas vamos a su-
poner que tanto $ como SI son inicialmente ambas matrices dia-
gonales con I (|). . I < 1. Adoptando la notación de (14) tenemos
                            AxC




entonces que la matriz de covarianza de los UJ. viene dada por


                            fí'^A^ = I ^It^lS-l/ul^^-^^l^                                    ^''^
                                           A.= l
donde T = t - S es el rezago.
           Queremos obtener estimadores para los valores de $ y Í2 .
Harvey [l985] sugiere proceder de la siguiente forma: reempla-
zar E(Uj.u ) por UJU.. y estimar por MC las siguientes dos regre-
sxones;


               "í = ^l-'^2^Lí+V3í+^;t                                                        ^"^
      /\   A

      V;C-1 = ' ^ l ' ' ^ 2 ' ' u h , t - l ^ ' ^ 3 ^ 3 t h , t - l ' ' ^ t                  ^26)
                                                              2
           Resolviendo por (24) las ecuaciones c- = w • ./(1-<1)/•) y
                                  rt                                    ^     "VA..   A-A.

d . = ü). .(J). ./(1-<|) yy) para (úyy y ^ . . se obtinen estimadores para
 A<            A>^   i^^^         A.^i>       Á.^j.       A^'


estos parámetros. Tenemos por lo tanto gasta este punto valores
iniciales para los elementos de la diagonal de las matrices <j) y ñ¿j.
    Nuestro objetivo es minimizar la función G de (17). Para ob
                                               2
tener valores iniciales de   ^ (6¿.) = y y de a es natural esti-
mar la ecuación (22) omitiendo el carácter estocástico de esta

                                                                               101


especificación por MC. Es decir hacemos de cuenta que tenemos
un modelo tradicional. El resultado específico de este ejerci-
cio arrojó los siguientes resultados:


   I ^ = - 4 . 9 7 + 0 . 1 9 y ^ - 4.88R^_j

                   (0.019)        (1.72)


   R^.   = 0.93            a^ = 2 0 . 3 1
    ajus

   (l)jj = 0.3357       (f)22 = - 0 . 7 9       (Jj^j = 0.6376   w^^ = 8.855

   0)22 " 0-0000085         u^g = 0.1243.


    La parte final de este trabajo es utilizar los resultados
obtenidos como valores iniciales en la minimización de la fun-
ción G de (17) y analizar los resultados. Como ya se dijo es-
to es esencialmente un problema numérico, por cierto bastante
complejo, en el cual aún estamos trabajando.
BIBLIOGRAFÍA
Ansley, C.F. y Kohn, R., (1982). A geometrical derivation of the fixed interval smoothing algorithm. Biometrika 69.
Chow, C , (1983). Econometrios Mac-Graw Hill, New York.
Fuller, W.A. (1976). Introduction to Statistical Time Series. J. Wiley, New York.
Harvey, A.C.(1981). Time Series Models, Phillip Alian, London.
Newbold, P. y Bos, Th., (1985). Stochastic Parameter Regression Models. Sage University Papers N- 51.
Wilks, S., (1962). Mathematical Statistics. J. Wiley New York.

