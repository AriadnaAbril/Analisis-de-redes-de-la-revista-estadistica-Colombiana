EL ANALISIS DE REGRESION PERSPECTIVA HISTÓRICA
Universidad Nacional de Colombia
Resumen. Este artículo presenta un esbozo histórico del desarrollo de la regresión desde Legendre hasta los recientes logros que se han alcanzado en esta área de la Estadística, pretendiendo dar un marco general de referencia sobre la tematica.
Se divide la historia en tres eras, para señalar principalmente algunos aspectos de los trabajos de Legendre y Gauss como pioneros en las ideas de regresión y destacar los avances más relevantes de la época actual, señalando algunos de los más renombrados investigadores de cada tema.
Introducción. Tratar aspectos de la historia de la Estadística, es una tarea poco usual en nuestro ámbito académico, pero al abordar tal labor, de por si difícil, resulta interesante y apasionante. En verdad, el indagar por el origen, por el nacimiento, por aquella primera oportunidad en que se utilizó un procedimiento estadístico, resultan actividades muy gratas, pues despiertan una curiosidad insaciable por encontrar ese secreto que invitó a las mentes de grandes hombres a crear ideas estructurales de esta rama del saber humano.
Las más antiguas disciplinas parece hubieran hecho un pacto especial, para contribuir (casi todas), cada una desde su óptica, a la "gestación, nacimiento y crianza" de unas unidades, que luego se asociarían para constituir ese cuerpo conceptual llamado Estadística.

     Así, la Economía y la Meteorología por suparte, fueron la
fuente del Análisis de Series de Tiempo; la Biología admite la
cuantificacion y dS origen al Análisis de Correlación; el Aná-
                                      . . "a

lisis de Frecuencias, son su.popular distribución Ji-cuadrado,
se deriva a partir de la constitución de la sociedad humana en
otro sujeto de Análisis Estadístico. En fin, la idea de regu-
laridad en el comportamiento de los eventos aleatorios y en las
mediciones en los fenómenos naturales, llamo a la puerta de nni
chas disciplinas y actividades hiunanas.

     En forma similar, el cautivante mundo de la Astronomía con
sus abundantes registros y el rigor en su consignación, tam-
bién fue visitado por esa idea pilar de la regularidad esta-
dística, en momentos en los cuales a nuestros antepasados les
asistía el espíritu de independencia de la corona española,
dando paso a un punto de vista de análisis con presupuesto
propio, que hoy llamamos A n á l i s i s de R e g r e s i ó n , distinto al
tambiién reciente T e o r í a de E r r o r e s y que en justicia debería
contener en su denominación el apellido del mas destacado de
sus pioneros Karl Friedrich Gauss.

     El triunvirato de Gauss en Alemania, Legendre en Francia
y el menos nombrado Adrain en América, inicia la primera era
de la historia del desarrollo de los conceptos, métodos y usos
del Análisis de Regresión, la cual cubre el período que se

inicia en 1805 y que termina alrededor de 1935, año en el cual
Fisher publica se texto de diseño experimental,-y simultanea-
mente Aitken formula la teoría de los mínimos cuadrados en for-
ma matricial.

    La segunda época se caracteriza por el desarrollo concep-
tual, impulsado por el hecho de producirse en este periodo, en
la Teoría de la Estadística un fulgurante avance, que contri-
buye con la consolidación de   la estructura de la Teoría de la
Regresión.

    Alrededor de 1960, como consecuencia del desarrollo tecno-
lógico en el computo electrónico, comienza una tercera época
en la historia de la regresión, en la cual el interés se cen-
tra en el aspecto metodológico y los resultados de estos pro-
cesos, son rápidamnente iiiq>lementados en el "soft-ware" de los
sistemas de computación.



Era Inicial: Origen del Modelo Lineal

    En el año 1805, Adrián Legendre, expone las ideas inicia-
les del método de mínimos cuadrados, en el apéndice de sií^ tr¿
bajo "Nouvelles Iféthodes pour la Determination des Orbites des
Cometes" en el cual trata sobre "la méthode des moindre ca-
rras", sin ofrecer   alguna prueba del método.

    Al siguiente año, Gauss reclama el derecho de haber utili-
zado desde hace 12 años, ese método que Legendre llamó de los
mínimos cuadrados y se compromete a presentar sus resultados
posteriormente.

    Dos años después, R. Adrain, aparentemente sin conocer el

III                                - ' — ' — • -   ^ - —        - - ' — '   " • •   '




 trabajo de Legendre y del aun no publicado método de Gatjss,
 desarrolló el, método de los muimos ctiadrados y lo utilizó pa-
 ra resolver varios problemas.

      Sin entrar a determinar quien fue realmente el iniciador
 del método y atendiendo solo a las referencias cronológicas de
 las publicaciones básicas, se presentan a continuación los
 principales aportes de los precursores de la regresión.

      En su citado trabajo, Legendre afirmaba que en problemas
 en los cuales se requiere obtener las mas exactas conclusiones
 a partir de observaciones numéricas, casi siempre se termina
 por establecer xm sistema de ecuaciones de la forma:


               ^y " I ^ i ^ i r ' ^ M ' i»2        '^jW > q)
                -^ j ^ l i S'^ -^
 Donde Z.. "coeilclentes conocidos"
         jA,

        ""I    "la observación numírlca"
               "tos desconocidos"
               "tos errores"

 El objetivo era "determinar" (estimar?) los q "desconocidos"
 (parámetros?), de tal forma que cada "error" (residual?) fue-
 se "muy pequeño" y "los errores extremos", sin                considerar el
 signo, estuviesen "dentro de limites muy cercanos".

      El "principio" que Legendre propuso para tal propósito fue
 la minimización, por variación en los B, de la suma de cuadra-
 dos de los errores, procediendo luego a obtener las ecuaciones
normales.

      En 1809 Gauss, en su trabajo "Theoria Motus Corporum coe-
 lestium in sectionibus conicis solem ambiemtium", cumple con

el compromiso de publicar sus resultados, ubicándose en otra
perspectiva, al considerar explícitamiente la distribución pro-
babilística de los errores del modelo presentado por Legendre.

    El aspecto fundamental de este trabajo de Gauss, esta re-
lacionado con el hecho de haber encontrado que la distribución
de los errores, asimiida continua, era necesariamente normal,
cuando aquellos eran muestreados independientemente de un uni-
verso normal de media cero y varianza fija o como Gauss prefe-
                                    2 - 2
ría, con precisión fija h , donde 2o = h .

    En términos generales el desarrollo de Gauss, fue el si-
guiente: bajo el supesto de que los errores observacionales
son variables aleatorias independientes, con distribución de
probabilidad H e ) y que los B tienen a-priori distribución
uniforme, la distribución a posteriori de los mismos, condi-^
clonada a las observaciones, es según la teoría Bayesiana,
proporcional a la distribución conjunta de los errores. Como
estimador de los 0, Gauss toma la moda de esta distribución.

    Cuando existe un solo parámetro dicho valor taaa probable,
corresponde a la media aritmética de los errores, la distribu:-
ción conjunta de estos es necesariamente normal y la distribu-
ción de los 6 es entonces proporcional a la función de densi-
dad de una normal multivariante. Esta probabilidad se maximiza
cuando se minimiza la suma de los cuadrados de los residuales,
lo cual corresponde exactamente al principio de los mínimos
cuadrados. En 1810, Gauss complementa este trabajo con la pre-
sentación de un algoritmo computaclonal del proceso en su "Dis
quisitio Palladis".

   Posteriormente, entre 1821 y 1826, presenta varios escri-
tos bajo el nombre genérico de "Theoria Combinationis Obser-

vationum erroribus minimis obnoxiae", con los cuales culmina
sus aportes al modelo. Caben destacar de este conjunto de tra-
bajos los siguientes hechos:

a. El abandono del supuesto de normalidad, al descubrir una
desigualdad general aplicable a cualquier distribución de pro-
babilidad continua unimodal y simétrica alrededor de la moda.

b. La determinación de los estimadores insesgados de los 3»
que además resultan ser de mínima varianza.

c. La extensión del método, para combinación lineal de los 3
y la derivación de la varianza del estimador, obteniéndose lo
que hoy en día se llaman covarianzas entre dos B estimados.

d. La obtención de la expresión para determinar la suma de cu¿
drados residuales a partir de los B estimados.

e. La presentación de un procedimiento para incluir otros B al
modelo, sin tener que recalcular los B ya estimados.

f. La generación de una forma aproximada para estimar la des-
viación estándar de los errores y la deducción del error es--
tándar de tal estimador.

    Aunque Gauss desarrollo los métodos para calcular el error
estándar del estimador de cualquier B, parece no haberse refe-
rido al problema de decidir si uno o más términos del modelo li-
neal deberían ser legítimamente descartados por no significan-
tes. Este problema fue considerado por Augustin Louis Cauchy,
pero sin referencia alguna a la distribución probabilística de
los errores residuales. En este sentido, el trabajo de Cauchy
es una regresión a las ideas menos sofisticadas de Legendre y
Cauchy en sus memorias litografiadas en 1835, lo presenta como

un problema de interpolación, acompañado de un dispendioso al-
goritmo de solución.

    Bienaymé, Chebyshev, y Gram entre otros, durante 1853 a
1883, al tratar de simplificar el cálculo numérico de algorit-
mo de Cauchy, introducen y dan los primeros pasos en la utili-
zación de los procesos de ortogonalización, para algunos pro-
blemas del modelo lineal. Sin embargo estos resultados inicia-
les no se llevaron a la práctica, e inclusive fueron por algún
tiempo olvidados, para ser retomados en épocas más recientes
por autores como Romanovsky (1925-1927) y por Rao (1949), en-
tre otros.

    Además contribuyeron con sus estudios al desarrollo de la
regresión^en esta época, autores como Helmert quien en 1875 al
analizar la distribución de l a varianza muestral, baje norma-
lidadi deduce la Ji-cuadrado, la cual es redescubierta poste-
riormente    por K. Pearson, en 1900 para pruebas de bondad de
ajuste y Pizzetti quien en 1889 hace una extensión del proceso
de Helmert, que lo lleva a encontrar la distribución de la Su-
ma de los cuadrados de los errores.

   A mediados del siglo diecinueve, aparece dentro de la Es-
tadística im nuevo concepto, el de la distribución de probabi-
lidad multivariante y más especificamente el de la distribu-
ción normal multivariante o ley. multinormal, que tendría una
gran influencia en el mayor desarrollo de los procesos de re-
gresión y cocrelación.

    August Bravais, considera por primera vez en 1846, la dis-
tribución de probabilidad conjunta de dos y tres variables, y
dentro de su trabajo utiliza tangencialmente el concepto de
correlación entre dos variables, aunque sin preocuparse dema-

siado por la aplicación práctica de sus resultados. Posterior-
mente Schols (1875), amplia un poco los trabajos para tres va-
riables y discute la aplicación de la normal bivariante.

    Pero fue Edgeworth, en 1892 el primero en proveer una for-
mulación completamente general de la normal multivariante, de-
safortunadamente con una notación engorrosa y difícil de mane-
jar, tanto que él mismo solo pudo aplicarla nuáiéricamente para
cuatro variables y solo K. Pearson, cuatro años después, la
presentó con una notación mejorada.

    El pionero en la utilización de las "palabras"regresión y
correlación fue Sir Francis Galton, al estudiar en "HereditaiTr
Genius" en 1869, la herencia de la estatura. Efectivamente en-
contró que las estaturas de los hijos cuyos padres tenían es-
taturas fuera de la "medianía", tendían a regresar a la media-
nía de la estatura de su propia generación hecho que Galton
llamó entonces "regresión o reversión", palabra que infortuna-
damente se generalizó para referirse al estudio de la relación
funcional entre variables, en contra de la misma etimología
del vocablo.

    En 1877, introdujo una medida de tal regresión, que luego
renombra co-relación o la actual correlación, asignándole el
síiri)olo " r " , inicial de regresión. Es de anotar que los apor-
tes de Galton fueron fundameitfalmente empíricos y la mayor crí-
tica negativa que se le hace, es haber pretendido hacer arit-
mética "por olfato". Posteriormente, a finales de 1890, Weldon
introduce el concepto de "coeficiente de correlación" y sugie-
re que, por lo menos dentro del campo de los estudios genéti-
cos en los cuales tuvieron origen estos temas, este parámetro
deberá ser constante.

     Como se anotó anteriormente, fue Rarl Pearson en 1896 quien
presentó una formulación definitiva de estas investigaciones
 la cual fue integrada al cuerpo conceptual de la Estadística.
 Pearson, extendiendo las ideas de Galton y Weldon a "p" varia-
bles correlacionadas deriva la "superficie normal multivariada"
.y encuentra que si las desviaciones con respecto a la corres-
pondiente media de p-1 de las variables, toman valores deter-
minados, la distribución condicional de la restante variable
es normal univariada alrededor de un valor esperado.

     La estructura de tal valor esperado, corresponde a una com
binación lineal de las desviaciones determinadas de           las otras
variables y cuyos coeficientes son los regresores o "coeficien
tes de regresión"; a tal ccHubinación se le dio el nombre de
" r e g r e s i ó n de X^ en Xj,X2,... ,Xp_|". En la estimación de los pa-
rámetros de la expresión utilizó un método que es el actualmeñ
te conocido como de Máxima Verosimiltud.

     En 1897, Yule mostró que para dos, tres y cuatro variables,
la ecuación de Pearson, era la misma que resulta de estimar
por el método de mínimos cuadrados, un valor de la vairiable )L,.
por medio de una combinación lineal de valores "dados" de las
otras variables. Pearson probó posteriormente (1899), que esta
combinación lineal es la que ti«tíe máximo coeficiente de corre^
lación con Xp.

     Karl Pearson amplió la aplicación del iiK>delo lineal de Ga-
uss a vina clase más extensa de problemas que la reladva a la
de medición de errores, y por ende permitid una interpretación
más general que la del modelo de Legendre.

     Con Ronald A. Fisher, culmina esta primera etapa del desa-
rrollo de la regresión, recibiendo de este científico, espe-

                                                                   •^T-pn»Bm



10


cialmente entre 1922 y 1935, dos de los mayores aportes para la
teoría del muestreo en regresión, especificamente a nivel de la
inferencia. Tales aportes fueron la formulación y la presenta-
ción del proceso para pruebas de hipótesis relativas a los coe-
ficientes de regresión, y el desarrollo del análisis de varian-
za, con la consiguiente deducción de la distribución F y la uti^
lización de la recién deducida distribución de U. (absset.

     Como es de común decir, R. Fisher "no necesita presentacioa"
y realmente tratar de presentar exahustivamente todas sus con-
tribuciones, no solo al desarrollo de la regreSion sino de la
Estadística en general, se sale de las posibilidades de esta
breve exposición, mereciendo quizás un espacio aparte.

     Terminamos en este punto el somero esbozo acerca del origen
de este proceso estadístico, para dar paso a continuación a la
presentación muy esquemática dé la época moderna, de ima forma
ua poco diferente a la anterior,debido a la gran cantidad de es
tudiosos que han contribuido al avance de la misma, lo que im-
pide hacer una pormenorizada referencia a los logros de cada
uno de ellos, limitándonos a identificar algunos nombres aso-
ciados con los principales teimas.

     Pasando por alto un cuarto de siglo, en el cual, por el mis_
mo desarrollo de la Teoría Estadística se consolidan y refinan
los aspectos teóricos de la regresión, pero con una metodología
relativam^ite incipiente referiremos la que llamamos era meto-
dológica que cubre los aikjs desde 1960 hasta nuestros días, la
cual consideramos por su naturaleza más interesante de comentar.

                                                                     11

Era Moderna: Desarrollo Metodológico.

    La complejidad de los procesos de cálculo y la carencia de
herramientas de computo hasta finales de la década de los cin-
cuenta impidieron un mayor desarrollo de la metodología del
análisis de regresión y el avance de varias ideas, algunas de
ellas manifiestas en publicaciones del siglo pasado, que perma_
necieron latentes hasta encontrar el auxilio del computador pa-
ra permitir su implementación. Por esta razón conceptos corriai
tes hoy día como robustez, colinealidad, selección de variables,
entre otros no pertenecían al conjunto de términos de la regre-
sión o solo fueron citados por aquellos años, de una manera muy
tímida.

    Este último período de unos treinta años y que aún no ter-
mina, es una era caracterizada por los avances significativos en
la metodología de la regresión. Rico en'publicaciones y con un
elevado número de nombres asociados a sus avances tanto en la
estructuración de .los conceptos, como en la generación de los
procedimientos para su aplicación, logrando de esta manera ha-
cer del análisis de la regresión una herramienta accesible y
que con el soporte de la computación, la convierte en una de
las más importantes áreas del análisis estadístico.

    El preguntar por la adecuación de los supestos de normali-
dad, independencia y homocedasticidad, buscando la construcción
adecuada de un modelo, puede tener respuesta en el Examen de
R e s i d u a l e s , aspecto que ha interesado a varios autores. Tuckey,
Anscombe, Pastemack, Luizzi, Behnken, Snee y Draper, trazan
las pautas que permiten este examen, con sus publicaciones en-
tre 1961 y 1972 principalmente.

    En el mismo sentido del cumplimiento de los supuestos del

                                                          ^m^fí



12


modelo. Box hacia el año 1962 conjuntamente con Tidwell y pos-
teriormente con Cox, impulsan la idea y proponen los métodos
para la trasformación de variables. Se destacan también en es-
te tema los aportes        de Atkinson, desde 1973, e igualmente los
artículos de Cook y Weisbérgen en 1982.

     La vieja idea de S e l e c c i ó n de V a r i a b l e s , que se puede deri
var indirectamente del trabajo de Cauchy, presentada por él en
forma no estadística, pero con una raíz común, es entendida hoy
en día como la búsqueda del "mejor modelo" y surge de la pregunta
sobre el efecto de añadir o excluir variables en el modelo de
regresión. En 1960 aparecen ideas al respecto con el trabajo
de Efro5nnson, en el cual se hace referencia al proceso "Step-
wise". Los sentidos de "Forward" y "Backward" de este proceso,
fueron analizados 10 ^bs después por Mantel, quien expresa razones
de preferencia por el sentido Backward. Los detalles del proce^
so y el uso de la estadística F como eje del mismo son comenta^
dos por Pope y Webster en 1972.

     Alrededor de Hocking hay un grupo de autores como Leslie,
Pendlenton y La Motte, quienes han publicado varios artículos
referentes a la selección de variables, que incluyen los pro-
cedimientos computacionales para llevarla a cabo. Es convenien
te anotar que este es uno de los temas de regresión, sobre el
cual la publicación ha sido bastante prolija y fructífera.

     La C o l i n e a l i d a d , e s otro concepto que impulsa el desarro-
llo de procedimientos y criterios para analizarla y/o remover-
la. Ejemplo de ello es la utilización de la Regresión Rldge, como
alternativa al método de mínimos cuadrados ordinarios, propues.
ta por Hoerl y Rennard        en 1970 (con orígenes en el año 1962 e
implementada computacionalmente en 1981), convirtiéndose                   en
otra de las áreas de la regresión, más trabajadas y por tanto

                                                                                  13


de gran producción bibliográfica.

    El estudio de los residuales, dio origen a su vez en 1973,
a la aplicación en regresión de la Robustez, campo de la teo-
ría de la estimación estadistica recientemente originada y en
pleno desarrollo, aplicable a los problemas que surgen por la
no aceptación del supuesto de normalidad en la distribución de
los errores. Con el trabajo en Regresión Robusta de Huber pre-
sentado en el citado año y con la publicación de Andrews en el
año siguiente se cuenta con un conjunto de estimadores robus-
tos , cuyos procedimientos computacionales son analizados por
Holland, Welsch, Denby, Mallows y Larson alrededor de 1977.

    Con el termino V l a g n ó s t l c o s en R e g r e s i ó n , se distinguen
varios procedimientos que tienden a detectar "valores anóma-
los", como son los "outliers", "casos influencíales", "valores^
extremos" que dé una u otra manera afectan las estimaciones del
modelo, con el fin de determinar si un valor o un conjunto de
valores tales, debe ser removido, modificado o retenido dentro
del mismo. Algunos autores como Hoalglin, Welsch, Andrews, Pre^
gibon, Weiberg y muy especialmente Cook, han ideado a partir
de 1977, Bffididas de diagnóstico que permiten detectar casos in-
dividuales o grupos de casos que puedan diferir de la genera-
lidad. Draper y John (1981) discuten los n^ritos relativos de
algunos de estos métodos.

    El "problema de enmascaramiento", en el cual la combina-
ción de dos o más casos influencíales, pueden dar un diagnós-
tico aceptable, en tanto que uno solo no, genera la considerar-
ción de medidas de diagnóstico para subconjuntos de casos. Los
conceptos de las medidas para estas situaciones, se pueden ex-
tender fácilmente, pero como lo anotan Andrews y Pregibon
(1978), los cálculos son extensos, por lo cual ha sido necesa-

14


rio utilizar soluciones gráficas aproximadas, como las sugeri-
das por Larsen y McCleary en 1972 o por Belsley, Kuh y Welsch
en 1980 y resumidas por Mallows en 1982.

     Otros temas relativos a la regresión, tales como la regre-
sión no lineal con toda su enorme complejidad especialmente en
los procesos inf réndales, la regresión con restricciones, y
la muy reciente idea de cointegración, han tenido y siguen te-
niendo un desarrollo notable en esta época y son muchos los au
tores contemporáneos, varios de ellos en nuestro país, que coii
tinúan cuestionando y ampliando estos procesos.



Epílogo.

     Es deseo de los autores, que estas breves notas históricas
referentes primordialmente a la Teoría y Métodos de la Regre-
sión, hayan logrado el fin principal de motivar a los lec-
tores a mirar con mayor interés el estudio de la perspectiva
histórica, no solo de la Regresión, sino de la Estadística en
general, pues el conocer el pasado es fundamental para enten-
der el presente y construir el futuro, verdad que tiene cabi-
da en todos los hechos del ser humano y en particular en el
caso de la Estadística, algunas preguntas sobre diversos as-
pectos solo tienen respuesta por el conocimiento del desarro-
llo del pensamiento estadístico a través del tiempo.
BIBLIOGRAFÍA
Drapper, N.R. y Smith, H. (1981). Applied Regression Analysis (2da. Ed.). John Wiley, New York.
Fisher, B.J. 1978. R.A. Fisher, the life of scientist. John Wiley and Sons. New York.
Hocking, R.R. 1983. Developments in linear regression methodology: 1956/1982. Technometrics, Vol. 25 N°- 3, Agosto.
Kotz, S.; Johnson, M.L, 1982. Encyclopedia of Statistical Sciences. John Wiley and Sons.
Pearson, E.S., Kendall, M.G. 1970. Studies in the history of statistics and probability. Hafner Publishing Company.