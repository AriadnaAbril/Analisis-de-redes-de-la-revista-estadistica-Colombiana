LA CONSTRUCCIÓN DE UN MODELO DE REGRESIÓN
Centro Internacional de Agricultura Tropical CIAT
Resumen
La construcción de un modelo de regresión es un proceso iterativo entre las etapas de identificación, estimación y chequeo del modelo. Se presentan varios conceptos que contribuyen a la apropiada formulación y escogencia de la ecuación de regresión que mejor se ajuste aun conjunto de datos.
Supuestos del Modelo.
El modelo de regresión lineal se basa en los siguientes supuestos:

 - La varianza de los errores es constante.
 - No existe dependencia entre los errores de observación a ob-
 servación: no correlacionados.
 - Para efectos de inferencia estadística los errores se dis-
 tribuyen normalmente.
 - La forma del modelo es la correcta.

     Este último supuesto se refiere a la adecuada expresión

                                                                17


matemática que representa la relación y acción de los predic-
tores en la respuesta. Los factores Xi son cuantitativos o al
menos corresponden a una variable indicadora DUMMT con valores
uno o cero, para señalar la presencia o ausencia de alguna ca-
racterística que afecta la respuesta.



2. El Modelo de Regresión.

    Debido a que no necesariamente se conoce de antemano la
forma correcta del modelo de regresión, se considera en un sen^
tido amplio, el modelo Lineal en los parámetros:


            TÍY.) = U j i X ^ , X ^ , . . . ,X^.) ^^. + r .   (2)

donde T(y.) es una transformación de la variable de respues-
ta, que necesariamente afecta los supuestos distribucionales de
los errores. Las funciones 9y(Xi .-.X»-,... ,X .) son iwsdifica-
ciones o nuevas expresiones de las variables predictoras aue
son de interés encontrar y que mejoran la capacidad predictiva
de la respuesta. Por convención se define g ( ) como cero o
uno, según se desee incluir o no en el modelo el coeficiente
6 denominado intercepto. Los 6.- son coeficientes de regresión,
desconocidos y uno de los propósitos del análisis de regresión
es su estimación;' " q " , e s el tamaño del modelo o número de
variables y sus transformaciones que afectan la respuesta; r .
es una discrepancia específica a la observación I entre la
respuesta observada o su transformación y la respuesta espera-
da. Para fines de dócimas de hipótesis, se deben hacer supues-
tos acerca de la distribución de los errores.

    Para simplificar la notación, en adelante Z • indicará cual^
quier función no nula 3 ;(X. ,X2,... ,X ) , j = 1 , 2 q < n; T.

18

es la correspondiente transformación T Í Y . ) ; por lo tanto la ex-
presión del modelo planteada en [z] será equivalente a:


                    h'^^.l^^j'jl-''-l                         (3>

Cuando no se requiere transformación para la respuesta, T = V»
es la identidad.

     Algunas formas especiales de modelos de regresión son las
siguientes:

Lineal simple                   / = B +B,X,+ e
                                     o i l
Cuadrático                      i' = B +B,X,+BoX?+e
                                      o 1 1 2 1
Polinomial de orden p           Y = &^+&^X^+&2X^+...+B X^+ e
                                                    2
Superficie de respuesta         Y = B +B,X^+B„X +B_X^
de orden 2

                                       +B¿^x2+B5X^.X2+e

Semilogarítmico                 ^ = B +B,logX. + e

Exponencial                     l o g ^ = B^+B^X^ + ^t

Logarítmico                     logi' = B +BilogX.. + / i

Bilineal                        V = ÍB^+B^X^)+ÍB2+B^X^)X^+ e^

Inverso                         y = B +B,/X,+ e
                                      o l í
Geométrico                      logi' = B +B*X^+r con B* = l o g ^ P j

Lineal múltiple                 V = B +B,X,+BoX-+.. .+B X + e
                                     o 1 1 22          P P
Senoidal                        Y = B +B,senX,+BoCosX,+ e
                                     o 1     1 ¿     1

                                                                        19

Multiplicativo                      lo%y = Bjj+B.logXj^+...+BplogX +A.

Recíproco                           1/V = B +B,X,+...+B X + r

Inverso general                     Y = B +B,/X.+...+B /X + e

Nótese que las funciones g -i ) toman distintas formas según la
expresión del modelo a analizar y las transformaciones en la
respuesta modifican los supuestos de distribución de probabi-
lidad de la misma y de los errores o residuos.



2.1. Características de un buen modelo.

    El modelo de regresión que se escoja debe minimizar alguna
medida de discrepancia entre los datos observados y los esti-
mados, por ejemplo.




Es importante t£mtbién, la simplicidad y parsimonia del modelo,
en el sentido de retener sólo el menor número de predictores
que garanticen una buena estimación de la respuesta sin que
ello implique eliminación de predictores importantes. La buena
formulación del modelo en el caso de regresión polinomial debe
tener en cuenta el concepto jerarquía, es decir, al incluir en
el modelo el término XL.Xr-...Xr         con ¿j,¿2»'• • »^>w exponentes
enteros, se deben incluir todos los términos que contengan las
combinaciones de los productos de X. , ,X. «,... ,Xi         con su co-
rrespondiente potencia de igual y de menor orden, Peixoto
(1987). Por ejemplo, los modelos que incluyan los términos
|_1 ,X. ,X2 ,X.X_ ,X-J   o   [1 ,X^ ,X-,X.J   o   |_1,X. ,X_ ,X2,X2 ,X.X2,
X.X2] son jerárquicamente bien formulados, mientras que el mo-
délo [l,X ,X..,X X ] no lo es, porque no incluye los términos

20


Xj^ y X-X-. Bajo condiciones muy generales, el espacio de esti-
mación de un modelo de regresión polinomial es invariante bajo
transformaciones de escala, si el modelo está jerárquicamente
bien formulado, Peixoto (1990).

     McCullagh y Nelder (1983) y Peixoto (1987) puntualizan que
la no inclusión de términos con potencia de menor orden puede
ser aceptable como parsimonioso, cuando el modelo sea usado
fínicamente con fines predictivos y no se esperan reparametri-
zaciones o cambios de escala, y no son erróneos cuando el mo-
delo describe leyes exactas de fenómenos físicos y químicos,
Peixoto (1990).
     Todas las variables predictoras que se piense incluir en
el modelo deben ser linealmente independientes, es decir no se
pueden encontrar constantes C-, j ^ 1 , 2 , . . . , q , no todas cero que
cumplan

                              I C.l. = O                                   (5)
                             y=i ^ ^
     Hay ciertas relaciones entre variables que pueden producir
el efecto "ALIASING" que identificadas permiten establecer un
mejor modelo en el sentido de parsimonia. McCullagh y Nelder
(1983) ilustran este efecto mediante el siguiente ejemplo:

Nominando Z = l o g i l a r g o ) , 1 = lo%íancho), Z- = l o g i d r e a ) ,
Si á r e a = cOYistante X largo X ancho entonces el modelo




se puede reescribir, debido a que Z_ = C+Z.+Z-, como


              T - (6^+330)+(Bi+B3)Zj+(32+63)Z2+e                               (7)


lo que implica estimar sólo tres parámetros y no cuatro como

                                                                  21


inicialiMnte se planteó. El buen juicio del analista de datos
permitirá identificar en cada caso éste efecto. Del modelo se
deben eliminar las variables redundantes: Flury (1989) define
1 . como tma variable redundante en un modelo de regresión múl-
tiple si la correlación múltiple entre T y Z,,Z_,...,Z    no de-
crece al eliminar I - , Este criterio de redundancia a veces se
entiende como la existencia de correlación entre variables pé-
dictoras, pero Hamilton (1987) mostró que este hecho no siem-
pre implica redundancia.

    El modelo de regresión debe tener especificado su campo de
acción o dominio de los predictores, sobre los cuales da bue-
nas predicciones. Si el dominio de los predictores es suficien
temente amplio, se puede asemejar esta situación con invarian-
za paramétrica, es decir, si con otro conjimto de datos rela-
cionados al mismo fenómeno se estiman los parámetros para el
mismo modelo, las nuevas estimaciones no cambian sustanciaftmen
te. Brooks, et al. (1988) definen el dominio de un modelo de
regresión en términos de superficies convexas, que comparati-
vamente a otras definiciones es más conservadora y presentan
un programa IML/SAS para determinar si una predicción proviene
o no del dominio 4c^ modelo y cae dentro del rango válido.

    Finalmente, para propósitos jde inferenc^^ia estadística el
modelo no debe violar los supuestos acerca de independencia y.
distribución de probabilidad de los errores. ^



2.2. Proceso 4e Gonstrucci6n del Modelo.

    La construcción del modelo. Box, et al. (1978), Box y Jen-
kins (1976), McCullagh y Nelder (1983), se asemeja a un proce-
so iterativo entre las etapas: Especificación o identificación.

22


Selección y ajuste y chequeo de diagnósticos. La prims^r^ se de
sarrolla informalmente con las ayuda de gráficos y un análisis
preliminar de los datos buscando las características del mode-
lo para llegar a una clase de modelos candidatos. La segunda,,
conlleva estimación de parámetros y discri;n^nación entre los
modelos candidatos. para seleccionar el piejor 05 los mejores-mo-
delos y en la última se 4naliz3>que tan adecuado o inadecuado
es el modelo seleccionado; los gráficos de los residuos indica
rán que modificaciones se deben hacer para los siguientes ci-
clos del proceso o si es suficientemente bueno el modelo como
para terminar el proceso. Box et al. (1978) ilustran mediante
un diagrama de flujo las posibles acciones a tomar en estudios
conducentes a plantear modelos de superficies de respuesta.




3. Consecuencias de la Incorrecta Especificación del
   Modelo de Regresión.

     Hocking (1976) establece la incorrecta especificación del
modelo en los casos en que: se incluyen variables irrelevantes
o se descartan variables que relevantes. Rao (1971) lo presen-
ta para los modelos de regresión donde.todas las variables pr£
dictoras no son estocásticas y los téinninos de error tienen va-
rianza constante e independientes, considera que: la omisión de
ima Variable relevante introduce sesgo en todas las estimacio-
nes de los parámetros' por mínimos cuadrado y arroja menor va-
rianza de los mismos. La inclusión de tma variable irrelevante
no introduce sesgo en las estimaciones de los parámetros, pero
incrementa la variaiíaía de tód^^las estimaciohés.

     La especificación del modelo incluyendo variables irrele-
vantes sacrifica el tamaño reducido o parsimonia del modelo e
incrementa el riesgo de establecer fluctuaciones aleatorias

                                                                                        23


que produzcan efectos engañosos o falsos.



4. Medidas de Buen Ajuste del Modelo.

    La e s c o g e n c i a d e l mejor modelo que se a j u s t e a l o s d a t o s se
hace con base en alguna medida que indique que tan' bueno es el
ajusté, la cual puede ser según Hocking (1976): Cuadrado Medio
de Residuos, K-cuadrado, Suma de Cuadrados de la Predicción
FRESS, Simia de Cuadrados Residuales Estandarizados, RSS , Cua-
                                                                             p
drado Medio de Error de Predicción Promedio S , el Error Cua-
                                                           ^   P
drático Total C , Varianza de una Predicción Promedio. Se dis-
cuten a continuación los de uso más frecuente.
a. Cuadrado Medio de Residuos;
Esta.medida sugiere cual es el tamaño apropiado del modelo, de^
bido a que tiende a estabilizarse cuando se presenta ajuste
con inclusión progresiva de más predictores que los necesarios,
Draper y Smith (1981).' La escogencia entre varios modelos can-
didatos se i inclinará por aquel modelo que de menor cuadrado me-
dio de residuos. Su desventaja radica en la comparación de mo-
delos que tengan distintas transformaciones para la respuesta.
b. Error Cuadrático total C , definido por:

                                   SCRp-                                         "
                              C^ = — f - - i n - 2 p )                               (9)

donde SCR e s la Suma de Cuadrados de residuos de un modelo
         P        ^             ^                   2
que contiene p parámetros, incluido el intercepto. S es el
cuadrado medio de residuos del modelo de mayor tamaño propues-
que contenga todos los predictores, tomado como estimación de
la varianza de error. Se escoge p como mejor tamaño de subcon-
junto de variables a retener en el modelo cuando C sea igual
                                                                      r
O próximo a p. Draper y Smith (1981) sugieren intentar estra-

                               i,y   • ly^^j^^^'g-



24


tegias de construcción de modelos más racionales que la de exa-
minar todos los posibles subconjuntos de variables a retener en
el modelo.

c. R-Cuadrado:
Es la medida de buen ajuste de un modelo de regresión. Kvalseth
(1985) hace una revisión de las distintas formas de cálculo de
 2
R y analiza las circunstancias bajo las cuales una u otra for
ma puede ser mal empleada. Chang y Afifi (1987) complementan la
                        ^      2
lista con una modificación de R cuando se tienen repeticiones
                                                      2
para las respuestas. Willett y Singer (1988) definen R adecúa
do cuando se estiman los parámetros por el método de mínimos
cuadrados ponderados. Theil y Chung (1988) sugieren otra co-
rrección por número de parámetros en el modelo como factor en
                                                2     ^
exponente, obviando asi situaciones en los que R podría ser
negativo. El ctiadro 1 presenta las distintas definiciones de
R\
                                 2
     La apropiada escogencia de R , Kwalseth (1985), depende:
del tipo de modelo, presencia de 3 o uso de transformaciones,
de la técnica de modelaje utilizada, del propósito con el cual
se usa R^ y de las propiedades que se consideren deseables pa-
ra R^.

                                          2
     Una situación frecuente es calcular R para un modelo li-
                 r   ""I                        2
nealizado con [T,TJ y compararlo con el R            para un modelo li-
neal con [^>V]. £1 primero brinda una medida del buen ajuste
para el modelo linealizado y no para él modelo no lineal en los
parámetros. Para hacer comparables las medidas de ajuste de los
                              2
dos modelos se debe calcular R para el modelo linealizado tisan
    Y,Y = T (T)] donde T (T) es la transformación inversa de
T( ) que opera sobre los valores de predicción del modelo li-
nealizado ajustado.

                                                                         25


                              CUADRO 1.
     Definiciones de R' como medida de buen ajuste de una
     regresión.
Nota: en (5): Cuadrado de la correlación múltiple entre
      T y Zi,Z2,...,Zq.
      en (6): Cuadrado de la correlación múltiple entre
      y y 9.

26

                                      2 -  2
    Otro error frecuente es utilizar R_ ó R„ para modelos que
                                 2
no incluyen 3 y compararlo con R para modelos que si lo inclu
yen. En el primer caso por razones teóricas se forza el modelo
a no tener intercepto, así los datos sugieran que para el domi-
nio del modelo, 3 debe ser incluido, Kwalseth (1985). Se reco-
mienda usar modelos sin 3   cuando tanto las consideraciones t ^
ricas como el dominio del modelo y los datos lo sugieren.




5. La Construcción del Modelo.

     La selección del modelo seria más fácil de realizar si se
tuviera de antemano un conocimiento exacto de la varianza de
los residuos. Como esta varianza es desconocida y un estimador
es el cuadrado medio de residuos, surge la incertidumbre sobre
si su magnitud es debida a falta de ajuste del modelo o si es
natural en los datos. Hay varios métodos expuestos en la lite-
ratura sobre la escogencia de jlá mejor ecuación de regresión,
pero varías son las dificultades a resolver én este proceso,
siendo algunas de ellas el establecer: Cuál es la transforma-
ción más adecuada de la respuesta? . Son todos los predictores
observados o controlados los que afectan de una manera impor-
tante la respuesta?. Cuál es el número de predictores ideal pa^
ra el modelo?. Los predictores afectan linealmente la respues-
ta? . Hay interacciones lineales o de mayor orden entre los di^
tintos predictores?




5.1. Transformación en la Respuesta.

     Cook y Weisberg (1982) identifican tres clases de situaci£
nes que motivan transformaciones de la respuesta.

                                                                     27


m. Las respuestas son independientes y provienen de una pobla-
ción con distribución no normal y el objetivo de la transforma
ción es buscar que la nueva forma de la rspuesta y sus errores
sea suficientemente cercana a la distribución normal con la fi-
nalidad de aplicar los métodos basados en esa distribución de
probabilidad. McCullagh y Nelder (1983), dentro de los modelos
generalizados nominan ésta transformación como fxincíones de en-
lace y dependen del supuesto inicial de la distribución de pro-
babilidad de la respuesta y sus errores. (Cuadro 2).


Cuadro 2. Distribución de probabilidad de la respuesta y su
           posible transformación.

Distribución       Transformación                    Nombre
Normal             T = y                             Identidad

Poisson           T = log(/),    Y>O                 Logarítmica
                  T =/T,     y >.0                   Raíz cuadrada

Binomial           T = lozíY/Íl-Y)) ,   O < i' < 1 Logit
                  T = *~^(y),    O « y< 1            Probit
                  T = log(-log(l-y)), O < y < 1 Log-log
                                                complementario

                  T = arcoseno (/Y) , O < V <: 1 Arco Seno

                  T = 1/y,    y » O                  Inversa
Gauss Inversa     T = l/y2, y jí O                   Inversa
                                                     cuadrática

Binomial Negativa T = /(l-y)-/(l-y)-^/3', 0< {/< 1
Nota: $" ( ) función inversa de la Distribución Normal acumu-
      lada.

b. Las respuestas esperadas están relacionadas con las varia-
bles predictoras por una función conocida y no lineal en los
parámetros y el objetivo de la transformación es línealizar la

28


función de la respuesta, y así poder usar el método de mínimos
cuadrados usuales; por ejemplo la ecuación del modelo dé regre-
sión exponencial. Aqui es necesario revisar los supuestos acer-
ca de la apropiada estructura de los errores en el modelo no
linealizado: aditivos, multiplicativos u otra composición.

c. Tanto la forma funcional de la relación entre la respuesta
y Sus predictores como la distribución de probabilidad de los
errores no son conocidas exactamente. Lo ideal seria obtener
una transformación de   la respuesta que conduzca a un modelo
donde los errores se distribuyan normalmente con media cero y
varianza constante. En el Cuadro No. 3 se presentan varias fa-
milias de transformaciones de la respuesta sugeridas por Box y
Cox, John y Draper, Mosteller y Tukey. La determinación de las
constantes   que definen la transformación sé hace por el n^o-
do de máxima verosimilitud, suponiendo que los errores de la
respuesta transformada son independientes y se distribuyen nor-
malmente con media cero y varianza constante. Algunas de estas
transformaciones buscan estabilizar la varianza de la respuesta
y así dejarla independiente de su valor medio.

     Con el objeto de establecer relaciones aproximadamente li-
neales entre la respuesta y los predictores, Mosteller y Tukey
(1977) sugieren algtinas técnicas prácticas para encontrar trans
formaciones apropiadas para las respuestas según sean cantida-
des y conteos, usando log(y+C) o ÍY+C)   con d y C constantes.
Para terminar C, Alien y Cady (1982) sugieren para un predic-
tor, tomar 3 puntos suficientemente amplios y equidistantes x,
con sus correspondientes valores Y, por ejemplo Y., Y^, y»,
escogiéndose C tal que cimipla con: log(y2+C)-log(y.+C) » log
(y,+C)-log(y_+C) o, definiendo previamente <í,(y„+C)*^-(y,+C)''^
      d        d                                 •
(y.+C) -ÍY^+C) . Para varios valores C y d, se pueden comparar
distintos ajustes y escoger el que satisfaga algún criterio;
                   2
por ejemplo mayor R .

                                                                                   29


Cuadro 3. Familias de transformaciones.

Familia de
Transformaciones Transformación
Potencia         (y^-D/i.                                   con y > 0,    L # 0
                    log(y)                                               i - 0
                                                            con y+L2 > 0, Lj / 0
Potencia            ((y+L2)'-^-l)/Ll
extendida                                                                   1      n
                    log(y+L2)

Módulo              s i ^ o ( y ) [ ( | y | + i ) ' - - i ] / L L ?É o
                    signo(y)[iog(|y|+i)]                        -1 = 0
Potencia Cruzada     [Y-ib-y)^]/L                           con o < y « faL ^ o
                    log[y/(b-y)]                                           L - O




5.2. Predictores a Incluir en el Modelo.

    En Draper y Smith (1981) se discuten los distintos métodos
tradicionalmente recomendados para la escogencia de subeonjun-
tos de predictores que ajustan el modelo, pero la mayoría su-
ponen que la relación entre los predictores y las respuesta es
lineal. Algunos de ellos han perdido aceptación; por ej,emplo,
inclusión de variables por pasos (Forward) o el método de to-
das las regresiones posibles que se considera ineficiente en
la medida en que el numero de predictores es grande ysapresu-
men formas no lineales.

    Otros procedimientos como exclusión de variables por pasos,
 >.     2                          ^
máximo R    y el criterio de la estadística C                    de Mallow, son
útiles cuando la forma de cada uno de los predictores es la
correcta según, está expresado en el modelo general.

30

5.3. Análisis de Residuos.

     Recientemente se han desarrollado métodos que mediante gra^
ficación y la identificación de patrones en ^1 análisis de re-
siduos permiten decidir sobre el adecuado ajuste y la necesi-"
dad de incluir al modelo nuevos predictores o nuevas re-expre-
siones de los predictores.

     Cook y Weisberg (1982) exponen ampliamente las bases teóri^
cas del análisis de residuos. Para la construcción del modelo
resultan de utilidad -los residuos ordinarios, los residuos de
la variable adicionada, los residuos parciales y los residuos
recursivos.



5.3.1. Gráficas de los Residuos Ordinarios.

     Estas gráficas consisten en ubicar en un plano con ordena-
da los residuos ÍY-Y); en la ábcisa se intercambian tanto la
respuesta esperada y como cada uno de los predictores Xy. La
primera gráfica [e*y]j permite determinar si existe falta de
ajuste en el modelo al detectarse que los puntos están siguien-
do algún patrón o secuencia. En caso contrario, si los puntos
sugieren que están aleatoriamente distribuidos en el gráfico,
el análisis podría terminarse porque el ajuste logrado hasta
ahora no se puede mejorar o porque las variables utilizadas no
tienen capacidad predictiva de la respuesta. Si se determina
algún patrón eh la gráfica [e.*X. ] se sugiere adicionar al mo-
delo términos X, con potencia o alguna reexpresión de este pre-
dictor. Estos gráficos no son muy adecuados para detectar po-
sibles interacciones entre los predictores.

                                                                  31


5.3.2. Gráficas de Residuos de Variable Adicionada.

    Otro i^todo para determinar la adeciaada expresión del pre-
dictor X. en el modelo se logra graficaaidto [(y-'t)*CX{.-X. )] ,
donde y. = 3* +E3*-, con j      ^fe,es el valor de predicción de
la respuesta exclii^do el prédictor X. del modelo, X^ = a +
Za .X V, con j ^ fe, es el valor de predicción de la variable X^
en \xn modelo de regresión en términos de los instantes predic-
tores. El coeficiente de la regresión lineal entre ÍY-yy) y
(X-Xr) coincide con Bt.» el coeficiente de X. en el modelo con-
pleto. El análisis del gráfico muestra si la variable Xj. está
relacion'ada linealmente con la respuesta: los puntos estaríao
distribuidos irregularmente alrededor de la recta (y-yi,) *
3L(X-XI ) .   Alguna curvatura o separación sistemática de la meor-
cionada recta indica que la variable X. se debe reexpresar o
se deben adicionar otros términos X. con potencia. Esta gráfi-
ca es la que el programa REG/SAS imprime con la opción PARTIAL.




5.3.3. Gráficas de Residuos Parciales.

    ún Sustituto del gráfico de variable adicionada es el grá-
fico de residuos parciales; ambos utilizan los mismos residuos
pero su alpariencia puede ser diferente. Consiste en graficar
[(e4§LXi ) Xir] , donde Z = Y-Y son los residuos ordinarios, B,
es la estiniáción del coeficiente de regresión de X, en el mo-
delo completo [iJ . Una particularidad importante de e+^uX, es
que adiciona la parte irregular del modelo e> con la parte sis-
temática debida a X L *

    Mansfleld y Conerly (1987) muestran la bondad del análisis
de residuos parciales simultáneo con los residuos ordinarios;
por ejemplo hay situaciones en las que el patrón de los resi-

32


dúos ordinarios sugiere adicionar XL con término cusdrático y
el patrón de los residuos parciales muestra que podría ser más
adecuada la transformación logarítmica O una función inversa.
En otras ocasiones a a t a a gráficas sugieren le misma transfor-
ción. Si las dos gráficas no presentan algún patrón por su
irregularidad en la distribución de los puntos, se concluye
que esa variable o alguna re-e^resión de ella no contribuye
como buena predictora de la respuesta, por lo tanto Bu " O , o
que las otras variables predictoras distintas a X> no permiten
describir la adecuada forma de X L .

     La excepción de la utilidad de    los residuos parciales se
presenta cuando existe colinealidad extrema entre una predic-
tora mal especificada en el modelo y otras variables, mostran-
do el mismo patrón o curvatura en la gráfica de los residuos
parciales de todas las variables involucradas, Mansfleld y
Conerly (1987).

     Las gráficas 1 a 6 muestran alguno patrones que en el aná-
lisis de residuos y de residuos parciales sugieren transformar
el correspondiente predictor. Las zonas extremas de cada grá-
fica muestran generalmente la curvatura y es de utilidad en el
residuo parcial dibujar la parte sistemática debida al predic-
tor X ) , como una línea de referencia. Si en los residuos los
puntos están aleatoriamente distribuidos y en los residuos par-
ciales muestran tendencia lineal, entonces el predictor está
bien especificado en el modelo; si en ésta gráfica muestra un
patrón distinto, entonces debe ser transformado. Finalmente,
si se presume la existencia de interacciones entre los predic-
tores, se pueden incorporar al modelo los productos de esos
predictores y hacer el análisis de residuos           ''

                                       33




                  GRÁFICA 1
                  Transformación: Identidad

GRÁFICA 2
Transformación:

                  GRÁFICA 5
                  Transformación:

GRÁFICA 6
Transformación:
BIBLIOGRAFÍA
Alien D M,Cady F B.Analyzing Experimental Data by Regression.(1982). Lifetime Learning Publications. Belmont, CA.
Box G,Hunter V,Hunter J.Statistics for Experimenters: An Introduction to Design, Data Analysis and Model Building.(1978). John Wiley, New York.
Brooks D G,Carroll S S,Verdini W A.Characterizing the Domain afa Regression Model.(1988). The American Statistician 42: 187-190.
Cook R D,Weisberg S.Residuals and Influence in Regression.(1982).Chapman y Hall, New York.
Flury B.Understanding Partial Statistics and Redundancy of Variables in Regression and Discriminant Analysis.(1989). The American Statistician 43: 27-31.
Chang P C,Afifi A A.Goodness of Fit Statistics for General Linear Regression Equations in the Presence of Replicated Responses.(1987). The American Statistician, 41, 195-199.
Drapper N R,Smith H.Applied Regression Analysis.(1981).(2da. Ed.). John Wiley, New York.
Hamilton D.Correrlated Variables are not always Redundant.(1987). The American Statistician. 41: 129-132.
Healy M J.The Use of R2 as a Measure of Goodness of Fit.(1984). Journal Royal Statistical Society. A. 147: 608-609.
Hocking R R.The Analysis and Selection of Variables in Linear Regression.(1976).Biometrics 32: 1-49.
Kvalseth T.Cartionary Note About R2.(1985).The American Statistician, 39: 279-285.
Mansfleld E,Conerly M D.Diagnostic value of Residuals and Partial Residual of lots..The American Statistician 41: 107-116.
McCullagh P,Nelder J A.Generalized Linear Models.(1983).Chapman y Hall, London.
Mosteller F,Tukey J W.Data Analysis and Regression.(1977).Adisson-Wesley, Reading, Mass.
Peixoto J L.A property of Well-Formulated Polymomial Regression Models.(1990).The American Statistician 44: 26-30.
Peixoto J L.Heirarchical Variable Selection in Polynomial Regression Models.(1987).The American Statistician, 41: 311-313.
Rao P.Some notes on Misspeficiation in Multiple Regression.(1971).The American Statistician, 37-39.
Theil H,Chung Ch.Information Theoretic Measures of Fit for Univariate and Multivariate Linear Regression.(1988).The American Statistician, 42; 249-252.
Willett J B,Singer J D.Another Cautionary Note about R : Its use in Weighted Least-Squares Regression Analysis.(1988).The American Statistician, 42: 236-238.